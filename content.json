{"pages":[{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"徐靖峰","text":"「技术分享」某种程度上，是让作者和读者，不那么孤独的东西。「Kirito的技术分享」致力于探讨 Java 生态的知识点，内容覆盖分布式服务治理，微服务，性能调优，各类源码分析。追求有深度并兼具表达力的文字。 联系方式 Email：kirito.moe@foxmail.com 微信号：xiayimiaoshenghua（添加好友请注明：姓名+公司+来意） 个人信息 徐靖峰/男/1995 Github：https://github.com/lexburner 目前在杭州，就职于阿里巴巴，主要负责方向：分布式服务治理框架 微信公众号Kirito的技术分享","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"一个 DDD 指导下的实体类设计案例","text":"1 引子项目开发中的工具类代码总是随着项目发展逐渐变大，在公司诸多的公用代码中，笔者发现了一个简单的，也是经常被使用的类：BaseDomain，引起了我的思考。在我们公司的开发习惯中，数据库实体类通常会继承一个叫做 BaseDomain 的类，这个类很简单，主要用来填充一些数据库实体公用的属性，它的设计如下： 12345678910111213141516171819202122232425@MappedSuperclass &lt;1&gt;public class BaseDomain {&lt;!-- more --&gt; private Boolean deleteFlag; &lt;2&gt; private Date deleteDate; private Date lastUpdateDate; private Date createDate; @Version &lt;3&gt; private Integer version; @PrePersist &lt;4&gt; public void init(){ Date now = new Date(); deleteFlag = false; createDate = lastUpdateDate = now; } @PreUpdate &lt;4&gt; public void update(){ lastUpdateDate = new Date(); } } 小小的一个类其实还是蕴含了不少的知识点在里面，至少可以包含以下几点： &lt;1&gt; 被其他类继承后，父类的字段不会被忽略，也就意味着子类没有必要自己写这一堆公用的属性了。 &lt;2&gt; 逻辑删除标识，业务类的删除必须是这种打标识的行为，不能进行物理删除。值得一提的是，公司原先的该字段被命名成了 isDelete，这不符合变量命名的规范，会导致一些序列化框架出现问题，而 delete 是数据库的保留字，所以本文中用 deleteFlag。 &lt;3&gt; 使用 version 作为乐观锁的实现，version 的自增以及版本失效异常受 @Version 该注解的影响，是由框架控制的。 &lt;4&gt; 创建日期，更新日期等等属性，在我们使用 JPA 的 save 方法后，框架会自动去填充相应的值。 2 发现问题与解决问题这个基类使用的频次是怎么样的呢？every class！是的，公司的每个开发者在新增一个实体类时总是优先写上 Xxx extends BaseDomain 。初级开发者总是有什么学什么，他们看到公司原来的代码都是会继承这个类，以及周围的同事也是这么写着，他们甚至不知道 version 乐观锁的实现，不知道类的创建日期更新日期是在基类中被声明的；高级开发者能够掌握我上面所说的那些技术要点，尽管开发中因此遇到一些不适，但也是尽可能的克服。等等，上面说到添加这个基类后，对开发造成了不适感，这引起了我的思考，下面就来谈谈直观的有哪些不适感以及解决方案。 2.1 没有物理删除，只有逻辑删除真正 delete 操作不会再出现了, 物理删除操作被 setDeleteFlag(true) 代替。在列表展示中，再也不能使用 findAll()操作了，而是需要使用 findByDeleteFlagFalse()。更多的数据库查询操作，都要考虑到，deleteFlag=true 的那些记录，不应该被影响到。 ** 解决问题 **：在 DDD 中，值得推崇的方式是使用 specification 模式来解决这个问题，对应到实际开发中，也就是 JPA 的 Predicate，或者是熟悉 Hibernate 的人所了解的 Criteria。但不可避免的一点是由于只有逻辑删除，导致了我们的数据库越来越大（解决方法不是没有，正是 EventSouring+CQRS 架构，这属于 DDD 的高级实践，本文不进行讨论）。从技术开发角度出发，这的确使得我们的编码变得稍微复杂了一点，但是其业务意义远大于这点开发工作量，所以是值得的。 2.2 级联查询变得麻烦一个会员有多个通信地址，多个银行卡。反映到实体设计，便是这样的： 1234567891011public class Member extends BaseDomain{ private String username; @OneToMany private List&lt;MemberAddress&gt; memberAddresses; @OneToMany private List&lt;BankCard&gt; bankCards; } 其中，MemberAddress 及 BankCard 都继承了 BaseDomain。使用 orm 框架自带的级联功能，我们本可以查询出会员信息时，顺带查出其对应的通讯地址列表和银行卡列表。但现在不是那么的美好了，使用级联查询，可能会查询出已经被删除的 MemberAddress，BankCard，只能在应用层进行 deleteFlag 的判断，从而过滤被删除的信息，这无法避免，因为框架不认识逻辑删除标识！ ** 解决问题 ：这个问题和 2.3 节的问题，恰恰是促成我写这篇文章的初衷，这与 DDD 有着密不可分的关联。DDD 将对象划分成了 entity（实体）和 value object（值对象）。如果仔细分析下上面的业务并且懂一点 DDD，你会立刻意识到。Member 对象就是一个 entity，而 MemberAddress 以及 BankCard 则是 value object（username 也是 value object）。value object 的一个重要特点，就是作为 entity 的修饰，从业务角度出发，MemberAddress 和 BankCard 的确是为了更好描述 Member 信息，而抽象出的一个集合。而 value object 的另一特性，不可变性，指导了我们， 不应该让 MemberAddress，BankCard 继承 BaseDomain**。说了这么多，就是想从一个理论的高度，让那些设计一个新实体便继承 BaseDomain 的人戒掉这个习惯。在 value object 丧失了 deleteFlag，lastUpdateDate 等属性后，可能会引发一些的质疑，他们会声称：“数据库里面 member_address 这张表没有 lastUpdateDate 字段了，我再也无法得知这条会员地址最后修改的时间了!”。是的，从逻辑意义上看，地址并没有改变，而改变的只是会员自己的地址，这个 UpdateDate 字段在地址上极为不合理，应该是会员的修改。也就是说 lastUpdateDate 应该反映到 Member 上。实际的开发经验告诉我，从前那么多的 value object 继承了 BaseDomain，99% 不会使用到其中的相关属性，如果真的需要使用，那么请单独为类添加，而不是继承 BaseDomain。其次这些人犯了另一个错误，我们设计一个系统时，应该是 entity first，而不应该 database first。DDD 告诉我们一个软件开发的大忌，到现在 2017 年，仍然有大帮的人在问：“我要实现 xxxx 功能，我的数据库应该如何设计？”这些人犯了根本性的错误，就是把软件的目的搞错了，软件研究的是什么？是研究如何使用计算机来解决实际（领域）问题，而不是去研究数据应该如何保存更合理。我的公司中有不少的程序员新人，希望这番话能够帮助那些“步入歧途”的从业人员 “走上正路”。软件设计应该从“数据库驱动”走向“领域驱动”，而 DDD 的实践经验正是为设计和开发大型复杂的软件系统提供了实践指导。 2.3 乐观锁的尴尬地位再说回 BaseDomain 中的 version 字段，由于 MemberAddress 和 BankCard 这样的 value object 也被赋予了乐观锁的行为，这意味着加锁的粒度变小了。DDD 的指导下，改动也可以理解为由 Member 这个根发出，统一由 Member 中的 version 来控制，这使锁的粒度变大了。换言之，从技术开发角度，对 value object 加上 version 可以允许同时（操作系统级别真正的同时）修改一个用户的地址信息和银行卡信息，甚至是多个银行卡中不同的银行卡，而单独由 Member 控制，则意味着，系统在同一时刻只能进行单独一项操作。在业务并发的一般角度上考虑，一个用户是不会出现多线程修改行为的。而从软件设计的角度，单独为 value object 添加 version，破坏了 value object 的不可变性，若要修改，应当是被整个替换。 ** 解决方案 **：在一般情况下，请不要为 value object 添加乐观锁。如果有一个场景下，你的 value object 需要出现版本控制，那可能有两种情况：1 你的 value object 是压根不是 value object，可能是一个 entity 2 聚合根划分错误 …. 这，要真是这样源头都弄错了，压根没法聊了对吧 3 总结BaseDomain 这样的设计本身并不是我想要强调的重点，但是既然出现了 BaseDomain 这样的设计，那么它究竟应该被什么样的实体继承，就是需要被考虑的了。DDD 下，识别 aggregate root，entity，value object，是整个软件设计的核心点，在本文中，判别是否继承 BaseDomain 的前提，就是这个对象是 entity，还是 value object。大家都是存在数据库中的，但是地位是不一样的。 本文若有什么不足之处，欢迎 DDD 爱好者指出。","link":"/DDD-practice/"},{"title":"南京 IAS 架构师峰会观后感","text":"上周六，周日在南京举办了 IAS 架构师峰会，这么多人的技术分享会还是头一次参加，大佬云集，涨了不少姿势。特此一篇记录下印象深刻的几场分享。由于全凭记忆叙述，故只能以流水账的形式还原出现场的收获。 大型支付交易平台的演进过程 陈斌，《架构即未来》译者，易宝支付 CTO。 交易系统具备以下特点，交易量大，并发度高，业务敏感度高，响应速度容忍度低… 从而使得支付交易平台需要有以下的特点： 高可用：7X24*365 随时可用 高安全：需满足 PCI-DSS 要求 高效率：每笔交易的成本要低 高扩展：随业务的快速发展扩张 从以上几点话题引申出了系统扩展的三个阶段 **X 轴扩展 – 扩展机器 ** 也就是通俗意义中集群方案，横向扩展，通过添加多台机器负载均衡，从而扩展计算能力，这是最简单粗暴，也是最直接易用的方案。 **Y 轴扩展 – 拆分服务 ** 当水平扩展遇到瓶颈后，可以进行服务的拆分，将系统按照业务模块进行拆分，从而可以选择性定制化地扩展特定的模块。如电商系统中拆分出订单模块，商品模块，会员模块，地址模块… 由于各个模块的职责不同，如订单模块在双 11 时压力很大，可以多部署一些订单模块，而其他压力不大的模块，则进行少量地部署。 **Z 轴扩展 – 拆分数据 ** 服务拆分之后仍然无法解决与日俱增的数据量问题，于是引发了第三层扩展，数据的分片，我理解的 sharding，不仅仅存在于数据库，还包含了 redis，文件等。 另外陈斌老师还聊了一个有意思的话题，系统可用性下降的原因根源是什么？最终他给出的答案是：人。系统升级后引发的事故 80% 是由于人的误操作或者触发了 bug 等人为因素导致的，是人就会手抖。借此引出了单元测试，持续集成，持续交付的重要性。健全这三者是保障系统可用性的最大利器。 在技术晚宴，陈斌老师又分享了一些管理经验：** 如何打造一支优秀的技术团队 **。 分析了构成团队的四要素： 人员：健全职级体系，区别考评，挖掘潜能，及时鼓励，扁平化管理 组织：面向产出，利于创新，敏捷小团队 过程：聚集问题的根源，适当地使⽤用 ITIL，不断优化过程，自动化取代人工 文化：鼓励分享，打破 devops 的边界，鼓励创新，树⽴立正确的技术负债观 对于技术人员来说可能有点抽象，不过对于立志于要成为 CIO 的人肯定是大有裨益的，具体的理解可以参考《架构即未来》中的具体阐释。(ps：这里的架构并不是指技术架构，别问我为什么知道，问问我看了一半后在落灰的那本书，你什么都明白了) 轻量级微服务架构实践之路 黄勇，特赞科技 CTO，《轻量级微服务架构》作者。 非常具有人格魅力的一位演讲者，这可能是当天最有价值的一场分享。 他首先提出了一个问题：什么是微服务？怎么理解这个 ‘微’ 字。随后他给出了自己的理解：微 = 合理。一知半解的微服务实践者可能盲目地拆分服务，微并不是代表颗粒越小越好，用领域驱动的术语来说，微服务模块需要用合适的限界上下文。黄勇老师给出了 4 个微服务拆分的技巧： 业务先行 由粗到细 避免耦合 持续改进 非常实用且具有指导意义的 4 个思想，当你还在犹豫到底该如何拆分你的模块时，可以尝试先从单体式开始开发，业务发展会指引你拆分出合适模块，合适的粒度。当一个个业务被剥离出 Monolithic 这个怪物，持续重构，持续改进，这样可以指引你深入理解微服务。 随后给出了轻量级微服务架构的技术选型，非常有参考价值。 其 PPT 总结了很多经验 list，可以在文末链接获取。 顺带一提，没记错的话黄勇老师介绍到其公司的语言栈有：Java，Node，Go，在后面其他老师的分享中集中介绍多语言栈的意义。 《轻量级微服务架构》上下册一起购买，赠送“技能图谱”，感兴趣的朋友可以阅读一下他的书籍。购买链接 ps: 谁让我白得了一本上册呢。 Cloud Native 架构一致性问题及解决方案 王启军，华为架构部资深架构师。 王启军老师则是带来了如今微服务架构最难的一个技术点的分享：分布式中的一致性问题。 他的分享中涵盖了很多经典的分布式一致性问题的案例，如两军问题，拜占庭将军问题。引出了经典的 CAP 理论，NWR，Lease，Replicated state machine，Paxos 算法。由于时间问题，45 分钟根本无法详细地介绍他们的流程，实属可惜。 一致性问题被分成了两类，包括： ** 以数据为中心的一致性模型 ** 严格一致性 顺序一致性 因果一致性 FIFO 一致性 弱一致性 释放一致性 入口一致性 ** 以用户为中心的一致性模型 ** 单调读一致性 单调写一致性 写后读一致性 读后写一致性 这么多一致性分类太过于学术范，所以业界通常将他们简单的归为了三类： 弱一致性 最终一致性 强一致性 对于各个一致性模型的科普，以及一些事务模型和解决方案如 2PC，3PC，TCC 型事务，PPT 中都给出了简单的介绍。 技术架构演变全景图 - 从单体式到云原生 千米网首席架构师，曹祖鹏（右） &amp; 当当网首席架构师，张亮（左）。知名开源框架 sharding-jdbc，elastic-job 作者。 别开生面的面向对象技术分享。也是我本次大会最期待的一场分享，分享涵盖的知识点很多，深度和广度得兼，其分享中阐释了云原生，服务编排、治理、调度等 2017 年处于潮流前线的技术热点，通俗易懂地介绍了 service mesh 的概念，让观众在惊叹于互联网技术变化如此之快的同时，也带来了很多思考。 分享中还对比了 Spring Cloud 和 Dubbo，当当网和千米网的团队都向 Dubbo 贡献过代码，Spring Cloud 又是国内话题最多的框架之一，台下观众对这样的话题自然是非常感兴趣。张亮老师着重介绍了 Spring Cloud 相关的组件，而曹祖鹏老师重点对比了其与 Dubbo 的区别。 Spring Cloud 的出现同时宣告了 Cloud Native 云原生的首映，其为微服务的构架带来了一整套初具雏形的解决方案，包含了 Zuul 网关，Ribbon 客户端负载均衡，Eureka 服务注册与发现，Hystrix 熔断… 并且有强大的 Spring 终端组件支持，活跃的社区，丰富的文档。 随后，介绍了云原生的技术全景图： 之后，简单解释了治理，编排，调度的概念后，并重点介绍了服务治理，编排相关的技术栈，老牌的 nginx，netflix ribbon，zuul 等产品，如今风靡的 k8s。尤其是介绍到 service mesh 这一比较新的概念时，分析了服务的治理，编排，调度从应用层转移到基础设施层的趋势，无疑是非常 exciting 的一件事。如 dubbo 等 rpc 框架的服务注册发现依赖于 zk，consul，而 spring cloud 的服务注册发现组件 eureka，以及其客户端路由组件 ribbon，服务端路由组件 zuul 等都是从应用层解决了服务的相关问题，而 service mesh 提供了一个新的思路，从基础设施层解决服务的相关问题： 如果 service mesh 的开源产品 Linkerd 和 Lstio 能够保持好的势头，配合 k8s 在运维层的大一统，很有可能带来架构的新格局。与此同时，java 一枝独秀的时代即将宣告终结，多语言的优势将会被 service mesh 发扬光大，使用 go 编写高并发的模块，使用 java 编写业务型模块，nodejs 打通前端模块，python 处理性能要求不高模块提升开发效率… 而不用关心多语言交互的问题，这都交由 service mesh 解决，这几乎是 2017 最潮流的知识点，没有之一。 （引用一张 jimmysong 博客中的图片） 如上图所示，得知 Spring Cloud 竟然是 2015 兴起的技术栈时，可能还会有些吃惊，等到可以预见的 2018，运维层的技术栈开始向上侵蚀应用层的技术栈，不得不感叹互联网技术的日新月异。 两位老师从可追溯的历史到可预见的未来展现了云原生架构的演进史，着实给小白们好好科普了一番。 番外此次技术分享会收获颇丰，不枉我早上 5 点起来赶高铁去南京了。但还是得吐槽一句，这门票真 tl 的贵啊，就不能便宜点吗！！！[微笑 face] 其他分享者的话题也很有意思，不仅包含了微服务方向，还囊括了人工智能，机器学习，运维，领导力，架构演变，游戏架构等多个方向，笔者选择性的介绍了一些，全部的 PPT 可以在下方的链接中获得。 https://pan.baidu.com/s/1eSbCu5c","link":"/NJIAS2017/"},{"title":"Re：从零开始的领域驱动设计","text":"前言领域驱动的火爆程度不用我赘述，但是即便其如此得耳熟能详，但大多数人对其的认识，还只是停留在知道它的缩写是 DDD，知道它是一种软件思想，或者知道它和微服务有千丝万缕的关系。Eric Evans 对 DDD 的诠释是那么地惜字如金，而我所认识的领域驱动设计的专家又都是行业中的资深前辈，他们擅长于对软件设计进行高屋建瓴的论述，如果没有丰富的互联网从业经验，是不能从他们的分享中获取太多的营养的，可以用曲高和寡来形容。1000 个互联网从业者，100 个懂微服务，10 个人懂领域驱动设计。 可能有很多和我一样的读者，在得知 DDD 如此火爆之后，尝试去读了开山之作《领域驱动设计——软件核心复杂性应对之道》，翻看了几张之后，晦涩的语句，不明所以的专业术语，加上翻译导致的语句流畅性，可以说观看体验并不是很好，特别是对于开发经验不是很多的读者。我总结了一下，为何这本书难以理解： 没有阅读软件设计丛书的习惯，更多人偏向于阅读偏应用层面的书籍，“talk is cheap，show me the code”往往更符合大多数人的习惯。 没有太多的开发经验支撑。没有踩过坑，就不会意识到设计的重要性，无法产生共情。 年代有些久远，这本书写于 2004 年，书中很多软件设计的反例，在当时是非常流行的，但是在现在已经基本绝迹了。大师之所以为大师，是因为其能跨越时代的限制，预见未来的问题，这也是为什么 DDD 在十几年前就被提出，却在微服务逐渐流行的现阶段才被大家重视。 诚然如标题所示，本文是领域驱动设计的一个入门文章，或者更多的是一个个人理解的笔记，笔者也正在学习 DDD 的路上，可能会有很多的疏漏。如有理解有偏颇的地方，还望各位指摘。 认识领域驱动设计的意义领域驱动设计并不会绝对地提高项目的开发效率。 遵循领域驱动设计的规范使得项目初期的开发甚至不如不使用它来的快，原因有很多，程序员的素质，代码的规范，限界上下文的划分… 甚至需求修改后导致需要重新建模。但是遵循领域驱动设计的规范，在项目越来越复杂之后，可以不至于让项目僵死。这也是为什么很多系统不断迭代着，最终就黄了。书名的副标题“软件核心复杂性应对之道”正是阐释了这一点 模式： smart ui 是个反模式可能很多读者还不知道 smart ui 是什么，但是在这本书写作期间，这种设计风格是非常流行的。在与一位领域驱动设计方面的资深专家的交谈中，他如下感慨到软件发展的历史： 2003 年时，正是 delphi，vb 一类的 smart ui 程序大行其道，java 在那个年代，还在使用 jsp 来完成大量的业务逻辑操作，4000 行的 jsp 是常见的事；2005 年 spring hibernate 替换了 EJB，社区一片欢呼，所有人开始拥护 action，service，dao 这样的贫血模型（充血模型，贫血模型会在下文论述）；2007 年，Rails 兴起，有人发现了 Rails 的 activeRecord 是涨血模型，引起了一片混战；直到现在的 2017 年，微服务成为主流系统架构。 在现在这个年代，不懂个 MVC 分层，都不好意思说自己是搞 java 的，也不会有人在 jsp 里面写业务代码了（可以说模板技术 freemarker,thymeleaf 已经取代 jsp 了），但是在那个年代，还没有现在这么普遍地强调分层架构的重要性。 这个章节其实并不重要，因为 mvc 一类的分层架构已经是大多数 java 初学者的“起点”了，大多数 DDD 的文章都不会赘述这一点，我这里列出来是为了让大家知晓这篇文章的时代局限性，在后续章节的理解中，也需要抱有这样的逻辑：这本书写于 2004 年。 模式： Entity 与 Value Object我在不了解 DDD 时，就对这两个术语早有耳闻。entity 又被称为 reference object，我们通常所说的 java bean 在领域中通常可以分为这两类，（可别把 value object 和常用于前台展示的 view object，vo 混为一谈）entity 的要义在于生命周期和标识，value object 的要义在于无标识，通常情况下，entity 在通俗意义上可以理解为数据库的实体，（不过不严谨），value object 则一般作为一个单独的类，构成 entity 的一个属性。 举两个例子来加深对 entity 和 value object 的理解。 例 1：以电商微服务系统中的商品模块，订单模块为例。将整个电商系统划分出商品和订单两个限界上下文（Bound Context）应该是没有争议的。如果是传统的单体应用，我们可以如何设计这两个模块的实体类呢？会不会是这样？ 1234567891011121314151617181920212223class Product{ String id;// 主键 String skuId;// 唯一识别号 String productName; Bigdecimal price; Category category;// 分类 List&lt;Specification&gt; specifications;// 规格 ... }class Order{ String id;// 主键 String orderNo;// 订单号 List&lt;OrderItem&gt; orderItems;// 订单明细 BigDecimal orderAmount;// 总金额 ...}class OrderItem{ String id; Product product;// 关联商品 BigDecimal snapshotPrice;// 下单时的价格} 看似好像没问题，考虑到了订单要保存下单时候的价格（当然，这是常识）但这么设计却存在诸多的问题。在分布式系统中，商品和订单这两个模块必然不在同一个模块，也就意味着不在同一个网段中。上述的类设计中直接将 Product 的列表存储到了 Order 中，也就是一对多的外键关联。这会导致，每次访问订单的商品列表，都需要发起 n 次远程调用。 反思我们的设计，其实我们发现，订单 BC 的 Product 和商品 BC 的 Product 其实并不是同一个 entity，在商品模块中，我们更关注商品的规格，种类，实时价格，这最直接地反映了我们想要买什么的欲望。而当生成订单后，我们只关心这个商品买的时候价格是多少，不会关心这个商品之后的价格变动，还有他的名称，仅仅是方便我们在订单的商品列表中定位这个商品。 如何改造就变得明了了 12345678class OrderItem{ String id; String productId;// 只记录一个 id 用于必要的时候发起 command 操作 String skuId; String productName; ... BigDecimal snapshotPrice;// 下单时的价格} 是的，我们做了一定的冗余，这使得即使商品模块的商品，名称发生了微调，也不会被订单模块知晓。这么做也有它的业务含义，用户会声称：我买的时候他的确就叫这个名字。记录 productId 和 skuId 的用意不是为了查询操作，而是方便申请售后一类的命令操作（command）。 在这个例子中，Order 和 Product 都是 entity，而 OrderItem 则是 value object（想想之前的定义，OrderItem 作为一个类，的确是描述了 Order 这个 entity 的一个属性集合）。关于标识，我的理解是有两层含义，第一个是作为数据本身存储于数据库，主键 id 是一个标识，第二是作为领域对象本身，orderNo 是一个标识，对于人而言，身份证是一个标识。而 OrderItem 中的 productId，id 不能称之为标识，因为整个 OrderItem 对象是依托于 Order 存在的，Order 不存在，则 OrderItem 没有意义。 例子 2： 汽车和轮胎的关系是 entity 和 value object 吗？这个例子其实是一个陷阱题，因为他没有交代限界上下文（BC），场景不足以判断。对于用户领域而言，的确可以成立，汽车报废之后，很少有人会关心轮胎。轮胎和发动机，雨刮器，座椅地位一样，只是构成汽车的一些部件，和用户最紧密相关的，只有汽车这个 entity，轮胎只是描述这个汽车的属性（value object）；场景切换到汽修厂，无论是汽车，还是轮胎，都是汽修厂密切关心的，每个轮胎都有自己的编号，一辆车报废了，可以安置到其他车上，这里，他们都是 entity。 这个例子是在说明这么一个道理，同样的事物，在不同的领域中，会有不同的地位。 在单体应用中，可能会有人指出，这直接违背了数据库范式，但是领域驱动设计的思想正如他的名字那样，不是基于数据库的，而是基于领域的。微服务使得数据库发生了隔离，这样的设计思想可以更好的指导我们优化数据库。 模式： Repository 哲学家分析自然规律得出规范，框架编写者根据规范制定框架。有些框架，可能大家一直在用，但是却不懂其中蕴含的哲学。 ------ 来自于笔者的口胡 记得在刚刚接触 mvc 模式，常常用 DAO 层表示持久化层，在 JPA+springdata 中，抽象出了各式各样的 xxxRepository，与 DDD 的 Repository 模式同名并不是巧合，jpa 所表现出的正是一个充血模型（如果你遵循正确的使用方式的话），可以说是领域驱动设计的一个最佳实践。 开宗明义，在 Martin Fowler 理论中，有四种领域模型： 失血模型 贫血模型 充血模型 胀血模型详细的概念区别不赘述了，可以参见专门讲解 4 种模型的博客。他们在数据库开发中分别有不同的实现，用一个修改用户名的例子来分析。12345class User{ String id; String name; Integer age;} 失血模型：跳过，可以理解为所有的操作都是直接操作数据库，在 smart ui 中可能会出现这样的情况。 贫血模型： 123456789101112131415161718class UserDao { @Autowired JdbcTemplate jdbcTemplate; public void updateName(String name,String id){ jdbcTemplate.excute(&quot;update user u set u.name = ? where id=?&quot;,name,id); }}class UserService{ @Autowired UserDao userDao; void updateName(String name,String id){ userDao.updateName(name,id); } } 贫血模型中，dao 是一类 sql 的集合，在项目中的表现就是写了一堆 sql 脚本，与之对应的 service 层，则是作为 Transaction Script 的入口。观察仔细的话，会发现整个过程中 user 对象都没出现过。 充血模型： 1234567891011121314interface UserRepository extends JpaRepository&lt;User,String&gt;{ //springdata-jpa 自动扩展出 save findOne findAll 方法}class UserService{ @Autowoird UserRepository userRepository; void updateName(String name,String id){ User user = userRepository.findOne(id); user.setName(name); userRepository.save(user); }} 充血模型中，整个修改操作是“隐性”的，对内存中 user 对象的修改直接影响到了数据库最终的结果，不需要关心数据库操作，只需要关注领域对象 user 本身。Repository 模式就是在于此，屏蔽了数据库的实现。与贫血模型中 user 对象恰恰相反，整个流程没有出现 sql 语句。 涨血模型：没有具体的实现，可以这么理解： 12345void updateName(String name,String id){ User user = new User(id); user.setName(name); user.save();} 我们在 Repository 模式中重点关注充血模型。为什么前面说：如果你遵循正确的使用方式的话，springdata 才是对 DDD 的最佳实践呢？因为有的使用者会写出下面的代码： 1234567interface UserRepository extends JpaRepository&lt;User,String&gt;{ @Query(&quot;update user set name=? where id=?&quot;) @Modifying(clearAutomatically = true) @Transactional void updateName(String name,String id);} 历史的车轮在滚滚倒退。本节只关注模型本身，不讨论使用中的一些并发问题，再来聊聊其他的一些最佳实践。 1234567interface UserRepository extends JpaRepository&lt;User,String&gt;{ User findById();//√ 然后已经存在 findOne 了，只是为了做个对比 User findBy 身份证号 ();// 可以接受 User findBy 名称 ();//× List&lt; 权限 &gt; find 权限 ByUserId();//×} 理论上，一个 Repository 需要且仅需要包含三类方法 loadBy 标识，findAll，save（一般 findAll（）就包含了分页，排序等多个方法，算作一类方法）。标识的含义和前文中 entity 的标识是同一个含义，在我个人的理解中，身份证可以作为一个用户的标识（这取决于你的设计，同样的逻辑还有订单中有业务含义的订单编号，保单中的投保单号等等），在数据库中，id 也可以作为标识。findBy 名称为什么不值得推崇，因为 name 并不是 User 的标识，名字可能会重复，只有在特定的现场场景中，名字才能具体对应到人。那应该如何完成“根据姓名查找可能的用户”这一需求呢？最方便的改造是使用 Criteria，Predicate 来完成视图的查询，哪怕只有一个非标识条件。在更完善的 CQRS 架构中，视图的查询则应该交由专门的 View 层去做，可以是数据库，可以是 ES。findByUserId 不值得推崇则是因为他违背了聚合根模式（下文会介绍），User 的 Repository 只应该返回 User 对象。 软件设计初期，你是不是还在犹豫：是应该先设计数据库呢，还是应该设计实体呢？在 Domain-Driven 的指导下，你应当放弃 Data-Driven。 模式 聚合和聚合根难住我的还有英文单词，初识这个概念时，忍不住发问：Aggregate 是个啥。文中使用聚合的概念，来描述对象之间的关联，采用合适的聚合策略，可以避免一个很长，很深的对象引用路径。对划分模块也有很大的指导意义。 在微服务中我们常说划分服务模块，在领域驱动设计中，我们常说划分限界上下文。在面向对象的世界里，用抽象来封装模型中的引用，聚合就是指一组相关对象的集合，我们把它作为数据修改的单元。每个聚合都有一个聚合根 (root) 和一个边界(boundary)。边界定义了聚合内部有什么，而根则是一个特定的 entity，两个聚合之间，只允许维护根引用，只能通过根引用去向深入引用其他引用变量。 例子还是沿用电商系统中的订单和商品模块。在聚合模式中，订单不能够直接关联到商品的规格信息，如果一定要查询，则应该通过订单关联到的商品，由商品去访问商品规格。在这个例子中，订单和商品分别是两个边界，而订单模块中的订单 entity 和商品模块中的商品 entity 就是分别是各自模块的 root。遵循这个原则，可以使我们模块关系不那么的盘根错节，这也是众多领域驱动文章中不断强调的划分限界上下文是第一要义。 模式 包结构微服务有诸多的模块，而每个模块并不一定是那么的单一职责，比模块更细的分层，便是包的分层。我在阅读中，隐隐觉得这其中蕴含着一层哲学，但是几乎没有文章尝试解读它。领域驱动设计将其单独作为了一个模式进行了论述，篇幅不小。重点就是论述了一个思想：包结构应当具有高内聚性。 这次以一个真实的案例来介绍一下对高内聚的包结构的理解，项目使用 maven 多 module 搭建。我曾经开发过一个短信邮件平台模块，它在整个微服务系统中有两个职责，一：负责为其他模块提供短信邮件发送的远程调用接口，二：有一个后台页面，可以让管理员自定义发送短信，并且可以浏览全部的一，二两种类型发送的短信邮件记录。 在设计包结构之前，先是设计微服务模块。| module 名 | 说明 | package 类型 | 顶级包名 || ——- | ————— | ————– | ———————— || api | api 接口定义，用于暴露服务 | jar | sinosoftgz.message.api || app | api 实现者，真正的服务提供者 | executable jar | sinosoftgz.message.app || admin | 管理端应用 | executable jar | sinosoftgz.message.admin || model | 实体 | jar | sinosoftgz.message.model |api 层定义了一系列的接口和接口依赖的一些 java bean，model 层也就是我们的领域层。这两个模块都会打成 jar 包，外部服务依赖 api，api 则由 app 模块使用 rpc 框架实现远程调用。admin 和 app 连接同一个数据源，可以查询出短信邮件记录，admin 需要自定义发送短信也是通过 rpc 调用。简单介绍完了这个项目后，重点来分析下需求，来看看如何构建包结构。mvc 分层天然将 controller，service，model，config 层分割开，这符合 DDD 所推崇的分层架构模式（这个模式在原文中有描述，但我觉得和现在耳熟能详的分层结构没有太大的出入，所以没有放到本文中介绍），而我们的业务需求也将短信和邮件这两个领域拆分开了。那么，到底是 mvc 应该包含业务包结构呢？还是说业务包结构包含 mvc 呢？ mvc 高于业务分层 123456789101112131415161718192021// 不够好的分层sinosoftgz.message.admin config CommonConfig.java service CommonService.java mail MailTemplateService.java MailMessageService.java sms SmsTemplateService.java SmsMessageService.java web IndexController.java mail MailTemplateController.java MailMessageController.java sms SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 业务分层包含 mvc 123456789101112131415161718192021222324252627// 高内聚的分层sinosoftgz.message.admin config CommonConfig.java service CommonService.java web IndexController.java mail config MailConfig.java service MailTemplateService.java MailMessageService.java web MailTemplateController.java MailMessageController.java sms config Smsconfig.java service SmsTemplateService.java SmsMessageService.java web SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 业务并不是特别复杂，但应该可以发现第二种（业务分层包含 mvc）的包结构，才是一种高内聚的包结构。第一种分层会让人有一种将各个业务模块（如 mail 和 sms）的 service 和 controller 隔离开了的感觉，当模块更多，每个模块的内容更多，这个“隔得很远”的不适感会逐渐侵蚀你的开发速度。一种更加低内聚的反例是不用包分层，仅仅依赖前缀区分，由于在项目开发中真的发现同事写出了这样的代码，我觉得还是有必要拿出来说一说： 12345678910111213141516171819// 反例sinosoftgz.message.admin config CommonConfig.java MailConfig.java Smsconfig.java service CommonService.java MailTemplateService.java MailMessageService.java SmsTemplateService.java SmsMessageService.java web IndexController.java MailTemplateController.java MailMessageController.java SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 这样的设计会导致 web 包越来越庞大，逐渐变得臃肿，是什么使项目僵化，项目经理为何一看到代码就头疼，规范的高内聚的包结构，遵循业务 &gt;mvc 的原则，可以知道我们的项目庞大却有条理。 其他模式《领域驱动设计》这本书介绍了众多的模式，上面只是介绍了一部分重要的模式，后续我会结合各个模式，尽量采用最佳实践 + 浅析设计的方式来解读。 微服务之于领域驱动设计的一点思考技术架构诚然重要，但不可忽视领域拆解和业务架构，《领域驱动设计》中的诸多失败，成功案例的总结，是支撑其理论知识的基础，最终汇聚成众多的模式。在火爆的微服务架构潮流下，我也逐渐意识到微服务不仅仅是技术的堆砌，更是一种设计，一门艺术。我的本科论文本想就微服务架构进行论述，奈何功底不够，最后只能改写成一篇分布式网站设计相关的文章，虽然是一个失败的过程，但让我加深了对微服务的认识。如今结合领域驱动设计，更加让我确定，技术方案始终有代替方案，决定微服务的不是框架的选择，不仅仅是 restful 或者 rpc 的接口设计风格的抉择，而更应该关注拆解，领域，限界上下文，聚合根等等一系列事物，这便是我所理解的领域驱动设计对微服务架构的指导意义。 参考文章多研究些架构，少谈些框架 —- 曹祖鹏 DDD 领域驱动设计基本理论知识总结 - netfocus ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/Re-DDD/"},{"title":"Re：从零开始的 Spring Security OAuth2（一）","text":"前言今天来聊聊一个接口对接的场景，A 厂家有一套 HTTP 接口需要提供给 B 厂家使用，由于是外网环境，所以需要有一套安全机制保障，这个时候 oauth2 就可以作为一个方案。 关于 oauth2，其实是一个规范，本文重点讲解 spring 对他进行的实现，如果你还不清楚授权服务器，资源服务器，认证授权等基础概念，可以移步 理解 OAuth 2.0 - 阮一峰，这是一篇对于 oauth2 很好的科普文章。 需要对 spring security 有一定的配置使用经验，用户认证这一块，spring security oauth2 建立在 spring security 的基础之上。第一篇文章主要是讲解使用 springboot 搭建一个简易的授权，资源服务器，在文末会给出具体代码的 github 地址。后续文章会进行 spring security oauth2 的相关源码分析。java 中的安全框架如 shrio，已经有 跟我学 shiro - 开涛，非常成体系地，深入浅出地讲解了 apache 的这个开源安全框架，但是 spring security 包括 oauth2 一直没有成体系的文章，学习它们大多依赖于较少的官方文档，理解一下基本的使用配置；通过零散的博客，了解一下他人的使用经验；打断点，分析内部的工作流程；看源码中的接口设计，以及注释，了解设计者的用意。spring 的各个框架都运用了很多的设计模式，在学习源码的过程中，也大概了解了一些套路。spring 也在必要的地方添加了适当的注释，避免了源码阅读者对于一些细节设计的理解产生偏差，让我更加感叹，spring 不仅仅是一个工具框架，更像是一个艺术品。 概述使用 oauth2 保护你的应用，可以分为简易的分为三个步骤 配置资源服务器 配置认证服务器 配置 spring security 前两点是 oauth2 的主体内容，但前面我已经描述过了，spring security oauth2 是建立在 spring security 基础之上的，所以有一些体系是公用的。 oauth2 根据使用场景不同，分成了 4 种模式 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials） 本文重点讲解接口对接中常使用的密码模式（以下简称 password 模式）和客户端模式（以下简称 client 模式）。授权码模式使用到了回调地址，是最为复杂的方式，通常网站中经常出现的微博，qq 第三方登录，都会采用这个形式。简化模式不常用。 项目准备主要的 maven 依赖如下 12345678910111213141516171819&lt;!-- 注意是 starter, 自动配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 不是 starter, 手动配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 将 token 存储在 redis 中 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 我们给自己先定个目标，要干什么事？既然说到保护应用，那必须得先有一些资源，我们创建一个 endpoint 作为提供给外部的接口： 123456789101112131415161718@RestControllerpublic class TestEndpoints { @GetMapping(&quot;/product/{id}&quot;) public String getProduct(@PathVariable String id) { //for debug Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); return &quot;product id :&quot; + id; } @GetMapping(&quot;/order/{id}&quot;) public String getOrder(@PathVariable String id) { //for debug Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); return &quot;order id :&quot; + id; }} 暴露一个商品查询接口，后续不做安全限制，一个订单查询接口，后续添加访问控制。 配置资源服务器和授权服务器由于是两个 oauth2 的核心配置，我们放到一个配置类中。为了方便下载代码直接运行，我这里将客户端信息放到了内存中，生产中可以配置到数据库中。token 的存储一般选择使用 redis，一是性能比较好，二是自动过期的机制，符合 token 的特性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475@Configurationpublic class OAuth2ServerConfig { private static final String DEMO_RESOURCE_ID = &quot;order&quot;; @Configuration @EnableResourceServer protected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter { @Override public void configure(ResourceServerSecurityConfigurer resources) { resources.resourceId(DEMO_RESOURCE_ID).stateless(true); } @Override public void configure(HttpSecurity http) throws Exception { // @formatter:off http // Since we want the protected resources to be accessible in the UI as well we need // session creation to be allowed (it's disabled by default in 2.0.6) .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.IF_REQUIRED) .and() .requestMatchers().anyRequest() .and() .anonymous() .and() .authorizeRequests()// .antMatchers(&quot;/product/**&quot;).access(&quot;#oauth2.hasScope('select') and hasRole('ROLE_USER')&quot;) .antMatchers(&quot;/order/**&quot;).authenticated();// 配置 order 访问控制，必须认证过后才可以访问 // @formatter:on } } @Configuration @EnableAuthorizationServer protected static class AuthorizationServerConfiguration extends AuthorizationServerConfigurerAdapter { @Autowired AuthenticationManager authenticationManager; @Autowired RedisConnectionFactory redisConnectionFactory; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception { // 配置两个客户端, 一个用于 password 认证一个用于 client 认证 clients.inMemory().withClient(&quot;client_1&quot;) .resourceIds(DEMO_RESOURCE_ID) .authorizedGrantTypes(&quot;client_credentials&quot;, &quot;refresh_token&quot;) .scopes(&quot;select&quot;) .authorities(&quot;client&quot;) .secret(&quot;123456&quot;) .and().withClient(&quot;client_2&quot;) .resourceIds(DEMO_RESOURCE_ID) .authorizedGrantTypes(&quot;password&quot;, &quot;refresh_token&quot;) .scopes(&quot;select&quot;) .authorities(&quot;client&quot;) .secret(&quot;123456&quot;); } @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception { endpoints .tokenStore(new RedisTokenStore(redisConnectionFactory)) .authenticationManager(authenticationManager); } @Override public void configure(AuthorizationServerSecurityConfigurer oauthServer) throws Exception { // 允许表单认证 oauthServer.allowFormAuthenticationForClients(); } }} 简单说下 spring security oauth2 的认证思路。 client 模式，没有用户的概念，直接与认证服务器交互，用配置中的客户端信息去申请 accessToken，客户端有自己的 client_id,client_secret 对应于用户的 username,password，而客户端也拥有自己的 authorities，当采取 client 模式认证时，对应的权限也就是客户端自己的 authorities。 password 模式，自己本身有一套用户体系，在认证时需要带上自己的用户名和密码，以及客户端的 client_id,client_secret。此时，accessToken 所包含的权限是用户本身的权限，而不是客户端的权限。 我对于两种模式的理解便是，如果你的系统已经有了一套用户体系，每个用户也有了一定的权限，可以采用 password 模式；如果仅仅是接口的对接，不考虑用户，则可以使用 client 模式。 配置 spring security在 spring security 的版本迭代中，产生了多种配置方式，建造者模式，适配器模式等等设计模式的使用，spring security 内部的认证 flow 也是错综复杂，在我一开始学习 ss 也产生了不少困惑，总结了一下配置经验：使用了 springboot 之后，spring security 其实是有不少自动配置的，我们可以仅仅修改自己需要的那一部分，并且遵循一个原则，直接覆盖最需要的那一部分。这一说法比较抽象，举个例子。比如配置内存中的用户认证器。有两种配置方式 planA： 1234567@Beanprotected UserDetailsService userDetailsService(){ InMemoryUserDetailsManager manager = new InMemoryUserDetailsManager(); manager.createUser(User.withUsername(&quot;user_1&quot;).password(&quot;123456&quot;).authorities(&quot;USER&quot;).build()); manager.createUser(User.withUsername(&quot;user_2&quot;).password(&quot;123456&quot;).authorities(&quot;USER&quot;).build()); return manager;} planB： 12345678910111213141516171819@Configuration@EnableWebSecuritypublic class SecurityConfiguration extends WebSecurityConfigurerAdapter { @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication() .withUser(&quot;user_1&quot;).password(&quot;123456&quot;).authorities(&quot;USER&quot;) .and() .withUser(&quot;user_2&quot;).password(&quot;123456&quot;).authorities(&quot;USER&quot;); } @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception { AuthenticationManager manager = super.authenticationManagerBean(); return manager; }} 你最终都能得到配置在内存中的两个用户，前者是直接替换掉了容器中的 UserDetailsService，这么做比较直观；后者是替换了 AuthenticationManager，当然你还会在 SecurityConfiguration 复写其他配置，这么配置最终会由一个委托者去认证。如果你熟悉 spring security，会知道 AuthenticationManager 和 AuthenticationProvider 以及 UserDetailsService 的关系，他们都是顶级的接口，实现类之间错综复杂的聚合关系… 配置方式千差万别，但理解清楚认证流程，知道各个实现类对应的职责才是掌握 spring security 的关键。 下面给出我最终的配置： 123456789101112131415161718192021222324@Configuration@EnableWebSecuritypublic class SecurityConfiguration extends WebSecurityConfigurerAdapter { @Bean @Override protected UserDetailsService userDetailsService(){ InMemoryUserDetailsManager manager = new InMemoryUserDetailsManager(); manager.createUser(User.withUsername(&quot;user_1&quot;).password(&quot;123456&quot;).authorities(&quot;USER&quot;).build()); manager.createUser(User.withUsername(&quot;user_2&quot;).password(&quot;123456&quot;).authorities(&quot;USER&quot;).build()); return manager; } @Override protected void configure(HttpSecurity http) throws Exception { // @formatter:off http .requestMatchers().anyRequest() .and() .authorizeRequests() .antMatchers(&quot;/oauth/*&quot;).permitAll(); // @formatter:on }} 重点就是配置了一个 UserDetailsService，和 ClientDetailsService 一样，为了方便运行，使用内存中的用户，实际项目中，一般使用的是数据库保存用户，具体的实现类可以使用 JdbcDaoImpl 或者 JdbcUserDetailsManager。 获取 token进行如上配置之后，启动 springboot 应用就可以发现多了一些自动创建的 endpoints： 123456{[/oauth/authorize]}{[/oauth/authorize],methods=[POST]{[/oauth/token],methods=[GET]}{[/oauth/token],methods=[POST]}{[/oauth/check_token]}{[/oauth/error]} 重点关注一下 /oauth/token，它是获取的 token 的 endpoint。启动 springboot 应用之后，使用 http 工具访问password 模式： http://localhost:8080/oauth/token?username=user_1&amp;password=123456&amp;grant_type=password&amp;scope=select&amp;client_id=client_2&amp;client_secret=123456 响应如下：{&quot;access_token&quot;:&quot;950a7cc9-5a8a-42c9-a693-40e817b1a4b0&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;refresh_token&quot;:&quot;773a0fcd-6023-45f8-8848-e141296cb3cb&quot;,&quot;expires_in&quot;:27036,&quot;scope&quot;:&quot;select&quot;} client 模式：http://localhost:8080/oauth/token?grant_type=client_credentials&amp;scope=select&amp;client_id=client_1&amp;client_secret=123456 响应如下：{&quot;access_token&quot;:&quot;56465b41-429d-436c-ad8d-613d476ff322&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;expires_in&quot;:25074,&quot;scope&quot;:&quot;select&quot;} 在配置中，我们已经配置了对 order 资源的保护，如果直接访问:http://localhost:8080/order/1 会得到这样的响应:{&quot;error&quot;:&quot;unauthorized&quot;,&quot;error_description&quot;:&quot;Full authentication is required to access this resource&quot;}（这样的错误响应可以通过重写配置来修改） 而对于未受保护的 product 资源 http://localhost:8080/product/1 则可以直接访问，得到响应 product id : 1 携带 accessToken 参数访问受保护的资源： 使用 password 模式获得的 token:http://localhost:8080/order/1?access_token=950a7cc9-5a8a-42c9-a693-40e817b1a4b0，得到了之前匿名访问无法获取的资源：order id : 1 使用 client 模式获得的 token:http://localhost:8080/order/1?access_token=56465b41-429d-436c-ad8d-613d476ff322，同上的响应 order id : 1 我们重点关注一下 debug 后，对资源访问时系统记录的用户认证信息，可以看到如下的 debug 信息 password 模式： client 模式： 和我们的配置是一致的，仔细看可以发现两者的身份有些许的不同。想要查看更多的 debug 信息，可以选择下载 demo 代码自己查看，为了方便读者调试和验证，我去除了很多复杂的特性，基本实现了一个最简配置，涉及到数据库的地方也尽量配置到了内存中，这点记住在实际使用时一定要修改。 到这儿，一个简单的 oauth2 入门示例就完成了，一个简单的配置教程。token 的工作原理是什么，它包含了哪些信息？spring 内部如何对身份信息进行验证？以及上述的配置到底影响了什么？这些内容会放到后面的文章中去分析。 示例代码下载全部的代码可以在我的 github 上进行下载，项目使用 springboot+maven 构建：https://github.com/lexburner/oauth2-demo ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/Spring-Security-OAuth2-1/"},{"title":"Re：从零开始的 Spring Security OAuth2（二）","text":"本文开始从源码的层面，讲解一些 Spring Security Oauth2 的认证流程。本文较长，适合在空余时间段观看。且涉及了较多的源码，非关键性代码以… 代替。 准备工作首先开启 debug 信息： 123logging: level: org.springframework: DEBUG 可以完整的看到内部的运转流程。 client 模式稍微简单一些，使用 client 模式获取 token http://localhost:8080/oauth/token?client_id=client_1&amp;client_secret=123456&amp;scope=select&amp;grant_type=client_credentials 由于 debug 信息太多了，我简单按照顺序列了一下关键的几个类： 1234ClientCredentialsTokenEndpointFilterDaoAuthenticationProviderTokenEndpointTokenGranter @EnableAuthorizationServer上一篇博客中我们尝试使用了 password 模式和 client 模式，有一个比较关键的 endpoint：/oauth/token。从这个入口开始分析，spring security oauth2 内部是如何生成 token 的。获取 token，与第一篇文章中的两个重要概念之一有关，也就是 AuthorizationServer 与 ResourceServer 中的 AuthorizationServer。 在之前的配置中 123@Configuration@EnableAuthorizationServerprotected static class AuthorizationServerConfiguration extends AuthorizationServerConfigurerAdapter {} 出现了 AuthorizationServerConfigurerAdapter 关键类，他关联了三个重要的配置类，分别是 1234567891011121314public class AuthorizationServerConfigurerAdapter implements AuthorizationServerConfigurer { @Override public void configure(AuthorizationServerSecurityConfigurer security &lt;1&gt;) throws Exception{ } @Override public void configure(ClientDetailsServiceConfigurer clients &lt;2&gt;) throws Exception { } @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints &lt;3&gt;) throws Exception { }} &lt;1&gt; 配置 AuthorizationServer 安全认证的相关信息，创建 ClientCredentialsTokenEndpointFilter 核心过滤器 &lt;2&gt; 配置 OAuth2 的客户端相关信息 &lt;3&gt; 配置 AuthorizationServerEndpointsConfigurer 众多相关类，包括配置身份认证器，配置认证方式，TokenStore，TokenGranter，OAuth2RequestFactory 我们逐步分析其中关键的类 客户端身份认证核心过滤器 ClientCredentialsTokenEndpointFilter（掌握）截取关键的代码，可以分析出大概的流程在请求到达 /oauth/token 之前经过了 ClientCredentialsTokenEndpointFilter 这个过滤器，关键方法如下 1234567891011121314public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException { ... String clientId = request.getParameter(&quot;client_id&quot;); String clientSecret = request.getParameter(&quot;client_secret&quot;); ... clientId = clientId.trim(); UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken(clientId, clientSecret); return this.getAuthenticationManager().authenticate(authRequest);} 顶级身份管理者 AuthenticationManager（掌握）用来从请求中获取 client_id,client_secret，组装成一个 UsernamePasswordAuthenticationToken 作为身份标识，使用容器中的顶级身份管理器 AuthenticationManager 去进行身份认证（AuthenticationManager 的实现类一般是 ProviderManager。而 ProviderManager 内部维护了一个 List, 真正的身份认证是由一系列 AuthenticationProvider 去完成。而 AuthenticationProvider 的常用实现类则是 DaoAuthenticationProvider，DaoAuthenticationProvider 内部又聚合了一个 UserDetailsService 接口，UserDetailsService 才是获取用户详细信息的最终接口，而我们上一篇文章中在内存中配置用户，就是使用了 UserDetailsService 的一个实现类 InMemoryUserDetailsManager）。UML 类图可以大概理解下这些类的关系，省略了授权部分。 图 1 认证相关 UML 类图 可能机智的读者会发现一个问题，我前面一篇文章已经提到了 client 模式是不存在“用户”的概念的，那么这里的身份认证是在认证什么呢？debug 可以发现 UserDetailsService 的实现被适配成了 ClientDetailsUserDetailsService，这个设计是将 client 客户端的信息（client_id,client_secret）适配成用户的信息 (username,password)，这样我们的认证流程就不需要修改了。 经过 ClientCredentialsTokenEndpointFilter 之后，身份信息已经得到了 AuthenticationManager 的验证。接着便到达了TokenEndpoint。 Token 处理端点 TokenEndpoint（掌握）前面的两个 ClientCredentialsTokenEndpointFilter 和 AuthenticationManager 可以理解为一些前置校验，和身份封装，而这个类一看名字就知道和我们的 token 是密切相关的。 1234567891011121314151617181920@FrameworkEndpointpublic class TokenEndpoint extends AbstractEndpoint { @RequestMapping(value = &quot;/oauth/token&quot;, method=RequestMethod.POST) public ResponseEntity&lt;OAuth2AccessToken&gt; postAccessToken(Principal principal, @RequestParam Map&lt;String, String&gt; parameters) throws HttpRequestMethodNotSupportedException { ... String clientId = getClientId(principal); ClientDetails authenticatedClient = getClientDetailsService().loadClientByClientId(clientId);//&lt;1&gt; ... TokenRequest tokenRequest = getOAuth2RequestFactory().createTokenRequest(parameters, authenticatedClient);//&lt;2&gt; ... OAuth2AccessToken token = getTokenGranter().grant(tokenRequest.getGrantType(), tokenRequest);//&lt;3&gt; ... return getResponse(token); } private TokenGranter tokenGranter;} &lt;1&gt; 加载客户端信息 &lt;2&gt; 结合请求信息，创建 TokenRequest &lt;3&gt; 将 TokenRequest 传递给 TokenGranter 颁发 token 省略了一些校验代码之后，真正的 /oauth/token 端点暴露在了我们眼前，其中方法参数中的 Principal 经过之前的过滤器，已经被填充了相关的信息，而方法的内部则是依赖了一个 TokenGranter 来颁发 token。其中 OAuth2AccessToken 的实现类 DefaultOAuth2AccessToken 就是最终在控制台得到的 token 序列化之前的原始类:​ 12345678910public class DefaultOAuth2AccessToken implements Serializable, OAuth2AccessToken { private static final long serialVersionUID = 914967629530462926L; private String value; private Date expiration; private String tokenType = BEARER_TYPE.toLowerCase(); private OAuth2RefreshToken refreshToken; private Set&lt;String&gt; scope; private Map&lt;String, Object&gt; additionalInformation = Collections.emptyMap(); //getter,setter} 1234567891011121314@org.codehaus.jackson.map.annotate.JsonSerialize(using = OAuth2AccessTokenJackson1Serializer.class)@org.codehaus.jackson.map.annotate.JsonDeserialize(using = OAuth2AccessTokenJackson1Deserializer.class)@com.fasterxml.jackson.databind.annotation.JsonSerialize(using = OAuth2AccessTokenJackson2Serializer.class)@com.fasterxml.jackson.databind.annotation.JsonDeserialize(using = OAuth2AccessTokenJackson2Deserializer.class)public interface OAuth2AccessToken { public static String BEARER_TYPE = &quot;Bearer&quot;; public static String OAUTH2_TYPE = &quot;OAuth2&quot;; public static String ACCESS_TOKEN = &quot;access_token&quot;; public static String TOKEN_TYPE = &quot;token_type&quot;; public static String EXPIRES_IN = &quot;expires_in&quot;; public static String REFRESH_TOKEN = &quot;refresh_token&quot;; public static String SCOPE = &quot;scope&quot;; ...} 一个典型的样例 token 响应, 如下所示，就是上述类序列化后的结果： 1234567{ &quot;access_token&quot;:&quot;950a7cc9-5a8a-42c9-a693-40e817b1a4b0&quot;, &quot;token_type&quot;:&quot;bearer&quot;, &quot;refresh_token&quot;:&quot;773a0fcd-6023-45f8-8848-e141296cb3cb&quot;, &quot;expires_in&quot;:27036, &quot;scope&quot;:&quot;select&quot; } TokenGranter（掌握）先从 UML 类图对 TokenGranter 接口的设计有一个宏观的认识 图 2 TokenGranter 相关 UML 类图 TokenGranter 的设计思路是使用 CompositeTokenGranter 管理一个 List 列表，每一种 grantType 对应一个具体的真正授权者，在 debug 过程中可以发现 CompositeTokenGranter 内部就是在循环调用五种 TokenGranter 实现类的 grant 方法，而 granter 内部则是通过 grantType 来区分是否是各自的授权类型。 123456789101112131415161718public class CompositeTokenGranter implements TokenGranter { private final List&lt;TokenGranter&gt; tokenGranters; public CompositeTokenGranter(List&lt;TokenGranter&gt; tokenGranters) { this.tokenGranters = new ArrayList&lt;TokenGranter&gt;(tokenGranters); } public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) { for (TokenGranter granter : tokenGranters) { OAuth2AccessToken grant = granter.grant(grantType, tokenRequest); if (grant!=null) { return grant; } } return null; }} 五种类型分别是： ResourceOwnerPasswordTokenGranter ==&gt; password 密码模式 AuthorizationCodeTokenGranter ==&gt; authorization_code 授权码模式 ClientCredentialsTokenGranter ==&gt; client_credentials 客户端模式 ImplicitTokenGranter ==&gt; implicit 简化模式 RefreshTokenGranter ==&gt;refresh_token 刷新 token 专用 以客户端模式为例，思考如何产生 token 的，则需要继续研究 5 种授权者的抽象类：AbstractTokenGranter 123456789101112131415161718192021222324252627282930313233343536public abstract class AbstractTokenGranter implements TokenGranter { protected final Log logger = LogFactory.getLog(getClass()); // 与 token 相关的 service，重点 private final AuthorizationServerTokenServices tokenServices; // 与 clientDetails 相关的 service，重点 private final ClientDetailsService clientDetailsService; // 创建 oauth2Request 的工厂，重点 private final OAuth2RequestFactory requestFactory; private final String grantType; ... public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) { ... String clientId = tokenRequest.getClientId(); ClientDetails client = clientDetailsService.loadClientByClientId(clientId); validateGrantType(grantType, client); logger.debug(&quot;Getting access token for:&quot; + clientId); return getAccessToken(client, tokenRequest); } protected OAuth2AccessToken getAccessToken(ClientDetails client, TokenRequest tokenRequest) { return tokenServices.createAccessToken(getOAuth2Authentication(client, tokenRequest)); } protected OAuth2Authentication getOAuth2Authentication(ClientDetails client, TokenRequest tokenRequest) { OAuth2Request storedOAuth2Request = requestFactory.createOAuth2Request(client, tokenRequest); return new OAuth2Authentication(storedOAuth2Request, null); } ...} 回过头去看 TokenEndpoint 中，正是调用了这里的三个重要的类变量的相关方法。由于篇幅限制，不能延展太多，不然没完没了，所以重点分析下 AuthorizationServerTokenServices 是何方神圣。 AuthorizationServerTokenServices（了解）AuthorizationServer 端的 token 操作 service，接口设计如下： 12345678910public interface AuthorizationServerTokenServices { // 创建 token OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException; // 刷新 token OAuth2AccessToken refreshAccessToken(String refreshToken, TokenRequest tokenRequest) throws AuthenticationException; // 获取 token OAuth2AccessToken getAccessToken(OAuth2Authentication authentication);} 在默认的实现类 DefaultTokenServices 中，可以看到 token 是如何产生的，并且了解了框架对 token 进行哪些信息的关联。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Transactionalpublic OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException { OAuth2AccessToken existingAccessToken = tokenStore.getAccessToken(authentication); OAuth2RefreshToken refreshToken = null; if (existingAccessToken != null) { if (existingAccessToken.isExpired()) { if (existingAccessToken.getRefreshToken() != null) { refreshToken = existingAccessToken.getRefreshToken(); // The token store could remove the refresh token when the // access token is removed, but we want to // be sure... tokenStore.removeRefreshToken(refreshToken); } tokenStore.removeAccessToken(existingAccessToken); } else { // Re-store the access token in case the authentication has changed tokenStore.storeAccessToken(existingAccessToken, authentication); return existingAccessToken; } } // Only create a new refresh token if there wasn't an existing one // associated with an expired access token. // Clients might be holding existing refresh tokens, so we re-use it in // the case that the old access token // expired. if (refreshToken == null) { refreshToken = createRefreshToken(authentication); } // But the refresh token itself might need to be re-issued if it has // expired. else if (refreshToken instanceof ExpiringOAuth2RefreshToken) { ExpiringOAuth2RefreshToken expiring = (ExpiringOAuth2RefreshToken) refreshToken; if (System.currentTimeMillis() &gt; expiring.getExpiration().getTime()) { refreshToken = createRefreshToken(authentication); } } OAuth2AccessToken accessToken = createAccessToken(authentication, refreshToken); tokenStore.storeAccessToken(accessToken, authentication); // In case it was modified refreshToken = accessToken.getRefreshToken(); if (refreshToken != null) { tokenStore.storeRefreshToken(refreshToken, authentication); } return accessToken;} 简单总结一下 AuthorizationServerTokenServices 的作用，他提供了创建 token，刷新 token，获取 token 的实现。在创建 token 时，他会调用 tokenStore 对产生的 token 和相关信息存储到对应的实现类中，可以是 redis，数据库，内存，jwt。 总结本篇总结了使用客户端模式获取 Token 时，spring security oauth2 内部的运作流程，重点是在分析 AuthenticationServer 相关的类。其他模式有一定的不同，但抽象功能是固定的，只是具体的实现类会被相应地替换。阅读 spring 的源码，会发现它的设计中出现了非常多的抽象接口，这对我们理清楚内部工作流程产生了不小的困扰，我的方式是可以借助 UML 类图，先从宏观理清楚作者的设计思路，这会让我们的分析事半功倍。 下一篇文章重点分析用户携带 token 访问受限资源时，spring security oauth2 内部的工作流程。即 ResourceServer 相关的类。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/Spring-Security-OAuth2-2/"},{"title":"Re：从零开始的 Spring Security OAuth2（三）","text":"上一篇文章中我们介绍了获取 token 的流程，这一篇重点分析一下，携带 token 访问受限资源时，内部的工作流程。 @EnableResourceServer 与 @EnableAuthorizationServer还记得我们在第一节中就介绍过了 OAuth2 的两个核心概念，资源服务器与身份认证服务器。我们对两个注解进行配置的同时，到底触发了内部的什么相关配置呢？ 上一篇文章重点介绍的其实是与身份认证相关的流程，即如果获取 token，而本节要分析的携带 token 访问受限资源，自然便是与 @EnableResourceServer 相关的资源服务器配置了。 我们注意到其相关配置类是 ResourceServerConfigurer，内部关联了 ResourceServerSecurityConfigurer 和 HttpSecurity。前者与资源安全配置相关，后者与 http 安全配置相关。（类名比较类似，注意区分，以 Adapter 结尾的是适配器，以 Configurer 结尾的是配置器，以 Builder 结尾的是建造器，他们分别代表不同的设计模式，对设计模式有所了解可以更加方便理解其设计思路） 1234567891011public class ResourceServerConfigurerAdapter implements ResourceServerConfigurer { @Override public void configure(ResourceServerSecurityConfigurer resources &lt;1&gt;) throws Exception { } @Override public void configure(HttpSecurity http) throws Exception { http.authorizeRequests().anyRequest().authenticated(); }} &lt;1&gt; ResourceServerSecurityConfigurer 显然便是我们分析的重点了。 ResourceServerSecurityConfigurer（了解）其核心配置如下所示： 123456789101112131415161718192021222324public void configure(HttpSecurity http) throws Exception { AuthenticationManager oauthAuthenticationManager = oauthAuthenticationManager(http); resourcesServerFilter = new OAuth2AuthenticationProcessingFilter();//&lt;1&gt; resourcesServerFilter.setAuthenticationEntryPoint(authenticationEntryPoint); resourcesServerFilter.setAuthenticationManager(oauthAuthenticationManager);//&lt;2&gt; if (eventPublisher != null) { resourcesServerFilter.setAuthenticationEventPublisher(eventPublisher); } if (tokenExtractor != null) { resourcesServerFilter.setTokenExtractor(tokenExtractor);//&lt;3&gt; } resourcesServerFilter = postProcess(resourcesServerFilter); resourcesServerFilter.setStateless(stateless); // @formatter:off http .authorizeRequests().expressionHandler(expressionHandler) .and() .addFilterBefore(resourcesServerFilter, AbstractPreAuthenticatedProcessingFilter.class) .exceptionHandling() .accessDeniedHandler(accessDeniedHandler)//&lt;4&gt; .authenticationEntryPoint(authenticationEntryPoint); // @formatter:on} 这段是整个 oauth2 与 HttpSecurity 相关的核心配置，其中有非常多的注意点，顺带的都强调一下： &lt;1&gt; 创建 OAuth2AuthenticationProcessingFilter，即下一节所要介绍的 OAuth2 核心过滤器。 &lt;2&gt; 为 OAuth2AuthenticationProcessingFilter 提供固定的 AuthenticationManager 即 OAuth2AuthenticationManager，它并没有将 OAuth2AuthenticationManager 添加到 spring 的容器中，不然可能会影响 spring security 的普通认证流程（非 oauth2 请求），只有被 OAuth2AuthenticationProcessingFilter 拦截到的 oauth2 相关请求才被特殊的身份认证器处理。 &lt;3&gt; 设置了 TokenExtractor 默认的实现 —-BearerTokenExtractor，这个类在下一节介绍。 &lt;4&gt; 相关的异常处理器，可以重写相关实现，达到自定义异常的目的。 还记得我们在一开始的配置中配置了资源服务器，是它触发了相关的配置。 123@Configuration@EnableResourceServerprotected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter {} 核心过滤器 OAuth2AuthenticationProcessingFilter（掌握）回顾一下我们之前是如何携带 token 访问受限资源的：http://localhost:8080/order/1?access_token=950a7cc9-5a8a-42c9-a693-40e817b1a4b0唯一的身份凭证，便是这个 access_token，携带它进行访问，会进入 OAuth2AuthenticationProcessingFilter 之中，其核心代码如下： 1234567891011121314151617181920212223242526272829303132public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain){ final HttpServletRequest request = (HttpServletRequest) req; final HttpServletResponse response = (HttpServletResponse) res; try { // 从请求中取出身份信息，即 access_token Authentication authentication = tokenExtractor.extract(request); if (authentication == null) { ... } else { request.setAttribute(OAuth2AuthenticationDetails.ACCESS_TOKEN_VALUE, authentication.getPrincipal()); if (authentication instanceof AbstractAuthenticationToken) { AbstractAuthenticationToken needsDetails = (AbstractAuthenticationToken) authentication; needsDetails.setDetails(authenticationDetailsSource.buildDetails(request)); } // 认证身份 Authentication authResult = authenticationManager.authenticate(authentication); ... eventPublisher.publishAuthenticationSuccess(authResult); // 将身份信息绑定到 SecurityContextHolder 中 SecurityContextHolder.getContext().setAuthentication(authResult); } } catch (OAuth2Exception failed) { ... return; } chain.doFilter(request, response);} 整个过滤器便是 oauth2 身份鉴定的关键，在源码中，对这个类有一段如下的描述 A pre-authentication filter for OAuth2 protected resources. Extracts an OAuth2 token from the incoming request and uses it to populate the Spring Security context with an {@link OAuth2Authentication} (if used in conjunction with an {@link OAuth2AuthenticationManager}). OAuth2 保护资源的预先认证过滤器。如果与 OAuth2AuthenticationManager 结合使用，则会从到来的请求之中提取一个 OAuth2 token，之后使用 OAuth2Authentication 来填充 Spring Security 上下文。 其中涉及到了两个关键的类 TokenExtractor，AuthenticationManager。相信后者这个接口大家已经不陌生，但前面这个类之前还未出现在我们的视野中。 OAuth2 的身份管理器 –OAuth2AuthenticationManager（掌握）在之前的 OAuth2 核心过滤器中出现的 AuthenticationManager 其实在我们意料之中，携带 access_token 必定得经过身份认证，但是在我们 debug 进入其中后，发现了一个出乎意料的事，AuthenticationManager 的实现类并不是我们在前面文章中聊到的常用实现类 ProviderManager，而是 OAuth2AuthenticationManager。 图 1 新的 AuthenticationManager 实现类 OAuth2AuthenticationManager 回顾我们第一篇文章的配置，压根没有出现过这个 OAuth2AuthenticationManager，并且它脱离了我们熟悉的认证流程（第二篇文章中的认证管理器 UML 图是一张经典的 spring security 结构类图），它直接重写了容器的顶级身份认证接口，内部维护了一个 ClientDetailService 和 ResourceServerTokenServices，这两个核心类在 Re：从零开始的 Spring Security Oauth2（二）有分析过。在 ResourceServerSecurityConfigurer 的小节中我们已经知晓了它是如何被框架自动配置的，这里要强调的是 OAuth2AuthenticationManager 是密切与 token 认证相关的，而不是与获取 token 密切相关的。 其判别身份的关键代码如下： 123456789101112131415161718public Authentication authenticate(Authentication authentication) throws AuthenticationException { ... String token = (String) authentication.getPrincipal(); // 最终还是借助 tokenServices 根据 token 加载身份信息 OAuth2Authentication auth = tokenServices.loadAuthentication(token); ... checkClientDetails(auth); if (authentication.getDetails() instanceof OAuth2AuthenticationDetails) { OAuth2AuthenticationDetails details = (OAuth2AuthenticationDetails) authentication.getDetails(); ... } auth.setDetails(authentication.getDetails()); auth.setAuthenticated(true); return auth;} 说到 tokenServices 这个密切与 token 相关的接口，这里要强调下，避免产生误解。tokenServices 分为两类，一个是用在 AuthenticationServer 端，第二篇文章中介绍的 123456789public interface AuthorizationServerTokenServices { // 创建 token OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException; // 刷新 token OAuth2AccessToken refreshAccessToken(String refreshToken, TokenRequest tokenRequest) throws AuthenticationException; // 获取 token OAuth2AccessToken getAccessToken(OAuth2Authentication authentication);} 而在 ResourceServer 端有自己的 tokenServices 接口： 12345678public interface ResourceServerTokenServices { // 根据 accessToken 加载客户端信息 OAuth2Authentication loadAuthentication(String accessToken) throws AuthenticationException, InvalidTokenException; // 根据 accessToken 获取完整的访问令牌详细信息。 OAuth2AccessToken readAccessToken(String accessToken);} 具体内部如何加载，和 AuthorizationServer 大同小异，只是从 tokenStore 中取出相应身份的流程有点区别，不再详细看实现类了。 TokenExtractor（了解）这个接口只有一个实现类，而且代码非常简单 1234567891011121314151617181920212223242526272829303132333435363738public class BearerTokenExtractor implements TokenExtractor { private final static Log logger = LogFactory.getLog(BearerTokenExtractor.class); @Override public Authentication extract(HttpServletRequest request) { String tokenValue = extractToken(request); if (tokenValue != null) { PreAuthenticatedAuthenticationToken authentication = new PreAuthenticatedAuthenticationToken(tokenValue, &quot;&quot;); return authentication; } return null; } protected String extractToken(HttpServletRequest request) { // first check the header... String token = extractHeaderToken(request); // bearer type allows a request parameter as well if (token == null) { ... // 从 requestParameter 中获取 token } return token; }/** * Extract the OAuth bearer token from a header. */ protected String extractHeaderToken(HttpServletRequest request) { Enumeration&lt;String&gt; headers = request.getHeaders(&quot;Authorization&quot;); while (headers.hasMoreElements()) {// typically there is only one (most servers enforce that) ... // 从 Header 中获取 token } return null; }} 它的作用在于分离出请求中包含的 token。也启示了我们可以使用多种方式携带 token。1 在 Header 中携带 123http://localhost:8080/order/1Header：Authentication：Bearer f732723d-af7f-41bb-bd06-2636ab2be135 2 拼接在 url 中作为 requestParam 1http://localhost:8080/order/1?access_token=f732723d-af7f-41bb-bd06-2636ab2be135 3 在 form 表单中携带 123http://localhost:8080/order/1form param：access_token=f732723d-af7f-41bb-bd06-2636ab2be135 异常处理OAuth2 在资源服务器端的异常处理不算特别完善，但基本够用，如果想要重写异常机制，可以直接替换掉相关的 Handler，如权限相关的 AccessDeniedHandler。具体的配置应该在 @EnableResourceServer 中被覆盖，这是适配器 + 配置器的好处。 总结到这儿，Spring Security OAuth2 的整个内部流程就算是分析结束了。本系列的文章只能算是揭示一个大概的流程，重点还是介绍相关设计 + 接口，想要了解更多的细节，需要自己去翻看源码，研究各个实现类。在分析源码过程中总结出的一点经验，与君共勉： 先掌握宏观，如研究 UML 类图，搞清楚关联 分析顶级接口，设计是面向接口的，不重要的部分，具体实现类甚至都可以忽略 学会对比，如 ResourceServer 和 AuthenticationServer 是一种对称的设计，整个框架内部的类非常多，但分门别类的记忆，会加深记忆。如 ResourceServerTokenServices ，AuthenticationServerTokenServices 就一定是作用相关，但所属领域不同的两个接口 熟悉设计模式，spring 中涉及了大量的设计模式，在框架的设计中也是遵循着设计模式的规范，如以 Adapter 结尾，便是运用了适配器模式；以 Factory 结尾，便是运用了适配器模式；Template 结尾，便是运用了模板方法模式；Builder 结尾，便是运用了建造者模式… 一点自己的理解：对源码的理解和灵感，这一切都建立自身的编码经验之上，自己遵循规范便能更好的理解别人同样遵守规范的代码。相对的，阅读好的源码，也能帮助我们自身提升编码规范。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/Spring-Security-OAuth2-3/"},{"title":"浅析 Open API 设计规范","text":"背景最近由于业务需求，我负责的一块系统需要对外开放 Open API，原本不是什么难事，因为阿里云内部的 Open API 开放机制已经非常成熟了，根本不需要我去设计，但这次的需求主要是因为一些原因，需要自己设计一些规范，那就意味着，需要对 Open API 进行一些规范约束了，遂有此文。 Open API 和前端页面一样，一直都是产品的门面， Open API 不规范，会拉低产品的专业性。在云场景下，很多用户会选择自建门户，对接云产品的 Open API，这对我们提出的诉求便是构建一套成熟的 Open API 机制。 站在业务角度，有一些指导原则，指导我们完善 Open API 机制： 前端页面使用的接口和 Open API 提供的接口是同一套接口 任意的前端页面接口都应该有对应的 Open API 站在技术角度，有很多的 API 开放标准可供我们参考，一些开源产品的 Open API 文档也都非常完善。一方面，我会取其精华，另一方面，要考虑自身产品输出形态的特殊性。本文将围绕诸多因素，尝试探讨出一份合适的 Open API 开放规范。 Open API 设计考虑因素一个完善的 Open API 规范到底应该规范哪些东西？ 站在设计角度，需要考虑：命名规范，构成规范，路径规范，出入参规范，数据类型规范，统一返回值规范，错误码规范，分页规范。 站在团队角度，团队中的后端初级中级开发以及前端研发是否有足够的经验，领悟并落地好制定的 API 规范。同时，伴随着人员流动，这份 Open API 规范是否可以很好地被传承下去。 站在行业角度，需要考虑提供 Open API 的产品所在的市场是否已经成熟，API 风格可能已经有了对应的规范。 站在产品角度，每个产品适合的 API 风格是不同的，下文会着重探讨这一角度。 总之，Open API 的设计是很难形成定论的一个东西，我在介绍自身产品最终采用的 Open API 规范之前，会先来聊一下大家耳熟能详的一些概念，例如 restful。 restful 规范之争有人的地方就会有江湖。 有代码的地方也是如此。 如果你在码圈混，一定听说过 restful 规范： 增删改查应分别声明为：POST、DELETE、PUT、PATCH、GET 不应该出现动词，动词统一由 HTTP Method 表示 体现出“资源”的抽象 利用 pathVariable，queryParam，header，statusCode 表达很多业务语义 restful 规范看似美好，但如果你真正尝试过落地，一定会遇到一些类似的问题： 以用户登录接口为例，此类接口难以映射到资源的增删改查 以查询最近 7 个小时内的接口请求错误率为例，衍生到诸如 graphQL 这类复杂的查询场景，往往需要 json 结构，GET 是无法实现这一点的，只有 POST 才可以传递 基于此，restful 规范逐渐有了反对的声音： 强行让所有的事物都“资源”化一下，有悖于开发常识，接口不一定都能够通过简单的增删改查来映射 复杂的查询语义不一定能够用 GET 表达 restful 风格的拥趸者，不乏对这些反对言论进行抨击，社区中不免有“拒绝 restful 风格的主要是低水平不思进取的架构师和前后端程序员们，不会设计是人的问题，不是规范的问题”此类的言论。同时对 restful 进行了升华：复杂参数的检索问题，在 restful 语义中本就应当归类为 post，因为该行为并不是对资源的定位（GET），而是对资源的检索（POST） 这显然刺激了 restful 风格反对者的神经，不屑道：呵，愚蠢的 restful 原教旨主义者呀。 不知道你是 restful 的拥趸者还是反对者？亦或是，中立者。 restful 之争暂时到此为止，这番争论纯属虚构，看官不必计较。无论你如何看待 restful，下面我的论述，你都可以作为一个中立者，否则效果减半。 ROA 与 RPCAPI 设计并不只有 restful 一种规范，在更大的视角中，主流的 API 设计风格其实可以分为 面向资源的设计，即 ROA（Resource oriented architecture） 面向过程的设计，即 RPC（Remote Procedure Call） restful 便是 ROA 风格的典型例子，而 RPC 风格则相对而言不太容易被大家熟知，但实际上可能大多数的系统的接口是 RPC 风格的，只不过 RPC 风格这个概念不太为人所知。 以用户模块的 CRUD 为例，对比下两个风格： ROA 风格创建用户（POST）123456789Request:POST /users{&quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18}Response:HTTP 201 Created{&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18} 查询用户（GET）1234567Request:GET /users/1Response:HTTP 200 OK{&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18} 查询用户列表（GET）1234567Request:GET /usersResponse:HTTP 200 OK{[{&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18}], &quot;next&quot;: &quot;/users?offset=1&quot;} 创建/修改用户(PUT)123456789Request:PUT /users/1{&quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 19}Response:HTTP 200 OK{&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 19} 修改用户（PATCH）123456789Request:PATCH /users/1{&quot;age&quot;: 20}Response:HTTP 200 OK{&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 20} 删除用户（DELETE）12345Request:DELETE /users/1Response:HTTP 204 No Content ROA 风格和 restful 规范说明的是一回事，为方便其与 RPC 风格接口的对比，特此说明上面示例的一些值得关注的点： 使用 HTTP 响应码（200，201，204），完成 HTTP 语义与业务语义的映射，异常流也出现 404，401 等情况（出于篇幅考虑，本文未做异常流的介绍） PATCH 部分修改资源，请求体是修改部分的内容；PUT 创建/修改资源，请求体是新资源全部的内容 id 是资源定位符，而 age、name 则为属性 RPC 风格创建用户（POST）123456789Request:POST /user/createUser{&quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18}Response:HTTP 200 OK{&quot;code&quot;: 0, &quot;message&quot;: &quot;&quot;, &quot;data&quot;: {&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18}} 查询用户（POST）123456789Request:POST /user/getUser{&quot;id&quot;: 1}Response:HTTP 200 OK{&quot;code&quot;: 0, &quot;message&quot;: &quot;&quot;, &quot;data&quot;: {&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18}} 查询用户列表（POST）1234567Request:POST /user/listUsersResponse:HTTP 200 OK{&quot;code&quot;: 0, &quot;message&quot;: &quot;&quot;, &quot;data&quot;: {&quot;user&quot;: [{&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 18}], &quot;next&quot;: &quot;/user/listUsers?offset=1&quot;}} 修改用户(POST)123456789Request:POST /user/modifyUser{&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 19}Response:HTTP 200 OK{&quot;code&quot;: 0, &quot;message&quot;: &quot;&quot;, &quot;data&quot;: {&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 19}} 修改用户名称(POST)123456789Request:POST /user/modifyUserAge{&quot;id&quot;: 1, &quot;age&quot;: 20}Response:HTTP 200 OK{&quot;code&quot;: 0, &quot;message&quot;: &quot;&quot;, &quot;data&quot;: {&quot;id&quot;: 1, &quot;name&quot;: &quot;kirito&quot;, &quot;age&quot;: 20}} 删除用户（DELETE）1234567Request:POST /user/deleteUser{&quot;id&quot;: 1}Response:{&quot;code&quot;: 0, &quot;message&quot;: &quot;&quot;} RPC 风格不像 restful 一类的 ROA 风格存在一些约定俗称的规范，每个业务系统在落地时，都存在差异，故此处只是笔者个人的经验之谈，但愿读者能够求同存异： user 为模块名，不需要像 ROA 风格使用复数形式 使用明确的动宾结构，而不是将 CRUD 映射到 HTTP Method，HTTP Method 统一使用 POST，查询场景也可以使用 GET 返回值中携带 code、message 和 data，来映射响应状态及响应信息，一般可以自行定义 code 的状态码，本文使用 0 标识请求成功，message 仅在业务响应失败时有意义，data 代表业务响应结果 如何选择 RPC 和 ROA，则需要根据产品自身的业务情况进行决策。有如下的指导原则： 有复杂业务逻辑的 API ，无法使用简单的增、删、改、查描述时宜使用 RPC 风格。 如果业务所属行业标准要求 restful 风格 API 或 ROA 能够满足业务需求，宜使用 ROA 风格。 AWS 主要采用 RPC 风格，Azure、Google 主要采用 ROA（restful）风格，阿里云 OpenAPI 同时支持 RPC 和 ROA，以 RPC 为主。 尽管规范是无罪的，但在 ROA 风格在实践过程中，我还是见识过不少“坑”的： 要求资源先行，即先设计资源，后设计接口，对软件开发流程要求较高 错误的 ROA 设计案例 1：tomcat 等应用服务器在处理 DELETE 方法的 HTTP 请求时，默认不允许携带 request body，需要显式开启，导致删除失败。（此案例为设计者的问题，复杂的删除场景，不应当映射成 DELELE，而应改成 POST，DELETE 不应当携带 request body） 错误的 ROA 设计案例 2：restful 路径中携带的参数，可能会引发正则匹配的问题，例如误将邮箱作为路径参数，或者多级路径匹配的冲突问题（此案例为设计者的问题，复杂的查询场景，不应当映射成 GET，而应改成 POST，path 中只应该出现资源定位符，而不应当携带属性） 响应码为 404 时，较难区分是真的 path 不存在，还是资源不存在 不利于对接网关等需要配置路由转发的场景 我负责的产品所需要的 Open API 规范需要满足以下的需求： 后端开发设计接口时，有明确的设计思路，不至于因为一个接口到底用 POST 还是 GET 实现而纠结，不用花费太多时间在资源的抽象上（这并不是说明资源是不需要被设计的） 前端开发对接接口时，能够较快地与后端协同，并且利于前端接口的封装 用户对接 Open API 时，整体风格一致，模块清晰 综上，在设计风格选择上，我计划采取 RPC 的设计规范。总结一下 RPC 风格的优势： 满足独立输出，CNStack、企业版、公安部的规范 API 设计难度较低，容易落地 阿里云大多数成熟的 IAAS 层产品使用 RPC 规范 适合复杂业务场景 一个详细的 RPC 接口文档示例创建服务请求参数 序号 字段中文名 字段英文名 数据类型 必填 说明 1 名称 name string 是 显示名称 2 协议 protocol string 是 枚举值：http/grpc/webservice 3 负载均衡 lb string 是 枚举值：random/roundrobin 4 上游类型 upstreamType string 是 枚举值：fixed/discovery 5 节点列表 nodes array 否 upstreamType=fixed 时必填，示例：[{“host”: “1.1.1.1”,”port”: “80”,”weight”: “1”}] 6 来源id originId string 否 7 服务名称 serviceName string 否 注册中心中的名称，upstreamType=discovery 时必填 8 服务描述 description string 否 9 网关id gatewayId string 是 返回参数 序号 字段中文名 字段英文名 数据类型 说明 1 响应码 code int 0 标识成功；1 标识失败 2 响应信息 message string 3 响应结果 data string 返回服务 id 请求示例123456789101112131415161718192021222324POST /service/createServiceRequest:{ &quot;name&quot;: &quot;httpbin&quot;, &quot;protocol&quot;: &quot;http&quot;, &quot;lb&quot;: &quot;random&quot;, &quot;upstreamType&quot;: &quot;fixed&quot;, &quot;nodes&quot;: [ { &quot;host&quot;: &quot;httpbin.org&quot;, &quot;port&quot;: &quot;80&quot;, &quot;weight&quot;: &quot;1&quot; } ], &quot;gatewayId&quot;: &quot;gw-1qw2e3e4&quot;}Response:{ &quot;code&quot;: 0, &quot;message&quot;: &quot;&quot;, &quot;serviceId&quot;: &quot;s-1qw2e3e4&quot;} API 命名规范 API 应使用拼写正确的英文，符合语法规范，包括单复数、时态和语言习惯 不能出现多个含义相近但功能无实际差别的 API，如同时存在 /user/getUser 和 /user/describeUser 语言习惯：禁止使用拼音 如下常见场景的命名规则是固定的 日期时间类型的参数应命名为 XxxxTime。例如：CreateTime 常用操作名称规范 create：创建 modify：变更 delete：删除 get：获取单个资源详情 list：获取资源列表 establishRelation：建立资源关系 destroyRelation：销毁资源关系 总结以本文推崇的一条规范为例：”所有接口全部使用 POST”，这不是为了迁就低水平不思进取的架构师和前后端程序员们（我在社区论坛上看到的言论），而是为了提高开发效率，降低沟通成本，降低运维和错误定位成本，把瞎折腾的成本，投入到了其他比如业务架构设计，测试体系，线上监控，容灾降级等领域上。 接口规范也并非我总结的那样，只有 RPC 和 ROA，也有一些言论将 GraphQL 单独归为一类 API 设计风格，用于复杂查询场景，有兴趣的同学可以参考 es 的 API 文档。 综上，我计划采用 RPC 的 API 设计风格。 参考资料kong：https://docs.konghq.com/gateway/2.8.x/admin-api/ google restful api design：https://cloud.google.com/apis/design?hl=zh-cn https://www.zhihu.com/question/336797348","link":"/api-design/"},{"title":"ACK 部署 Apache apisix-ingress-cotroller","text":"背景Ingress 是 Kubernetes 中一个值得关注的模块，作为外部访问 Kubernetes 集群服务的入口，市面上已经有了多种 Ingress controller 的实现。国产实时、高性能的 API 网关 Apache APISIX 推出的 Apache/apisix-ingress-controller 就是其中一员，作为功能更加强大的 ingress 对外提供服务。笔者准备在阿里云 ACK 集群上部署测试。 主题描述本文主要介绍在阿里云 ACK 部署 apisix-ingress-controller，并且使用 httpbin 测试一个简单的场景。 部署拓扑 依赖项阿里云的 ACK 集群 ；推荐最低配置：3个 master 节点：CPU 2核 内存 4G2个 worker 节点：CPU 4核 内存 8G 安装步骤apisix 2.1 release通过 helm 安装 apisix 2.1 release 123456789$ kubectl create ns apisix$ git clone https://github.com/apache/apisix-helm-chart.git$ cd ./apisix-helm-chart$ helm repo add bitnami https://charts.bitnami.com/bitnami$ helm dependency update ./chart/apisix$ helm install apisix ./chart/apisix \\ --set gateway.type=LoadBalancer \\ --set allow.ipList=&quot;{0.0.0.0/0}&quot; \\ --namespace apisix tips: etcd 安装时指定 PVC， PVC 在阿里云部署时，需要指定 PV 为云盘， 请在 PVC 的 annotations 中增加：volume.beta.kubernetes.io/storage-class: alicloud-disk-ssd。(关于 PVC 和 PV 的关系请参考这里) apisix-ingress-controller通过 helm 安装 apisix-ingress-controller 12345678$ git clone https://github.com/apache/apisix-ingress-controller.git$ cd ./apisix-ingress-controller$ helm install ingress-apisix-base -n apisix ./charts/base$ helm install ingress-apisix ./charts/ingress-apisix \\ --set ingressController.image.tag=dev \\ --set ingressController.config.apisix.baseURL=http://apisix-admin:9180/apisix/admin \\ --set ingressController.config.apisix.adminKey=edd1c9f034335f136f87ad84b625c8f1 \\ --namespace apisix 测试检查集群是否部署成功 配置一个简单的路由做测试1234567891011121314apiVersion: apisix.apache.org/v1kind: ApisixRoutemetadata: name: httpbin-route namespace: apisixspec: rules: - host: httpbin.apisix.com http: paths: - backend: serviceName: httpbin servicePort: 80 path: /hello* 通过 apisix admin api 查看结果，发现路由已经正确配置。 12345678910111213141516171819202122232425262728293031323334{ &quot;action&quot;: &quot;get&quot;, &quot;count&quot;: &quot;2&quot;, &quot;header&quot;: { &quot;revision&quot;: &quot;46&quot;, &quot;cluster_id&quot;: &quot;8320356269565269865&quot;, &quot;raft_term&quot;: &quot;2&quot;, &quot;member_id&quot;: &quot;3807956127770623265&quot; }, &quot;node&quot;: { &quot;key&quot;: &quot;/apisix/upstreams&quot;, &quot;dir&quot;: true, &quot;modifiedIndex&quot;: 27, &quot;createdIndex&quot;: 3, &quot;nodes&quot;: [ { &quot;key&quot;: &quot;/apisix/upstreams/00000000000000000041&quot;, &quot;modifiedIndex&quot;: 42, &quot;value&quot;: { &quot;nodes&quot;: { &quot;172.20.1.12:80&quot;: 100 }, &quot;type&quot;: &quot;roundrobin&quot;, &quot;pass_host&quot;: &quot;pass&quot;, &quot;hash_on&quot;: &quot;vars&quot;, &quot;desc&quot;: &quot;apisix_httpbin_80&quot;, &quot;create_time&quot;: 1608561159, &quot;update_time&quot;: 1608561159 }, &quot;createdIndex&quot;: 42 } ] }} 扩容 httpbin 查看 k8s 中 httpbin 查看 apisix 中 httpbin upstream 12345678910111213// 格式化后{ ... &quot;nodes&quot;: { &quot;172.20.1.12:80&quot;: 100, &quot;172.20.0.198:80&quot;: 100, &quot;172.20.0.197:80&quot;: 100 }, &quot;id&quot;: &quot;00000000000000000041&quot;, &quot;key&quot;: &quot;/apisix/upstreams/00000000000000000041&quot;, &quot;desc&quot;: &quot;apisix_httpbin_80&quot;, ...} 总结 本文在 ACK 集群环境依次安装了 Etcd、 Apache APISIX、Apache apisix-ingress-controller，并且使用 httpbin 服务验证 ingress 的基本配置功能，通过 CRD 配置了路由，检测了后端服务在扩缩容时服务注册发现机制。 另外值得一提的是 apisix-ingress-controller 可以完整的支持 Apache APISIX 提供的所有插件，甚至是自定义插件。功能丰富且扩展能力强，是一款不错的 Ingress 项目。","link":"/apisix-ingress/"},{"title":"不会吧？不会还有人不知道 Arthas 可以条件过滤进行 watch 吧？","text":"前言Arthas 的 watch 指令一直是我排查线上问题时使用最多的指令，没有之一。而按照条件进行 watch 也是很常见的一个需求，例如线上一个方法会有大量的调用，而我们可以按照指定的条件，watch 到我们希望观察的那一次调用。 说实话，我对 Arthas 也没有什么研究，一开始还真不清楚原来 Arthas watch 可以按条件过滤，翻看一下官方文档：https://arthas.aliyun.com/doc/watch#id6 条件表达式的例子 1234567$ watch demo.MathGame primeFactors &quot;{params[0],target}&quot; &quot;params[0]&lt;0&quot;Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 68 ms.ts=2018-12-03 19:36:04; [cost=0.530255ms] result=@ArrayList[ @Integer[-18178089], @MathGame[demo.MathGame@41cf53f9],] 喏，这不是这么明显的例子吗？但是上面的例子，貌似只给出了一些简单的讯息 可以直接用 watch 命令的参数项中增加条件表达式进行过滤 可以进行数值类型的过滤 但是，这个简单的示例，并没有解答我内心其他的疑惑： 我可以进行字符串、集合等复杂类型的判断吗？ 这个表达式是 el、ognl 或者其他类型的表达式吗？ 带着这些疑问，我记录下了这篇文章，给不了解 Arthas watch 条件表达式的读者们一些参考。 一些条件表达式的示例有一些读者可能仅仅是想知道“我该怎么实现使用 Arthas 条件 Watch”，为此，我在本文的第二节先介绍下我平时积累的一些实践命令。 示例方法 12public void methodForWatch(int id, User user) {} User 结构 123456@Datapublic class User { private String name; private int age; private List&lt;String&gt; hobbies;} 另外准备一些请求，我会在每个示例中执行相同的调用。示例请求： 12341, new User(&quot;hanmeimei&quot;, 16, Arrays.asList(&quot;pubg&quot;, &quot;lol&quot;));2, new User(&quot;liming&quot;, 17, Collections.singletonList(&quot;pubg&quot;));3, new User(&quot;tom&quot;, 18, Collections.singletonList(&quot;running&quot;));4, new User(&quot;jacky&quot;, 19, Collections.singletonList(&quot;food&quot;)); 示例 1：过滤 int 类型；过滤 id &gt; 0 的请求 这其实就是官方的示例，我拿过来再贴一遍 1watch moe.cnkirito.arthas.WatchDemo methodForWatch &quot;{params,returnObj}&quot; &quot;params[0]&gt;0&quot; -x 2 示例 2：过滤对象中的字符串类型；过滤 User 中 name = haimeimei 的请求 1watch moe.cnkirito.arthas.WatchDemo methodForWatch &quot;{params,returnObj}&quot; &quot;params[1].getName().equals('liming')&quot; -x 2 这里有三个注意点 使用 params[1] 这种数组访问的方式，对应到 methodForWatch 方法的第二个参数 User user 使用 getName() 这种方法调用的方式拿到 name 字段，并且使用 String 的 equals 方法进行字符串比对 由于 condition 表达式整体使用了双引号 “”，在 hanmeimei 该字面量上需要使用单引号 ‘’ 示例 3：过滤集合中的元素;过滤对 pubg 感兴趣的 User 相关的请求 1watch moe.cnkirito.arthas.WatchDemo methodForWatch {params,returnObj} &quot;params[1].getHobbies().contains('pubg')&quot; -x 2 示例 4：多个条件表达式 增加请求示例 15, new User(&quot;kirito&quot;, 20, null); 按照示例 3 的 watch 语句执行，会发现 Arthas 直接抛了一个空指针： 1watch failed, condition is: params[1].getHobbies().contains('pubg'), express is: {params,returnObj}, java.lang.NullPointerException: target is null for method contains, visit /Users/xujingfeng/logs/arthas/arthas.log for more details. 需要增加空指针的判断： 1watch moe.cnkirito.actuator.demo.HelloController methodForWatch {params,returnObj} &quot;params[1].getHobbies() != null &amp;&amp; params[1].getHobbies().contains('pubg')&quot; -x 2 呐，很简单，可以直接使用 &amp;&amp; 增加判断条件。 ognl 实现条件过滤可能有人要说了，Kirito 啊！你的公众号最近是不是广告发的太多了，深感愧疚，写了一篇没啥深度的原创文章来充数啊！那我当然要反驳拉，其实我看 Arthas 文档的时候也是踩了坑的好吧，索性我将这个过程也分享一下。 可能大家看了上面的示例会觉得这个 condition 表达式不就是跟 Java 里面的表达式差不多吗？但其实我作为一个不太了解 Arthas 的弱鸡，上面的用法纯粹是我摸索出来的，在最开始的时候，参考 github 中的 issue，我使用的其实是其他的方式来实现的条件查询，参考 issue：https://github.com/alibaba/arthas/issues/71。 看下 github 中的 Arthas 开源作者提供的按条件过滤的示例，可以发现跟上文中我介绍的过滤方式好像，有那么一点点的不同。注意上文的示例 1$ watch demo.MathGame primeFactors &quot;{params[0],target}&quot; &quot;params[0]&lt;0&quot; watch 后的参数是由 4 部分组成的，分别是类名表达式，方法名表达式，观察表达式，条件表达式。 而 issue 中给出的表达式 1$ watch com.taobao.container.Test test &quot;params[0].{? #this.name == null }&quot; -x 2 没有第四部分：条件表达式。过滤条件被放到了观察表达式的对象后，并且不是 Java 里面的表达式，而是 ognl 表达式。 ognl 表达式官方参考文档：https://commons.apache.org/proper/commons-ognl/language-guide.html 例如使用 ognl 表达式实现上面的示例 2，需要这么写 示例 5：使用 ognl 表达式过滤对象中的字符串类型；过滤 User 中 name = haimeimei 的请求 1watch moe.cnkirito.actuator.demo.HelloController methodForWatch &quot;params[1].{? #this.name == 'hanmeimei'}&quot; -x 2 示例 2 和示例 5 的对比聊到这里，如果你对 Arthas 比较熟悉，应该已经意识到示例 5 ognl 过滤和示例 2 直接使用条件过滤表达式的区别了。ognl 这种过滤的方式，是针对对象的属性的过滤，无论是否匹配，都会被算进 watch 的匹配次数中，只不过没有匹配到的对象没有输出；而示例 2 中直接使用条件过滤表达式这种方式，更匹配我文首提出的需求，只有被条件表示式命中的请求，才会被算进 watch 次数中。你可以使用 -n 1 来限定 watch 匹配次数，直观地观察到这两个匹配方式的差异。 总结本文简单介绍了使用 Arthas 条件表达式使用中可能踩到的一些坑，示例 1~4 可以参考，用于过滤一些指定的请求，让线上问题的定位变得更加高效。 Kirito 对 Arthas 的研究并不是特别深，如果还有其他关于条件表达式的问题或者本文存在的问题，欢迎留言交流~","link":"/arthas-condition-watch/"},{"title":"lambda 表达式导致 Arthas 无法 redefine 的问题","text":"原文出处：https://m.jb51.net/article/188155.htm 作者：鲁严波 这篇文章主要介绍了 lambda 表达式导致 Arthas 无法 redefine 的问题,本文通过图文实例相结合给大家介绍的非常详细，对大家的学习或工作具有一定的参考借鉴价值，需要的朋友可以参考下。 通过 arthas 的 redefine 命令，可以做到不用重新发布，就可以改变程序行为。 但是用多了，发现很多时候，我们就改了几行代码，甚至有的时候就添加了一行日志，就无法 redefine 了。提示： redefine error! java.lang.UnsupportedOperationException: class redefinition failed: attempted to add a method 它提示我们新增加方法，那我们就看看是不是新增加了方法。通过 javap 来查看定义的方法： 这是老的类： 这是新的类： 对比之后发现，新的类，即本地编译的类，其中的 lambda 对应的方法名都是 lambda$getAllCity$0 这样的，最后的编号是从 0 开始的。 而旧的类，即现在在运行的类，其中的同一个 lambda 的方法名是 lambda$getAllCity$121，最后的编号是一个非常大的数字。 在仔细对比下，发现是 jdk 的版本问题，不同的 jdk 版本对与 lamdba 的处理可能不一致。 具体来说，线上编译的 jdk 版本是 1.8.0_66-b17， 而本地是 1.8.0_222-b10，而这两个版本对 lambda 对应的方法命名是不一样的。 首先，为了调试方便，写一个最小复现用例来看看： 123456789101112131415161718192021222324252627282930313233343536373839404142434445// Compile.java// 编译LamdbaTest1.java和LamdbaTest2.javaimport javax.tools.*;import java.io.File;public class Compile { public static void main(String[] args) { String path1 = &quot;/path/to/LamdbaTest1.java&quot;; String path2 = &quot;/path/to/LamdbaTest2.java&quot;; JavaCompiler javaCompiler = ToolProvider.getSystemJavaCompiler(); DiagnosticCollector diagnostics = new DiagnosticCollector(); StandardJavaFileManager fileManager = javaCompiler.getStandardFileManager(diagnostics, null, null); Iterable&lt;? extends JavaFileObject&gt; compilationUnits = fileManager.getJavaFileObjects( new File(path1), new File(path2) ); JavaCompiler.CompilationTask task = javaCompiler.getTask(null, fileManager, diagnostics, null, null, compilationUnits); boolean success = task.call(); System.out.println(success); }}//LamdbaTest1.javapublic class LamdbaTest1 { private void test(Runnable runnable) { runnable.run(); } private void main() throws Throwable { test(() -&gt; { System.out.println(11); }); }}//LamdbaTest2.javapublic class LamdbaTest2 { private void test(Runnable runnable) { runnable.run(); } private void main() throws Throwable { test(() -&gt; { System.out.println(22); }); }} 使用 1.8.0_222-b10（新版本 jdk）跑完了之后，发现 LamdbaTest2 中的 lambda 方法是： 1private static void lambda$main$0(); 而换版本 1.8.0_66-b17（旧版本 jdk）之后，lambda 的方法就成了： 1private static void lambda$main$1(); 多尝试几个文件同时编译，我们就可以发现：对于旧版本的 javac，末尾这个数字是全局递增的，50 个类有 100 个 lambda，那最后一个 lambda 的编号就是 99；而新的版本是每个类重新计数的，和总共多少个类没有关系。 确认了问题之后，接下来就是不断的打断点、重试了。后来发现不同版本的 javac 逻辑确实不同。 首先，查看 jdk 源码可以知道，lambda 的方法名都是： 1lambda$&lt;methodname&gt;$&lt;lambdaCount&gt; 代码见 LambdaToMethod.java 不同的地方在于： 新版本的 javac，在处理一个新的类的时候，会保存上一个 lambdaCount，后续再恢复： 而旧版本则没有这个逻辑： 这就说明旧版本的编译器确实是 lambda 全局编号的。 那，问题来了，这个行为是从哪个版本变掉的呢？ 对比之后发现这个变更是 jdk8u74-b02 引入的。对应的 bug 是 https://bugs.openjdk.java.net/browse/JDK-8067422，基本上就是每个类内的 lambda 单独编号，确保编译顺序不会影响 lambda 的方法名字。 所以，解决方案很简单，升级编译环境的 jdk 版本就好。 非常巧合的是，前两天为了更好的适配 Docker 运行环境（通俗的讲，就是在容器内获取到 docker 的 cpu 配额，而不是物理机器的 cpu 数量），我找运维添加了一个新的j dk 版本 1.8.0_231-b11，这样只需要直接将编译环境的 jdk 版本切换到 8u231 就行！","link":"/arthas-lambda-redefine/"},{"title":"Arthas | 热更新线上代码","text":"前言本文是我介绍 Arthas 系列文章的第一篇。 一般线上问题比开发环境的问题更难解决，一个主要的原因便在于开发态可以任意 debug 断点调试，而线上环境一般不允许远程调试，所以在实践中，我一般习惯用 Arthas 来定位线上的问题。 Arthas 是阿里巴巴开源的 Java 应用诊断利器 Arthas 可以完成很多骚操作，今天给大家介绍的 Arthas 诊断技巧便是 – 热更新线上代码。在生产环境热更新代码，并不是很好的行为，可能会引发一些问题 黑屏化的操作可能会导致误操作 不符合安全生产的规范，不满足可监控、可回滚、可降级 但有时候也有一些场景可以考虑使用 Arthas 来热更，例如开发环境无法复现的问题、找到修复思路后临时验证等。 本文以 Arthas 3.1.7 版本为例，主要使用到 jad/mc/redefine 三个指令。 示例在 arthas-demo 示例中，一共有两个类，一个 HelloService 类，sayHello 方法负责不断的打印 hello world： 1234567public class HelloService { public void sayHello() { System.out.println(&quot;hello world&quot;); }} HelloService 用于模拟我们日常开发的一些业务 Service，另外还有一个 Main 函数，负责启动进程，并循环调用 1234567891011public class Main { public static void main(String[] args) throws InterruptedException { HelloService helloService = new HelloService(); while (true) { Thread.sleep(1000); helloService.sayHello(); } }} 需求假设这段代码运行在线上，我们希望通过 Arthas 将 hello world 的输出更改为 hello arthas。 Arthas 修改热更的逻辑主要分为三步： jad 命令反编译出内存中的字节码，生成 class 文件 修改代码，使用 mc 命令内存编译新的 class 文件 redefine 重新加载新的 class 文件 从而达到热更新的效果 jad 反编译当挂载上 Arthas 之后，执行 1$ jad --source-only moe.cnkirito.arthas.demo.HelloService &gt; /tmp/HelloService.java 将字节码文件输出到指定的位置，查看其内容，与示例中的源码内容一致： 123456789101112/* * Decompiled with CFR. */package moe.cnkirito.arthas.demo;import java.io.PrintStream;public class HelloService { public void sayHello() { System.out.println(&quot;hello world&quot;); }} 命令中 --source-only 的含义为，只输出源码部分，如果不加这个参数，在反编译出的内容头部会携带类加载器的信息： 123456ClassLoader:+-sun.misc.Launcher$AppClassLoader@18b4aac2 +-sun.misc.Launcher$ExtClassLoader@20d5ad12Location:/Users/xujingfeng/IdeaProjects/arthas-demo/target/classes/ 在服务器上可以直接使用 vi 等编辑器对源码进行编辑。将 hello world 改为 hello arthas，为下一步做准备。 sc 查找类加载器mc 命令编译文件需要传入该类对应类加载器的 hash 值，需要先使用 sc 命令查看 HelloService 的累加器信息 1$ sc -d moe.cnkirito.arthas.demo.HelloService 输出： 1234567891011121314151617181920class-info moe.cnkirito.arthas.demo.HelloService code-source /Users/xujingfeng/IdeaProjects/arthas-demo/target/classes/ name moe.cnkirito.arthas.demo.HelloService isInterface false isAnnotation false isEnum false isAnonymousClass false isArray false isLocalClass false isMemberClass false isPrimitive false isSynthetic false simple-name HelloService modifier public annotation interfaces super-class +-java.lang.Object class-loader +-sun.misc.Launcher$AppClassLoader@18b4aac2 +-sun.misc.Launcher$ExtClassLoader@20d5ad12 classLoaderHash 18b4aac2 最后一行 classLoaderHash 即为 HelloService 的类加载器 hash 值。 Arthas 支持 grep，你也可以简化该操作为： sc -d moe.cnkirito.arthas.demo.HelloService | grep classLoaderHash mc 内存编译123$ mc -c 18b4aac2 /tmp/HelloService.java -d /tmpMemory compiler output:/tmp/moe/cnkirito/arthas/demo/HelloService.class 使用 -c 指定类加载器的 hash 值。编译完成后，/tmp 目录下会生成对应的 class 字节码文件 redefine 热更新代码1$ redefine /tmp/moe/cnkirito/arthas/demo/HelloService.class 检查结果12345678hello worldhello worldhello worldhello worldhello arthashello arthashello arthashello arthas 热更新成功 常见问题redefine 使用限制 不允许新增或者删除 field/method 会出现类似下面的提示 1redefine error! java.lang.UnsupportedOperationException: class redefinition failed: attempted to change the schema (add/remove fields) 运行中的方法不会立刻生效，会在下一次进入该方法时才能生效。 很好理解，并发问题 mc 常见问题 mc 命令有可能失败 因为运行时环境和编译时环境的 JDK 可能有版本差异，mc 可能会失败。如果编译失败可以在本地编译好 .class 文件，再上传到服务器 当存在内部类时，一次会生成多个 class 文件 123456789101112public class HelloService { public void sayHello() { Inner.test(); } public static class Inner { public static void test() { System.out.println(&quot;hello inner&quot;); } }} 执行 mc 1234$ mc -c 18b4aac2 /tmp/HelloService.java -d /tmpMemory compiler output:/tmp/moe/cnkirito/arthas/demo/HelloService$Inner.class/tmp/moe/cnkirito/arthas/demo/HelloService.class 注意 redefine 时也可以同时传入多个入参 12$ redefine /tmp/moe/cnkirito/arthas/demo/HelloService$Inner.class /tmp/moe/cnkirito/arthas/demo/HelloService.classredefine success, size: 2 参考文章https://blog.csdn.net/hengyunabc/article/details/87718469 Arthas 钉钉交流群","link":"/arthas-redefine/"},{"title":"Arthas | 定位线上 Dubbo 线程池满异常","text":"前言本文是 Arthas 系列文章的第二篇。 Dubbo 线程池满异常应该是大多数 Dubbo 用户都遇到过的一个问题，本文以 Arthas 3.1.7 版本为例，介绍如何针对该异常进行诊断，主要使用到 dashboard/thread 两个指令。 Dubbo 线程池满异常介绍理解线程池满异常需要首先了解 Dubbo 线程模型，官方文档：http://dubbo.apache.org/zh-cn/docs/user/demos/thread-model.html。简单概括下 Dubbo 默认的线程模型：Dubbo 服务端每次接收到一个 Dubbo 请求，便交给一个线程池处理，该线程池默认有 200 个线程，如果 200 个线程都不处于空闲状态，则 客户端会报出如下异常： 1Caused by: java.util.concurrent.ExecutionException: org.apache.dubbo.remoting.RemotingException: Server side(192.168.1.101,20880) threadpool is exhausted ... 服务端会打印 WARN 级别的日志： 1[DUBBO] Thread pool is EXHAUSTED! 引发该异常的原因主要有以下几点： 客户端/服务端超时时间设置不合理，导致请求无限等待，耗尽了线程数 客户端请求量过大，服务端无法及时处理，耗尽了线程数 服务端由于 fullgc 等原因导致处理请求较慢，耗尽了线程数 服务端由于数据库、Redis、网络 IO 阻塞问题，耗尽了线程数 … 原因可能很多，但纠其根本，都是因为业务上出了问题，导致 Dubbo 线程池资源耗尽了。所以出现该问题，首先要做的是： 排查业务异常 紧接着针对自己的业务场景对 Dubbo 进行调优： 调整 Provider 端的 dubbo.provider.threads 参数大小，默认 200，可以适当提高。多大算合适？至少 700 不算大；不建议调的太小，容易出现上述问题 调整 Consumer 端的 dubbo.consumer.actives 参数，控制消费者调用的速率。这个实践中很少使用，仅仅一提 客户端限流 服务端扩容 Dubbo 目前不支持给某个 service 单独配置一个隔离的线程池，用于保护服务，可能在以后的版本中会增加这个特性 另外，不止 Dubbo 如此设计线程模型，绝大多数服务治理框架、 HTTP 服务器都有业务线程池的概念，所以理论上它们都会有线程池满异常的可能，解决方案也类似。 那竟然问题都解释清楚了，我们还需要排查什么呢？一般在线上，有很多运行中的服务，这些服务都是共享一个 Dubbo 服务端线程池，可能因为某个服务的问题，导致整个应用被拖垮，所以需要排查是不是集中出现在某个服务上，再针对排查这个服务的业务逻辑；需要定位到线程堆栈，揪出导致线程池满的元凶。 定位该问题，我的习惯一般是使用 Arthas 的 dashboard 和 thread 命令，而在介绍这两个命令之前，我们先人为的构造一个 Dubbo 线程池满异常的例子。 复现 Dubbo 线程池满异常配置服务端线程池大小1dubbo.protocol.threads=10 默认大小是 200，不利于重现该异常 模拟服务端阻塞123456789101112131415161718@Service(version = &quot;1.0.0&quot;)public class DemoServiceImpl implements DemoService { @Override public String sayHello(String name) { sleep(); return &quot;Hello &quot; + name; } private void sleep() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } }} sleep 方法模拟了一个耗时操作，主要是为了让服务端线程池耗尽。 客户端多线程访问12345678910111213141516for (int i = 0; i &lt; 20; i++) { new Thread(() -&gt; { while (true){ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } try { demoService.sayHello(&quot;Provider&quot;); } catch (Exception e) { e.printStackTrace(); } } }).start();} 问题复现客户端 服务端 问题得以复现，保留该现场，并假设我们并不知晓 sleep 的耗时逻辑，使用 Arthas 来进行排查。 dashboard 命令介绍1$ dashboard 执行效果 可以看到如上所示的面板，显示了一些系统的运行信息，这里主要关注 THREAD 面板，介绍一下各列的含义： ID: Java 级别的线程 ID，注意这个 ID 不能跟 jstack 中的 nativeID 一一对应 NAME: 线程名 GROUP: 线程组名 PRIORITY: 线程优先级, 1~10 之间的数字，越大表示优先级越高 STATE: 线程的状态 CPU%: 线程消耗的 CPU 占比，采样 100ms，将所有线程在这 100ms 内的 CPU 使用量求和，再算出每个线程的 CPU 使用占比。 TIME: 线程运行总时间，数据格式为分：秒 INTERRUPTED: 线程当前的中断位状态 DAEMON: 是否是 daemon 线程 在空闲状态下线程应该是处于 WAITING 状态，而因为 sleep 的缘故，现在所有的线程均处于 TIME_WAITING 状态，导致后来的请求被处理时，抛出了线程池满的异常。 在实际排查中，需要抽查一定数量的 Dubbo 线程，记录他们的线程编号，看看它们到底在处理什么服务请求。使用如下命令可以根据线程池名筛选出 Dubbo 服务端线程： 1dashboard | grep &quot;DubboServerHandler&quot; thread 命令介绍使用 dashboard 筛选出个别线程 id 后，它的使命就完成了，剩下的操作交给 thread 命令来完成。其实，dashboard 中的 thread 模块，就是整合了 thread 命令，但是 dashboard 还可以观察内存和 GC 状态，视角更加全面，所以我个人建议，在排查问题时，先使用 dashboard 纵观全局信息。 thread 使用示例： 查看当前最忙的前 n 个线程 1$ thread -n 3 显示所有线程信息 1$ thread 和 dashboard 中显示一致 显示当前阻塞其他线程的线程 123$ thread -bNo most blocking thread found!Affect(row-cnt:0) cost in 22 ms. 这个命令还有待完善，目前只支持找出 synchronized 关键字阻塞住的线程， 如果是 java.util.concurrent.Lock， 目前还不支持 显示指定状态的线程 1$ thread --state TIMED_WAITING 线程状态一共有 [RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, NEW, TERMINATED] 6 种 查看指定线程的运行堆栈 1$ thread 46 介绍了几种常见的用法，在实际排查中需要针对我们的现场做针对性的分析，也同时考察了我们对线程状态的了解程度。我这里列举了几种常见的线程状态： 初始(NEW)新创建了一个线程对象，但还没有调用 start() 方法。 运行(RUNNABLE)Java 线程将就绪（ready）和运行中（running）两种状态笼统的称为“运行” 阻塞(BLOCKED)线程阻塞于锁 等待(WAITING)进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断） Object#wait() 且不加超时参数 Thread#join() 且不加超时参数 LockSupport#park() 超时等待(TIMED_WAITING)该状态不同于 WAITING，它可以在指定的时间后自行返回 Thread#sleep() Object#wait() 且加了超时参数 Thread#join() 且加了超时参数 LockSupport#parkNanos() LockSupport#parkUntil() 终止(TERMINATED)标识线程执行完毕 状态流转图 问题分析分析线程池满异常并没有通法，需要灵活变通，我们对下面这些 case 一个个分析： 阻塞类问题。例如数据库连接不上导致卡死，运行中的线程基本都应该处于 BLOCKED 或者 TIMED_WAITING 状态，我们可以借助 thread --state 定位到 繁忙类问题。例如 CPU 密集型运算，运行中的线程基本都处于 RUNNABLE 状态，可以借助于 thread -n 来定位出最繁忙的线程 GC 类问题。很多外部因素会导致该异常，例如 GC 就是其中一个因素，这里就不能仅仅借助于 thread 命令来排查了。 定点爆破。还记得在前面我们通过 grep 筛选出了一批 Dubbo 线程，可以通过 thread ${thread_id} 定向的查看堆栈，如果统计到大量的堆栈都是一个服务时，基本可以断定是该服务出了问题，至于说是该服务请求量突然激增，还是该服务依赖的某个下游服务突然出了问题，还是该服务访问的数据库断了，那就得根据堆栈去判断了。 总结本文以 Dubbo 线程池满异常作为引子，介绍了线程类问题该如何分析，以及如何通过 Arthas 快速诊断线程问题。有了 Arthas，基本不再需要 jstack 将 16 进制转来转去了，大大提升了诊断速度。 Arthas 钉钉交流群","link":"/arthas-thread/"},{"title":"Arthas | 追踪线上耗时方法","text":"前言本文是 Arthas 系列文章的第三篇。 本文主要介绍 trace 指令，用于定位两种类型的问题： 线上服务 RT 比较高，但没有打印日志，无法确定具体是哪个方法比较耗时 线上服务出现异常，需要追踪到方法的堆栈 模拟线上耗时方法总结本文以 Dubbo 线程池满异常作为引子，介绍了线程类问题该如何分析，以及如何通过 Arthas 快速诊断线程问题。有了 Arthas，基本不再需要 jstack 将 16 进制转来转去了，大大提升了诊断速度。 Arthas 钉钉交流群","link":"/arthas-trace/"},{"title":"博客搬家","text":"陆陆续续，写博客已经写了有 4 年多了，之前一直在 CSDN 维护博客（博客旧址），最近有了点空余时间，使用 hexo 搭了这个博客，的确比 CSDN 清爽多了，首先感谢 @程序猿 DD 推荐的 icarus 模板，国人开发的一个 hexo 模板，插件支持可能不是很完善，但是样式非常让人喜欢。 作为一个前端弱渣，搭建博客的过程还是遇到了不少的困难。原先是打算直接使用 github 个人主页作为博客地址，hexo 对 git 有很好的支持，源代码和博客静态页面都托管在了 github，master 分支放静态页面，hexo 分支放源文件。可惜的是国内坑爹的网速,github.io 的访问速度不尽如人意（github.com 倒还好），于是在宇泽学妹 @ntzyz 的帮助下，搞了 github 的 hook，本地提交到 github 时，代理服务器自动向 master 分支拉取页面，同时设置反向代理和 https。由于 hexo 是静态文件搭建的博客，这种方式可以说是非常合适的。所以，国内的朋友浏览本博客可以直接访问 https://www.cnkirito.moe，如果有国外代理的朋友可以直接访问我的 github 个人主页 https://lexburner.github.io。 目前博客功能还不算完善，缺少评论，分享，和一些小插件，以后逐渐完善，不过不影响主要功能。以后这儿就作为我主要更新博客的地方了！","link":"/blog-migration/"},{"title":"“字节序”是个什么鬼","text":"转载自：https://zhuanlan.zhihu.com/p/21388517 论顺序的重要性做饭的故事今天女朋友加班，机智的她早已在昨晚准备好食材，回家只需下锅便可。谁知开会就是个无底洞，到了B1，还有B2，无穷匮也。 辛苦如她，为了能让她一回家就吃上热腾腾的饭菜，我准备亲自下厨，奉献出我的第一次。食材都已备好，我相信没有那么难，估摸着应该和我习以为常的流程处理差不多，开火 | 加食材 | 上配料 | 翻炒 | 出锅，啊哈，想想还有点小激动。 今天的晚饭是西红柿炒鸡蛋和胡萝卜炒肉，实际操作才发现，又遇到了一个大坑…… 食材是这样的： 12案板1号（西红柿炒鸡蛋的食材），从左向右依次放着：西红柿、鸡蛋、葱案板2号（胡萝卜炒肉的食材），从左向右依次放着：蒜、胡萝卜丝、肉 食材在案板上整齐划一依次排开，我是先放西红柿呢，还是先放鸡蛋呢，还是先放葱呢？简单沟通后得知，案板上的食材是按顺序放好的，我只需要按顺序下锅即可。听着电视哼着90年代的老歌，三下五除二，两道菜如期完成。 闻着怪味，我知道第一次就这么失败了。 等她回家，一番检讨后，才知道是顺序放错了。每道菜都应该是从右往左依次放食材，即葱-&gt;鸡蛋-&gt;西红柿。这是逗我的么！？一般人所理解的按默认顺序不应该是从左往右嘛！ 朋友们，到底应该是从左往右还是从右往左？ 剥鸡蛋的故事《格列佛游记》中记载了两个征战的强国，你不会想到的是，他们打仗竟然和剥鸡蛋的姿势有关。 很多人认为，剥鸡蛋时应该打破鸡蛋较大的一端，这群人被称作“大端（Big endian）派”。可是当今皇帝的祖父小时候吃鸡蛋的时候碰巧将一个手指弄破了。所以，他的父亲（当时的皇帝）就下令剥鸡蛋必须打破鸡蛋较小的一端，违令者重罚，由此产生了“小端（Little endian）派”。 老百姓们对这项命令极其反感，由此引发了6次叛乱，其中一个皇帝送了命，另一个丢了王位。据估计，先后几次有11000人情愿受死也不肯去打破鸡蛋较小的一端！ 看到没有，仅仅是剥鸡蛋就能产生这么大的分歧，“大端”和“小端”有这么重要嘛！ 字节序字节字节（Byte）作为计算机世界的计量单位，和大家手中的人民币多少多少“元”一个意思。反正，到了计算机的世界，说字节就对了，使用人家的基本计量单位，这是入乡随俗。 比如，一个电影是1G个字节（1GB），一首歌是10M个字节（10MB），一张图片是1K个字节（1KB）。 字节序一元钱可以干嘛？啥也干不了，公交都不够坐的。一个字节可以干嘛？至少可以存一个字符。 当数据太大，一个字节存不下的时候，我们就得使用多个字节了。比如，我有两个分别需要4个字节存储的整数，为了方便说明，使用16进制表示这两个数，即0x12345678和0x11223344。有的人采用以下方式存储这个两个数字： 这个方案看起来不错，但是，又有人采用了以下方式： 蒙圈了吧，到底该用哪一种方式来存！两种方案虽有不同，但也有共识，即依次存储每一个数字，即先存0x12345678，再存0x11223344。大家的分歧在于，对于某一个要表示的值，因为只能一个字节一个字节的存嘛，我是把值的低位存到低地址，还是把值的高位存到低地址。前者使用的是“小端（Little endian）”字节序，即先存低位的那一端（两个数字的最低位分别是0x78、0x44），如上图中的第一个图；后者使用的是“大端（Big endian）”字节序，即先存高位的那一端（两个数字的最高位分别是0x12、0x11）,如上图中的第二个图。 由此也引发了计算机界的大端与小端之争，不同的CPU厂商并没有达成一致： x86，MOS Technology 6502，Z80，VAX，PDP-11等处理器为Little endian。 Motorola 6800，Motorola 68000，PowerPC 970，System/370，SPARC（除V9外）等处理器为Big endian。 ARM, PowerPC (除PowerPC 970外), DEC Alpha, SPARC V9, MIPS, PA-RISC and IA64的字节序是可配置的。 大端也好，小端也罢，就权当是个人爱好吧，只要你不影响别人就行，对不？ 网络字节序前面的大端和小端都是在说计算机自己，也被称作主机字节序。其实，只要自己能够自圆其说是没啥问题的。问题是，网络的出现使得计算机可以通信了。通信，就意味着相处，相处必须得有共同语言啊，得说普通话，要不然就容易会错意，下了一个小时的小电影发现打不开，理解错误了！ 但是每个计算机都有自己的主机字节序啊，还都不依不饶，坚持做自己，怎么办？ TCP/IP协议隆重出场，RFC1700规定使用“大端”字节序为网络字节序，其他不使用大端的计算机要注意了，发送数据的时候必须要将自己的主机字节序转换为网络字节序（即“大端”字节序），接收到的数据再转换为自己的主机字节序。这样就与CPU、操作系统无关了，实现了网络通信的标准化。突然觉得，TCP/IP协议好任性啊有木有！ 为了程序的兼容，你会看到，程序员们每次发送和接受数据都要进行转换，这样做的目的是保证代码在任何计算机上执行时都能达到预期的效果。 这么常用的操作，BSD Socket提供了封装好的转换接口，方便程序员使用。包括从主机字节序到网络字节序的转换函数：htons、htonl；从网络字节序到主机字节序的转换函数：ntohs、ntohl。当然，有了上面的理论基础，也可以编写自己的转换函数。 下面的一段代码可以用来判断计算机是大端的还是小端的，判断的思路是确定一个多字节的值（下面使用的是4字节的整数），将其写入内存（即赋值给一个变量），然后用指针取其首地址所对应的字节（即低地址的一个字节），判断该字节存放的是高位还是低位，高位说明是Big endian，低位说明是Little endian。 123456789101112#include &lt;stdio.h&gt;int main (){ unsigned int x = 0x12345678; char *c = (char*)&amp;x; if (*c == 0x78) { printf(&quot;Little endian&quot;); } else { printf(&quot;Big endian&quot;); } return 0;} 身边的字节序字符编码方式UTF-16、UTF-32同样面临字节序的问题，因为他们分别使用2个字节和4个字节编码Unicode字符，一旦某个值用多个字节表示，就必须要考虑存储的顺序了。于是，采用了最简单粗暴的方式，给文件头部写几个字符，用来表示是大端呢还是小端： 头部的字符 编码 字节序 FF FE UTF-16/UCS-2 Little endian FE FF UTF-16/UCS-2 Big endian FF FE 00 00 UTF-32/UCS-4 Little endian 00 00 FE FF UTF-32/UCS-4 Big-endian 这里不得不提一下UTF-8啊，明明人家是单个字节的，不存在什么字节序的问题。微软为了统一UTF-X，硬生生给他的头部也加了几个字符！是的，这几个字符就是BOM（Byte Order Mark），这就是Windows下的UTF-8。 相信很多人都被UTF-8的BOM给坑过，多了这个BOM的UTF-8文件，会导致很多问题啊。比如，写的Shell脚本，内容为#!/usr/bin/env bash，在UTF-8有BOM和UTF-8无BOM的编码下，对应的16进制为： 所以，有BOM的话，Shell解释器就报错啦。原因在于，解释器希望遇到#!/usr/bin/env bash，而使用UTF-8有BOM进行编码的内容会多了3个字节的EF BB BF。 对于UTF-8和UTF-8无BOM两种编码格式，我们更多的使用UTF-8无BOM。","link":"/byteorder/"},{"title":"JAVA 拾遗 — CPU Cache 与缓存行","text":"最近的两篇文章，介绍了我参加的中间件比赛中一些相对重要的优化，但实际上还存在很多细节优化，出于篇幅限制并未提及，在最近的博文中，我会将他们整理成独立的知识点，并归类到我的系列文章「JAVA 拾遗」中。 引言123456789101112131415161718192021222324public class Main { static long[][] arr; public static void main(String[] args) { arr = new long[1024 * 1024][8]; // 横向遍历 long marked = System.currentTimeMillis(); for (int i = 0; i &lt; 1024 * 1024; i += 1) { for (int j = 0; j &lt; 8; j++) { sum += arr[i][j]; } } System.out.println(&quot;Loop times:&quot; + (System.currentTimeMillis() - marked)+ &quot;ms&quot;); marked = System.currentTimeMillis(); // 纵向遍历 for (int i = 0; i &lt; 8; i += 1) { for (int j = 0; j &lt; 1024 * 1024; j++) { sum += arr[j][i]; } } System.out.println(&quot;Loop times:&quot; + (System.currentTimeMillis() - marked)+ &quot;ms&quot;); }} 如上述代码所示，定义了一个二维数组 long[][] arr 并且使用了横向遍历和纵向遍历两种顺序对这个二位数组进行遍历，遍历总次数相同，只不过循环的方向不同，代码中记录了这两种遍历方式的耗时，不妨先卖个关子，他们的耗时会有区别吗？ 这问题问的和中小学试卷中的：“它们之间有区别吗？如有，请说出区别。”一样没有水准，没区别的话文章到这儿就结束了。事实上，在我的机器上（64 位 mac）多次运行后可以发现：横向遍历的耗时大约为 25 ms，纵向遍历的耗时大约为 60 ms，前者比后者快了 1 倍有余。如果你了解上述现象出现的原因，大概能猜到，今天这篇文章的主角便是他了— CPU Cache&amp;Cache Line。 在学生生涯时，不断收到这样建议：《计算机网络》、《计算机组成原理》、《计算机操作系统》、《数据结构》四门课程是至关重要的，而在我这些年的工作经验中也不断地意识到前辈们如此建议的原因。作为一个 Java 程序员，你可以选择不去理解操作系统，组成原理（相比这二者，网络和数据结构跟日常工作联系得相对紧密），这不会降低你的 KPI，但了解他们可以使你写出更加计算机友好（Mechanical Sympathy）的代码。 下面的章节将会出现不少操作系统相关的术语，我将逐个介绍他们，并最终将他们与 Java 联系在一起。 什么是 CPU 高速缓存？CPU 是计算机的心脏，最终由它来执行所有运算和程序。主内存（RAM）是数据（包括代码行）存放的地方。这两者的定义大家应该不会陌生，那 CPU 高速缓存又是什么呢？ 在 计算机 系统中，**CPU 高速缓存 ** 是用于减少处理器访问内存所需平均时间的部件。在金字塔式 存储体系 中它位于自顶向下的第二层，仅次于CPU 寄存器。其容量远小于内存，但速度却可以接近处理器的频率。 当处理器发出内存访问请求时，会先查看缓存内是否有请求数据。如果存在（命中），则不经访问内存直接返回该数据；如果不存在（失效），则要先把内存中的相应数据载入缓存，再将其返回处理器。 缓存之所以有效，主要是因为程序运行时对内存的访问呈现局部性（Locality）特征。这种局部性既包括空间局部性（Spatial Locality），也包括时间局部性（Temporal Locality）。有效利用这种局部性，缓存可以达到极高的命中率。 在处理器看来，缓存是一个透明部件。因此，程序员通常无法直接干预对缓存的操作。但是，** 确实可以根据缓存的特点对程序代码实施特定优化，从而更好地利用缓存 **。 — 维基百科 左图为最简单的高速缓存的架构，数据的读取和存储都经过高速缓存，CPU 核心与高速缓存有一条特殊的快速通道；主存与高速缓存都连在系统总线上（BUS），这条总线还用于其他组件的通信。简而言之，CPU 高速缓存就是位于 CPU 操作和主内存之间的一层缓存。 为什么需要有 CPU 高速缓存？随着工艺的提升，最近几十年 CPU 的频率不断提升，而受制于制造工艺和成本限制，目前计算机的内存在访问速度上没有质的突破。因此，CPU 的处理速度和内存的访问速度差距越来越大，甚至可以达到上万倍。这种情况下传统的 CPU 直连内存的方式显然就会因为内存访问的等待，导致计算资源大量闲置，降低 CPU 整体吞吐量。同时又由于内存数据访问的热点集中性，在 CPU 和内存之间用较为快速而成本较高（相对于内存）的介质做一层缓存，就显得性价比极高了。 为什么需要有 CPU 多级缓存？结合 图片 – CPU 缓存架构，再来看一组 CPU 各级缓存存取速度的对比 各种寄存器，用来存储本地变量和函数参数，访问一次需要 1cycle，耗时小于 1ns； L1 Cache，一级缓存，本地 core 的缓存，分成 32K 的数据缓存 L1d 和 32k 指令缓存 L1i，访问 L1 需要 3cycles，耗时大约 1ns； L2 Cache，二级缓存，本地 core 的缓存，被设计为 L1 缓存与共享的 L3 缓存之间的缓冲，大小为 256K，访问 L2 需要 12cycles，耗时大约 3ns； L3 Cache，三级缓存，在同插槽的所有 core 共享 L3 缓存，分为多个 2M 的段，访问 L3 需要 38cycles，耗时大约 12ns； 大致可以得出结论，缓存层级越接近于 CPU core，容量越小，速度越快，同时，没有披露的一点是其造价也更贵。所以为了支撑更多的热点数据，同时追求最高的性价比，多级缓存架构应运而生。 什么是缓存行 (Cache Line)？上面我们介绍了 CPU 多级缓存的概念，而之后的章节我们将尝试忽略“多级”这个特性，将之合并为 CPU 缓存，这对于我们理解 CPU 缓存的工作原理并无大碍。 缓存行 (Cache Line) 便是 CPU Cache 中的最小单位，CPU Cache 由若干缓存行组成，一个缓存行的大小通常是 64 字节（这取决于 CPU），并且它有效地引用主内存中的一块地址。一个 Java 的 long 类型是 8 字节，因此在一个缓存行中可以存 8 个 long 类型的变量。 试想一下你正在遍历一个长度为 16 的 long 数组 data[16]，原始数据自然存在于主内存中，访问过程描述如下 访问 data[0]，CPU core 尝试访问 CPU Cache，未命中。 尝试访问主内存，操作系统一次访问的单位是一个 Cache Line 的大小 — 64 字节，这意味着：既从主内存中获取到了 data[0] 的值，同时将 data[0] ~ data[7] 加入到了 CPU Cache 之中，for free~ 访问 data[1]~data[7]，CPU core 尝试访问 CPU Cache，命中直接返回。 访问 data[8]，CPU core 尝试访问 CPU Cache，未命中。 尝试访问主内存。重复步骤 2 CPU 缓存在顺序访问连续内存数据时挥发出了最大的优势。试想一下上一篇文章中提到的 PageCache，其实发生在磁盘 IO 和内存之间的缓存，是不是有异曲同工之妙？只不过今天的主角— CPU Cache，相比 PageCache 更加的微观。 再回到文章的开头，为何横向遍历 arr = new long[1024 * 1024][8] 要比纵向遍历更快？此处得到了解答，正是更加友好地利用 CPU Cache 带来的优势，甚至有一个专门的词来修饰这种行为 — Mechanical Sympathy。 伪共享通常提到缓存行，大多数文章都会提到伪共享问题（正如提到 CAS 便会提到 ABA 问题一般）。 伪共享指的是多个线程同时读写同一个缓存行的不同变量时导致的 CPU 缓存失效。尽管这些变量之间没有任何关系，但由于在主内存中邻近，存在于同一个缓存行之中，它们的相互覆盖会导致频繁的缓存未命中，引发性能下降。伪共享问题难以被定位，如果系统设计者不理解 CPU 缓存架构，甚至永远无法发现 — 原来我的程序还可以更快。 伪共享 正如图中所述，如果多个线程的变量共享了同一个 CacheLine，任意一方的修改操作都会使得整个 CacheLine 失效（因为 CacheLine 是 CPU 缓存的最小单位），也就意味着，频繁的多线程操作，CPU 缓存将会彻底失效，降级为 CPU core 和主内存的直接交互。 伪共享问题的解决方法便是字节填充。 伪共享 - 字节填充 我们只需要保证不同线程的变量存在于不同的 CacheLine 即可，使用多余的字节来填充可以做点这一点，这样就不会出现伪共享问题。在代码层面如何实现图中的字节填充呢？ Java6 中实现字节填充1234public class PaddingObject{ public volatile long value = 0L; // 实际数据 public long p1, p2, p3, p4, p5, p6; // 填充} PaddingObject 类中需要保存一个 long 类型的 value 值，如果多线程操作同一个 CacheLine 中的 PaddingObject 对象，便无法完全发挥出 CPU Cache 的优势（想象一下你定义了一个 PaddingObject[] 数组，数组元素在内存中连续，却由于伪共享导致无法使用 CPU Cache 带来的沮丧）。 不知道你注意到没有，实际数据 value + 用于填充的 p1~p6 总共只占据了 7 * 8 = 56 个字节，而 Cache Line 的大小应当是 64 字节，这是有意而为之，在 Java 中，** 对象头还占据了 8 个字节 **，所以一个 PaddingObject 对象可以恰好占据一个 Cache Line。 Java7 中实现字节填充在 Java7 之后，一个 JVM 的优化给字节填充造成了一些影响，上面的代码片段 public long p1, p2, p3, p4, p5, p6; 会被认为是无效代码被优化掉，有回归到了伪共享的窘境之中。 为了避免 JVM 的自动优化，需要使用继承的方式来填充。 1234567abstract class AbstractPaddingObject{ protected long p1, p2, p3, p4, p5, p6;// 填充}public class PaddingObject extends AbstractPaddingObject{ public volatile long value = 0L; // 实际数据} Tips: 实际上我在本地 mac 下测试过 jdk1.8 下的字节填充，并不会出现无效代码的优化，个人猜测和 jdk 版本有关，不过为了保险起见，还是使用相对稳妥的方式去填充较为合适。 如果你对这个现象感兴趣，测试代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public final class FalseSharing implements Runnable { public final static int NUM_THREADS = 4; // change public final static long ITERATIONS = 500L * 1000L * 1000L; private final int arrayIndex; private static VolatileLong[] longs = new VolatileLong[NUM_THREADS]; static { for (int i = 0; i &lt; longs.length; i++) { longs[i] = new VolatileLong(); } } public FalseSharing(final int arrayIndex) { this.arrayIndex = arrayIndex; } public static void main(final String[] args) throws Exception { final long start = System.currentTimeMillis(); runTest(); System.out.println(&quot;duration =&quot; + (System.currentTimeMillis() - start)); } private static void runTest() throws InterruptedException { Thread[] threads = new Thread[NUM_THREADS]; for (int i = 0; i &lt; threads.length; i++) { threads[i] = new Thread(new FalseSharing(i)); } for (Thread t : threads) { t.start(); } for (Thread t : threads) { t.join(); } } public void run() { long i = ITERATIONS + 1; while (0 != --i) { longs[arrayIndex].value = i; } } public final static class VolatileLong { public volatile long value = 0L; public long p1, p2, p3, p4, p5, p6; // 填充，可以注释后对比测试 }} Java8 中实现字节填充12345@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.FIELD, ElementType.TYPE})public @interface Contended { String value() default &quot;&quot;;} ** 注意需要同时开启 JVM 参数：-XX:-RestrictContended=false** @Contended 注解会增加目标实例大小，要谨慎使用。默认情况下，除了 JDK 内部的类，JVM 会忽略该注解。要应用代码支持的话，要设置 -XX:-RestrictContended=false，它默认为 true（意味仅限 JDK 内部的类使用）。当然，也有个 –XX: EnableContented 的配置参数，来控制开启和关闭该注解的功能，默认是 true，如果改为 false，可以减少 Thread 和 ConcurrentHashMap 类的大小。参加《Java 性能权威指南》210 页。 — @Im 的补充 Java8 中终于提供了字节填充的官方实现，这无疑使得 CPU Cache 更加可控了，无需担心 jdk 的无效字段优化，无需担心 Cache Line 在不同 CPU 下的大小究竟是不是 64 字节。使用 @Contended 注解可以完美的避免伪共享问题。 一些最佳实践可能有读者会问：作为一个普通开发者，需要关心 CPU Cache 和 Cache Line 这些知识点吗？这就跟前几天比较火的话题：「程序员有必要懂 JVM 吗？」一样，仁者见仁了。但确实有不少优秀的源码在关注着这些问题。他们包括： ConcurrentHashMap 面试中问到要吐的 ConcurrentHashMap 中，使用 @sun.misc.Contended 对静态内部类 CounterCell 进行修饰。另外还包括并发容器 Exchanger 也有相同的操作。 12345678910/* ---------------- Counter support -------------- *//** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */@sun.misc.Contended static final class CounterCell { volatile long value; CounterCell(long x) {value = x;}} Thread Thread 线程类的源码中，使用 @sun.misc.Contended 对成员变量进行修饰。 1234567891011121314151617// The following three initially uninitialized fields are exclusively// managed by class java.util.concurrent.ThreadLocalRandom. These// fields are used to build the high-performance PRNGs in the// concurrent code, and we can not risk accidental false sharing.// Hence, the fields are isolated with @Contended./** The current seed for a ThreadLocalRandom */@sun.misc.Contended(&quot;tlr&quot;)long threadLocalRandomSeed;/** Probe hash value; nonzero if threadLocalRandomSeed initialized */@sun.misc.Contended(&quot;tlr&quot;)int threadLocalRandomProbe;/** Secondary seed isolated from public ThreadLocalRandom sequence */@sun.misc.Contended(&quot;tlr&quot;)int threadLocalRandomSecondarySeed; RingBuffer 来源于一款优秀的开源框架 Disruptor 中的一个数据结构 RingBuffer ， 我后续会专门花一篇文章的篇幅来介绍这个数据结构 123456abstract class RingBufferPad{ protected long p1, p2, p3, p4, p5, p6, p7;}abstract class RingBufferFields&lt;E&gt; extends RingBufferPad{} 使用字节填充和继承的方式来避免伪共享。 面试题扩展问：说说数组和链表这两种数据结构有什么区别？ 了解了 CPU Cache 和 Cache Line 之后想想可不可以有一些特殊的回答技巧呢？ 参考资料高性能队列——Disruptor 神奇的缓存行填充 伪共享和缓存行填充 关于 CPU Cache – 程序猿需要知道的那些事 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/cache-line/"},{"title":"Spring Cloud 终于改了，为什么要用日期来做版本号？","text":"Spring Cloud 终于改了最近 Spring Cloud 把版本号从 A 到 Z 的伦敦地铁站，改成用日期命名了。 https://spring.io/blog/2020/04/17/spring-cloud-2020-0-0-m1-released 也就是从 Greenwich.SR6, Hoxton.SR9 这样的风格改成了 2020.0.0 的形式。广大人民终于不用为 Spring Cloud 的版本号烦恼了。 Spring Cloud 推广不力，固然有自身复杂的原因，版本号太复杂也是一个坑。 以日期为版本号，即所谓的 Calendar Versioning，可以参考这个网站： https://calver.org/overview_zhcn.html 何时使用 CalVer如果你和很多素不相识的人协同开发某个项目，那么使用一个严谨的版本命名方式是一个合适的选择，恰巧 CalVer 就是选择之一。 该项目是否具有较大或不断变化的范围？ 大型系统和框架，如 Ubuntu 和 Twisted。 没有实际边界的实用工具集合，如 Boltons。 该项目是否对时间敏感？是否有其他的外部变化驱动项目新版本的发布？ 业务需求，例如 Ubuntu 的支持计划。 安全更新，例如 certifi 对证书更新的需求。 政治变化，例如 pytz 对时区变化的处理。 如果你对这些问题中的任何一个回答是肯定的，CalVer 都可以成为你项目的有力选择。 但上面这些理由我觉得都不够充分。 在我看来最重要的理由是：以日期为版本号，让依赖库的开发方和下游依赖方达成了默契。 阿里巴巴的实践Pandora 是阿里巴巴内部的隔离容器。在 14 年时，Pandora 包版本号是这样子的： 2_1_0_3 , 2_1_0_4_10-LOG 后面改为 Pandora 版本 + 日期 2_2_140825, 2_2_140905 但实际上应用方并不关心 Pandora 的版本，所以改成了现在的风格： 2020-04-release-fix , 2020-10-release 好处是： 按时间节点推动升级 电商的业务都是时间为关键节点的，比如 618/双 11。中间件和应用方达成了一个默契：到关键时间点，业务方使用中间件推出的稳定版本，如果出了事故那么就是中间件的锅。不升级，则是业务方自己的锅。 推动升级的阻力变小 当业务方遇到问题时，一看版本号是 1 年多前的，很自然就会想到升级。 依赖提供方要按时间保持更新 维护人员本身要不断发版本证明自己的生命力。下游用户也可以根据时间选择是否要切换到其它的新技术路线上去了。 对于一些总体的依赖，比如公司内部的 maven bom，都建议使用时间做日期。 比如 Spring 2.5.6 版本，大部分开发都知道它是比较旧的依赖，但不会有太大的动力去管。 但是如果你说，这是 12 年前的代码（绝大部分开发还没毕业），那么开发人员就知道很容易会出现不兼容的问题，他自己就知道应该要升级了。 以时间为版本号，既是对用户的承诺，也是对开发者自己的鞭策。","link":"/calendar-versioning/"},{"title":"中文文案排版指北","text":"统一中文文案、排版的相关用法，降低团队成员之间的沟通成本，增强网站气质。原文出处：https://github.com/mzlogin/chinese-copywriting-guidelines 目录 空格 中英文之间需要增加空格 中文与数字之间需要增加空格 数字与单位之间无需增加空格 全角标点与其他字符之间不加空格 -ms-text-autospace to the rescue? 标点符号 不重复使用标点符号 全角和半角 使用全角中文标点 数字使用半角字符 遇到完整的英文整句、特殊名词，其內容使用半角标点 名词 专有名词使用正确的大小写 不要使用不地道的缩写 争议 链接之间增加空格 简体中文使用直角引号 工具 谁在这样做？ 参考文献 空格 「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。 与大家共勉之。」——vinta/paranoid-auto-spacing 中英文之间需要增加空格 正确： 在 LeanCloud 上，数据存储是围绕 AVObject 进行的。 错误： 在LeanCloud上，数据存储是围绕AVObject进行的。 在 LeanCloud上，数据存储是围绕AVObject 进行的。 完整的正确用法： 在 LeanCloud 上，数据存储是围绕 AVObject 进行的。每个 AVObject 都包含了与 JSON 兼容的 key-value 对应的数据。数据是 schema-free 的，你不需要在每个 AVObject 上提前指定存在哪些键，只要直接设定对应的 key-value 即可。 例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。 中文与数字之间需要增加空格 正确： 今天出去买菜花了 5000 元。 错误： 今天出去买菜花了 5000元。 今天出去买菜花了5000元。 数字与单位之间无需增加空格 正确： 我家的光纤入户宽带有 10Gbps，SSD 一共有 10TB。 错误： 我家的光纤入户宽带有 10 Gbps，SSD 一共有 20 TB。 另外，度／百分比与数字之间不需要增加空格： 正确： 今天是 233° 的高温。 新 MacBook Pro 有 15% 的 CPU 性能提升。 错误： 今天是 233 ° 的高温。 新 MacBook Pro 有 15 % 的 CPU 性能提升。 全角标点与其他字符之间不加空格 正确： 刚刚买了一部 iPhone，好开心！ 错误： 刚刚买了一部 iPhone ，好开心！ -ms-text-autospace to the rescue? Microsoft 有个 -ms-text-autospace 的 CSS 属性可以实现自动为中英文之间增加空白。不过目前并未普及，另外在其他应用场景，例如 OS X、iOS 的用户界面目前并不存在这个特性，所以请继续保持随手加空格的习惯。 标点符号不重复使用标点符号 正确： 德国队竟然战胜了巴西队！ 她竟然对你说「喵」？！ 错误： 德国队竟然战胜了巴西队！！ 德国队竟然战胜了巴西队！！！！！！！！ 她竟然对你说「喵」？？！！ 她竟然对你说「喵」？！？！？？！！ 全角和半角 不明白什么是全角（全形）与半角（半形）符号？请查看维基百科词条『全角和半角』。 使用全角中文标点 正确： 嗨！你知道嘛？今天前台的小妹跟我说「喵」了哎！ 核磁共振成像（NMRI）是什么原理都不知道？JFGI！ 错误： 嗨! 你知道嘛? 今天前台的小妹跟我说 “喵” 了哎! 嗨!你知道嘛?今天前台的小妹跟我说”喵”了哎! 核磁共振成像 (NMRI) 是什么原理都不知道? JFGI! 核磁共振成像(NMRI)是什么原理都不知道?JFGI! 数字使用半角字符 正确： 这件蛋糕只卖 1000 元。 错误： 这件蛋糕只卖 １０００ 元。 例外：在设计稿、宣传海报中如出现极少量数字的情形时，为方便文字对齐，是可以使用全角数字的。 遇到完整的英文整句、特殊名词，其內容使用半角标点 正确： 乔布斯那句话是怎么说的？「Stay hungry, stay foolish.」 推荐你阅读《Hackers &amp; Painters: Big Ideas from the Computer Age》，非常的有趣。 错误： 乔布斯那句话是怎么说的？「Stay hungry，stay foolish。」 推荐你阅读《Hackers＆Painters：Big Ideas from the Computer Age》，非常的有趣。 名词专有名词使用正确的大小写 大小写相关用法原属于英文书写范畴，不属于本 wiki 讨论內容，在这里只对部分易错用法进行简述。 正确： 使用 GitHub 登录 我们的客户有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。 错误： 使用 github 登录 使用 GITHUB 登录 使用 Github 登录 使用 gitHub 登录 使用 gｲんĤЦ8 登录 我们的客户有 github、foursquare、microsoft corporation、google、facebook, inc.。 我们的客户有 GITHUB、FOURSQUARE、MICROSOFT CORPORATION、GOOGLE、FACEBOOK, INC.。 我们的客户有 Github、FourSquare、MicroSoft Corporation、Google、FaceBook, Inc.。 我们的客户有 gitHub、fourSquare、microSoft Corporation、google、faceBook, Inc.。 我们的客户有 gｲんĤЦ8、ｷouЯƧquﾑгє、๓เςг๏ร๏Ŧt ς๏гק๏гคtเ๏ภn、900913、ƒ4ᄃëв๏๏к, IПᄃ.。 注意：当网页中需要配合整体视觉风格而出现全部大写／小写的情形，HTML 中请使用标准的大小写规范进行书写；并通过 text-transform: uppercase;／text-transform: lowercase; 对表现形式进行定义。 不要使用不地道的缩写 正确： 我们需要一位熟悉 JavaScript、HTML5，至少理解一种框架（如 Backbone.js、AngularJS、React 等）的前端开发者。 错误： 我们需要一位熟悉 Js、h5，至少理解一种框架（如 backbone、angular、RJS 等）的 FED。 争议 以下用法略带有个人色彩，即：无论是否遵循下述规则，从语法的角度来讲都是正确的。 链接之间增加空格 用法： 请 提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请 点击这里 进行订阅！ 对比用法： 请提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请点击这里进行订阅！ 简体中文使用直角引号 用法： 「老师，『有条不紊』的『紊』是什么意思？」 对比用法： “老师，‘有条不紊’的‘紊’是什么意思？” 工具 仓库 语言 vinta/paranoid-auto-spacing JavaScript huei90/pangu.node Node.js huacnlee/auto-correct Ruby sparanoid/space-lover PHP (WordPress) nauxliu/auto-correct PHP ricoa/copywriting-correct PHP hotoo/pangu.vim Vim sparanoid/grunt-auto-spacing Node.js (Grunt) hjiang/scripts/add-space-between-latin-and-cjk Python 谁在这样做？ 网站 文案 UGC Apple 中国 Yes N/A Apple 香港 Yes N/A Apple 台湾 Yes N/A Microsoft 中国 Yes N/A Microsoft 香港 Yes N/A Microsoft 台湾 Yes N/A LeanCloud Yes N/A 知乎 Yes 部分用户达成 V2EX Yes Yes SegmentFault Yes 部分用户达成 Apple4us Yes N/A 豌豆荚 Yes N/A Ruby China Yes 标题达成 PHPHub Yes 标题达成 少数派 Yes N/A 力扣 LeetCode Yes Yes 参考文献 Guidelines for Using Capital Letters Letter case - Wikipedia Punctuation - Oxford Dictionaries Punctuation - The Purdue OWL How to Use English Punctuation Corrently - wikiHow 格式 - openSUSE 全角和半角 - 维基百科 引号 - 维基百科 疑问惊叹号 - 维基百科","link":"/chinese-copywriting-guidelines/"},{"title":"IDEA 插件推荐：Cloud Toolkit 测评","text":"产品介绍Cloud Toolkit 是一款 IDE 插件，帮助开发者更高效地开发、测试、诊断并部署应用。开发者能够方便地将本地应用一键部署到任意机器，或 ECS、EDAS、Kubernetes；并内置 Arthas 诊断、高效执行终端命令和 SQL 等。 对这款产品最直观的感受：这是一款发布工具，帮助用户在 IDE 中直接打包应用并部署到各种终端。原本看到其产品介绍位于阿里云的页面中，以为是一款和阿里云服务强绑定的产品，但试用过后发现，即使对于普通的云主机，其也非常适用，可以解决很多开发运维的痛点，非阿里云用户可以放心使用。 在 Cloud Toolkit 出现之前作为一个 Java 程序员，我们现在大多数都会在 Intellij IDEA 中基于 SpringBoot 来开发 WEB 应用，所以本文中的测评将会基于如下架构 开发环境：IDEA 项目组织方式：Maven 开发框架：SpringBoot 来构建。在接触 Cloud Toolkit 之前，可以怎么部署一个 SpringBoot 应用呢？作为一个偏正经的测评人员，我不会为了凸显出 Cloud Toolkit 的强大而去翻出一些上古的部署工具来做对比，而是直接使用 Intellij IDEA 的内置功能与之对比。 第一步：配置服务器信息在 Tools -&gt; Deployment 中可以找到 IDEA 对项目部署支持的内置插件 我们可以在其中进行服务器信息的配置，包括服务器地址和权限认证，并且在 Mapping 选项卡中完成本地工程与服务器路径的映射。 第二步：配置 Maven 打包插件12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 由于是 SpringBoot 应用，配置专用的打包插件后，可以将整个工程打成一个 fatjar，示例工程非常简单： 123456789101112@SpringBootApplication@RestControllerpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } @RequestMapping(&quot;/hello&quot;) public String hello() { return &quot;hello world~~~~~~~~~~~~~~~~&quot;; }} 之后，只要执行 install，即可得到一个可运行的 jar 包： 第三步：部署 jar 包 由于我们在第一步已经配置过项目路径与服务器路径的映射，可以选择直接对 fatjar 右键，upload 到远程服务器上。 第四步：启动应用 上图中展示的是 IDEA 中两个非常棒的内置功能，可以在 Tools -&gt; Start SSH session 中开启远程服务器的终端，在 IDEA 下方可以执行远程指令；也可以在 Tools -&gt; Deployment -&gt;Browse Remote Host 中展开如图右侧的结构，可视化地浏览服务器上的文件列表，检查应用是否部署成功。 在远程终端中，找到对应的 fatjar，执行 java -jar spring-demo-1.0-SNAPSHOT.jar 便完成了整个部署流程。 IDEA 内置插件总结IDEA 内置插件已经提供了相当强大的能力，整个部署过程我们完全没有离开 IDEA！避免了频繁切换窗口，装各种部署工具，可以说已经很方便了，Cloud Toolkit 必须要比这个部署过程做的更加强大才行，那下面就让我们来体验下 Cloud Toolkit 是怎么优化的吧。 Cloud Toolkit 初体验我们不急着用 Cloud Toolkit 来部署应用。虽然笔者是一位开发，但还是从产品的角度来研究下它的菜单项，看看它的产品定位。IDEA 安装插件的过程省略，详情可以参考 《Intellij IDEA 安装 Cloud Toolkit 教程》。 其他菜单项暂且抛到一边，这 5 个核心能力应该就是 Cloud Toolkit 的核心了。 即使作为一个插件小白，应该也能够望名知意，猜到这几个菜单对应的功能： Deploy to Host：部署到任意服务器。这一个功能决定了 Cloud Toolkit 强大的功能可以使得每个开发者受益，它其实并不是和阿里云厂商强绑定的。在下文也会重点测评下这个功能。 Deploy to ECS：这里的 ECS 指的阿里云的 ECS，如果你的服务部署在阿里云 ECS 上，可以选择使用这个功能，获得比 Deploy to Host 更加丰富的功能。在下文我也会简单测评下这个功能。 Deploy to EDAS，Deploy to EDAS Serverless：EDAS &amp; EDAS Serverless 是阿里云上提供的分布式服务治理服务，可以理解为商业版的 Dubbo，具有强大的服务治理、服务调度能力，Cloud Toolkit 对 EDAS 做了个性化的部署支持，使得使用者无需登录控制台，在 IDEA 中即可完成 EDAS 的部署。 Deploy to CS K8S：云原生时代很多应用使用容器化的方式进行部署，Cloud Toolkit 这一点做的还是不错的，已经具备了容器化部署的能力，具有一定的前瞻性。 其实从简单的功能介绍就可以看出，Cloud Toolkit 相比 IDEA 内置的部署能力的确是高出一大截了，甚至可以说，Deploy to Host 这一能力完全就可以覆盖 IDEA 插件的所有能力，并且对流程还进行了一些简化。下面我重点测评下 Deploy to Host 这一能力，与之前的部署流程进行一个对比。 使用 Cloud Toolkit 部署应用到任意服务器 上图展示的 Deploy to Host 功能的配置项，实际上涵盖了 远程服务器配置 部署方式：Maven 构建，直接上传文件（目前还不支持 Gradle 构建，可能在后续的版本会支持） 本地文件与服务器路径的映射配置 启动脚本的集成 账号管理SSH 登录账户可以在 Preferences -&gt; Alibaba Cloud Toolkit -&gt; SSH Profile 中管理，找不到也没关系，需要设置的时候一般都会有超链接跳转，这点做得很人性化。 主机管理服务信息可以在 Tools -&gt; Alibaba Cloud -&gt;Alibaba Cloud View 中展开，如下图所示 Deploy to Host配置完账号信息和主机信息，然后只需要右键项目选择 Alibaba Cloud -&gt; Deploy to Host-&gt; Run ，一切就搞定了。这个过程相比之前变得非常简易 不需要自己打包。Cloud Toolkit 集成了 Maven 插件。 不需要登录远程终端去执行脚本启动服务。Cloud Toolkit 提供了应用部署生命周期必要的钩子，只需要设置好启动脚本即可。 修改完本地代码，点击下 Deploy to Host，即可完成改动代码的部署。 经过如上的测评过程，相信即使没有使用过 Cloud Toolkit 的用户，也可以直观体会到这是怎么样一款插件了，并且它的功能是多么的实用。 使用 Cloud Toolkit 部署应用到 ECS从产品设计的角度来分析，Cloud Toolkit 提供如此众多的部署能力，可以想到是其直接预设了使用人群。例如一个阿里云的 ECS 用户，在选择部署方式时，既可以使用 Deploy to Host 也可以使用 Deploy to ECS；例如一个 EDAS 用户，在选择部署方式时，既可以使用 Deploy to Host、Deploy to ECS，也可以使用 Deploy to EDAS（EDAS 可以理解为一个定制化的 ECS）。从产品的角度，越定制化的功能服务的人群越少，同时功能更强大；从用户体验的角度，其实也透露了云服务的一个特点，云厂商正在为其所提供的云服务提供更好的用户体验，借助于此类插件，可以降低使用者的开发运维门槛。 可以预见的一件事是，对于非阿里云用户来说，Deploy to Host 是使用 Cloud Toolkit 最大的诱惑了。作为一个测评文章，除了 Deploy to Host 之外，我还选择了 Deploy to ECS 这一功能来进行测评。为此我购买了一台阿里云的 ECS 来部署与上文相同的应用。 在阿里云控制台可以获取到账号的 Access Key/Access Key Secret，在 IDEA 中的 Preferences -&gt; Alibaba Cloud Toolkit -&gt; Accounts 中可以设置账号。 在账号设置完毕后，Cloud Toolkit 看起来是通过内置的 API 直接关联到了我的 ECS 实例，在选择部署时，可以直接根据 region 选择实例列表中的机器进行部署。 其余的部署流程和 Deploy to Host 相差无几。也就是说，Deploy to ECS 更多的其实完成了权限管理和主机管理，ECS 用户使用这个功能就显得非常高效了。 Cloud Toolkit 的亮点功能Cloud Toolkit 除了主打的部署能力，还提供了不少亮点功能，我选择了其中的 3 个功能：上传文件，远程 Terminal，内置应用诊断功能来进行评测。 上传文件 有些脚本我们希望在本地编辑之后上传到服务器上，Cloud Toolkit 对每一个主机都提供了一个 Upload 操作，可以将本地的文件上传到远程主机上，并且还可以触发一个 commond，这个功能也是很人性化的，因为上传脚本后，往往需要运行一次，避免了我们再登录到远程主机上执行一次运行操作。 远程 Terminal特别是在 Mac 中，我一直苦恼的一件事便是如何管理众多的远程机器，我需要偶尔去搭建了博客的主机上查看下个人博客为什么挂了，偶尔又要去看看我的 VPN 主机排查下为什么无法转发流量了，在开发测试阶段，又要经常去测试主机上简单的执行一些命令。所有这一切通过 ssh 工具去完成都不麻烦，但所有的麻烦事集合到一起时往往会让我变得焦头烂额，这一点，Cloud Toolkit 简直是一个 Life Saver。 事实上，在前面的测评中我们已经了解到 IDEA 内置了远程 Terminal 这个功能，Cloud Toolkit 是进一步优化了它的体验，用户可以直接在可视化的页面选择想要远程登录的主机，在对主机加了 Tag 之后，这个过程会更加直观。 内置应用诊断功能在测评体验过程中，意外地发现了 Cloud Toolkit 的一个功能支持，就是前面的截图有显示，但我未提到的 Diagnostic （诊断）功能。Cloud Toolkit 集成了阿里巴巴开源的一款应用诊断框架 – Arthas。 对于本地主机，可以直接通过 Tools -&gt; Alibaba Cloud -&gt; Diagnostic Tools 开启诊断。 对于远程主机，可以通过主机管理中的 Diagnostic 选项卡，开启远程诊断。 在过去，我们想要进行诊断，必须要手动在服务器上安装 Arthas，Cloud Toolkit 借助于 Remote Terminal 和 Arthas 的集成，让这一切都可以在 IDEA 中完成，似乎是想要贯彻：彻底杜绝第三方工具，一切都用插件完成。 当你遇到以下类似问题而束手无策时，Arthas 可以帮助你解决： 这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？ 我改的代码为什么没有执行到？难道是我没 commit？分支搞错了？ 遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？ 线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！ 是否有一个全局视角来查看系统的运行状况？ 有什么办法可以监控到 JVM 的实时运行状态？ 作为一个偏正经的评测，我们试用一下远程诊断的功能，选取比较直观的 trace 命令来进行评测 如上图所示，我们构造了一个慢请求，其中 invokeServiceA_B() 相对于其他方法十分耗时，我们希望通过 Cloud Toolkit 定位到慢调用的源头，找出 invokeServiceA_B 这个罪魁祸首。 点击 IDEA 中对应部署服务器的 Diagnostic 菜单项，就会出现如上图所示的一个 Arthas 诊断页面，它会自动关联到用户的 Java 进程，用户只需要选择相应诊断的进程即可。 在关联到相应的进程之后，我们执行 trace 指令 trace moe.cnkirito.demo.Application * -j 这个指令的含义是当 moe.cnkirito.demo.Application 中的任意方法被触发调用后，会打印出相应的调用栈，并计算耗时，-j 的含义是过滤掉 JDK 内置的类，简化堆栈。正如上图所示，我们定位到是 invokeServiceA 的 invokeServiceA_B 最为耗时。用户可以自行监控对应的方法，把 * 替换为想要监控的方式即可。更多的监控指令可以参考：Arthas 文档 测评中发现的不足是软件就必然有 bug，或者是用户体验不好的地方，花费了一个下午进行测评，简单罗列下我认为的缺陷。 远程连接容易出现异常这个问题不是特别容易复现，表现是长时间运行项目后，再部署，会提示远程连接失败，在重启 IDEA 之后可以解决这个问题，原因未知。在后面想要复现时一直无法复现，但的确耗费了我很长的时间，不知道有没有其他的用户遇到同样的问题。 文件浏览器过于简陋 当尝试配置 SSH 公私钥以实现免密登录时，发现 Browse 打开的文件浏览器无法正常显示 Mac 中的 .ssh 隐藏文件夹，大多数情况下用户会将 SSH 公私钥存放在 ~/.ssh 中，这个用户体验不是很好，或许有办法在这个文件浏览器中访问到隐藏文件夹，但至少我还没找到方法。 缺少远程主机的可视化功能IDEA 的默认插件支持 Remote Host 这个可以提升用户体验，Cloud Toolkit 提供了远程主机的管理，额外实现一个 ftp 协议可能会更方便用户查看自己的部署结果。从连接协议的选择上也可以发现，Cloud Toolkit 目前只支持 sftp 协议，而 IDEA 内置的 Deployment 插件还支持 ftp、ftps 等方式。 产品定位 &amp; 评价 &amp; 竞品其实本文基本是围绕 IDEA 的内置 Deployment 顺带着 Cloud Toolkit 的测评一起进行的。实际上我并不觉得 Cloud Toolkit 存在什么竞品 xftp 或者 xshell 吗？它们只是一款 ssh 工具罢了，人家压根没想着跟你竞争。 jenkins 吗？jenkins 有自己的 devops 流程，侧重在持续集成，而 Cloud Toolkit 定位是在日常开发中完成部署验证等行为。 在我的测评过程中，能够感受到这款产品的匠心，几乎为所有用户可能遇到的问题都做配备了文档：不知道启动脚本怎么写？链接了常用的 Java 应用启动脚本；不清楚该使用哪种部署方式？每种方式都有完整的部署文档；多语言？同时提供了 Go、NodeJS 的部署案例… 同时还支持了一些赠品功能：查看实时日志，文件上传，SQL 执行等。 以个人愚见，聊聊这款产品的定位，一方面是云厂商无关的特性，Cloud Toolkit 提供了 Deploy to Host、内置 Arthas 诊断等功能，造福了广大的开发者，另一方面是阿里云服务绑定的一些功能，Cloud Toolkit 为 ECS、EDAS 用户带来了福音，可以享受比普通应用部署更加便捷的操作。前者为 Cloud Toolkit 积累了业界口碑，后者为阿里云付费用户增加了信心，同时也为潜在的阿里云用户埋下了种子。","link":"/cloud-toolkit-benchmark/"},{"title":"八个层面比较 Java 8, RxJava, Reactor","text":"前言这是一篇译文，原文出处 戳这里。其实很久以前我就看完了这篇文章，只不过个人对响应式编程研究的不够深入，羞于下笔翻译，在加上这类译文加了原创还有争议性，所以一直没有动力。恰逢今天交流群里两个大佬对响应式编程的话题辩得不可开交，趁印象还算深刻，借机把这篇文章翻译一下。说道辩论的点，不妨也在这里抛出来： 响应式编程在单机环境下是否鸡肋？ 结论是：没有结论，我觉得只能抱着怀疑的眼光审视这个问题了。另外还聊到了 RSocket 这个最近在 SpringOne 大会上比较火爆的响应式 “ 新“网络协议，github 地址 戳这里，为什么给”新“字打了个引号，仔细观察下 RSocket 的 commit log，其实三年前就有了。有兴趣的同学自行翻阅，说不定就是今年这最后两三个月的热点技术哦。 Java 圈子有一个怪事，那就是对 RxJava，Reactor，WebFlux 这些响应式编程的名词、框架永远处于渴望了解，感到新鲜，却又不甚了解，使用贫乏的状态。之前转载小马哥的那篇《Reactive Programming 一种技术，各自表述》时，就已经聊过这个关于名词之争的话题了，今天群里的讨论更是加深了我的映像。Java 圈子里面很多朋友一直对响应式编程处于一个了解名词，知道基本原理，而不是深度用户的状态 (我也是之一)。可能真的和圈子有关，按石冲兄的说法，其实 Scala 圈子里面的那帮人，不知道比咱们高到哪里去了（就响应式编程而言）。 实在是好久没发文章了，向大家说声抱歉，以后的更新频率肯定是没有以前那么勤了（说的好像以前很勤快似的），一部分原因是在公司内网写的文章没法贴到公众号中和大家分享讨论，另一部分是目前我也处于学习公司内部框架的阶段，不太方便提炼成文章，最后，最大的一部分原因还是我这段时间需要学 (tou) 习(lan)其 (da) 他(you)东 (xi) 西啦。好了，废话也说完了，下面是译文的正文部分。 引言关于响应式编程 (Reactive Programming)，你可能有过这样的疑问：我们已经有了 Java8 的 Stream, CompletableFuture, 以及 Optional，为什么还必要存在 RxJava 和 Reactor？ 回答这个问题并不难，如果在响应式编程中处理的问题非常简单，你的确不需要那些第三方类库的支持。 但随着复杂问题的出现，你写出了一堆难看的代码。然后这些代码变得越来越复杂，难以维护，而 RxJava 和 Reactor 具有许多方便的功能，可以解决你当下问题，并保障了未来一些可预见的需求。本文从响应式编程模型中抽象出了 8 个标准，这将有助于我们理解标准特性与这些库之间的区别： Composable（可组合） Lazy（惰性执行） Reusable（可复用） Asynchronous（异步） Cacheable（可缓存） Push or Pull（推拉模型） Backpressure（回压）(译者注：按照石冲老哥的建议，这个词应当翻译成 “回压” 而不是 “背压”) Operator fusion（操作融合） 我们将会对以下这些类进行这些特性的对比： CompletableFuture（Java 8） Stream（Java 8） Optional（Java 8） Observable (RxJava 1) Observable (RxJava 2) Flowable (RxJava 2) Flux (Reactor Core) 让我们开始吧 ~ 1. Composable（可组合）这些类都是支持 Composable 特性的，使得各位使用者很便利地使用函数式编程的思想去思考问题，这也正是我们拥趸它们的原因。 CompletableFuture - 众多的 .then*() 方法使得我们可以构建一个 pipeline, 用以传递空值，单一的值，以及异常. Stream - 提供了许多链式操作的编程接口，支持在各个操作之间传递多个值。 Optional - 提供了一些中间操作 .map(), .flatMap(), .filter(). Observable, Flowable, Flux - 和 Stream 相同 2. Lazy（惰性执行）CompletableFuture - 不具备惰性执行的特性，它本质上只是一个异步结果的容器。这些对象的创建是用来表示对应的工作，CompletableFuture 创建时，对应的工作已经开始执行了。但它并不知道任何工作细节，只关心结果。所以，没有办法从上至下执行整个 pipeline。当结果被设置给 CompletableFuture 时，下一个阶段才开始执行。 Stream - 所有的中间操作都是延迟执行的。所有的终止操作 (terminal operations)，会触发真正的计算 (译者注：如 collect() 就是一个终止操作 )。 Optional - 不具备惰性执行的特性，所有的操作会立刻执行。 Observable, Flowable, Flux - 惰性执行，只有当订阅者出现时才会执行，否则不执行。 3. Reusable（可复用）CompletableFuture - 可以复用，它仅仅是一个实际值的包装类。但需要注意的是，这个包装是可更改的。.obtrude*() 方法会修改它的内容，如果你确定没有人会调用到这类方法，那么重用它还是安全的。 Stream - 不能复用。Java Doc 注释道： A stream should be operated on (invoking an intermediate or terminal stream operation) only once. A stream implementation may throw IllegalStateException if it detects that the stream is being reused. However, since some stream operations may return their receiver rather than a new stream object, it may not be possible to detect reuse in all cases. （译者注：Stream 只能被调用一次。如果被校测到流被重复使用了，它会跑出抛出一个 IllegalStateException 异常。但是某些流操作会返回他们的接受者，而不是一个新的流对象，所以无法在所有情况下检测出是否可以重用） Optional - 完全可重用，因为它是不可变对象，而且所有操作都是立刻执行的。 Observable, Flowable, Flux - 生而重用，专门设计成如此。当存在订阅者时，每一次执行都会从初始点开始完整地执行一边。 4. Asynchronous（异步）CompletableFuture - 这个类的要点在于它异步地把多个操作连接了起来。CompletableFuture 代表一项操作，它会跟一个 Executor 关联起来。如果不明确指定一个 Executor，那么会默认使用公共的 ForkJoinPool 线程池来执行。这个线程池可以用 ForkJoinPool.commonPool() 获取到。默认设置下它会创建系统硬件支持的线程数一样多的线程（通常和 CPU 的核心数相等，如果你的 CPU 支持超线程 (hyperthreading)，那么会设置成两倍的线程数）。不过你也可以使用 JVM 参数指定 ForkJoinPool 线程池的线程数， 1-Djava.util.concurrent.ForkJoinPool.common.parallelism=? 或者在创建 CompletableFuture 时提供一个指定的 Executor。 Stream - 不支持创建异步执行流程，但是可以使用 stream.parallel() 等方式创建并行流。 Optional - 不支持，它只是一个容器。 Observable, Flowable, Flux - 专门设计用以构建异步系统，但默认情况下是同步的。subscribeOn 和 observeOn 允许你来控制订阅以及接收（这个线程会调用 observer 的 onNext / onError / onCompleted 方法）。 subscribeOn 方法使得你可以决定由哪个 Scheduler 来执行 Observable.create 方法。即便你没有调用创建方法，系统内部也会做同样的事情。例如： 12345678910111213Observable .fromCallable(() -&gt; { log.info(&quot;Reading on thread:&quot; + currentThread().getName()); return readFile(&quot;input.txt&quot;); }) .map(text -&gt; { log.info(&quot;Map on thread:&quot; + currentThread().getName()); return text.length(); }) .subscribeOn(Schedulers.io()) // &lt;-- setting scheduler .subscribe(value -&gt; { log.info(&quot;Result on thread:&quot; + currentThread().getName()); }); 输出： 123Reading file on thread: RxIoScheduler-2Map on thread: RxIoScheduler-2Result on thread: RxIoScheduler-2 相反的，observeOn() 控制在 observeOn() 之后，用哪个 Scheduler 来运行下游的执行阶段。例如： 1234567891011121314Observable .fromCallable(() -&gt; { log.info(&quot;Reading on thread:&quot; + currentThread().getName()); return readFile(&quot;input.txt&quot;); }) .observeOn(Schedulers.computation()) // &lt;-- setting scheduler .map(text -&gt; { log.info(&quot;Map on thread:&quot; + currentThread().getName()); return text.length(); }) .subscribeOn(Schedulers.io()) // &lt;-- setting scheduler .subscribe(value -&gt; { log.info(&quot;Result on thread:&quot; + currentThread().getName()); }); 输出： 123Reading file on thread: RxIoScheduler-2Map on thread: RxComputationScheduler-1Result on thread: RxComputationScheduler-1 5. Cacheable（可缓存）可缓存和可复用之间的区别是什么？假如我们有 pipeline A，重复使用它两次，来创建两个新的 pipeline B = A + X 以及 C = A + Y 如果 B 和 C 都能成功执行，那么这个 A 就是是可重用的。 如果 B 和 C 都能成功执行，并且 A 在这个过程中，整个 pipeline 只执行了一次，那么我们便称 A 是可缓存的。这意味着，可缓存一定代表可重用。 CompletableFuture - 跟可重用的答案一样。 Stream - 不能缓存中间操作的结果，除非调用了终止操作。 Optional - 可缓存，所有操作立刻执行，并且进行了缓存。 Observable, Flowable, Flux - 默认不可缓存的，但是可以调用 .cache() 把这些类变成可缓存的。例如： 123456Observable&lt;Integer&gt; work = Observable.fromCallable(() -&gt; { System.out.println(&quot;Doing some work&quot;); return 10;});work.subscribe(System.out::println);work.map(i -&gt; i * 2).subscribe(System.out::println); 输出： 1234Doing some work10Doing some work20 使用 .cache()： 123456Observable&lt;Integer&gt; work = Observable.fromCallable(() -&gt; { System.out.println(&quot;Doing some work&quot;); return 10;}).cache(); // &lt;- apply cachingwork.subscribe(System.out::println);work.map(i -&gt; i * 2).subscribe(System.out::println); 输出： 123Doing some work1020 6. Push or Pull（推拉模型）Stream 和 Optional - 拉模型。调用不同的方法（.get(), .collect() 等）从 pipeline 拉取结果。拉模型通常和阻塞、同步关联，那也是公平的。当调用方法时，线程会一直阻塞，直到有数据到达。 CompletableFuture, Observable, Flowable, Flux - 推模型。当订阅一个 pipeline ，并且某些事件被执行后，你会得到通知。推模型通常和非阻塞、异步这些词关联在一起。当 pipeline 在某个线程上执行时，你可以做任何事情。你已经定义了一段待执行的代码，当通知到达的时候，这段代码就会在下个阶段被执行。 7. Backpressure（回压） 支持回压的前提是 pipeline 必须是推模型。* Backpressure（回压） 描述了 pipeline 中的一种场景：某些异步阶段的处理速度跟不上，需要告诉上游生产者放慢速度。直接失败是不能接受的，这会导致大量数据的丢失。 Stream &amp; Optional - 不支持回压，因为它们是拉模型。 CompletableFuture - 不存在这个问题，因为它只产生 0 个或者 1 个结果。 Observable(RxJava 1), Flowable, Flux - 支持。常用策略如下： Buffering - 缓冲所有的 onNext 的值，直到下游消费它们。 Drop Recent - 如果下游处理速率跟不上，丢弃最近的 onNext 值。 Use Latest - 如果下游处理速率跟不上，只提供最近的 onNext 值，之前的值会被覆盖。 None - onNext 事件直接被触发，不做缓冲和丢弃。 Exception - 如果下游处理跟不上的话，抛出异常。 Observable(RxJava 2) - 不支持。很多 RxJava 1 的使用者用 Observable 来处理不适用回压的事件，或者是使用 Observable 的时候没有配置任何策略，导致了不可预知的异常。所以，RxJava 2 明确地区分两种情况，提供支持回压的 Flowable 和不支持回压的 Observable。 8. Operator fusion（操作融合）操作融合的内涵在于，它使得生命周期的不同点上的执行阶段得以改变，从而消除类库的架构因素所造成的系统开销。所有这些优化都在内部被处理完毕，从而让外部用户觉得这一切都是透明的。 只有 RxJava 2 和 Reactor 支持这个特性，但支持的方式不同。总的来说，有两种类型的优化： Macro-fusion - 用一个操作替换 2 个或更多的相继的操作 Micro-fusion - 一个输出队列的结束操作，和在一个输入队列的开始操作，能够共享一个队列的实例。比如说，与其调用 request(1) 然后处理 onNext()`： 不然让订阅者直接从父 observable 拉取值。 更多信息可以参考 Part1 和 Part2 总结一图胜千言 Stream，CompletableFuture 和 Optional 这些类的创建，都是为了解决特定的问题。 并且他们非常适合用于解决这些问题。 如果它们满足你的需求，你可以立马使用它们。 然而，不同的问题具有不同的复杂度，并且某些问题只有新技术才能很好的解决，新技术的出现也是为了解决那些高复杂度的问题。 RxJava 和 Reactor 是通用的工具，它们帮助你以声明方式来解决问题，而不是使用那些不够专业的工具，生搬硬套的使用其他的工具来解决响应式编程的问题，只会让你的解决方案变成一种 hack 行为。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/comparing-rxjava/"},{"title":"【参赛总结】第二届云原生编程挑战赛-冷热读写场景的RocketMQ存储系统设计","text":"前言人总是这样，年少时，怨恨自己年少，年迈时，怨恨自己年迈，就连参加一场比赛，都会纠结，工作太忙怎么办，周末休息怎么办，成年人的任性往往就在那一瞬间，我只是单纯地想经历一场酣畅的性能挑战赛。所以，云原生挑战赛，我来了，Kirito 带着他的公众号来了。 读完寥寥数百多字的赛题描述，四分之一炷香之后一个灵感出现在脑海中，本以为这个灵感是开篇，没想到却是终章。临近结束，测试出了缓存命中率更高的方案，但评测已经没有了日志，在茫茫的方案之中，我错过了最大的那一颗麦穗，但在一个月不长不短的竞赛中，我挑选到了一颗不错的麦穗，从此只有眼前路，没有身后身，最终侥幸跑出了内部赛第一的成绩。 传统存储引擎类型的比赛，主要是围绕着两种存储介质：SSD 和 DRAM，不知道这俩有没有熬过七年之痒，Intel 就已经引入了第三类存储介质：AEP（PMem 的一种实现）。AEP 的出现，让原本各司其职的 SSD 和 DRAM 关系变得若即若离起来，它既可以当做 DRAM 用，也可以当做 SSD 用。蕴含在赛题中的”冷热存储“这一关键词，为后续风起云涌的赛程埋下了伏笔，同时给了 AEP 一个名分。 AEP 这种存储介质不是第一次出现在我眼前，在 ADB 比赛中就遇到过它，此次比赛开始时，脑子里面对它仅存的印象便是”快”。这个快是以 SSD 为参照物，无论是读还是写，都高出传统 SSD 1~n 个数量级。但更多的认知，只能用 SSD 来类比，AEP 特性的理解和使用方法，无疑是这次的决胜点之一。 曾经的我喜欢问，现在的我喜欢试。一副键盘，一个深夜，我窥探到了 AEP 的奥秘，多线程读写必不可少，读取速度和写入速度近似 DRAM，但细究之下写比读慢，从整体吞吐来看，DRAM 的读写性能略优于 AEP，但 DRAM 和 AEP 的读写都比 SSD 快得多的多。我的麦穗也有了初步的模样：第一优先级是降低 SSD 命中率，在此基础上，提高 DRAM 命中率，AEP 起到平衡的效果，初期不用特别顾忌 AEP 和 DRAM 的命中比例。 参赛历程万事开头难，中间难，后段也难。 和参加比赛的大多数人一样，我成功卡在了第一步：出分。AEP 先别用了，缓存方案也暂且搁置，不愧是 RocketMQ 团队出的题目，队列众多，那就先用 RocketMQ 的经典方案，“一阶段多线程绑定分区文件，顺序追加写，索引好办，内存和文件各存一份；二阶段随机读”，说干就干，两个小时后，果然得到了“正确性检测失败”的提示。把时间轴拉到去年，相比之下今年的参赛队伍并没有降低很多，但出分的队伍明显下降，大多数人是卡在了正确性检测这一关。咨询出题人，得知这一次的正确性检测是实实在在的”掉电“，PageCache 是指望不上了，只能派 FileChannel#force() 出场，成功获得了第一份成绩：1200s，一份几乎快超时的成绩。 使用 force 解决掉电数据不丢失可以参考我赛程初段写的文章：https://www.cnkirito.moe/filechannel_force/ 优化成功的喜悦片刻消散，初次出分的悸动至死不渝。那种感觉就像一觉醒来，发现今天是周六，可以睡到中午，甚至后面还有一个周日一样放肆。有了 baseline，下面便可以开始着手优化了，刚起头儿，有的是功夫，有的是希望。 掉电的限制，使得 SSD 的写入方案十分受限，每一条数据都要 force，使得 pageCache 近似无效，pageCache 从来没有受过这样的委屈，我走还不行吗？ 随着赛程持续推进，也就推进了一天吧，我开始着手优化合并写入方案。传统方案中，聚合写入往往使用 DRAM 作为写入缓冲，聚合同一线程内前后几条消息，以减少写入放大问题，也有诸如 MongoDB 之流，会选择合并同一批次多个线程之间的数据，按照题意，多个线程合并写入，一起 force 已然是题目的唯一解。团结就是力量，经过测试，n 个线程聚合在一起 force，一起返回，可以显著降低 force 的总次数，缓解写入放大问题，有效地提升了分数。n 多少合适呢？理论分析，n 过大，实际 IO 线程就会变少；n 过小，force 次数多，解决调参问题，最合适的是机器学习，其次是 benchmark，经过多轮 benchmark，4 组 x 10 线程最为合适，4 个 IO 线程正好 = CPU core，这非常的合理。 合并写入后，效果显著，成绩来到了 700~800s。如果没有 AEP，这个比赛也就到头了，AEP 的加入，像草一样，一切都卷了起来。 赛程中段，有朋友在微信交流群中问我，AEP 你有用起来吗？每个人都会经历这个阶段：看见一座山，就想知道山后面是什么。我很想告诉他，可能翻过去山后面，你会发觉没有什么特别。回头看，会觉得这一边更好。但我知道他不会听，以他的性格，自己不试过，又怎么会甘心？我的 trick 无足轻重，他最终还是使用了 AEP，感受到了蒸汽火车到高铁那般速度的提升。总数据 125G，AEP 容量 60G，即使固定存储最后的 60G 数据，也可以确保热读部分的数据全部命中 AEP，SSD 会因为你的刻意保持距离而感到失落，你的分数不会。 即便是这样不经任何设计的 AEP 缓存方案，得益于 AEP 的读写速度和较大的容量加持，也可以获得 600s+ 的分数。 分数这个东西，总是比昨天高一点，比明天少一点，但要想维持这样的分数增长，需要持续付出极大的努力。600s 显然不足以支撑进入决赛，AEP 缓存固定的数据也显得有点呆，就像你意外获得了一块金刚石，不经雕琢，则无法成为耀眼夺目的钻石。必须优化 AEP 的使用方案！因为有热数据的存在，写入的一部分数据会在较短的一段时间内被消费，缓存方案也需要联动起来，写入-&gt; 消费 -&gt; 写入 -&gt; 消费，大脑中飞速地模拟、推测评测程序的流程，操作系统、计算机网络的概念被一遍遍检索，最终锁定在了一个耳熟能详的概念：TCP 滑动窗口。AEP 保存最热的 60G 数据，使得热读全部命中，根据测试，发现冷读也会很快变成热读，在思路和方案连接的那一刻，代码流程也直接显现了出来，三又二分之一小时后，我提交了这份 AEP 滑动窗口的方案，没有什么比一次 Acccept 更爽的事了，一边赞叹自己的编码能力，一边自负地停止了优化，成绩停留在了 504s。 十月八号零点八分，钟楼敲响后的八分钟，我手握着一杯水，打开了排行榜，看到不少 500+ 的分数，懊恼、恐慌、焦虑一下子涌上了心头，水也越饮越寒。我本有七天时间，优化我的方案，但我没有；我在等一个奇迹，期待大家忘掉这场比赛，放弃优化，让排行榜锁定在十月一号那一天，但它没有。我将这份烦恼倾诉给妻子，换来了她的安慰，在她心目中，我永远是最棒的。我内心忐忑地依附道，那当然…在之后的晚上，世上少了一个 WOT 的玩家，多了一个埋头在 IDEA 中追求极致性能的码农。 在很长的一段时间里，我一直在追求降低 SSD 的命中率，每降低一点，我的分数总能够提升几秒。不知道从哪一天起，我看到排行榜中出现了一些 450s 的成绩，起初这并没有引起的我的警觉，因为 hack 可以很容易达到 300s，我一开始预估的极限成绩，不过也就是 470s，对，这一定是 hack，心里一遍遍地默念着。但，万一不是呢？ 太想伸手摘取星星的人，常常忘记脚下的鲜花。我开始翻阅赛题描述，以寻找是否有遗漏的信息；一遍遍 review 自己的代码，调整代码结构，优化代码细节；检索自己过往的博文，以寻找可能擦肩而过的优化点。往后的几个晚上，我做的是同一个梦，梦里面重复播放着自己曾经的优化经验：4kb 对齐、文件预分配、缓存行填充…忽然间想起，自己总结的优化经验还没有完全尝试过。这次比赛是从第一次写入开始计时的，选手们可以在构造函数中恣意地预先分配一些数据，例如对象提前 new 好，内存提前 allocate，减少 runtime 时期的耗时，而这其中最有用的优化，当属 SSD 文件的预分配和 4kb 填充了。在 append 之前，事先把文件用 0 填充，得到总长度略大于 125G 的空白文件，在写入时，不足 4kb 的部分使用 4kb 填充，即使多写了一部分数据，速度还是能够提升，换算成实际的写入速度，可以达到 310M/s，而在此之前，force 的存在使得写入瓶颈始终卡在 275M/s。宁可一思进，莫在一思留，方案调通后，成绩锁定在了 440s。 内部赛结束前的两周，我又萌生了一个大胆的想法，考虑到 getRange 命中 SSD 时，系统采用的是抽样检测，那是不是意味着，用 mmap 读取就变成了一种懒加载呢？这个思路虽然在实际生产中不太通用，但在赛场上，那可以一把利器，这把利器斩下了 412s 的分数，也割伤了自己，评委不让用！我的天，我浪费了宝贵的两周，浪费在了一个无法通过的方案上。天知道评测是在抽样检测，我只是认为 mmap 读会更快呀 ：） 不知道从什么时候开始，在什么东西上面都有个日期，秋刀鱼会过期，肉罐头会过期，比赛也在 10.26 号这天迎来了结束。未竟的优化,设想的思路，没能完成方案改造的遗憾都在这一刻失去了意义。我已经很久没有打过比赛了，也很久没有这样为一个方案绞尽脑汁了，这场比赛就这样任性地画上了一个句号。 最终方案SSD 写入方案 缓存架构是制胜点，SSD 的写入方案则是基本面，相信绝大多数前排的选手，都采用了上述的架构。性能评测阶段固定有 40 个线程，我将线程分为了 4 组，每组 10 个线程，进行 IO 合并。为什么是 4组在参赛历程中也介绍过，尊重 benchmark 的结果。1~9 号线程写入缓冲区完毕之后就 await 进入阻塞态，留下 10 号线程进行 IO，刷盘之后，notify 其他线程返回结果，如此往复，是一个非常经典的生产者消费者模式。 由于这次比赛，recover 阶段是不计入得分的，为了降低 force 的开销，我选择将索引的持久化和数据存在一起，这样避免了单独维护索引文件。在我的方案中，索引需要维护 topic，queue，length 三个信息，只需要定长的 5 个字节，和 100b~17Kb 的数据相比，微不足道，合并之后收益是很明显的。 选择使用 JUC 提供的 Lock + Condition 实现 wait/notify，一则是自己比较习惯这么控制并发，二则是复用其中一个 append 线程做刷盘的 IO 线程，相比其他并发方案的线程切换，要少一点。事实上，这次比赛中，CPU 是非常富余的，不会成为瓶颈，该模式的优势并没有完全发挥出来。 4kb 对齐是 SSD 经典的优化技巧，尽管并不是每一次性能挑战赛它都能排上用场，但请务必不要忘记尝试它。它对于人们的启发是使用 4kb 整数倍的写入缓冲聚合数据，整体刷盘，从而避免读写放大问题。此次比赛稍显特殊，由于赛题数据的随机分布特性，10 个线程聚合后的数据，往往不是 4KB 的整数倍，但这不妨碍我们做填充，看似多写入了一部分无意义的数据，但实际上会使得写入速度得到提升，尤其是在 force 情况下。 我曾和 @chender 交流过 4KB 填充这个话题，尝试分析出背后的原因，这里的结论不一定百分之百正确。4KB 是 SSD 的最小读写单元，这涉及硬件的操作，如果不填充，考虑以下执行流程，写入 9KB，force，写入 9 Kb，force，如果不做填充，相当于 force 了 9+3+3+9+3=27 kb，中间交叉的 3 kb，在 force 时会被重复计算，而填充过后，一定是 force 了 9+3+9+3=24 kb，整体 force 量降低。还有一个可能的依据是，没有填充的情况下，其实一定程度破坏了顺序写，写入实际写入了 12kb，但第二次写入并没有从 12kb 开始写入，而是从 9kb 写入。总之在 benchmark 下，4kb 对齐确实带来了 15s+ 的收益。 写入阶段还有一个致胜的优化，文件预分配。在 C 里面，有 fallocate，而 Java 并没有对应的 JNI 调用，不过可以取巧，利用 append 开始计分这个评测特性，在 SSD 上先使用字节 0 填充整个文件。在预分配过后，使用 force 也可以获得跟不使用 force 一样的写入速度，几乎打满了 320M/s 的 IO 速度。这个优化点，我在之前的博客中也分享过，不知道有没有其他选手看到并利用了起来，如果漏掉了这个优化，真的有点可惜，因为它足足可以让方案快 50s 左右。 缓存架构 上图是全局缓存架构，整体方案的思路是多级缓存，采用滑动窗口的思想，AEP 永远缓存最新的 60G 数据，以确保热数据一定不会命中 SSD。同时，堆外的 2G DRAM 与 AEP 息息相关，这部分 DRAM 有两个作用，其一是作为 AEP 的写入缓冲，规避 AEP 写入放大的问题，其二是作为热数据的 DRAM 缓存，最热的一部分数据，可以保证直接命中 DRAM，规避 AEP 的访问。另外富余的 3G 的堆内内存，可以用于缓存由于滑动而导致被覆盖的数据，这部分 DRAM 同时配备引用计数，从而达到复用的效果。 在具体实现中，我将 60G 平均分配给 40 个线程，每个线程持有 1.5G 的 AEP 可用缓存，50M 的 DRAM 缓存。可以发现，在我的方案中，SSD 写入方案和 AEP 是不同的，SSD 由于 force 的限制，采用了线程合并写入，而 AEP 本身就是可以丢失的缓存，所以不需要进行合并，每个线程维护自身的 AEP 和 DRAM 缓存即可。 每个线程除了配备 1.5G 的AEP，还分配了 50M 的 DRAM。这部分 DRAM 永远被优先写入，同样的，也会优先被读取，前提是命中了的话。50M 显然不是一个特别大的空间，所以在其充满时，将 50M 数据整体刷入 AEP 中，使用 ByteBuffer 作为 DRAM 的 manager，还可以利用其逻辑 clear 的操作，使得 DRAM 和 AEP 一样变成了一个 RingBuffer。这部分设计算是我方案中比较巧妙的一点。 当然，你永远可以相信 SSD，它是最后一道兜底逻辑，无论缓存设计的多么糟糕，保证最后能够命中 SSD 才能出分，所有人都清楚这一点。 AEP 滑动窗口的实现其实并不复杂，详见文末的代码，我就不过多介绍了。 程序优化预分配文件1234567891011121314151617181920212223public static void preAllocateFile(FileChannel fileChannel, long threadGroupPerhapsFileSize) { int bufferSize = 4 * 1024; ByteBuffer byteBuffer = ByteBuffer.allocateDirect(bufferSize); for (int i = 0; i &lt; bufferSize; i++) { byteBuffer.put((byte)0); } byteBuffer.flip(); long loopTimes = threadGroupPerhapsFileSize / bufferSize; for (long i = 0; i &lt; loopTimes; i++) { try { fileChannel.write(byteBuffer); } catch (IOException e) { e.printStackTrace(); } byteBuffer.flip(); } try { fileChannel.force(true); fileChannel.position(0); } catch (IOException e) { e.printStackTrace(); }} 简单实用的一个优化技巧，属于发现就可以很快实现的一个优化，但可能不容易发现。 4KB 对齐12345678910111213private void force() { // 4kb 对齐 int position = writeBuffer.position(); int mod = position % Constants._4kb; if (mod != 0) { writeBuffer.position(position + Constants._4kb - mod); } writeBuffer.flip(); fileChannel.write(writeBuffer); fileChannel.force(false); writeBuffer.clear();} 同上 Unsafe其实感觉用了 Unsafe 也没有多少提升，因为后期抖动太大了，但最优成绩的确是用 Unsafe 跑出来的，还是罗列出来，万一下次有用呢？详见代码实现 io.openmessaging.NativeMemoryByteBuffer。 推荐阅读：《聊聊Unsafe的一些使用技巧》 ArrayMap可以使用数组重写一遍 Map，反正此次比赛调用到的 Map 的 API 也不多，换成数组实现之后，可以降低 HashMap 的 overheap，再优秀的实现，在数组面前也会变得黯淡无光，仅限于比赛。 详见代码实现 io.openmessaging.ArrayMap。 预初始化充分利用 append 之前的耗时不计入总分这一特性。除了将文件提前分配出来之外，Runtime 需要 new 的对象、DRAM 空间等，都提前在构造函数中完成，蚊子腿也是肉，分数总是这么一点点抠出来的。 并发 getRange读取阶段 fetchNum 最大为 100，串行访问的话，如果是命中缓存还好，要是 100 次 SSD 的 IO 都是串行，那可就太糟糕了。经过测试，仅当命中 SSD 时并发访问，和不区分内存、AEP、SSD 命中，均并发访问，效果差不多，但无论如何，并发 getRange 总是比串行好的。 ThreadLocal 复用性能挑战赛中务必要 check 的一个环节，便是在运行时有没有动态 new 对象，有没有动态 allocate 内存，出现这些可是大忌，建议全部用 ThreadLocal 缓存。这次的赛题中有很多关键性的数字，100 个 Topic、40 个线程，稍加不留意，可能把线程级别的一些操作，错当成 Topic 级别来设计，例如分配的写入缓冲也好，getRange 阶段复用的读取缓冲也好，都应该设计成线程级别的数据。ThreadLocal 第一是方便管理线程级别的资源，第二是因为线程相对于 Topic 是要少的，需要搞清楚，哪些资源是线程级别的，哪些是 Topic 级别的，避免资源浪费。 合并写入详见源码io.openmessaging.ThreadGroupManager#append AEP 滑动窗口详见源码io.openmessaging.AepManager 总结与反思一言以蔽之，滑动窗口缓存是我整个方案的核心，虽然这个方案经过细节的优化，让我取得了内部赛第一的成绩，但开篇我也提到过，它并不是缓存命中率最高的方案，在这个方案中，第一个明显的问题便是，堆外 DRAM 和 AEP 可能缓存了同一批数据，实际上，DRAM 和 AEP 缓存不重叠的方案肯定会有更高的缓存命中率；第二个问题，也是问题一连带的问题，在该方案中，堆内的 DRAM 无法被很高效地利用起来，所以我在本文中，只是稍带提了一下堆内的设计，没有详细介绍引用技术的逻辑。 我在赛程后半段，也尝试设计过 DRAM 和 AEP 不重叠并且动态分配回收的方案，缓存利用率的确会更高，但这意味着我要放弃滑动窗口方案中所有的细节调优！业余时间搞比赛，实在是精力时间有限，最终选择了放弃。 这像极了项目开发的技术债，如果你选择忍受，你可以得到一个尚可使用的系统，但你知道，重构之后，它可以更好；当然你也可以选择重构，死着皮，连着肉。 重赏之下，必有卷夫。内部赛还好，外部赛实在是卷，每次这种性能挑战赛，打到最后都是拼了命的抠细节，你被别人卷到了，就很累，你想到了优化点，卷到了别人，就很爽，这也太真实了。 最后说说收获，这次比赛，让我对 AEP 这个新概念有了比较深的理解，对存储设计、文件 IO 也有了更深的体会。这类比赛偶尔打打还是挺有意思的，一方面写项目代码容易疲乏，二是写出这么一个小的工程，还是挺有成就感的一件事。如果有下一场，也欢迎读者们一起来卷。 源码https://github.com/lexburner/aliyun-cloudnative-race-mq-2021.git","link":"/cloudnative-race-2021-rmq/"},{"title":"浅析项目中的并发 (一)","text":"前言控制并发的方法很多，从最基础的 synchronized，juc 中的 lock，到数据库的行级锁，乐观锁，悲观锁，再到中间件级别的 redis，zookeeper 分布式锁。特别是初级程序员，对于所谓的锁一直都是听的比用的多，第一篇文章不深入探讨并发，更多的是一个入门介绍，适合于初学者，主题是“根据并发出现的具体业务场景，使用合理的控制并发手段”。 什么是并发由一个大家都了解的例子引入我们今天的主题：并发 类共享变量遭遇并发123456789101112131415161718192021222324public class Demo { public Integer count = 0; public static void main(String[] args) { final Demo demo = new Demo(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executor.execute(new Runnable() { @Override public void run() { demo.count++; } }); } try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;final count value:&quot;+demo1.count); }} final count value:973 本例中创建了一个初始化时具有 10 个线程的线程池，多线程对类变量 count 进行自增操作。这个过程中，自增操作并不是线程安全的，happens-before 原则并不会保障多个线程执行的先后顺序，导致了最终结果并不是想要的 1000 下面，我们把并发中的共享资源从类变量转移到数据库中。 充血模型遭遇并发1234567891011121314@Componentpublic class Demo2 { @Autowired TestNumDao testNumDao; @Transactional public void test(){ TestNum testNum = testNumDao.findOne(&quot;1&quot;); testNum.setCount(testNum.getCount()+1); testNumDao.save(testNum); }} 依旧使用多线程，对数据库中的记录进行 +1 操作 Demo2 demo2; public String test(){ Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executor.execute(new Runnable() { @Override public void run() { demo2.test(); } }); } return &quot;test&quot;; } 数据库的记录 12id | count1 | 344 初窥门径的程序员会认为事务最基本的 ACID 中便包含了原子性，但是事务的原子性和今天所讲的并发中的原子操作仅仅是名词上有点类似。而有点经验的程序员都能知道这中间发生了什么，这只是暴露了项目中并发问题的冰山一角，千万不要认为上面的代码没有必要列举出来，我在实际项目开发中，曾经见到有多年工作经验的程序员仍然写出了类似于上述会出现并发问题的代码。 贫血模型遭遇并发1234567891011121314151617181920@RequestMapping(&quot;testSql&quot;) @ResponseBody public String testSql() throws InterruptedException { final CountDownLatch countDownLatch = new CountDownLatch(1000); long start = System.currentTimeMillis(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executor.execute(new Runnable() { @Override public void run() { jdbcTemplate.execute(&quot;update test_num set count = count + 1 where id ='1'&quot;); countDownLatch.countDown(); } }); } countDownLatch.await(); long costTime =System.currentTimeMillis() - start; System.out.println(&quot;共花费：&quot;+costTime+&quot;s&quot;); return &quot;testSql&quot;; } 数据库结果： count ： 1000 达到了预期效果这个例子我顺便记录了耗时, 控制台打印: 共花费：113 ms简单对比一下二，三两个例子，都是想对数据库的 count 进行 +1 操作，唯一的区别就是，** 后者的 +1 计算发生在数据库，而前者的计算依赖于事先查出来的值，并且计算发生在程序的内存中 **。而现在大部分的 ORM 框架，导致了写充血模型的程序员变多，不注意并发的话，就会出现问题。下面我们来看看具体的业务场景。 业务场景 修改个人信息 修改商品信息 扣除账户余额，扣减库存 业务场景分析第一个场景，互联网如此众多的用户修改个人信息，这算不算并发？答案是：算也不算。算，从程序员角度来看，每一个用户请求进来，都是调用的同一个修改入口，具体一点，就是映射到 controller 层的同一个 requestMapping，所以一定是并发的。不算，虽然程序是并发的，但是从用户角度来分析，每个人只可以修改自己的信息，所以，不同用户的操作其实是隔离的，所以不算“并发”。这也是为什么很多开发者，在日常开发中一直不注意并发控制，却也没有发生太大问题的原因，大多数初级程序员开发的还都是 CRM，OA，CMS 系统。 回到我们的并发，第一种业务场景，是可以使用如上模式的，对于一条用户数据的修改，我们允许程序员读取数据到内存中，内存计算修改（耗时操作），提交更改，提交事务。 1234567//Transaction startUser user = userDao.findById(&quot;1&quot;);user.setName(&quot;newName&quot;);user.setAge(user.getAge()+1);...// 其他耗时操作userDao.save(user);//Transaction commit 这个场景变现为：几乎不存在并发，不需要控制，场景乐观。 为了严谨，也可以选择控制并发，但我觉得这需要交给写这段代码的同事，让他自由发挥。 第二个场景已经有所不同了，同样是修改一个记录，但是系统中可能有多个操作员来维护，此时，商品数据表现为一个共享数据，所以存在微弱的并发，通常表现为数据的脏读，例如操作员 A，B 同时对一个商品信息维护，我们希望只能有一个操作员修改成功，另外一个操作员得到错误提示（该商品信息已经发生变化），否则，两个人都以为自己修改成功了，但是其实只有一个人完成了操作，另一个人的操作被覆盖了。 这个场景表现为：存在并发，需要控制，允许失败，场景乐观。 通常我建议这种场景使用乐观锁，即在商品属性添加一个 version 字段标记修改的版本，这样两个操作员拿到同一个版本号，第一个操作员修改成功后版本号变化，另一个操作员的修改就会失败了。 1234567891011121314151617class Goods{ @Version int version;}//Transaction starttry{ Goods goods = goodsDao.findById(&quot;1&quot;); goods.setName(&quot;newName&quot;); goods.setPrice(goods.getPrice()+100.00); ...// 其他耗时操作 goodsDao.save(goods);}catch(org.hibernate.StaleObjectStateException e){ // 返回给前台}//Transaction commit springdata 配合 jpa 可以自动捕获 version 异常，也可以自动手动对比。 第三个场景这个场景表现为：存在频繁的并发，需要控制，不允许失败，场景悲观。 ** 强调一下，本例不应该使用在项目中，只是为了举例而设置的一个场景，因为这种贫血模型无法满足复杂的业务场景，而且依靠单机事务来保证一致性，并发性能和可扩展性能不好。** 一个简易的秒杀场景，大量请求在短时间涌入，是不可能像第二种场景一样，100 个并发请求，一个成功，其他 99 个全部异常的。 设计方案应该达到的效果是：有足够库存时，允许并发，库存到 0 时，之后的请求全部失败；有足够金额时，允许并发，金额不够支付时立刻告知余额不足。 可以利用数据库的行级锁，update set balance = balance - money where userId = ? and balance &gt;= money;update stock = stock - number where goodsId = ? and stock &gt;= number ; 然后在后台 查看返回值是否影响行数为 1，判断请求是否成功，利用数据库保证并发。 需要补充一点，我这里所讲的秒杀，并不是指双 11 那种级别的秒杀，那需要多层架构去控制并发，前端拦截，负载均衡…. 不能仅仅依赖于数据库的，会导致严重的性能问题。为了留一下一个直观的感受，这里对比一下 oracle，mysql 的两个主流存储引擎：innodb，myisam 的性能问题。 123456oracle:10000 个线程共计 1000000 次并发请求：共花费：101017 ms =&gt;101sinnodb:10000 个线程共计 1000000 次并发请求：共花费：550330 ms =&gt;550smyisam:10000 个线程共计 1000000 次并发请求：共花费：75802 ms =&gt;75s 可见，如果真正有大量请求到达数据库，光是依靠数据库解决并发是不现实的，所以仅仅只用数据库来做保障而不是完全依赖。需要根据业务场景选择合适的控制并发手段。","link":"/concurrent-1/"},{"title":"浅析项目中的并发 (二)","text":"分布式遭遇并发在前面的章节，并发操作要么发生在单个应用内，一般使用基于 JVM 的 lock 解决并发问题，要么发生在数据库，可以考虑使用数据库层面的锁，而在分布式场景下，需要保证多个应用实例都能够执行同步代码，则需要做一些额外的工作，一个最典型分布式同步方案便是使用分布式锁。 分布式锁由很多种实现，但本质上都是类似的，即依赖于共享组件实现锁的询问和获取，如果说单体式应用中的 Monitor 是由 JVM 提供的，那么分布式下 Monitor 便是由共享组件提供，而典型的共享组件大家其实并不陌生，包括但不限于：Mysql，Redis，Zookeeper。同时他们也代表了三种类型的共享组件：数据库，缓存，分布式协调组件。基于 Consul 的分布式锁，其实和基于 Zookeeper 的分布式锁大同小异，都是借助于分布式协调组件实现锁，大而化之，这三种类型的分布式锁，原理也都差不多，只不过，锁的特性和实现细节有所差异。 Redis 实现分布式锁定义需求：A 应用需要完成添加库存的操作，部署了 A1，A2，A3 多个实例，实例之间的操作要保证同步。 分析需求：显然，此时依赖于 JVM 的 lock 已经没办法解决问题了，A1 添加锁，无法保证 A2，A3 的同步，这种场景可以考虑使用分布式锁应对。 建立一张 Stock 表，包含 id，number 两个字段，分别让 A1，A2，A3 并发对其操作，保证线程安全。 123456@Entitypublic class Stock { @Id private String id; private Integer number;} 定义数据库访问层： 12public interface StockRepository extends JpaRepository&lt;Stock,String&gt; {} 这一节的主角，redis 分布式锁，使用开源的 redis 分布式锁实现：Redisson。 引入 Redisson 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.5.4&lt;/version&gt;&lt;/dependency&gt; 定义测试类： 12345678910111213141516171819202122232425262728293031@RestControllerpublic class StockController { @Autowired StockRepository stockRepository; ExecutorService executorService = Executors.newFixedThreadPool(10); @Autowired RedissonClient redissonClient; final static String id = &quot;1&quot;; @RequestMapping(&quot;/addStock&quot;) public void addStock() { RLock lock = redissonClient.getLock(&quot;redisson:lock:stock:&quot; + id); for (int i = 0; i &lt; 100; i++) { executorService.execute(() -&gt; { lock.lock(); try { Stock stock = stockRepository.findOne(id); stock.setNumber(stock.getNumber() + 1); stockRepository.save(stock); } finally { lock.unlock(); } }); } }} 上述的代码使得并发发生在多个层面。其一，在应用内部，启用线程池完成库存的加 1 操作，本身便是线程不安全的，其二，在多个应用之间，这样的加 1 操作更加是不受约束的。若初始化 id 为 1 的 Stock 数量为 0。分别在本地启用 A1(8080)，A2(8081)，A3(8082) 三个应用，同时并发执行一次 addStock()，若线程安全，必然可以使得数据库中的 Stock 为 300，这便是我们的检测依据。 简单解读下上述的代码，使用 redisson 获取一把 RLock，RLock 是 java.util.concurrent.locks.Lock 接口的实现类，Redisson 帮助我们屏蔽 Redis 分布式锁的实现细节，使用过 java.util.concurrent.locks.Lock 的朋友都会知道下述的代码可以被称得上是同步的起手范式，毕竟这是 Lock 的 java doc 中给出的代码： 1234567Lock l = ...;l.lock();try { // access the resource protected by this lock} finally { l.unlock();} 而 redissonClient.getLock(&quot;redisson:lock:stock:&quot; + id) 则是以 &quot;redisson:lock:stock:&quot; + id 该字符串作痛同步的 Monitor，保证了不同 id 之间是互相不阻塞的。 为了保证发生并发，实际测试中我加入了 Thread.sleep(1000)，使竞争得以发生。测试结果： Redis 分布式锁的确起了作用。 锁的注意点如果仅仅是实现一个能够用于 demo 的 Redis 分布式锁并不难，但为何大家更偏向于使用开源的实现呢？主要还是可用性和稳定性，we make things work 是我在写博客，写代码时牢记在脑海中的，如果真的要细究如何自己实现一个分布式锁，或者平时使用锁保证并发，需要有哪些注意点呢？列举几点：阻塞，超时时间，可重入，可用性，其他特性。 阻塞意味着各个操作之间的等待，A1 正在执行增加库存时，A1 其他的线程被阻塞，A2，A3 中所有的线程被阻塞，在 Redis 中可以使用轮询策略以及 redis 底层提供的 CAS 原语 (如 setnx) 来实现。（初学者可以理解为：在 redis 中设置一个 key，想要执行 lock 代码时先询问是否有该 key，如果有则代表其他线程在执行过程中，若没有，则设置该 key，并且执行代码，执行完毕，释放 key，而 setnx 保证操作的原子性） 超时时间在特殊情况，可能会导致锁无法被释放，如死锁，死循环等等意料之外的情况，锁超时时间的设置是有必要的，一个很直观的想法是给 key 设置过期时间即可。 如在 Redisson 中，lock 提供了一个重载方法 lock(long t, TimeUnit timeUnit); 可以自定义过期时间。 可重入这个特性很容易被忽视，可重入其实并不难理解，顾名思义，一个方法在调用过程中是否可以被再次调用。实现可重入需要满足三个特性： 可以在执行的过程中可以被打断； 被打断之后，在该函数一次调用执行完之前，可以再次被调用（或进入，reentered)。 再次调用执行完之后，被打断的上次调用可以继续恢复执行，并正确执行。 比如下述的代码引用了全局变量，便是不可重入的： 12345678int t;void swap(int x, int y) { t = x; x = y; y = t; System.out.println(&quot;x is&quot; + x + &quot;y is&quot; + y);} 一个更加直观的例子便是，同一个线程中，某个方法的递归调用不应该被阻塞，所以如果要实现这个特性，简单的使用某个 key 作为 Monitor 是欠妥的，可以加入线程编号，来保证可重入。 使用可重入分布式锁的来测试计算斐波那契数列（只是为了验证可重入性）： 12345678910111213141516171819202122@RequestMapping(&quot;testReentrant&quot;)public void ReentrantLock() { RLock lock = redissonClient.getLock(&quot;fibonacci&quot;); lock.lock(); try { int result = fibonacci(10); System.out.println(result); } finally { lock.unlock(); }}int fibonacci(int n) { RLock lock = redissonClient.getLock(&quot;fibonacci&quot;); try { if (n &lt;= 1) return n; else return fibonacci(n - 1) + fibonacci(n - 2); } finally { lock.unlock(); }} 最终输出：55，可以发现，只要是在同一线程之内，无论是递归调用还是外部加锁 (同一把锁)，都不会造成死锁。 可用性借助于第三方中间件实现的分布式锁，都有这个问题，中间件挂了，会导致锁不可用，所以需要保证锁的高可用，这就需要保证中间件的可用性，如 redis 可以使用哨兵 + 集群，保证了中间件的可用性，便保证了锁的可用性、 其他特性除了可重入锁，锁的分类还有很多，在分布式下也同样可以实现，包括但不限于：公平锁，联锁，信号量，读写锁。Redisson 也都提供了相关的实现类，其他的特性如并发容器等可以参考官方文档。 新手遭遇并发基本算是把项目中遇到的并发过了一遍了，案例其实很多，再简单罗列下一些新手可能会遇到的问题。 使用了线程安全的容器就是线程安全了吗？很多新手误以为使用了并发容器如：concurrentHashMap 就万事大吉了，却不知道，一知半解的隐患可能比全然不懂更大。来看下面的代码： 123456789101112131415161718192021public class ConcurrentHashMapTest { static Map&lt;String, Integer&gt; counter = new ConcurrentHashMap(); public static void main(String[] args) throws InterruptedException { counter.put(&quot;stock1&quot;, 0); ExecutorService executorService = Executors.newFixedThreadPool(10); CountDownLatch countDownLatch = new CountDownLatch(100); for (int i = 0; i &lt; 100; i++) { executorService.execute(new Runnable() { @Override public void run() { counter.put(&quot;stock1&quot;, counter.get(&quot;stock1&quot;) + 1); countDownLatch.countDown(); } }); } countDownLatch.await(); System.out.println(&quot;result is&quot; + counter.get(&quot;stock1&quot;)); }} counter.put(&quot;stock1&quot;, counter.get(&quot;stock1&quot;) + 1) 并不是原子操作，并发容器保证的是单步操作的线程安全特性，这一点往往初级程序员特别容易忽视。 总结项目中的并发场景是非常多的，而根据场景不同，同一个场景下的业务需求不同，以及数据量，访问量的不同，都会影响到锁的使用，架构中经常被提到的一句话是：业务决定架构，放到并发中也同样适用：业务决定控制并发的手段，如本文未涉及的队列的使用，本质上是化并发为串行，也解决了并发问题，都是控制的手段。了解锁的使用很简单，但如果使用，在什么场景下使用什么样的锁，这才是价值所在。 同一个线程之间的递归调用不应该被阻塞，所以如果要实现这个特性，简单的使用某个 key 作为 Monitor 是欠妥的，可以加入线程编号，来保证可重入。","link":"/concurrent-2/"},{"title":"浅析项目中的并发","text":"前言控制并发的方法很多，我之前的两篇博客都有过介绍，从最基础的 synchronized，juc 中的 lock，到数据库的行级锁，乐观锁，悲观锁，再到中间件级别的 redis，zookeeper 分布式锁。今天主要想讲的主题是“根据并发出现的具体业务场景，使用合理的控制并发手段”。 什么是并发由一个大家都了解的例子引入我们今天的主题：并发 123456789101112131415161718192021222324252627public class Demo1 { public Integer count = 0; public static void main(String[] args) { final Demo1 demo1 = new Demo1(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executor.execute(new Runnable() { @Override public void run() { demo1.count++; } }); } try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;final count value:&quot;+demo1.count); }}console:final count value:973 这个过程中，类变量 count 就是共享资源，而 ++ 操作并不是线程安全的，而多个线程去对 count 执行 ++ 操作，并没有 happens-before 原则保障执行的先后顺序，导致了最终结果并不是想要的 1000 下面，我们把并发中的共享资源从类变量转移到数据库中。先来看看使用框架的情况，以 JPA 为例（充血模型） 1234567891011121314151617181920212223242526272829303132@Componentpublic class Demo2 { @Autowired TestNumDao testNumDao; @Transactional public void test(){ TestNum testNum = testNumDao.findOne(&quot;1&quot;); testNum.setCount(testNum.getCount()+1); testNumDao.save(testNum); }}controller: @Autowired Demo2 demo2; @RequestMapping(&quot;test&quot;) @ResponseBody public String test(){ Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executor.execute(new Runnable() { @Override public void run() { demo2.test(); } }); } return &quot;test&quot;; } 数据库的记录 id count 1 344 初窥门径的程序员会认为事务最基本的 ACID 中便包含了原子性，但是事务的原子性和今天所讲的并发中的原子操作仅仅是名词上有点类似。而有点经验的程序员都能知道这中间发生了什么（下面细说），这只是暴露了项目中并发问题的冰山一角。 改成直接用 sql 如何呢（贫血模型）？ 1234567891011121314151617181920@RequestMapping(&quot;testSql&quot;) @ResponseBody public String testSql() throws InterruptedException { final CountDownLatch countDownLatch = new CountDownLatch(1000); long start = System.currentTimeMillis(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executor.execute(new Runnable() { @Override public void run() { jdbcTemplate.execute(&quot;update test_num set count = count + 1 where id ='1'&quot;); countDownLatch.countDown(); } }); } countDownLatch.await(); long costTime =System.currentTimeMillis() - start; System.out.println(&quot;共花费：&quot;+costTime+&quot;s&quot;); return &quot;testSql&quot;; } 数据库结果： count ： 1000 达到了预期效果这个例子我顺便记录了耗时, 控制台打印: 共花费：113 ms简单对比一下二，三两个例子，都是想对数据库的 count 进行 +1 操作，唯一的区别就是，后者的 +1 计算发生在数据库，而前者的计算依赖于事先查出来的值，并且计算发生在程序的内存中。而现在大部分的 ORM 框架的兴起，导致了写第二种代码的程序员变多，不注意并发的话，就会出现问题。下面我们来看看具体的业务场景。 业务场景 修改个人信息 修改商品信息 扣除账户余额，扣减库存 业务场景分析第一个场景，互联网如此众多的用户修改个人信息，这算不算并发？答案是：算也不算。算，从程序员角度来看，每一个用户请求进来，都是调用的同一个修改入口，具体一点，就是映射到 controller 层的同一个 requestMapping，所以一定是并发的。不算，虽然程序是并发的，但是从用户角度来分析，每个人只可以修改自己的信息，所以，不同用户的操作其实是隔离的，所以不算“并发”。这也是为什么很多开发者，在日常开发中一直不注意并发控制，却也没有发生太大问题的原因，大多数初级程序员开发的还都是 CRM，OA，CMS 系统。 回到我们的并发，第一种业务场景，是可以使用如上模式的，对于一条用户数据的修改，我们允许程序员读取数据到内存中，内存计算修改（耗时操作），提交更改，提交事务。 1234567//Transaction startUser user = userDao.findById(&quot;1&quot;);user.setName(&quot;newName&quot;);user.setAge(user.getAge()+1);...// 其他耗时操作userDao.save(user);//Transaction commit 这个场景变现为：几乎不存在并发，不需要控制，场景乐观。 为了严谨，也可以选择控制并发，但我觉得这需要交给写这段代码的同事，让他自由发挥。第二个场景已经有所不同了，同样是修改一个记录，但是系统中可能有多个操作员来维护，此时，商品数据表现为一个共享数据，所以存在微弱的并发，通常表现为数据的脏读，例如操作员 A，B 同时对一个商品信息维护，我们希望只能有一个操作员修改成功，另外一个操作员得到错误提示（该商品信息已经发生变化），否则，两个人都以为自己修改成功了，但是其实只有一个人完成了操作，另一个人的操作被覆盖了。 这个场景表现为：存在并发，需要控制，允许失败，场景乐观。 通常我建议这种场景使用乐观锁，即在商品属性添加一个 version 字段标记修改的版本，这样两个操作员拿到同一个版本号，第一个操作员修改成功后版本号变化，另一个操作员的修改就会失败了。 1234567891011121314151617class Goods{ @Version int version;}//Transaction starttry{ Goods goods = goodsDao.findById(&quot;1&quot;); goods.setName(&quot;newName&quot;); goods.setPrice(goods.getPrice()+100.00); ...// 其他耗时操作 goodsDao.save(goods);}catch(org.hibernate.StaleObjectStateException e){ // 返回给前台}//Transaction commit springdata 配合 jpa 可以自动捕获 version 异常，也可以自动手动对比。 第三个场景这个场景表现为：存在频繁的并发，需要控制，不允许失败，场景悲观。 ** 强调一下，本例不应该使用在项目中，只是为了举例而设置的一个场景，因为这种贫血模型无法满足复杂的业务场景，而且依靠单机事务来保证一致性，并发性能和可扩展性能不好。** 一个秒杀场景，大量请求在短时间涌入，是不可能像第二种场景一样，100 个并发请求，一个成功，其他 99 个全部异常的。 设计方案应该达到的效果是：有足够库存时，允许并发，库存到 0 时，之后的请求全部失败；有足够金额时，允许并发，金额不够支付时立刻告知余额不足。 可以利用数据库的行级锁，update set balance = balance - money where userId = ? and balance &gt;= money;update stock = stock - number where goodsId = ? and stock &gt;= number ; 然后在后台 查看返回值是否影响行数为 1，判断请求是否成功，利用数据库保证并发。 需要补充一点，我这里所讲的秒杀，并不是指双 11 那种级别的秒杀，那需要多层架构去控制并发，前端拦截，负载均衡…. 不能仅仅依赖于数据库的，会导致严重的性能问题。为了留一下一个直观的感受，这里对比一下 oracle，mysql 的两个主流存储引擎：innodb，myisam 的性能问题。 123456oracle:10000 个线程共计 1000000 次并发请求：共花费：101017 ms =&gt;101sinnodb:10000 个线程共计 1000000 次并发请求：共花费：550330 ms =&gt;550smyisam:10000 个线程共计 1000000 次并发请求：共花费：75802 ms =&gt;75s 可见，如果真正有大量请求到达数据库，光是依靠数据库解决并发是不现实的，所以仅仅只用数据库来做保障而不是完全依赖。需要根据业务场景选择合适的控制并发手段。 后续，待补充分布式锁控制并发…浅析队列在并发场景中的地位…","link":"/concurrent-in-project/"},{"title":"如何向开源项目做贡献 (以 incubator-dubbo 为例)","text":"Github 上有众多优秀的开源项目，大多数 IT 从业者将其当做了予取予求的工具库，遇到什么需求，先去 Github 搜一把，但有没有想过有一天自己也可以给开源事业做一些贡献呢？本文将会以 incubator-dubbo 项目为例，向你阐释，给开源项目做贡献并不是一件难事。 1 为何要给开源贡献力量为开源项目做贡献得到的收益是多方面的，为了让你有足够的信心加入到开源项目中，我在文章最开始列举出它的诸多好处。 1.1 巩固技能无论你是提交代码，撰写文档，提交 Issue，组织活动，当你切身参与到一个开源项目中，相关的技能都会得到历练，并且在开源项目中找到自己的位置。一方面，日常工作中我们中的大多数人接触到的是业务场景，并没有太多机会接触到基础架构组件，开源项目为我们提供了一个平台，在这里，你可以尽情挑选自己熟悉的项目为它添砖加瓦（以 Dubbo 为例，并不是所有 IT 公司都有能力自研服务治理框架）；另一方面，你所提交的代码，会有管理员协助审核，他们会给出专业的建议，更好的代码规范以及更优的编程思路最终都会变成你的经验。 1.2 结交朋友开源社区为你提供了一个平台，在这里，你可以认识很多纯粹的技术爱好者，开源贡献者是最符合 geek 定义的那群人，你所接触到的往往是某个领域最厉害的那批人。 1.3 建立口碑这是一个很好的展示个人实力的地方，俗话说：talk is cheap，show me the code. 作为技术人员，没有什么比一个漂亮的 Github 主页更有说服力的了。如果你能够为开源项目做出可观的贡献，你也将收获到业界的知名度，此时开源项目的成就和你是密不可分的。 1.4 传承开源精神只有源源不断的贡献者给开源项目添砖加瓦，才可以为 Github 一类的开源社区形成良好的开源风气。否则，只有输出没有输入，开源会失去活力。 1.5 养成习惯相信我，一旦养成了每天提交代码的习惯，就像你不想中断打卡一样，你绝不想中断 commit。不止有英语打卡，健身打卡，还有开源打卡！ 2 贡献代码时的一些疑难杂症如果你是一名开源界的新手，可能会对贡献的流程心生畏惧。比如：我该怎么修改代码并提交？我的代码要是存在 bug 怎么办？我的代码别人会不会很 low？我该如何寻找合适的开源项目？开源社区那么多的工具和词汇都是什么意思？ 文章的第二部分将从一个 ** 小白 ** 的角度，介绍一下开源中的一些常见问题。 2.1 git 常规操作一般而言，我们选择使用 git 来作为版本管理的工具，你不一定要非常熟练的使用它，在我看来掌握 clone，add，commit，pull，push 即可，遇到复杂的场景，你还有谷歌。 fork 与 clone 如果你只是想下载源码，查看他的源码实现，使用 Clone or download 按钮即可。 如果你想要给开源项目做改动，并且最终请求合并，让开源项目存在你贡献的代码，就应该使用 fork。 fork 将会复制一份当前主分支的代码进入到你的仓库中，之后你所有的修改，应当基于自己的仓库进行，在功能开发 /bug 修复之后，可以使用你的仓库向源仓库提交 pull request。只有源仓库的管理员才有权利合并你的请求。 一些可能对你有帮助的高级指令。 1234567# 设置源仓库git remote add upstream https://github.com/apache/incubator-dubbo.git# 拉取源仓库的更新git fetch upstream# 将自己仓库的主分支合并源仓库的更新git checkout mastergit merge upstream/master pull request pull request 经常被缩写为 PR，指的是一次向源仓库请求合并的行为，如上是我 fork 了 incubator-dubbo 的仓库之后才存在的操作按钮。 ** 源仓库视角的 pull request** 管理者会对 pull request 涉及的改动进行 review，以确保你的代码是符合规范的，逻辑有没有偏差，以及符合框架的功能需求。 2.2 Travis CI一些自动化的 CI 流程被植入在每一次 pull request 的构建之中，用于给开源仓库去校验提交者的代码是否符合既定的规范，如：是否有编译问题，单元测试是否通过，覆盖率是否达标，代码风格是否合规等等。 一般情况下，必须通过 CI，你的 pull request 才会被管理 review。 2.3 Mailing list每个开源项目都会有自己的贡献规范，可以参考首页的 Contributing，来获取具体的信息。incubator-dubbo 作为一个孵化中的 apache 项目，遵守了 apache 的传统，在 Contributing 中描述道：当你有新特性想要贡献给 Dubbo 时，官方推荐使用 Mailing list 的方式描述一遍你想要做的改动。 Mailing list 简单来说，就是一个邮件通知机制，所有的 Dubbo 开发者都会订阅该邮箱：dev@dubbo.incubator.apache.org。有任何新特性的改动，或者什么建议想要通知其他开发者，都可以通过向该邮箱发送邮件来达到这个目的，相同地，你也会收到其转发的其他开发者的邮件。 或者你是一个 Dubbo 的使用者，你想要得知开发者的改造方向，也可以订阅，这个 指南 可以帮助你订阅 Dubbo 的 Mailing list。 作为一个 modern developer，你可能觉得 mailing list 的交流方式存在滞后性，这样的沟通方式不是特别的高效，但它作为 apache 项目的推荐交流方式存在其特殊的原因，在此不多赘述。总之遵循一个原则：bug fix 或者讨论，可以在 github issue 中进行，影响较大的特性和讨论则推荐在 mailing list 中展开。 3 其他贡献形式不仅仅只有贡献代码，修复 bug 等行为才算作为开源做贡献，以下这些行为也属于主要形式： 3.1 撰写文档 Dubbo 文档 是其开源组成成分的重要一环，其内容源文件位于：https://github.com/apache/incubator-dubbo-website。同样也是一个 Git 仓库，任何你想要对 dubbo 知识点的补充，都可以在这儿提交 pull request，只需要一些 markdown 的语法知识，和一些可有可无的 npm 语法即可。如果你觉得贡献代码对于现在的自己仍然有点难度，不妨从贡献文档开始接触开源。 3.2 ISSUE无论是 Github 中的 Issue 还是 mailing list 中的讨论，无论是提出问题，汇报 bug，还是回答问题（bugfix 则不仅仅需要 Issue 了），协助管理者 review pull request，都是贡献的一种形式，勿以善小而不为。 3.3 其他行为任何你能够想到的，可以帮助开源项目变得更好的的行为，都属于开源贡献。例如，给每个 Issue 打上合适的 tag，关闭重复的 Issue，链接相关联的 Issue，线下组织沙龙，回答 Stack Overflow 上相关的问题，以及文档中一个错别字的修改等等。 4 开源最佳实践4.1 有效沟通无论你处于什么样的目的：仅仅是一次性的贡献，亦或是永久性的加入社区，都的和他人进行沟通和交往，这是你要在开源圈发展必须修炼的技能。 在你开启一个 isse 或 PR 之前，或者是在聊天室问问题之前，请牢记下面所列出的几点建议，会让你的工作更加的高效。 ** 给出上下文 ** 以便于让其他人能够快速的理解。比方说你运行程序时遇到一个错误，要解释你是如何做的，并描述如何才能再现错误现象。又比方说你是提交一个新的想法，要解释你为什么这么想，对于项目有用处吗（不仅仅是只有你！） 😇 “当我做 Y 的时候 X 不能工作” 😢 “X 出问题! 请修复它。” ** 在进一步行动前，做好准备工作。** 不知道没关系，但是要展现你尝试过、努力过。在寻求帮助之前，请确认阅读了项目的 README、文档、问题（开放的和关闭的）、邮件列表，并搜索了网络。当你表现出很强烈的求知欲的时候，人们是非常欣赏这点的，会很乐意的帮助你。 😇 “我不确定 X 是如何实现的，我查阅了相关的帮助文档，然而毫无所获。” 😢 “我该怎么做 X ?” ** 保持请求内容短小而直接。** 正如发送一份邮件，每一次的贡献，无论是多么的简单，都是需要他人去查阅的。很多项目都是请求的人多，提供帮助的人少。相信我，保持简洁，你能得到他人帮助的机会会大大的增加。 😇 “我很乐意写 API 教程。” 😢 ” 有一天我驾驶汽车行驶在高速公路上，在某个加油站加油的时候，突发奇想，我们应该这么做，不过在我进一步解释之前，我先和大家展示一下。。。” ** 让所有的沟通都是在公开场合下进行。** 哪怕是很不起眼的小事，也不要去给维护者发私信，除非是你要分享一些敏感信息（诸如安全问题或严重的过失）。你若能够保持谈话是公开的，很多人可以你们交换的意见中学习和受益。 😇 (评论) “@维护者 你好！我们该如何处理这个 PR？” 😢 (邮件) “你好，非常抱歉给发信，但是我实在很希望你能看一下我提交的 PR。” ** 大胆的提问（但是要谨慎！）。** 每个人参与社区，开始的时候都是新手，哪怕是非常有经验的贡献者也一样，在刚进入一个新的项目的时候，也是新手。出于同样的原因, 甚至长期维护人员并不总是熟悉一个项目的每一部分。给他们同样的耐心, 你也会得到同样的回报。 😇 “感谢查看了这个错误，我按照您的建议做了，这是输出结果。” 😢 “你为什么不修复我的问题？这难道不是你的项目吗？” ** 尊重社区的决定。** 你的想法可能会和社区的优先级、愿景等有差异，他们可能对于你的想法提供了反馈和最后的决定的理由，这时你应该去积极的讨论，并寻求妥协的办法，维护者必须慎重的考虑你的想法。但是如果你实在是不能同意社区的做法，你可以坚持自己！保持自己的分支，或者另起炉灶。 😇 “你不能支持我的用例，我蛮失望，但是你的解释仅仅是对一小部分用户起作用，我理解是为什么。感谢你的耐心倾听。” 😢 “你为什么不支持我的用例？这是不可接受的！” ** 以上几点，要铭记在心。** 开源是由来自世界各地的人们共同协作实现的。面临的问题是跨语言、跨文化、不同的地理为止、不同的时区，另外，撰写文字的沟通更是难上加难，无法传达语气和情绪。请让这些会话都充满善意吧！在以下情形中请保持礼貌：推动一个想法、请求更多的上下文、进一步澄清你的立场。既然你在互联网找到了自己的所需，那么请尝试让它变得更好！ 4.2 创建 issue你应该在遇到下列情况下，去创建一个 issue： 报告你自己无法解决的错误 讨论一个高级主题或想法 期望实现某新的特性，或者其它项目的想法 在 issue 的沟通中几点实用的技巧: ** 如果你刚好看到一个开放的 issue，恰是你打算解决的，** 添加评论，告诉他人你将对此展开工作，并及时响应。这样的话，可以避免他人重复劳动。 ** 如果说某个 issue 已经开放很久了，** 这可能是已经有人正在解决中，又或者是早已经解决过了，所以也请添加评论，在打算开始工作之前，最好是确认一下。 ** 如果你创建了一个 issue，但是没多久自己解决了，** 也要添加评论，让其他人知道，然后关闭该 issue。记录本身就是对社区的贡献。 4.3 创建 pull request在下面的情形时，请你务必使用 PR： 提交补丁 ( 例如，纠正拼写错误、损坏的链接、或者是其它较明显的错误） 开始一项别人请求的任务，或者是过去在 issue 中早就讨论过的 一个 PR 并不代表着工作已经完成。它通常是尽早的开启一个 PR，是为了其他人可以观看或者给作者反馈意见。只需要在子标题标记为“WIP”（正在进行中）。作者可以在后面添加很多评论。 如果说项目是托管在 GitHub 上的，以下是我们总结出的提交 RP 的建议： **Fork 代码仓库 ** 并克隆到本地，在本地的仓库配置“上游”为远端仓库。这样你可以在提交你的 PR 时保持和“上游”同步，会减少很多解决冲突的时间。(更多关于同步的说明，请参考 这里.) ** 创建一个分支 ** 用于自己编辑。 ** 参考任何相关的 issue** 或者在你的 RP 中支持文档 (比如. “Closes #37.”) ** 包含之前和之后的快照 ** 如果你的改动是包含了不同的 HTML/CSS。在你的 PR 中拖拉相应的图片。 ** 测试你的改动！** 若测试用例存在的话，跑一遍，以覆盖你的更改，若没有的话，则创建相应的用例。无论测试是否存在，一定要确保你的改动不会破坏掉现有的项目。 ** 和项目现有的风格保持一致 ** 尽你最大的努力，这也就是意味着在使用缩进、分号、以及注释很可能和你自己的风格大相径庭，但是为了节省维护者的精力，以及未来他人更好的理解和维护，还请你容忍一下。 5 成为一个开源贡献者如果你有志于参与开源事业，可以尝试从自己最熟悉的项目开始，开源并不是属于高级开发者的专属词汇，它就是由你我这样的人在需求，修复，构建中演进下去的。Let’s try it ! ** 参考资料 ** 如何为开源做贡献 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/contribute-to-opensource/"},{"title":"一致性哈希负载均衡算法的探讨","text":"前言一致性哈希算法在很多领域有应用，例如分布式缓存领域的 MemCache，Redis，负载均衡领域的 Nginx，各类 RPC 框架。不同领域场景不同，需要顾及的因素也有所差异，本文主要讨论在 ** 负载均衡 ** 中一致性哈希算法的设计。 在介绍一致性哈希算法之前，我将会介绍一些哈希算法，讨论它们的区别和使用场景。也会给出一致性哈希算法的 Java 通用实现，可以直接引用，文末会给出 github 地址。 友情提示：阅读本文前，最好对一致性哈希算法有所了解，例如你最好听过一致性哈希环这个概念，我会在基本概念上缩短篇幅。 一致性哈希负载均衡介绍负载均衡这个概念可以抽象为：从 n 个候选服务器中选择一个进行通信的过程。负载均衡算法有多种多样的实现方式：随机、轮询、最小负载优先等，其中也包括了今天的主角：一致性哈希负载均衡。一致性哈希负载均衡需要保证的是“相同的请求尽可能落到同一个服务器上”，注意这短短的一句描述，却包含了相当大的信息量。“相同的请求” — 什么是相同的请求？一般在使用一致性哈希负载均衡时，需要指定一个 key 用于 hash 计算，可能是： 请求方 IP 请求服务名称，参数列表构成的串 用户 ID “尽可能” —为什么不是一定？因为服务器可能发生上下线，所以少数服务器的变化不应该影响大多数的请求。这也呼应了算法名称中的“一致性”。 同时，一个优秀的负载均衡算法还有一个隐性要求：流量尽可能均匀分布。 综上所述，我们可以概括出一致性哈希负载均衡算法的设计思路。 尽可能保证每个服务器节点均匀的分摊流量 尽可能保证服务器节点的上下线不影响流量的变更 哈希算法介绍哈希算法是一致性哈希算法中重要的一个组成部分，你可以借助 Java 中的 int hashCode() 去理解它。 说到哈希算法，你想到了什么？Jdk 中的 hashCode、SHA-1、MD5，除了这些耳熟能详的哈希算法，还存在很多其他实现，详见 HASH 算法一览。可以将他们分成三代： 第一代：SHA-1（1993），MD5（1992），CRC（1975），Lookup3（2006） 第二代：MurmurHash（2008） 第三代：CityHash， SpookyHash（2011） 这些都可以认为是广义上的哈希算法，你可以在 wiki 百科 中查看所有的哈希算法。当然还有一些哈希算法如：Ketama，专门为一致性哈希算法而设计。 既然有这么多哈希算法，那必然会有人问：当我们在讨论哈希算法时，我们再考虑哪些东西？我大概总结下有以下四点： 实现复杂程度 分布均匀程度 哈希碰撞概率 性能 先聊聊性能，是不是性能越高就越好呢？你如果有看过我曾经的文章 《该如何设计你的 PasswordEncoder?》 ，应该能了解到，在设计加密器这个场景下，慢 hash 算法反而有优势；而在负载均衡这个场景下，安全性不是需要考虑的因素，所以性能自然是越高越好。 优秀的算法通常比较复杂，但不足以构成评价标准，有点黑猫白猫论，所以 2，3 两点：分布均匀程度，哈希碰撞概率成了主要考虑的因素。 我挑选了几个值得介绍的哈希算法，重点介绍下。 MurmurHash 算法：高运算性能，低碰撞率，由 Austin Appleby 创建于 2008 年，现已应用到 Hadoop、libstdc++、nginx、libmemcached 等开源系统。2011 年 Appleby 被 Google 雇佣，随后 Google 推出其变种的 CityHash 算法。官方只提供了 C 语言的实现版本。 Java 界中 Redis，Memcached，Cassandra，HBase，Lucene 都在使用它。 在 Java 的实现，Guava 的 Hashing 类里有，上面提到的 Jedis，Cassandra 里都有相关的 Util 类。 FNV 算法：全名为 Fowler-Noll-Vo 算法，是以三位发明人 Glenn Fowler，Landon Curt Noll，Phong Vo 的名字来命名的，最早在 1991 年提出。 特点和用途：FNV 能快速 hash 大量数据并保持较小的冲突率，它的高度分散使它适用于 hash 一些非常相近的字符串，比如 URL，hostname，文件名，text，IP 地址等。 Ketama 算法：将它称之为哈希算法其实不太准确，称之为一致性哈希算法可能更为合适，其他的哈希算法有通用的一致性哈希算法实现，只不过是替换了哈希方式而已，但 Ketama 是一整套的流程，我们将在后面介绍。 以上三者都是最合适的一致性哈希算法的强力争夺者。 一致性哈希算法实现 一致性哈希的概念我不做赘述，简单介绍下这个负载均衡中的一致性哈希环。首先将服务器（ip+ 端口号）进行哈希，映射成环上的一个节点，在请求到来时，根据指定的 hash key 同样映射到环上，并顺时针选取最近的一个服务器节点进行请求（在本图中，使用的是 userId 作为 hash key）。 当环上的服务器较少时，即使哈希算法选择得当，依旧会遇到大量请求落到同一个节点的问题，为避免这样的问题，大多数一致性哈希算法的实现度引入了虚拟节点的概念。 在上图中，只有两台物理服务器节点：11.1.121.1 和 11.1.121.2，我们通过添加后缀的方式，克隆出了另外三份节点，使得环上的节点分布的均匀。一般来说，物理节点越多，所需的虚拟节点就越少。 介绍完了一致性哈希换，我们便可以对负载均衡进行建模了： 123public interface LoadBalancer { Server select(List&lt;Server&gt; servers, Invocation invocation);} 下面直接给出通用的算法实现： 12345678910111213141516171819202122232425262728293031323334353637public class ConsistentHashLoadBalancer implements LoadBalancer{ private HashStrategy hashStrategy = new JdkHashCodeStrategy(); private final static int VIRTUAL_NODE_SIZE = 10; private final static String VIRTUAL_NODE_SUFFIX = &quot;&amp;&amp;&quot;; @Override public Server select(List&lt;Server&gt; servers, Invocation invocation) { int invocationHashCode = hashStrategy.getHashCode(invocation.getHashKey()); TreeMap&lt;Integer, Server&gt; ring = buildConsistentHashRing(servers); Server server = locate(ring, invocationHashCode); return server; } private Server locate(TreeMap&lt;Integer, Server&gt; ring, int invocationHashCode) { // 向右找到第一个 key Map.Entry&lt;Integer, Server&gt; locateEntry = ring.ceilingEntry(invocationHashCode); if (locateEntry == null) { // 想象成一个环，超过尾部则取第一个 key locateEntry = ring.firstEntry(); } return locateEntry.getValue(); } private TreeMap&lt;Integer, Server&gt; buildConsistentHashRing(List&lt;Server&gt; servers) { TreeMap&lt;Integer, Server&gt; virtualNodeRing = new TreeMap&lt;&gt;(); for (Server server : servers) { for (int i = 0; i &lt; VIRTUAL_NODE_SIZE; i++) { // 新增虚拟节点的方式如果有影响，也可以抽象出一个由物理节点扩展虚拟节点的类 virtualNodeRing.put(hashStrategy.getHashCode(server.getUrl() + VIRTUAL_NODE_SUFFIX + i), server); } } return virtualNodeRing; }} 对上述的程序做简单的解读： Server 是对服务器的抽象，一般是 ip+port 的形式。 123public class Server { private String url;} Invocation 是对请求的抽象，包含一个用于 hash 的 key。 123public class Invocation { private String hashKey;} 使用 TreeMap 作为一致性哈希环的数据结构，ring.ceilingEntry 可以获取环上最近的一个节点。在 buildConsistentHashRing 之中包含了构建一致性哈希环的过程，默认加入了 10 个虚拟节点。 计算方差，标准差的公式： 123456789101112131415161718192021222324252627282930313233public class StatisticsUtil { // 方差 s^2=[(x1-x)^2 +...(xn-x)^2]/n public static double variance(Long[] x) { int m = x.length; double sum = 0; for (int i = 0; i &lt; m; i++) {// 求和 sum += x[i]; } double dAve = sum / m;// 求平均值 double dVar = 0; for (int i = 0; i &lt; m; i++) {// 求方差 dVar += (x[i] - dAve)* (x[i] - dAve); } return dVar / m; } // 标准差σ=sqrt(s^2) public static double standardDeviation(Long[] x) { int m = x.length; double sum = 0; for (int i = 0; i &lt; m; i++) {// 求和 sum += x[i]; } double dAve = sum / m;// 求平均值 double dVar = 0; for (int i = 0; i &lt; m; i++) {// 求方差 dVar += (x[i] - dAve)* (x[i] - dAve); } return Math.sqrt(dVar / m); }} 其中，HashStrategy 是下文中重点讨论的一个内容，他是对 hash 算法的抽象，我们将会着重对比各种 hash 算法给测评结果带来的差异性。 123public interface HashStrategy { int getHashCode(String origin);} 测评程序前面我们已经明确了一个优秀的一致性哈希算法的设计思路。这一节我们给出实际的量化指标：假设 m 次请求打到 n 个候选服务器上 统计每个服务节点收到的流量，计算方差、标准差。测量流量分布均匀情况，我们可以模拟 10000 个随机请求，打到 100 个指定服务器，测试最后个节点的方差，标准差。 记录 m 次请求落到的服务器节点，下线 20% 的服务器，重放流量，统计 m 次请求中落到跟原先相同服务器的概率。测量节点上下线的情况，我们可以模拟 10000 个随机请求，打到 100 个指定服务器，之后下线 20 个服务器并重放流量，统计请求到相同服务器的比例。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class LoadBalanceTest { static String[] ips = {...}; // 100 台随机 ip /** * 测试分布的离散情况 */ @Test public void testDistribution() { List&lt;Server&gt; servers = new ArrayList&lt;&gt;(); for (String ip : ips) { servers.add(new Server(ip+&quot;:8080&quot;)); } LoadBalancer chloadBalance = new ConsistentHashLoadBalancer(); // 构造 10000 随机请求 List&lt;Invocation&gt; invocations = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) { invocations.add(new Invocation(UUID.randomUUID().toString())); } // 统计分布 AtomicLongMap&lt;Server&gt; atomicLongMap = AtomicLongMap.create(); for (Server server : servers) { atomicLongMap.put(server, 0); } for (Invocation invocation : invocations) { Server selectedServer = chloadBalance.select(servers, invocation); atomicLongMap.getAndIncrement(selectedServer); } System.out.println(StatisticsUtil.variance(atomicLongMap.asMap().values().toArray(new Long[]{}))); System.out.println(StatisticsUtil.standardDeviation(atomicLongMap.asMap().values().toArray(new Long[]{}))); } /** * 测试节点新增删除后的变化程度 */ @Test public void testNodeAddAndRemove() { List&lt;Server&gt; servers = new ArrayList&lt;&gt;(); for (String ip : ips) { servers.add(new Server(ip)); } List&lt;Server&gt; serverChanged = servers.subList(0, 80); ConsistentHashLoadBalancer chloadBalance = new ConsistentHashLoadBalancer(); // 构造 10000 随机请求 List&lt;Invocation&gt; invocations = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) { invocations.add(new Invocation(UUID.randomUUID().toString())); } int count = 0; for (Invocation invocation : invocations) { Server origin = chloadBalance.select(servers, invocation); Server changed = chloadBalance.select(serverChanged, invocation); if (origin.getUrl().equals(changed.getUrl())) count++; } System.out.println(count / 10000D); } 不同哈希算法的实现及测评最简单、经典的 hashCode 实现： 123456public class JdkHashCodeStrategy implements HashStrategy { @Override public int getHashCode(String origin) { return origin.hashCode(); }} FNV1_32_HASH 算法实现： 1234567891011121314151617181920public class FnvHashStrategy implements HashStrategy { private static final long FNV_32_INIT = 2166136261L; private static final int FNV_32_PRIME = 16777619; @Override public int getHashCode(String origin) { final int p = FNV_32_PRIME; int hash = (int) FNV_32_INIT; for (int i = 0; i &lt; origin.length(); i++) hash = (hash ^ origin.charAt(i)) * p; hash += hash &lt;&lt; 13; hash ^= hash &gt;&gt; 7; hash += hash &lt;&lt; 3; hash ^= hash &gt;&gt; 17; hash += hash &lt;&lt; 5; hash = Math.abs(hash); return hash; }} CRC 算法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class CRCHashStrategy implements HashStrategy { private static final int LOOKUP_TABLE[] = {0x0000, 0x1021, 0x2042, 0x3063, 0x4084, 0x50A5, 0x60C6, 0x70E7, 0x8108, 0x9129, 0xA14A, 0xB16B, 0xC18C, 0xD1AD, 0xE1CE, 0xF1EF, 0x1231, 0x0210, 0x3273, 0x2252, 0x52B5, 0x4294, 0x72F7, 0x62D6, 0x9339, 0x8318, 0xB37B, 0xA35A, 0xD3BD, 0xC39C, 0xF3FF, 0xE3DE, 0x2462, 0x3443, 0x0420, 0x1401, 0x64E6, 0x74C7, 0x44A4, 0x5485, 0xA56A, 0xB54B, 0x8528, 0x9509, 0xE5EE, 0xF5CF, 0xC5AC, 0xD58D, 0x3653, 0x2672, 0x1611, 0x0630, 0x76D7, 0x66F6, 0x5695, 0x46B4, 0xB75B, 0xA77A, 0x9719, 0x8738, 0xF7DF, 0xE7FE, 0xD79D, 0xC7BC, 0x48C4, 0x58E5, 0x6886, 0x78A7, 0x0840, 0x1861, 0x2802, 0x3823, 0xC9CC, 0xD9ED, 0xE98E, 0xF9AF, 0x8948, 0x9969, 0xA90A, 0xB92B, 0x5AF5, 0x4AD4, 0x7AB7, 0x6A96, 0x1A71, 0x0A50, 0x3A33, 0x2A12, 0xDBFD, 0xCBDC, 0xFBBF, 0xEB9E, 0x9B79, 0x8B58, 0xBB3B, 0xAB1A, 0x6CA6, 0x7C87, 0x4CE4, 0x5CC5, 0x2C22, 0x3C03, 0x0C60, 0x1C41, 0xEDAE, 0xFD8F, 0xCDEC, 0xDDCD, 0xAD2A, 0xBD0B, 0x8D68, 0x9D49, 0x7E97, 0x6EB6, 0x5ED5, 0x4EF4, 0x3E13, 0x2E32, 0x1E51, 0x0E70, 0xFF9F, 0xEFBE, 0xDFDD, 0xCFFC, 0xBF1B, 0xAF3A, 0x9F59, 0x8F78, 0x9188, 0x81A9, 0xB1CA, 0xA1EB, 0xD10C, 0xC12D, 0xF14E, 0xE16F, 0x1080, 0x00A1, 0x30C2, 0x20E3, 0x5004, 0x4025, 0x7046, 0x6067, 0x83B9, 0x9398, 0xA3FB, 0xB3DA, 0xC33D, 0xD31C, 0xE37F, 0xF35E, 0x02B1, 0x1290, 0x22F3, 0x32D2, 0x4235, 0x5214, 0x6277, 0x7256, 0xB5EA, 0xA5CB, 0x95A8, 0x8589, 0xF56E, 0xE54F, 0xD52C, 0xC50D, 0x34E2, 0x24C3, 0x14A0, 0x0481, 0x7466, 0x6447, 0x5424, 0x4405, 0xA7DB, 0xB7FA, 0x8799, 0x97B8, 0xE75F, 0xF77E, 0xC71D, 0xD73C, 0x26D3, 0x36F2, 0x0691, 0x16B0, 0x6657, 0x7676, 0x4615, 0x5634, 0xD94C, 0xC96D, 0xF90E, 0xE92F, 0x99C8, 0x89E9, 0xB98A, 0xA9AB, 0x5844, 0x4865, 0x7806, 0x6827, 0x18C0, 0x08E1, 0x3882, 0x28A3, 0xCB7D, 0xDB5C, 0xEB3F, 0xFB1E, 0x8BF9, 0x9BD8, 0xABBB, 0xBB9A, 0x4A75, 0x5A54, 0x6A37, 0x7A16, 0x0AF1, 0x1AD0, 0x2AB3, 0x3A92, 0xFD2E, 0xED0F, 0xDD6C, 0xCD4D, 0xBDAA, 0xAD8B, 0x9DE8, 0x8DC9, 0x7C26, 0x6C07, 0x5C64, 0x4C45, 0x3CA2, 0x2C83, 0x1CE0, 0x0CC1, 0xEF1F, 0xFF3E, 0xCF5D, 0xDF7C, 0xAF9B, 0xBFBA, 0x8FD9, 0x9FF8, 0x6E17, 0x7E36, 0x4E55, 0x5E74, 0x2E93, 0x3EB2, 0x0ED1, 0x1EF0,}; /** * Create a CRC16 checksum from the bytes. implementation is from * mp911de/lettuce, modified with some more optimizations * * @param bytes * @return CRC16 as integer value */ public static int getCRC16(byte[] bytes) { int crc = 0x0000; for (byte b : bytes) { crc = ((crc &lt;&lt; 8) ^ LOOKUP_TABLE[((crc &gt;&gt;&gt; 8) ^ (b &amp; 0xFF)) &amp; 0xFF]); } return crc &amp; 0xFFFF; } public static int getCRC16(String key) { return getCRC16(key.getBytes(Charset.forName(&quot;UTF-8&quot;))); } @Override public int getHashCode(String origin) { // optimization with modulo operator with power of 2 // equivalent to getCRC16(key) % 16384 return getCRC16(origin) &amp; (16384 - 1); }} Ketama 算法： 123456789101112131415161718192021222324252627282930313233343536public class KetamaHashStrategy implements HashStrategy { private static MessageDigest md5Digest; static { try { md5Digest = MessageDigest.getInstance(&quot;MD5&quot;); } catch (NoSuchAlgorithmException e) { throw new RuntimeException(&quot;MD5 not supported&quot;, e); } } @Override public int getHashCode(String origin) { byte[] bKey = computeMd5(origin); long rv = ((long) (bKey[3] &amp; 0xFF)&lt;&lt; 24) | ((long) (bKey[2] &amp; 0xFF)&lt;&lt; 16) | ((long) (bKey[1] &amp; 0xFF)&lt;&lt; 8) | (bKey[0] &amp; 0xFF); return (int) (rv &amp; 0xffffffffL); } /** * Get the md5 of the given key. */ public static byte[] computeMd5(String k) { MessageDigest md5; try { md5 = (MessageDigest) md5Digest.clone(); } catch (CloneNotSupportedException e) { throw new RuntimeException(&quot;clone of MD5 not supported&quot;, e); } md5.update(k.getBytes()); return md5.digest(); }} MurmurHash 算法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MurmurHashStrategy implements HashStrategy { @Override public int getHashCode(String origin) { ByteBuffer buf = ByteBuffer.wrap(origin.getBytes()); int seed = 0x1234ABCD; ByteOrder byteOrder = buf.order(); buf.order(ByteOrder.LITTLE_ENDIAN); long m = 0xc6a4a7935bd1e995L; int r = 47; long h = seed ^ (buf.remaining() * m); long k; while (buf.remaining() &gt;= 8) { k = buf.getLong(); k *= m; k ^= k &gt;&gt;&gt; r; k *= m; h ^= k; h *= m; } if (buf.remaining() &gt; 0) { ByteBuffer finish = ByteBuffer.allocate(8).order( ByteOrder.LITTLE_ENDIAN); // for big-endian version, do this first: // finish.position(8-buf.remaining()); finish.put(buf).rewind(); h ^= finish.getLong(); h *= m; } h ^= h &gt;&gt;&gt; r; h *= m; h ^= h &gt;&gt;&gt; r; buf.order(byteOrder); return (int) (h &amp; 0xffffffffL); }} 测评结果： 方差 标准差 不变流量比例 JdkHashCodeStrategy 29574.08 171.97 0.6784 CRCHashStrategy 3013.02 54.89 0.7604 FnvHashStrategy 961.64 31.01 0.7892 KetamaHashStrategy 1254.64 35.42 0.7986 MurmurHashStrategy 815.72 28.56 0.7971 其中方差和标准差反映了均匀情况，越低越好，可以发现 MurmurHashStrategy，KetamaHashStrategy，FnvHashStrategy 都表现的不错。 不变流量比例体现了服务器上下线对原有请求的影响程度，不变流量比例越高越高，可以发现 KetamaHashStrategy 和 MurmurHashStrategy 表现最为优秀。 我并没有对小集群，小流量进行测试，样本偏差性较大，仅从这个常见场景来看，MurmurHashStrategy 是一个不错的选择，多次测试后发现 FnvHashStrategy，KetamaHashStrategy，MurmurHashStrategy 差距不是很大。 至于性能测试，MurmurHash 也十分的高性能，我并没有做测试（感兴趣的同学可以对几种 strategy 用 JMH 测评一下）, 这里我贴一下 MurmurHash 官方的测评数据： OneAtATime - 354.163715 mb/sec FNV - 443.668038 mb/sec SuperFastHash - 985.335173 mb/sec lookup3 - 988.080652 mb/sec MurmurHash 1.0 - 1363.293480 mb/sec MurmurHash 2.0 - 2056.885653 mb/sec 扩大虚拟节点可以明显降低方差和标准差，但虚拟节点的增加会加大内存占用量以及计算量 Ketama 一致性哈希算法实现Ketama 算法有其专门的配套实现方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class KetamaConsistentHashLoadBalancer implements LoadBalancer { private static MessageDigest md5Digest; static { try { md5Digest = MessageDigest.getInstance(&quot;MD5&quot;); } catch (NoSuchAlgorithmException e) { throw new RuntimeException(&quot;MD5 not supported&quot;, e); } } private final static int VIRTUAL_NODE_SIZE = 12; private final static String VIRTUAL_NODE_SUFFIX = &quot;-&quot;; @Override public Server select(List&lt;Server&gt; servers, Invocation invocation) { long invocationHashCode = getHashCode(invocation.getHashKey()); TreeMap&lt;Long, Server&gt; ring = buildConsistentHashRing(servers); Server server = locate(ring, invocationHashCode); return server; } private Server locate(TreeMap&lt;Long, Server&gt; ring, Long invocationHashCode) { // 向右找到第一个 key Map.Entry&lt;Long, Server&gt; locateEntry = ring.ceilingEntry(invocationHashCode); if (locateEntry == null) { // 想象成一个环，超过尾部则取第一个 key locateEntry = ring.firstEntry(); } return locateEntry.getValue(); } private TreeMap&lt;Long, Server&gt; buildConsistentHashRing(List&lt;Server&gt; servers) { TreeMap&lt;Long, Server&gt; virtualNodeRing = new TreeMap&lt;&gt;(); for (Server server : servers) { for (int i = 0; i &lt; VIRTUAL_NODE_SIZE / 4; i++) { byte[] digest = computeMd5(server.getUrl() + VIRTUAL_NODE_SUFFIX + i); for (int h = 0; h &lt; 4; h++) { Long k = ((long) (digest[3 + h * 4] &amp; 0xFF)&lt;&lt; 24) | ((long) (digest[2 + h * 4] &amp; 0xFF)&lt;&lt; 16) | ((long) (digest[1 + h * 4] &amp; 0xFF)&lt;&lt; 8) | (digest[h * 4] &amp; 0xFF); virtualNodeRing.put(k, server); } } } return virtualNodeRing; } private long getHashCode(String origin) { byte[] bKey = computeMd5(origin); long rv = ((long) (bKey[3] &amp; 0xFF)&lt;&lt; 24) | ((long) (bKey[2] &amp; 0xFF)&lt;&lt; 16) | ((long) (bKey[1] &amp; 0xFF)&lt;&lt; 8) | (bKey[0] &amp; 0xFF); return rv; } private static byte[] computeMd5(String k) { MessageDigest md5; try { md5 = (MessageDigest) md5Digest.clone(); } catch (CloneNotSupportedException e) { throw new RuntimeException(&quot;clone of MD5 not supported&quot;, e); } md5.update(k.getBytes()); return md5.digest(); }} 稍微不同的地方便在于：Ketama 将四个节点标为一组进行了虚拟节点的设置。 方差 标准差 不变流量比例 KetamaConsistentHashLoadBalancer 911.08 30.18 0.7936 实际结果并没有太大的提升，可能和测试数据的样本规模有关。 总结优秀的哈希算法和一致性哈希算法可以帮助我们在大多数场景下应用的高性能，高稳定性，但在实际使用一致性哈希负载均衡的场景中，最好针对实际的集群规模和请求哈希方式进行压测，力保流量均匀打到所有的机器上，这才是王道。 不仅仅是分布式缓存，负载均衡等等有限的场景，一致性哈希算法、哈希算法，尤其是后者，是一个用处很广泛的常见算法，了解它的经典实现是很有必要的，例如 MurmurHash，在 guava 中就有其 Java 实现，当需要高性能，分布均匀，碰撞概率小的哈希算法时，可以考虑使用它。 本文代码的 github 地址：https://github.com/lexburner/consistent-hash-algorithm 扩展阅读深入理解 RPC 之集群篇 《该如何设计你的 PasswordEncoder?》 参考文章MurmurHash memcached Java 客户端 spymemcached 的一致性 Hash 算法","link":"/consistent-hash-lb/"},{"title":"DevOps 的八荣八耻","text":"前言被群里的好友安利了一发，周日跑去参加了一个技术讲座《云上开发与运维最佳实践》，听完两个人的演讲之后才发现主题竟然是讲运维，好在有一个人干货不少，在此记录下所得。简单追溯了一下这个 DevOps 才发现并不是一个新的概念，早在 2010 年就能看到有相关的人在追捧这个概念了。DevOps 就是开发（Development）和运维（Operations）这两个领域的合并。（如果没错的话，DevOps 还包括产品管理、QA、winces 甚至销售等领域）。这种理念和现如今流行的微服务架构以及分布式特性的相关理念不谋而合。这篇文章主要就是转载记录了当时又拍云运维总监的演讲稿。 DevOps 的八荣八耻 DevOps 这个思想提出来已经五六年了，一直都是呼声很高，落地很难，为什么呢？这可能与各个公司的业务情况和技术发展路线有或多或少的关系，比如说创业的最早技术合伙人是运维出身或者技术出身，但是水平不高，为了公司持续发展，引入新鲜血液时，就会存在技术的先进性跟解决遗留烂摊子的矛盾。又或者业务本身偏向于用户，导致技术被边缘化，产品又没有好的架构，限制了快速发展等；所以，DevOps 的推进一定要自上而下，凭借挑战自我，颠覆传统的勇气才能去落实。 以可配置为荣，以硬编码为耻 △ 以可配置为荣，以硬编码为耻 hardcoding 一时爽，真正要做改动时，需要定位代码，做出调整，甚至可能会破坏功能。以下可以说是配置的一个进化史 • 本地配置, 程序⽣生成 (txt/ini/cfg)• 集中配置, 动态⽣生成 (Yaml/Json)• 环境变量量 (代码⽆无侵⼊入 &amp; 语⾔言⽆无关性)• 服务⾃自动发现,⾃自动注册 (zookeeper/consul) 以互备为荣，以单点为耻 △ 以互备为荣，以单点为耻 互容互备一直是优良架构的设计重点。 又拍云早期做架构设计，使用了 LVS+Keeplived+VRRP 做转换，这样可以方便负载均衡，动态升级，隔离故障。现在的又拍云第二代，已经在部分大节点使用 OSPF 和 Quagga 做等价路由的负载均衡和冗余保障。 Nginx 可以加 Haproxy 或 LVS 做负载均衡。MySQL 可以做主从切换，或者是 MMM 的高可用成熟解决方案。我们的消息队列之前用 rabbitmq 做，现在主要是 redis 和 kafka 集群化，其中 kafka 已经迁到了 Mesos 容器平台里。 服务的自动发现、注册，我们可以使用 consul、etcd、doozer（Heroku 公司产品），还有 zookeeper。主要区别是算法不一样，zookeeper 用的是 paxos 算法，而 consul 用的是 raft 算法。目前看来 consul 比较流行，因为 consul 的自动发现和自动注册更加容易使用。etcd 主要是 CoreOS 在主推，CoreOS 本身就是一个滚动发布的针对分布式部署的操作系统，大家可以去关注一下它。还有一个是 hadoop 和 elk，大数据平台的可扩展性是标配，很容易互备。 上面是举了一些常见互备的软件组件的造型，那我们如何是设计一个无单点的架构呢？主要掌握以下几点： 无状态 无状态意味着没有竞争，很容易做负载均衡，负载均衡的方式有很多种，F5，LVS，Haproxy，总能找到一种适合你的方式。 无共享 以前我们很喜欢用内存来保持临时信息，如进程间的交换，这种方式虽然效率很高，但是对程序的扩展性没什么好处，尤其是现在的互联网体量，光靠单机或者高性能机器是明显玩不转的。所以我们现在就需要使用类似消息队列的组件，把数据共享出去，利用多台机器把负载给承担下来。 松耦合 / 异步处理 以前我们用 Gearman 这样的任务框架。大家可以把任务丢进任务池里，生成多个消费者去取任务。当我的消费不够用时，可以平滑增加我的 work 资源，让他从更快的去拿任务。运维平台这边以 python/celery 的组合使用更多。 分布式 / 集群协作 像 Hadoop 这样的天生大数据 / 数据仓库解决方案，由于先前设计比较成熟，一般都是通过很多台机器扩容来实现 map/reduce 的扩展计算能力。 以随时重启为荣，以不能迁移为耻 △ 以随时重启为荣，以不能迁移为耻 关于这个点，我们讲三个方面： 1.Pet 到 Cow 观念的转变 以前我们说机器是 pet，也就是宠物模式，然后花了几万块钱去买的服务器，当宝一般供奉。但事实上却并不是这样，任何电子设备、服务器只要一上线，便开始了一个衰老的过程，你根本不知道在运行过程中会发生什么事，比如说质量差的电容会老化爆浆，电子元器件在机房的恶劣环境里会加速损坏，这些变化都是我们无法参与控制的，所以无论我们怎么努力，都无法保障机器有多么的牢靠。 谷歌指出的 Cow 模式就是指农场模式。就是要把机器发生故障当做常态，打个比方，比如说这头牛死了，那我就不要了，因为我有很多这样的牛，或者是再拉一头新的牛。这就是我们软件开发和运维需要做的转变，去适应这种变化。 2.OpenStack 虚拟机的编排 虚拟化是个好东西，通过 OpenStack 我们很容易就可以做出一些存储或者迁移的操作，但是在实施的过程中，也是一波三折的。 又拍云从 2014 年开始在内部推动 OpenStack，当然我们也踩过 OpenStack 网络的坑，那时候我们用双千兆的卡做内网通讯，因为使用 OpenStack 实现虚拟化后，一切都变成了文件，在网络上传输的话，对网络的压力会非常大，结果就导致部分服务响应缓慢（因为本身就是实验性质，所以在硬件上没有足够投入，内测时也没有推广，所以影响不大）。 2015 年又拍云再上的 OpenStack，全部都用双万兆的网卡做 bonding，交换机也是做了端口聚合和堆叠。目前来说，只有云存储没有上线，其它云处理，云网络的使用还是能够满足要求。 3.Docker 的导入导出 Docker 是更轻量级的资源隔离和复用技术，从 2016 年开始，又拍云同时也在尝试使用 Mesos/Docker 来实现云处理的业务迁移。 以整体交付为荣，以部分交付为耻 △ 以整体交付为荣，以部分交付为耻 以往开发运维要安装一个机器，首先要去申请采购，购买完了还要等待运输，在运输中要花去一天的时间，之后还需要配交换机和网络。在这个过程中你会发现，简单的给开发配台机器，光上架就涉及到运维的很多环节，更不要说系统安装，优化，软件配置等剩余工作了，所以大多数情况下你只能做到部分交付。 要如何解决这些问题？通过 OpenStack 可以做到云计算、云网络、云存储这三块搭建完成之后，进行整体交付。 根据一些经验总结，在整个云平台当中，云存储的坑最多，云计算、云网络相对来说比较成熟。现在云计算的硬件基本上是基于英特尔 CPU 的虚拟化技术来硬件指令穿透的，损耗大概 2%～5%，这是可以接受的。至于云网络，刚才胡凯（B 站运维总监）提到内网包转发效率，我做过一个测试，在 OpenStack 的内网中，如果 MTU 默认是 1500，万兆网卡的转发率大概为 6.7xxGbps。后来我在优化的过程中，也翻查一些文档，看到的数据是可以达到 9.5xxGbps，通过不断的摸索，对比测试后发现，如果把内网的 MTU 搞成大包，如 9000 时，万兆网卡的存储量直接达到了 9.72Gbps 左右的。不过，这个 MTU 需要提前在宿主机上调整好，需要重启生效。所以，这个问题发现得越早越好，这样就可以做到统一调度，分配资源。 Docker 的好处是可以做到 Build、Shipand Run，一气呵成。无论是对开发，测试，还是运维来说，Docker 都是同一份 Dockerfile 清单，所以使用 Docker 在公司里的推动就很顺畅。虽然 OpenStack 也可以一站式交付，整体交付，使用时非常方便。但是对开发来说，他还是拿到一台机器，还是需要去安装软件环境，配置，上线，运行，除了得到机器快一些，对上线服务没有什么大的帮助，所以又拍云现在的 Openstack 集群一般对内申请开发测试用，外网生产环境还是以 Docker 容器化部署为主，这也是大家都喜闻乐见的方式，但前提是开发那边能够适应编写 Dockerfile（目前是我在内部推动这种变革，如新的项目就强制要求用 docker）。 以无状态为荣，以有状态为耻 △ 以无状态为荣，以有状态为耻 有状态的服务真的很麻烦，无论是存在数据库、磁盘开销，还有各种锁等资源的竞争，横向扩展也很差，不能重启，也不能互备。所以，有姿态的服务对于扩展原则来说，就是一场恶梦。如果是说我们解决这个问题，那就要使用解耦和负载均衡的方法去解决问题。 使用可靠的中间件 中间件其实最早出现在金融公司、证券公司，后来随着互联网行业不断壮大以后，就用一些高可靠性的号称工业级的消息队列出现，如 RabbitMQ，一出来以后，就把中间件拉下神坛。随着中间件民用化，互联网蓬勃发展，是可以把一些服务变成无状态，方便扩展。 公共资源池 我们可以通过各种云，容器云、弹性云，做计算单元的弹性扩展。 能够被计算 如果你不想存状态，那也可以被计算，比如说 Ceph 存储，它的创新在于每个数据块都是可计算出来的，这就类似无状态的，每次都算，反正现在的 cpu 都这么强悍了，所以，无状态是一个命题，在做架构的时候，你脑海里一定要有这个意念，然后再看你用什么样的方式开动脑筋，预先的跟开发，运维沟通好，把应用拆分成一种无状态的最佳组合。 以标准化为荣，以特殊化为耻 △ 以标准化为荣，以特殊化为耻 在标准化方面，我们在这几个方面改良： 统一输入输出 统一入口是我加入又拍云后做的第一件事情，我们用一个统一的文本，到现在也在用，然后推送到所有的边缘，服务器上面的组件，要用到的参数，都能从配置里读出来。代码管理方面我们也使用 git，git wiki，批量部署我们用 ansible（早在 2012 年，我做了一些比较后，就在公司里推行 ansible，看来还是很明智的决定）。 统一的流程管理 运维中使用 python 最多，所以我们使用了 yaml 和 playbook。又拍云有自己的跳板机，通过 VPN 登陆，目前我们也在试用一个带有审计功能的堡垒机，可以把每个人的操作录制下来，然后再去回放观察，改进我们的工作流程。 抽象底层设计和复用组件 如果是开发者的话，就会写很多的复用函数，对于优秀的运维人员来说，也要有优秀的抽象业务的能力，也要去做一些重复工作的复用准备，如频繁的，繁琐易出错的手工操作抽象成若干运维的脚本化。 最后是巧妙的利用虚拟化、容器服务、server-less 微服务，这些服务是可以被备份，还原的，可以保持一个相对稳定的状态，我们要拒绝多的特殊管理操作。香农 - 信息熵理论里说，变量的不确定性越大，熵就越大，把它搞清楚所需要的信息量也就越大。理论上来说，如果是一个孤立的系统，他就会变得越来越乱。 以自动化工具为荣，以手动和人肉为耻 △ 以自动化工具为荣，以手动和人肉为耻 又拍云早期，用的是 bash、sed、awk，因为我之前有搞嵌入式的背景和经验，对一个十几兆的嵌入式系统来说，上面是不可能有 python/perl/nodejs 等环境。所以我们把服务器批量安装，部署，上线，做成了嵌入式的系统后，只要点亮以后，运行一个硬件检测的程序，会把机器的 CPU、内存、硬盘大小等都打印出来，供货商截图给我看，这个机器是否合格。合格的机器可以直接发到机房去，在机器到了机房通上网线以后会有一个 ansibleplaybook 的推动。 自从用了这种方法以后，我们在公司里面基本上没有见到服务器，一般直接产线上检测通过后发到机房。然后又拍云的运维人员就可以连上去远程管理，在过去的三年里我们服务器平均每年翻了三倍，节点翻了六倍多，但是人手并没有增加。 关于 tgz、rpm、pkg 的打包部署，我们用的是 tgz 的打包及 docker 镜像。优势在于，又拍云自有 CDN 网络，软件通过推动到 CDN 网络下可以加速下发。 关于集成测试、自动测试的发布，像 ELK 集中日志的分析、大数据的分析，我们现在使用 ELK 以后，只要有基础的运维技术知识便可看懂，不需要高深的运维知识和脚本编辑知识，大多数人都可以完成这份工作，好处就是你多了好多眼睛帮你一起来发现问题，定位问题。 最后是不要图形，不要交互，不要终端。一旦有了图形以后，很难实现自动化。原则就是，不要手工 hack，最好是用程序生成程序的方式去完成这个步骤。 以无人值守为荣，以人工介入为耻 △ 以无人值守为荣，以人工介入为耻 运维部门要做的事情有三件： 运维自动化 要有一定的业务抽象能力，要有标准化的流程。没有好的自动化，就很难把运维的工作效率提升了，只要做好这些，就可以节省时间，从容应对业务增长。而且运维自动化的另一个好处就是运维不会因为人的喜怒哀乐而受到影响稳定性，比如说我今天心情不好，你让我装一台机器我还可以忍，你让我装十台一百台就不行了。但如果公司有了运维自动化的流程，这个事情就可以避免，因为谁做都一样。 监控要常态 2016 年年初，又拍云特别成立大数据分析部门，我们把日志做了采样收集和过滤，通过大数据平台做日志的同构数据分析，重点关注 4xx/5xx/2xx 比例，响应时间分析如 100 毫秒、200 毫秒、500 毫秒，还有区域性的速率分布，讲真，这真是一个好东西。 性能可视化 数据的有效展示。现在 ELK 对我们的帮助很大，从监控图上来看相关的数据指标，一目了然。这里就不反复赘述了。 DevOps 的本质最后，我们谈一谈 DevOps 的本质。 弹性 像亚马逊推云时，那个单词叫 elastic，意思是，你要能够扩展，如横向扩展；你要能负载均衡，如果你是基于 openstack/docker 资源池，你的资源就可以复用，可以编排回滚。比如说 OpenStack 有模板，我打一个镜像包，稍微重了一点，Docker 的就轻一点，Docker 可以做一个滚动发布，可以保留原来的程序、原来的容器，你可以做快速切换，这也是一种变化的弹性。 无关性 如果是虚拟化资源，一切都可以在模板里面设置，可以把底层的硬件、系统、网络抚平差异，比如说不管物理磁盘是 1T(市面上缺货)/4T/6T 的盘，都可以划分 100G 容量，所以当把一切变成按需申请的服务，无论是开发还是运维，工作都会比较简单，因为它的无关性。 不可变的基础设施 这个对传统运维可能是一种打击，因为基础镜像可能已经做的足够安全，足够完美，足够精干，不需要基础运维过多的人工参与。但我认为恰恰能帮助传统运维减轻工作量，反而有更多的精力去迎接虚拟化、容器化，SDN 的挑战，掌握了新技能后，就可以随取随用。","link":"/devops-1/"},{"title":"Java 文件 IO 操作之 DirectIO","text":"在前文《文件 IO 操作的一些最佳实践》中，我介绍了一些 Java 中常见的文件操作的接口，并且就 PageCache 和 DIrect IO 进行了探讨，最近我自己封装了一个 Direct IO 的库，趁着这个机会，本文重点谈谈 Java 中 Direct IO 的意义，以及简单介绍下我自己的轮子。 Java 中的 Direct IO如果你阅读过我之前的文章，应该已经了解 Java 中常用的文件操作接口为：FileChannel，并且没有直接操作 Direct IO 的接口。这也就意味着 Java 无法绕开 PageCache 直接对存储设备进行读写，但对于使用 Java 语言来编写的数据库，消息队列等产品而言，的确存在绕开 PageCache 的需求： PageCache 属于操作系统层面的概念，用户层面很难干预，User BufferCache 显然比 Kernel PageCache 要可控 现代操作系统会使用尽可能多的空闲内存来充当 PageCache，当操作系统回收 PageCache 内存的速度低于应用写缓存的速度时，会影响磁盘写入的速率，直接表现为写入 RT 增大，这被称之为“毛刺现象” PageCache 可能会好心办坏事，采用 Direct IO + 自定义内存管理机制会使得产品更加的可控，高性能。 Direct IO 的限制在 Java 中使用 Direct IO 最终需要调用到 c 语言的 pwrite 接口，并设置 O_DIRECT flag，使用 O_DIRECT 存在不少限制 操作系统限制：Linux 操作系统在 2.4.10 及以后的版本中支持 O_DIRECT flag，老版本会忽略该 Flag；Mac OS 也有类似于 O_DIRECT 的机制 用于传递数据的缓冲区，其内存边界必须对齐为 blockSize 的整数倍 用于传递数据的缓冲区，其传递数据的大小必须是 blockSize 的整数倍。 数据传输的开始点，即文件和设备的偏移量，必须是 blockSize 的整数倍 查看系统 blockSize 大小的方式：stat /boot/|grep “IO Block” ubuntu@VM-30-130-ubuntu:~$ stat /boot/|grep “IO Block” Size: 4096 Blocks: 8 IO Block: 4096 directory 通常为 4kb Java 使用 Direct IO项目地址https://github.com/lexburner/kdio 引入依赖12345&lt;dependency&gt; &lt;groupId&gt;moe.cnkirito.kdio&lt;/groupId&gt; &lt;artifactId&gt;kdio-core&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 注意事项12345// file path should be specific since the different file path determine whether your system support direct iopublic static DirectIOLib directIOLib = DirectIOLib.getLibForPath(&quot;/&quot;);// you should always write into your disk the Integer-Multiple of block size through direct io.// in most system, the block size is 4kbprivate static final int BLOCK_SIZE = 4 * 1024; Direct IO 写12345678910111213private static void write() throws IOException { if (DirectIOLib.binit) { ByteBuffer byteBuffer = DirectIOUtils.allocateForDirectIO(directIOLib, 4 * BLOCK_SIZE); for (int i = 0; i &lt; BLOCK_SIZE; i++) { byteBuffer.putInt(i); } byteBuffer.flip(); DirectRandomAccessFile directRandomAccessFile = new DirectRandomAccessFile(new File(&quot;./database.data&quot;), &quot;rw&quot;); directRandomAccessFile.write(byteBuffer, 0); } else { throw new RuntimeException(&quot;your system do not support direct io&quot;); }} Direct IO 读12345678910111213public static void read() throws IOException { if (DirectIOLib.binit) { ByteBuffer byteBuffer = DirectIOUtils.allocateForDirectIO(directIOLib, 4 * BLOCK_SIZE); DirectRandomAccessFile directRandomAccessFile = new DirectRandomAccessFile(new File(&quot;./database.data&quot;), &quot;rw&quot;); directRandomAccessFile.read(byteBuffer, 0); byteBuffer.flip(); for (int i = 0; i &lt; BLOCK_SIZE; i++) { System.out.print(byteBuffer.getInt() + &quot; &quot;); } } else { throw new RuntimeException(&quot;your system do not support direct io&quot;); }} 主要 API DirectIOLib.java 提供 Native 的 pwrite 和 pread DirectIOUtils.java 提供工具类方法，比如分配 Block 对齐的 ByteBuffer DirectChannel/DirectChannelImpl.java 提供对 fd 的 Direct 包装，提供类似 FileChannel 的读写 API。 DirectRandomAccessFile.java 通过 DIO 的方式打开文件，并暴露 IO 接口。 总结这个简单的 Direct IO 框架参考了 smacke/jaydio，这个库自己搞了一套 Buffer 接口跟 JDK 的类库不兼容，且读写实现里面加了一块 Buffer 用于缓存内容至 Block 对齐有点破坏 Direct IO 的语义。同时，感谢尘央同学的指导，这个小轮子的代码量并不多，初始代码引用自他的一个小 demo（已获得本人授权）。为什么需要这么一个库？主要是考虑后续会出现像「中间件性能挑战赛」和「PolarDB 性能挑战赛」这样的比赛，Java 本身的 API 可能不足以发挥其优势，如果有一个库可以屏蔽掉 Java 和 CPP 选手的差距，岂不是美哉？我也将这个库发到了中央仓库，方便大家在自己的代码中引用。 后续会视需求，会这个小小的轮子增加注入 fadvise，mmap 等系统调用的映射，也欢迎对文件操作感兴趣的同学一起参与进来，pull request &amp; issue are welcome！ 扩展阅读《文件 IO 操作的一些最佳实践》 《PolarDB 数据库性能大赛 Java 选手分享》 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。","link":"/direct-io/"},{"title":"分布式限流","text":"前言最近正在为本科论文的事感到心烦，一方面是在调研期间，发现大部分的本科论文都是以 MVC 为架构，如果是使用了 java 作为开发语言则又是千篇一律的在使用 SSH，二方面是自己想就微服务，分布式方面写一篇论文，讲述一些技术点的实现，和一些中间件的使用，看到如八股文般的模板格式.. 不免让人望文生怯。退一步，投入模板化 ssh-web 项目的怀抱，落入俗套，可以省去自己不少时间，因为在外实习，琐事并不少；进一步，需要投入大量时间精力去研究，而且不成体系，没有论文参考。 突然觉得写博客，比写论文爽多了，可以写自己想写的，记录自己最真实的想法。可能会逐渐将之前博客维护的自己的一些想法，纳入到本科论文中去。 经典限流算法 说回正题，补上之前分布式限流的实现。先介绍一些现有的限流方案。 核心的算法主要就是四种：A 类：计数器法，滑动窗口法B 类：令牌桶法，漏桶法 这里的四种算法通常都是在应用级别讨论的，这里不重复介绍这四种算法的实现思路了，只不过我人为的将他们分成了 A，B 两类。 A 类算法，是否决式限流。即如果系统设定限流方案是 1 分钟允许 100 次调用，那么真实请求 1 分钟调用 200 次的话，意味着超出的 100 次调用，得到的是空结果或者调用频繁异常。 B 类算法，是阻塞式限流。即如果系统设定限流方案是 1 分钟允许 100 次调用，那么真实请求 1 分钟调用 200 次的话，意味着超出的 100 次调用，会均匀安排到下一分钟返回。（当然 B 类算法，也可以立即返回失败，也可以达到否决式限流的效果） B 类算法，如 Guava 包提供的 RateLimiter，内部其实就是一个阻塞队列，达到阻塞限流的效果。然后分布式场景下，有一些思路悄悄的发生了变化。多个模块之间不能保证相互阻塞，共享的变量也不在一片内存空间中。为了使用阻塞限流的算法，我们不得不将统计流量放到 redis 一类的共享内存中，如果操作是一系列复合的操作，我们还不能使用 redis 自带的 CAS 操作 (CAS 操作只能保证单个操作的原子性) 或者使用中间件级别的队列来阻塞操作，显示加分布式锁的开销又是非常的巨大。最终选择放弃阻塞式限流，而在分布式场景下，仅仅使用 redis+lua 脚本的方式来达到分布式 - 否决式限流的效果。redis 执行 lua 脚本是一个单线程的行为，所以不需要显示加锁，这可以说避免了加锁导致的线程切换开销。 锁的演变下面记录一下这个设计的演变过程。 单体式应用中显示加锁首先还是回到单体应用中对共享变量进行 +1 的例子。 12345678910111213141516171819Integer count = 0;//sychronized 锁public synchronized void synchronizedIncrement(){ count++; }//juc 中的 lockLock lock = new ReentrantLock(); public void incrementByLock(){ lock.lock(); try{ count++; }finally { lock.unlock(); } } 用 synchronized 或者 lock 同步的方式进行统计，当单位时间内到达限定次数后否决执行。限制：单体应用下有效，分布式场景失效，显示加锁，开销大。 单体式应用中 CAS 操作 12345public AtomicInteger atomicInteger = new AtomicInteger(0);public increamt(){ atomicInteger.incrementAndGet();} 虽然没有显示加锁，但是 CAS 操作有一定的局限性，限流中不仅要对计数器进行 +1，而且还要记录时间段，所以复合操作，还是无法避免加锁。 分布式应用中显示加锁 1234567891011RedisDistributeLock lock = new RedisDistributeLock();public void incrementByLock(){ lock.lock(); try{ count++; }finally { lock.unlock(); }} 分布式阻塞锁的实现，可以参考我之前的博客。虽然能达到多个模块之间的同步，但还是开销过大。不得已时才会考虑使用。 redis+lua 脚本限流（最终方案） 12345678910111213local key = KEYS[1] -- 限流 KEY（一秒一个）local limit = tonumber(ARGV[1]) -- 限流大小local current = tonumber(redis.call('get', key) or &quot;0&quot;)if current + 1 &gt; limit then -- 如果超出限流大小 redis.call(&quot;INCRBY&quot;, key,&quot;1&quot;) -- 如果不需要统计真是访问量可以不加这行 return 0else -- 请求数 +1，并设置 2 秒过期 redis.call(&quot;INCRBY&quot;, key,&quot;1&quot;) if tonumber(ARGV[2]) &gt; -1 then redis.call(&quot;expire&quot;, key,tonumber(ARGV[2])) -- 时间窗口最大时间后销毁键 end return 1end lua 脚本返回值比较奇怪，用 java 客户端接受返回值，只能使用 Long，没有去深究。这个脚本只需要传入 key（url+ 时间戳 / 预设时间窗口大小），便可以实现限流。这里也贴下 java 中配套的工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596package sinosoftgz.apiGateway.utils;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.script.RedisScript;import org.springframework.util.Assert;import java.util.Arrays;/** * Created by xujingfeng on 2017/3/13. * &lt;p&gt; * 基于 redis lua 脚本的线程安全的计数器限流方案 * &lt;/p&gt; */public class RedisRateLimiter { /** * 限流访问的 url */ private String url; /** * 单位时间的大小, 最大值为 Long.MAX_VALUE - 1, 以秒为单位 */ final Long timeUnit; /** * 单位时间窗口内允许的访问次数 */ final Integer limit; /** * 需要传入一个 lua script, 莫名其妙 redisTemplate 返回值永远是个 Long */ private RedisScript&lt;Long&gt; redisScript; private RedisTemplate redisTemplate; /** * 配置键是否会过期， * true：可以用来做接口流量统计，用定时器去删除 * false：过期自动删除，时间窗口过小的话会导致键过多 */ private boolean isDurable = false; public void setRedisScript(RedisScript&lt;Long&gt; redisScript) { this.redisScript = redisScript; } public void setRedisTemplate(RedisTemplate redisTemplate) { this.redisTemplate = redisTemplate; } public String getUrl() { return url; } public void setUrl(String url) { this.url = url; } public boolean isDurable() { return isDurable; } public void setDurable(boolean durable) { isDurable = durable; } public RedisRateLimiter(Integer limit, Long timeUnit) { this.timeUnit = timeUnit; Assert.isTrue(timeUnit &lt; Long.MAX_VALUE - 1); this.limit = limit; } public RedisRateLimiter(Integer limit, Long timeUnit, boolean isDurable) { this(limit, timeUnit); this.isDurable = isDurable; } public boolean acquire() { return this.acquire(this.url); } public boolean acquire(String url) { StringBuffer key = new StringBuffer(); key.append(&quot;rateLimiter&quot;).append(&quot;:&quot;) .append(url).append(&quot;:&quot;) .append(System.currentTimeMillis() / 1000 / timeUnit); Integer expire = limit + 1; String convertExpire = isDurable ? &quot;-1&quot; : expire.toString(); return redisTemplate.execute(redisScript, Arrays.asList(key.toString()), limit.toString(), convertExpire).equals(1l); }} 由此可以见，分布式场景下，一个小小的统计次数的需求，如果真想在分布式下做到最完善，需要花很大的精力。","link":"/distribute-ratelimit/"},{"title":"构建多系统架构支持的 Docker 镜像","text":"前言陪伴了我 3 年的 Mac 在几个月前迎来了它的退休时刻，我将其置换成了公司新发的 Mac M1。对电子产品并不太感冒的我，并没有意识到 M1 是 ARM 架构的（除了个别软件的安装异常之外），显然，Mac M1 做的是不错的，我并没有太多吐槽它的机会。这也是我第一次近距离接触 ARM 架构的机会。 很快，在工作上，我遇到了第二次跟 ARM 打交道的机会。我们越来越多的客户，开始选择 ARM 架构的服务器作为 IaaS 层资源，这给我们的交付带来了一些工作量。适配工作中比较重要的一环便是 Docker 镜像，需要产出支持 ARM 架构的版本。 本文主要记录笔者在构建多系统架构支持的 Docker 镜像时的一些经验，以及一些个人的理解。 前置知识点CPU 架构主流的 CPU 架构就两类：x86 和 ARM。但在发展过程中，他们的命名并不一定都是如此。例如 amd64、x86_64 指的都是 x86 的 64 位架构，arm64v8、aarch64、arm64 指的都是 ARM 的 64 位架构。 在 docker hub 中，主流的镜像都列出了支持的架构，你也可以通过 Architectures 来进行镜像筛选。 docker buildx在 docker buildx 出现之前，我们只能通过 docker build 来构建镜像。顾名思义，docker buildx 是对 docker 构建能力的一个扩展，它最大的一个亮点便是对多系统架构构建的支持。 docker buildx 适用于 Docker v19.03+ 版本 一个 docker buildx 的构建示例： 1docker buildx build -t cop/cop-demo --platform linux/amd64 . 我们将在下文详细介绍这一命令。 docker manifestdocker manifest 清单，该功能仍处于实验性阶段，也是多系统架构构建的一个关键命令。其可以让我们了解一个镜像的分层信息、大小、签名，最关键的，他可以让我们了解该镜像支持的架构信息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546~ docker manifest inspect openjdk{ &quot;schemaVersion&quot;: 2, &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.list.v2+json&quot;, &quot;manifests&quot;: [ { &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 954, &quot;digest&quot;: &quot;sha256:afbe5f6d76c1eedbbd2f689c18c1984fd67121b369fc0fbd51c510caf4f9544f&quot;, &quot;platform&quot;: { &quot;architecture&quot;: &quot;amd64&quot;, &quot;os&quot;: &quot;linux&quot; } }, { &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 954, &quot;digest&quot;: &quot;sha256:0722e5cd28b8834d2c2e6a3659ba4631c6f6aea6aa88361feff58032bb3514e3&quot;, &quot;platform&quot;: { &quot;architecture&quot;: &quot;arm64&quot;, &quot;os&quot;: &quot;linux&quot;, &quot;variant&quot;: &quot;v8&quot; } }, { &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 2983, &quot;digest&quot;: &quot;sha256:5ecbb996abc91a17257ae0192f2b69a0a3096279a5b9167aef656d6b88972b65&quot;, &quot;platform&quot;: { &quot;architecture&quot;: &quot;amd64&quot;, &quot;os&quot;: &quot;windows&quot;, &quot;os.version&quot;: &quot;10.0.20348.643&quot; } }, { &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 2983, &quot;digest&quot;: &quot;sha256:702402cac2a4e078ec1df8aa23e0f13c7155621dffc520e5ac21e44d94d9ca76&quot;, &quot;platform&quot;: { &quot;architecture&quot;: &quot;amd64&quot;, &quot;os&quot;: &quot;windows&quot;, &quot;os.version&quot;: &quot;10.0.17763.2803&quot; } } ]} platform 一栏，使我们最关注的架构支持信息。 对比 digest 信息，可以发现和 docker hub 的信息是一致的。 本文环境说明本文所有操作基于 Mac M1，Docker Desktop 进行。相关操作可能涉及 experiment 和 buildkit 特性，需要开启。我的配置参考： 123456789101112{ &quot;features&quot;: { &quot;buildkit&quot;: true }, &quot;builder&quot;: { &quot;gc&quot;: { &quot;enabled&quot;: true, &quot;defaultKeepStorage&quot;: &quot;20GB&quot; } }, &quot;experimental&quot;: true} 拉取多架构镜像在没有使用 Mac M1 / ARM 架构之前，拉取镜像似乎并没有那么多烦恼。 1docker pull openjdk 从前文可以得知，openjdk 在不同架构下有不同的 digest，docker 会自行判断当前机器的架构，拉取对应架构的版本。例如 Mac M1 上我拉取的便是 arm64 的版本： 12~ docker image inspect openjdk | grep Arch &quot;Architecture&quot;: &quot;arm64&quot;, 我们也可以通过 –platform 参数来指定拉取的操作系统&amp;架构对应的镜像 1docker pull --platform linux/amd64 openjdk 同一个镜像 tag，本地只会保存一份，再次查看本地镜像的架构信息，已经是 amd64 了： 12~ docker image inspect openjdk | grep Arch &quot;Architecture&quot;: &quot;amd64&quot;, hub 端支持根据按照 Arch 存储多份镜像，实际借助了 manifest 等机制，但并不是所有镜像都支持了 manifest，这也意味着， --platform 参数并不适用于所有镜像，你可以通过 docker manifest inspect 确认镜像的 Arch 支持情况。 构建多架构镜像在调研构建多架构镜像方案时，我有不少困惑，也踩过不少坑，最终我采用的是 docker buildx 构建多架构镜像，并通过 docker manifest 合并清单列表的方案。 寻找支持多架构的 parent 镜像以 openjdk 为例，其提供了 arm64 和 amd64 的版本，我们就用它来做 demo。 Java demo： 1234567public class Main { public static void main(String[] args) { System.out.println(&quot;hello world&quot;); }} Dockerfile: 12345FROM openjdk:17COPY . /usr/src/myappWORKDIR /usr/src/myappRUN javac Main.javaCMD [&quot;java&quot;, &quot;Main&quot;] 本地构建多架构镜像123456789~ docker buildx inspect --bootstrapName: defaultDriver: dockerNodes:Name: defaultEndpoint: defaultStatus: runningPlatforms: linux/arm64, linux/amd64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6 docker buildx 默认的构建器支持构建 linux/arm64, linux/amd64 等操作系统 &amp; 架构的镜像。Docker 通过交叉构建实现该能力，所以并不限制于构建机器的 CPU 架构。 而 docker buildx 支持 --platform 参数，该参数可以指定构建镜像的操作系统 &amp; CPU 架构 12docker buildx build -t kiritomoe/java-multi-arch-demo:1.0-aarch64 --platform linux/arm64 -o type=docker .docker buildx build -t kiritomoe/java-multi-arch-demo:1.0-x86_64 --platform linux/amd64 -o type=docker . 创建推送 Manifest 清单在上一步中，其实我们已经构建了多架构的镜像，但此时，不同架构对应了不同的 tag，这与我们熟悉的 openjdk 的方案还有些差别。openjdk 等镜像实现同一个 tag 绑定多架构版本正是使用了 docker manifest。 123docker manifest create kiritomoe/java-multi-arch-demo:1.0 kiritomoe/java-multi-arch-demo:1.0-x86_64 kiritomoe/java-multi-arch-demo:1.0-aarch64docker manifest push kiritomoe/java-multi-arch-demo:1.0docker manifest rm kiritomoe/java-multi-arch-demo:1.0 注意最终推送的是 kiritomoe/java-multi-arch-demo:1.0 该 manifest，并没有推送其他镜像。并且在 manifest 推送之后，需要删除本地副本，这使得我们今后在本地执行诸如 docker manifest inspect kiritomoe/java-multi-arch-demo:1.0 等操作时，确保是从远程仓库加载的，manifest 只有存在于远程仓库，才有意义。 查看远程仓库的多架构镜像 成功将多架构绑定到了同一个 tag。 使用命令行查看 12345678910111213141516171819202122232425~ docker manifest inspect kiritomoe/java-multi-arch-demo:1.0{ &quot;schemaVersion&quot;: 2, &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.list.v2+json&quot;, &quot;manifests&quot;: [ { &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 1574, &quot;digest&quot;: &quot;sha256:6cceb21f1a225c9f309f51413fdb7cf8d8ea3980a832c84c07ce3e30fed41628&quot;, &quot;platform&quot;: { &quot;architecture&quot;: &quot;arm64&quot;, &quot;os&quot;: &quot;linux&quot; } }, { &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 1574, &quot;digest&quot;: &quot;sha256:0dddf9a86e60de3fd56d074a8f535a90e391b35a6e503fedd09f87c8c32ca75a&quot;, &quot;platform&quot;: { &quot;architecture&quot;: &quot;amd64&quot;, &quot;os&quot;: &quot;linux&quot; } } ]} 一些谈不上最佳实践的实践如果你调研过多架构方案的支持，会发现其实上述的方案并不是唯一的支持方案，个人精力也有限，我没有详细考究 docker 对多架构支持的发展历史，要不是项目需要，天知道我竟然花了两天时间在研究这些东西。但上述的方案是我目前总结下来最简单的方案。 尽管 docker 实现了根据编译机器自动拉取适合本机的镜像，但该能力并不适用于所有的情况。例如 构建机器无法把控，那编译这一行为也将会变得不可控。 构建机器并不一定是最终运行镜像的机器 本地构建的测试开发场景 要想让这一切尽在掌控之中，我个人的建议是遵循两个原则： 业务镜像提供 multi-arch 支持。例如我的基础镜像选择了 centos（centos 是支持 multi-arch 的），我的本地环境是 Mac M1，而我们公司的构建机器是 x86，并不是每个人都是 docker 专家，我希望 From centos 这个拉取镜像的策略变的可控，我愿意为之而编写两个 Dockerfile: Dockerfile_amd64 和 Dockerfile_arm64。最终对我的两个制品进行 manifest 合并，实现 multi-arch。 其他通用镜像支持 multi-arch 的同时，提供不同 Arch 的 tag。例如业务场景中，一般需要提供几类基础镜像 适用于 java 应用的基础镜像：java-base:1.0、java-base:1.0-aarch64、java-base:1.0-x86_64 适用于前端应用的基础镜像：nginx-base:1.0、nginx-base:1.0-aarch64、nginx-base:1.0-x86_64 适用于通用应用的基础镜像：centos-base:1.0、centos-base:1.0-aarch64、centos-base:1.0-x86_64 尽管我清楚可以通过 sha256 精准拉取到指定 Arch 的镜像，但会徒增很多理解成本。 参考 docker docs","link":"/docker-multi-arch/"},{"title":"Service 层需要实现接口吗","text":"前几天看技术交流群的话题，又刷到了「Service 层和 Dao 层真的有必要每个类都加上接口吗？」这个问题，之前简单回答了一波，给出的观点是「看情况」 现在结合我参与的项目以及阅读的一些项目源码来看，如果项目中使用了像 Spring 这样的依赖注入框架，那可以不用接口！ 先来说说为什么使用了依赖注入框架以后，可以不使用接口。 我整理了支持 Service 层和 Dao 层需要加上接口的理由，总结下来就这么三个： 可以在尚未实现具体 Service 逻辑的情况下编写上层代码，如 Controller 对 Service 的调用 Spring 默认是基于动态代理实现 AOP 的，动态代理需要接口 可以对 Service 进行多实现 实际上，这三个理由都站不住脚！ 先说说第一个理由：「上层可以在下层逻辑没有实现的情况下进行编码」！很典型的面向接口编程，对层与层之间进行了解耦，看起来好像没有问题。 这种开发方式适合不同模块之间是由不同的人或项目组开发的，因为沟通的成本比较大。同时避免由于项目组之间开发进度的差异而相互影响。 不过让我们回想一下，在一般项目开发里面，有多少项目组是按层来切分开发任务的呢？实际上，大部分的项目都是按照功能划分的。即使是现在前后端分离的情况，单纯的后端开发也是按照功能模块进行任务划分，即一个人负责从 Controller 层到 DAO 层的完整逻辑处理。在这种情况下，每一层都先定义一个接口，再去实现逻辑，除了增加了开发人员的工作量（当然，如果代码量计入工作量的话，那开发人员应该也不是太排斥接口的！），实际没有任何用处。 如果开发人员想在下层逻辑没有完成的情况下，先开发上层逻辑，可以先编写下层类的空方法来先完成上层的逻辑。 这里推荐一个个人比较喜欢的开发流程，自上向下的编码流程： 先在 Controller 层编写逻辑，遇到需要委托 Service 调用的地方，直接先写出调用代码。 优先完成 Controller 层的流程 然后使用 IDE 的自动补全，对刚才调用下层的代码生成对应的类和方法，在里面添加 TODO 等所有的类和方法都补全了，再基于 TODO，按照上面的流程去一个个的完善逻辑。 此方法可以使你对业务流程有比较好的理解。 对于第二个理由，就完全不成立了。Spring 默认是基于动态代理的，不过通过配置是可以使用 CGLib 来实现 AOP。CGLib 是不需要接口的。 最后一个理由是「可以对 Service 进行多实现」。这个理由不充分，或者说没有考虑场景。实际上在大多数情况下是不需要多实现，或者说可以使用其它方式替代基于接口的多实现。 另外，对于很多使用了接口的项目，项目结构也是有待商榷的！下面，我们结合项目结构来说明。 一般项目结构都是按层来划分的，如下所示： Controller Service Dao 对于不需要多实现的情况，也就不需要接口了。上面的项目结构即可满足要求。 对于需要多实现的情况，无论是现在需要，还是后面需要。这种情况下，看起来好像是需要接口。此时的项目结构看起来像这样： Controller Service — 接口在一个包中 impl — 实现在另一个包里 Dao 对于上面的结构，我们来考虑多实现的情况下，该怎么处理？ 第一种方式，是在 Service 中新增一个包，在里面编写新的逻辑，然后修改配置文件，将新实现作为注入对象。 Controller Service —- 接口在一个包中 impl —实现在另一个包里 impl2 —新实现在另一个包里 Dao 第二种方式，是新增一个 Service 模块，在里面编写新的逻辑（注意这里的包和原来 Service 的包不能相同，或者包相同，但是类名不同，否则无法创建类。因为在加载时需要同时加载两个 Service 模块，如果包名和类名都相同，两个模块的类全限定名就是一样的了！），然后修改配置文件，将新逻辑作为注入对象。 Controller Service —- 接口在一个包中 impl —实现在另一个包里 Service2 impl2 —新实现在另一个包里 Dao 相对而言，实际第一种方式相对更简单一点，只需要关注包层面。而第二种方式需要关注模块和包两个层面。另外，实际这两种方式都导致了项目中包含了不需要的逻辑代码。因为老逻辑都会被打进包里。 不过，从结构上来看，实际方式二的结构要比方式一的结构更清晰，因为从模块上能区分逻辑。 那有没有办法来结合两者的优点呢？答案是肯定的，而且操作起来也不复杂！ 首先将接口和实现独立开，作为一个独立的模块： Controller Service — 接口模块 ServiceImpl impl —实现在另一个包里 ServiceImpl2 impl2 —新实现在另一个包里 Dao 其次，调整打包配置，ServiceImpl 和 ServiceImpl2 二选一。既然 ServiceImpl 和 ServiceImpl2 是二选一，那 ServiceImpl 和ServiceImpl2 的包结构就可以相同。包结构相同了，那调整了依赖以后，依赖注入相关的配置就不需要调整了。调整后，项目结构看起来像这样： Controller Service — 接口模块 ServiceImpl impl —实现在另一个包 ServiceImpl2 impl —新实现和老实现在相同的包中 Dao 现在，ServiceImpl 和 ServiceImpl2 模块中的包结构、类名都是一样的。那我们还需要接口模块吗？ 假设，我们把Service接口模块去掉，结构变成了如下所示： Controller Service1 — 老实现 Service2 — 新实现 Dao 单纯的通过调整模块依赖，是否能实现 Service 的多实现？答案显而易见吧？ 上面给出了不使用接口的理由。不过不使用接口并不是完全没有缺点的，主要问题就是在进行多实现的时候，没有一个强接口规范。即不能通过实现接口，借助 IDE 快速生成框架代码。对于没有实现的接口，IDE 也能给出错误提醒。 一个不太优雅的解决是，将原来的模块里的代码拷贝一份到新模块中，基于老代码来实现新的逻辑。 所以，如果一个项目需要多实现、且多实现数量较多（不过一般项目不会有多个实现的），则推荐使用接口。否则不需要使用接口。 本文针对「Service 层是否需要接口」这个问题，指出需要接口的理由的问题。以及个人对这个问题的观点，希望在评论区写出自己的理解 ！ 链接：https://urlify.cn/Vjua2e","link":"/does-service-module-need-interface/"},{"title":"Docker Network—Bridge 模式","text":"概述Docker 强大的原因之一在于多个 Docker 容器之间的互相连接。涉及到连接，就引出了网络通信的几种模式。Docker 默认提供了 5 种网络驱动模式。 bridge: 默认的网络驱动模式。如果不指定驱动程序，bridge 便会作为默认的网络驱动模式。当应用程序运行在需要通信的独立容器 (standalone containers) 中时，通常会选择 bridge 模式。 host：移除容器和 Docker 宿主机之间的网络隔离，并直接使用主机的网络。host 模式仅适用于 Docker 17.06+。 overlay：overlay 网络将多个 Docker 守护进程连接在一起，并使集群服务能够相互通信。您还可以使用 overlay 网络来实现 swarm 集群和独立容器之间的通信，或者不同 Docker 守护进程上的两个独立容器之间的通信。该策略实现了在这些容器之间进行操作系统级别路由的需求。 macvlan：Macvlan 网络允许为容器分配 MAC 地址，使其显示为网络上的物理设备。 Docker 守护进程通过其 MAC 地址将流量路由到容器。对于希望直连到物理网络的传统应用程序而言，使用 macvlan 模式一般是最佳选择，而不应该通过 Docker 宿主机的网络进行路由。 none：对于此容器，禁用所有联网。通常与自定义网络驱动程序一起使用。none 模式不适用于集群服务。 通过在 Docker 上安装和使用第三方网络插件可以算作额外的扩展方式。 默认网络12345kiritodeMacBook-Pro:~ kirito$ docker network lsNETWORK ID NAME DRIVER SCOPE15315759c263 bridge bridge locald72064d9febf host host local83ea989d3fec none null local 这 3 个网络包含在 Docker 实现中。运行一个容器时，可以使用 –network 参数指定在哪种网络模式下运行该容器。 这篇文章重点介绍 bridge 模式。 所有 Docker 安装后都存在的 docker0 网络，这在 Docker 基础中有过介绍。除非使用 docker run –network= 选项另行指定，否则 Docker 守护进程默认情况下会将容器连接到 docker0 这个网络。 创建自定义的网络使用如下命令就可以创建一个名称为 my-net ，网络驱动模式为 bridge 的自定义网络。 1$ docker network create my-net 再次查看存在的网络可以发现上述命令执行之后产生的变化： 123456kiritodeMacBook-Pro:~ kirito$ docker network lsNETWORK ID NAME DRIVER SCOPE15315759c263 bridge bridge locald72064d9febf host host local73e32007f19f my-net bridge local83ea989d3fec none null local 使用 busybox 测试容器连通性 BusyBox 是一个集成了一百多个最常用 Linux 命令和工具（如 cat、echo、grep、mount、telnet 、ping、ifconfig 等）的精简工具箱，它只需要几 MB 的大小，很方便进行各种快速验证，被誉为“Linux 系统的瑞士军刀”。 我们使用 busybox 来测试容器间的网络情况。(一开始我尝试使用 ubuntu 作为基础镜像来构建测试容器，但 ubuntu 镜像删减了几乎所有的常用工具，连同 ping，ifconfig 等命令都需要额外安装软件，而 busybox 则不存在这些问题。) 使用默认网桥 docker01234kiritodeMacBook-Pro:~ kirito$ docker run --name box1 -it --rm busybox sh/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:07 inet addr:172.17.0.7 Bcast:172.17.255.255 Mask:255.255.0.0 —rm 指令可以让我们在退出容器时自动销毁该容器，这样便于测试。查看自身的 ip 为 172.17.0.7，接下来创建第二个容器 box2。 1234kiritodeMacBook-Pro:~ kirito$ docker run --name box2 -it --rm busybox sh/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:08 inet addr:172.17.0.8 Bcast:172.17.255.255 Mask:255.255.0.0 查看自身的 ip 为 172.17.0.8。 在 box2 中执行 ping 命令测试与 box1 的连通性： 1234567891011使用 IP/ # ping 172.17.0.8PING 172.17.0.8 (172.17.0.8): 56 data bytes64 bytes from 172.17.0.8: seq=0 ttl=64 time=0.107 ms64 bytes from 172.17.0.8: seq=1 ttl=64 time=0.116 ms64 bytes from 172.17.0.8: seq=2 ttl=64 time=0.114 ms64 bytes from 172.17.0.8: seq=3 ttl=64 time=0.126 ms使用容器名称/ # ping box1无响应 我们发现使用默认网桥 docker0 的桥接模式下，ip 是通的，但是无法使用容器名作为通信的 host。 使用自定义网桥 my-net1234kiritodeMacBook-Pro:~ kirito$ docker run --name box3 -it --rm --network my-net busybox sh/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:15:00:02 inet addr:172.21.0.2 Bcast:172.21.255.255 Mask:255.255.0.0 使用 —network 指定使用的网络模式，my-net 便是在此之前我们通过 docker network create 命令新创建的网络。新启动一个 shell 创建 box4 1234kiritodeMacBook-Pro:~ kirito$ docker run -it --name box4 --rm --network my-net busybox sh/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:15:00:03 inet addr:172.21.0.3 Bcast:172.21.255.255 Mask:255.255.0.0 在 box4 中执行 ping 命令测试与 box3 的连通性： 123456789101112131415使用 IP/ # ping 172.21.0.2PING 172.21.0.2 (172.21.0.2): 56 data bytes64 bytes from 172.21.0.2: seq=0 ttl=64 time=0.203 ms64 bytes from 172.21.0.2: seq=1 ttl=64 time=0.167 ms64 bytes from 172.21.0.2: seq=2 ttl=64 time=0.169 ms64 bytes from 172.21.0.2: seq=3 ttl=64 time=0.167 ms使用容器名称/ # ping box3PING box3 (172.21.0.2): 56 data bytes64 bytes from 172.21.0.2: seq=0 ttl=64 time=0.229 ms64 bytes from 172.21.0.2: seq=1 ttl=64 time=0.170 ms64 bytes from 172.21.0.2: seq=2 ttl=64 time=0.165 ms64 bytes from 172.21.0.2: seq=3 ttl=64 time=0.168 ms 与默认的网络 docker0 不同的是，指定了自定义 network 的容器可以使用容器名称相互通信，实际上这也是 docker 官方推荐使用 —network 参数运行容器的原因之一。 对比自定义 bridge(my-net) 与默认 bridge(docker0)自定义 bridge 提供更好的隔离性和容器间的互操作性连接到同一个自定义 bridge 网络的容器会自动将所有端口相互暴露，并且无法连接到容器之外的网络。这使得容器化的应用能轻松地相互通信，并且与外部环境产生了良好的隔离性。 例如一个包含了 web 应用，数据库，redis 等组件的应用程序。很有可能只希望对外界暴露 80 端口，而不允许外界访问数据库端口和 redis 端口，而又不至于让 web 应用本身无法访问数据库和 redis， 便可以使用自定义 bridge 网络轻松实现。如果在默认 bridge 网络上运行相同的应用程序，则需要使用 -p 或 —publish 标志打开 web 端口，数据库端口，redis 端口。这意味着 Docker 宿主机需要通过其他方式阻止对数据库端口，redis 端口的访问，无意增大了工作量。 自定义 bridge 提供容器间的自动 DNS 解析这一点在上一节的实验中已经验证过了。默认 bridge 网络上的容器只能通过 IP 地址互相访问，除非使用在 docker run 时添加 —link 参数。这么做个人认为有两点不好的地方： 一：容器关系只要稍微复杂一些，便会对管理产生不便。 二： —link 参数在官方文档中已经被标记为过期的参数，不被建议使用。 在用户定义的桥接网络上，容器可以通过容器名称 (--name 指定的名称) 或别名来解析对方。可能有人说，在默认 bridge 模式下我可以去修改 /etc/hosts 文件呀，但这显然不是合理的做法。 容器可以在运行中与自定义 bridge 网络连接和分离在容器的生命周期中，可以在运行中将其与自定义网络连接或断开连接。 而要从默认 bridge 网络中移除容器，则需要停止容器并使用不同的网络选项重新创建容器。 每个自定义的 bridge 网络都会创建一个可配置的网桥如果容器使用默认 bridge 网络，虽然可以对其进行配置，但所有容器都使用相同的默认设置，例如 MTU 和防火墙规则。另外，配置默认 bridge 网络隔离于 Docker 本身之外，并且需要重新启动 Docker 才可以生效。 自定义的 bridge 是使用 docker network create 创建和配置的。如果不同的应用程序组具有不同的网络要求，则可以在创建时分别配置每个用户定义的 bridge 网络，这无疑增加了灵活性和可控性。 使用默认 bridge 容器共享所有的环境变量在 Docker 的旧版本中，两个容器之间共享环境变量的唯一方法是使用 —link 标志来进行链接。这种类型的变量共享对于自定义的网络是不存在的。但是，自定义网络有更好方式来实现共享环境变量： 多个容器可以使用 Docker 卷来挂载包含共享信息的文件或目录。 多个容器可以使用 docker-compose 一起启动，并且 docker-compose.yml 文件可以定义共享变量。 使用集群服务而不是独立容器，并利用共享密钥和配置。 结合上述这些论述和官方文档的建议，使用 bridge 网络驱动模式时，最好添加使用 —network 来指定自定义的网络。 参考资料https://docs.docker.com/network/bridge/#connect-a-container-to-the-default-bridge-network https://www.ibm.com/developerworks/cn/linux/l-docker-network/index.html","link":"/docker-network-bridge/"},{"title":"drools 用户指南 ----stateless session（无状态会话）的使用","text":"stateless session 无状态会话Drools 规则引擎中有如此多的用例和诸多功能，它变得令人难以置信。不过不用担心，复杂性是分层的，你可以用简单的用例来逐步了解 drools。 无状态会话，不使用推理，形成最简单的用例。无状态会话可以被称为函数传递一些数据，然后再接收一些结果。无状态会话的一些常见用例有以下但不限于： 验证这个人有资格获得抵押吗？ 计算计算抵押保费。 路由和过滤将传入的邮件（如电子邮件）过滤到文件夹中。将传入的邮件发送到目的地。 所以让我们从使用驾驶执照应用程序的一个非常简单的例子开始吧。 123456public class Applicant { private String name; private int age; private boolean valid; // getter and setter methods here} 现在我们有了我们的数据模型，我们可以写出我们的第一个规则。我们假设应用程序使用规则来拒绝不符合规则的申请。由于这是一个简单的验证用例，我们将添加一条规则来取消任何 18 岁以下的申请人的资格。 12345678package com.company.licenserule &quot;Is of valid age&quot;when $a : Applicant(age &lt; 18)then $a.setValid(false);end 为了使引擎了解数据，所以可以根据规则进行处理，我们必须插入数据，就像数据库一样。当申请人实例插入到引擎中时，将根据规则的约束进行评估，在这种情况下，这只是一个规则的两个约束条件。我们说两个，因为申请人类型是第一个对象类型约束，而 age &lt;18 是第二个字段约束。对象类型约束及其零个或多个字段约束被称为模式。当插入的实例同时满足对象类型约束和所有字段约束时，它被称为匹配。$a 是一个绑定变量，它允许我们引用匹配的对象。其属性可以更新。美元字符（’$’）是可选的，但它有助于区分变量名称和字段名称。匹配模式与插入数据的过程并不奇怪，通常被称为模式匹配。 要使用这个规则，有必要把它放在一个 Drools 文件中，只是一个带有.drl 扩展名的纯文本文件，简称为“Drools Rule Language”。我们来调用 licenseApplication.drl 这个文件，并将其存储在 Kie Project 中。 Kie 项目具有正常的 Maven 项目的结构，并附加一个可以创建的 KieBase 和 KieSession 文件（kmodule.xml）。该文件必须放在 Maven 项目的 resources/META-INF 文件夹中，而所有其他 Drools 工件（如包含前一规则的 licenseApplication.drl）必须存储在资源文件夹或其下的任何其他子文件夹中。 由于为所有配置方面提供了有意义的默认值，所以最简单的 kmodule.xml 文件只能包含一个空的 kmodule 标签，如下所示： 12&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;kmodule xmlns=&quot;http://www.drools.org/xsd/kmodule&quot;/&gt; 此时，可以从类路径创建一个 KieContainer 来读取要构建的文件。 12KieServices kieServices = KieServices.Factory.get();KieContainer kContainer = kieServices.getKieClasspathContainer(); 上面的代码段编译了类路径中找到的所有 DRL 文件，并将该编译结果 KieModule 放在 KieContainer 中。如果没有错误，我们现在可以从 KieContainer 创建我们的会话并执行一些数据： 12345StatelessKieSession kSession = kContainer.newStatelessKieSession();Applicant applicant = new Applicant(&quot;Mr John Smith&quot;, 16);assertTrue(applicant.isValid() );ksession.execute(applicant);assertFalse(applicant.isValid() ); 上述代码根据规则执行数据。由于申请人年龄未满 18 岁，申请被标记为无效。 到目前为止，我们只使用了一个实例，但是如果我们想要使用多个实例呢？我们可以执行任何实现 Iterable 的对象，如集合。我们再添加一个名为 Application 的类，它有应用程序的日期，我们还将布尔有效字段移到 Application 类。 1234567891011public class Applicant { private String name; private int age; // getter and setter methods here}public class Application { private Date dateApplied; private boolean valid; // getter and setter methods here} 我们还将添加另一条规则来验证申请是否在一段时间内进行。 12345678910111213141516package com.company.licenserule &quot;Is of valid age&quot;when Applicant(age &lt; 18) $a : Application() then $a.setValid(false);endrule &quot;Application was made this year&quot;when $a : Application(dateApplied &gt; &quot;01-jan-2009&quot;) then $a.setValid(false);end 不幸的是，Java 数组不实现 Iterable 接口，所以我们必须使用 JDK 转换器方法 Arrays.asList（…）。下面显示的代码针对一个可迭代列表执行，其中在触发任何匹配的规则之前插入所有集合元素。 123456StatelessKieSession kSession = kContainer.newStatelessKieSession();Applicant applicant = new Applicant(&quot;Mr John Smith&quot;, 16);Application application = new Application();assertTrue(application.isValid() );ksession.execute(Arrays.asList( new Object[] {application, applicant} ) );assertFalse(application.isValid() ); 执行的两个执行方法（Object object）和 execute（Iterable 对象）实际上是接口 BatchExecutor 的方法 execute（Command 命令）的便利方法。 KieCommands 命令工厂可以像 KIE A​​PI 的所有其他工厂一样从 KieServices 获取，用于创建命令，以便以下操作相当于执行（Iterable it）： 1ksession.execute(kieServices.getCommands().newInsertElements(Arrays.asList( new Object[] {application, applicant} ) ); 批处理执行器和命令工厂在使用多个命令和输出标识符以获取结果时特别有用。 123456KieCommands kieCommands = kieServices.getCommands();List&lt;Command&gt; cmds = new ArrayList&lt;Command&gt;();cmds.add(kieCommands.newInsert( new Person( &quot;Mr John Smith&quot;), &quot;mrSmith&quot;, true, null ) );cmds.add(kieCommands.newInsert( new Person( &quot;Mr John Doe&quot;), &quot;mrDoe&quot;, true, null ) );BatchExecutionResults results = ksession.execute(kieCommands.newBatchExecution( cmds) );assertEquals(new Person( &quot;Mr John Smith&quot;), results.getValue(&quot;mrSmith&quot;) );","link":"/drools-1/"},{"title":"drools 用户指南 ----stateful session（有状态会话）的使用","text":"stateful session 有状态会话有状态会话长期存在，并允许随着时间的推移进行迭代更改。 有状态会话的一些常见用例包括但不限于： 监测半自动买入股票市场监控与分析。 诊断故障查找，医疗诊断 物流包裹跟踪和送货配置 合规验证市场交易的合法性。 与无状态会话相反，必须先调用 dispose() 方法，以确保没有内存泄漏，因为 KieBase 包含创建状态知识会话时的引用。 由于状态知识会话是最常用的会话类型，所以它只是在 KIE API 中命名为 KieSession。 KieSession 还支持 BatchExecutor 接口，如 StatelessKieSession，唯一的区别是 FireAllRules 命令在有状态会话结束时不被自动调用。 我们举例说明了用于提高火灾报警器的监控用例。 只使用四个类，我们假设 Room 代表房子里的房间，每个 Room 都有一个喷头 Sprinkler。 如果在房间里发生火灾，我们用一个 Fire 实例来表示, 用 Alarm 代表警报 。 123456789101112131415161718public class Room { private String name // getter and setter methods here}public class Sprinkler { private Room room; private boolean on; // getter and setter methods here}public class Fire { private Room room; // getter and setter methods here}public class Alarm {} 在上一节无状态会话中介绍了插入和匹配数据的概念。 这个例子假设每个对象类型的都是单个实例被插入的，因此只使用了字面约束。 然而，房子有许多房间，因此 rules 必须表达实体类之间的关系，例如在某个房间内的喷洒器。 这最好通过使用绑定变量作为模式中的约束来完成。 这种“加入”过程产生了所谓的“cross products”，这在下一节中将会介绍。 当发生火灾时，会为该类别创建 Fire 类的实例，并将其插入到会话中。 该规则使用 Fire 对象的房间字段上的绑定来约束与当前关闭的房间的喷水灭火器的匹配。 当此规则触发并且执行结果时，喷头被打开。 12345678rule &quot;When there is a fire turn on the sprinkler&quot;when Fire($room : room) $sprinkler : Sprinkler(room == $room, on == false)then modify($sprinkler) {setOn( true) }; System.out.println(&quot;Turn on the sprinkler for room&quot; + $room.getName() );end 而无状态会话使用标准 Java 语法来修改字段，在上述规则中，我们使用 modify 语句，它作为一种“with”语句。 它可以包含一系列逗号分隔的 Java 表达式，即对由 modify 语句的控制表达式选择的对象的 setter 的调用。 这将修改数据，并使引擎意识到这些更改，以便它可以再次对其进行推理。 这个过程被称为推理，对于有状态会话的工作至关重要。 无状态会话通常不使用推理，因此引擎不需要意识到数据的更改。 也可以通过使用顺序模式显式地关闭推理。 到目前为止，我们有规则告诉我们匹配数据是否存在，但是当它不存在时呢？ 我们如何确定火已经熄灭了，即没有 Fire 对象呢？ 以前的约束是根据命题逻辑的句子，其中引擎限制个别的实例。 Drools 还支持 First Order Logic，允许您查看数据集。 当某个不存在时，关键字下的模式不匹配。 一旦这个房间的火灾消失，下面给出的规则会使喷水灭火。 123456789rule &quot;When the fire is gone turn off the sprinkler&quot;when $room : Room( ) $sprinkler : Sprinkler(room == $room, on == true) not Fire(room == $room)then modify($sprinkler) {setOn( false) }; System.out.println(&quot;Turn off the sprinkler for room&quot; + $room.getName() );end 每个 room 有一个喷水灭火器，house 只有一个警报。 当发生火灾时，会创建一个 alrm 对象，而不管发生多少火灾，整个建筑物都只需要一个警报 alrm。 1234567rule &quot;Raise the alarm when we have one or more fires&quot;when exists Fire()then insert(new Alarm() ); System.out.println(&quot;Raise the alarm&quot;);end 同样，当没有火灾时，我们想要删除警报，所以可以再次使用 not 关键字。 12345678rule &quot;Cancel the alarm when all the fires have gone&quot;when not Fire() $alarm : Alarm()then delete($alarm); System.out.println(&quot;Cancel the alarm&quot;);end 最后，当应用程序首次启动并且在报警消除并且所有喷头已关闭后，都会打印 Everything is ok。 1234567rule &quot;Status output when things are ok&quot;when not Alarm() not Sprinkler(on == true) then System.out.println(&quot;Everything is ok&quot;);end 正如我们在无状态会话示例中所做的那样，上述规则应放在单个 DRL 文件中，并保存到 Maven 项目或其任何子文件夹的资源文件夹中。 如前所述，我们可以从 KieContainer 获得 KieSession。 唯一的区别是，这次我们创建一个有状态会话，而之前我们创建的是一个无状态会话。 123KieServices kieServices = KieServices.Factory.get();KieContainer kContainer = kieServices.getKieClasspathContainer();KieSession ksession = kContainer.newKieSession(); 创建会话后，现在可以随着时间的推移迭代地使用它。 创建和插入四个房间对象，每个房间的对应一个 Sprinkler 对象。 此时，规则引擎已经完成了所有的匹配，但并没有触发。 调用 ksession.fireAllRules（）使得匹配的规则触发，但因为没有火灾，所以输出结果是 Everything is ok。 1234567891011String[] names = new String[]{&quot;kitchen&quot;, &quot;bedroom&quot;, &quot;office&quot;, &quot;livingroom&quot;};Map&lt;String,Room&gt; name2room = new HashMap&lt;String,Room&gt;();for(String name: names){ Room room = new Room(name); name2room.put(name, room); ksession.insert(room); Sprinkler sprinkler = new Sprinkler(room); ksession.insert(sprinkler);}ksession.fireAllRules(); Everything is ok 我们现在创造两个 Fire 并插入它们， 随着发动机内部的火灾，一旦调用了 fireAllRules（），报警器就会升高，并且相应的喷水灭火器打开。 123456Fire kitchenFire = new Fire(name2room.get( &quot;kitchen&quot;) );Fire officeFire = new Fire(name2room.get( &quot;office&quot;) );FactHandle kitchenFireHandle = ksession.insert(kitchenFire);FactHandle officeFireHandle = ksession.insert(officeFire);ksession.fireAllRules(); Raise the alarmTurn on the sprinkler for room kitchenTurn on the sprinkler for room office 一段时间之后，火灾将熄灭，并且 Fire 实例被撤回。 这导致喷头关闭，报警被取消，最后再次打印 Everything is ok。 1234ksession.delete(kitchenFireHandle);ksession.delete(officeFireHandle);ksession.fireAllRules(); Cancel the alarmTurn off the sprinkler for room officeTurn off the sprinkler for room kitchenEverything is ok","link":"/drools-2/"},{"title":"drools 用户指南 ----Methods vs Rules","text":"Methods vs Rules人们经常混淆方法和规则，初学者经常会问：“我如何理解规则的含义？“ 在最后一节之后，你会对规则的使用得心应手，答案也变得显而易见的，但在这之前，先让我们总结一下方法判断和规则的差异。 12345public void helloWorld(Person person) { if (person.getName().equals(&quot;Chuck&quot;) ) { System.out.println(&quot;Hello Chuck&quot;); }} 方法是被直接调用的 需要传递具体的实例 一个调用导致一次执行（One call results in a single execution）。 12345rule &quot;Hello World&quot; when Person(name == &quot;Chuck&quot;)then System.out.println(&quot;Hello Chuck&quot;);end 只要将其插入引擎，就可以通过匹配任何数据执行规则。 规则永远无法被直接调用，而只能触发 无法将特定的实例传递给规则 根据匹配，一个规则可能会触发一次或多次，或根本不被触发。","link":"/drools-3/"},{"title":"drools 用户指南 ----Cross Products","text":"Cross Products之前提到“Cross Products”一词，其实就是一个 join 操作（译者注：可以理解为笛卡尔积）。想象一下，火灾报警示例的数据与以下规则结合使用，其中没有字段约束： 1234567rule &quot;Show Sprinklers&quot; when $room : Room() $sprinkler : Sprinkler()then System.out.println(&quot;room:&quot; + $room.getName() + &quot;sprinkler:&quot; + $sprinkler.getRoom().getName() );end 在 SQL 术语中，这就像是执行了 select * from Room, Sprinkler，Sprinkler 表中的每一行将与 Room 表中的每一行相连接，从而产生以下输出： 12345678910111213141516room:office sprinkler:officeroom:office sprinkler:kitchenroom:office sprinkler:livingroomroom:office sprinkler:bedroomroom:kitchen sprinkler:officeroom:kitchen sprinkler:kitchenroom:kitchen sprinkler:livingroomroom:kitchen sprinkler:bedroomroom:livingroom sprinkler:officeroom:livingroom sprinkler:kitchenroom:livingroom sprinkler:livingroomroom:livingroom sprinkler:bedroomroom:bedroom sprinkler:officeroom:bedroom sprinkler:kitchenroom:bedroom sprinkler:livingroomroom:bedroom sprinkler:bedroom 这些连接结果显然会变得巨大，它们必然包含冗余数据。 cross products 的大小通常是新规则引擎产品性能问题的根源。 从这可以看出，我们希望约束 cross products，这便是用可变约束（the variable constraint）完成的。 12345678rulewhen $room : Room() $sprinkler : Sprinkler(room == $room)then System.out.println(&quot;room:&quot; + $room.getName() + &quot;sprinkler:&quot; + $sprinkler.getRoom().getName() );end 这就使得筛选结果只有寥寥几行, 这就为每一个 Room 筛选出了正确的 Sprinkler. 在 sql 中 (实际上是 HQL) 这样的查询约等于 select * from Room, Sprinkler where Room == Sprinkler.room.","link":"/drools-4/"},{"title":"Dubbo 迈向云原生的里程碑 | 应用级服务发现","text":"1 概述社区版本 Dubbo 从 2.7.5 版本开始，新引入了一种基于应用粒度的服务发现机制，这是 Dubbo 为适配云原生基础设施的一步重要探索。版本发布到现在已有近半年时间，经过这段时间的探索与总结，我们对这套机制的可行性与稳定性有了更全面、深入的认识；同时在 Dubbo 3.0 的规划也在全面进行中，如何让应用级服务发现成为未来下一代服务框架 Dubbo 3.0 的基础服务模型，解决云原生、规模化微服务集群扩容与可伸缩性问题，也已经成为我们当前工作的重点。 既然这套新机制如此重要，那它到底是怎么工作的，今天我们就来详细解读一下。在最开始的社区版本，我们给这个机制取了一个神秘的名字 - 服务自省，下文将进一步解释这个名字的由来，并引用服务自省代指这套应用级服务发现机制。 熟悉 Dubbo 开发者应该都知道，一直以来都是面向 RPC 方法去定义服务的，并且这也是 Dubbo 开发友好性、治理功能强的基础。既然如此，那我们为什么还要定义个应用粒度的服务发现机制那？这个机制到底是怎么工作的？它与当前机制的区别是什么？它能给我们带来哪些好处那？对适配云原生、性能提升又有哪些帮助？ 带着所有的这些问题，我们开始本文的讲解。 2 服务自省是什么？首先，我们先来解释文章开篇提到的问题： 应用粒度服务发现是到底是一种怎样的模型，它与当前的 Dubbo 服务发现模型的区别是什么？ 我们为什么叫它服务自省？ 所谓“应用/实例粒度” 或者“RPC 服务粒度”强调的是一种地址发现的数据组织格式。 以 Dubbo 当前的地址发现数据格式为例，它是“RPC 服务粒度”的，它是以 RPC 服务作为 key，以实例列表作为 value 来组织数据的： 1234567&quot;RPC Service1&quot;: [ {&quot;name&quot;:&quot;instance1&quot;, &quot;ip&quot;:&quot;127.0.0.1&quot;, &quot;metadata&quot;:{&quot;timeout&quot;:1000}}, {&quot;name&quot;:&quot;instance2&quot;, &quot;ip&quot;:&quot;127.0.0.1&quot;, &quot;metadata&quot;:{&quot;timeout&quot;:2000}}, {&quot;name&quot;:&quot;instance3&quot;, &quot;ip&quot;:&quot;127.0.0.1&quot;, &quot;metadata&quot;:{&quot;timeout&quot;:3000}},]&quot;RPC Service2&quot;: [Instance list of RPC Service2],&quot;RPC ServiceN&quot;: [Instance list of RPC ServiceN] 而我们新引入的“应用粒度的服务发现”，它以应用名（Application）作为 key，以这个应用部署的一组实例（Instance）列表作为 value。这带来两点不同： 数据映射关系变了，从 RPC Service -&gt; Instance 变为 Application -&gt; Instance 数据变少了，注册中心没有了 RPC Service 及其相关配置信息 12345&quot;application1&quot;: [ {&quot;name&quot;:&quot;instance1&quot;, &quot;ip&quot;:&quot;127.0.0.1&quot;, &quot;metadata&quot;:{}}, {&quot;name&quot;:&quot;instance2&quot;, &quot;ip&quot;:&quot;127.0.0.1&quot;, &quot;metadata&quot;:{}}, {&quot;name&quot;:&quot;instanceN&quot;, &quot;ip&quot;:&quot;127.0.0.1&quot;, &quot;metadata&quot;:{}}] 要进一步理解新模型带来的变化，我们看一下应用与 RPC 服务间的关系，显而易见的，1 个应用内可能会定义 n 个 RPC Service。因此 Dubbo 之前的服务发现粒度更细，在注册中心产生的数据条目也会更多（与 RPC 服务成正比），同时也存在一定的数据冗余。 简单理解了应用级服务发现的基本机制，接着解释它为什么会被叫做“服务自省”？其实这还是得从它的工作原理说起，上面我们提到，应用粒度服务发现的数据模型有几个以下明显变化：数据中心的数据量少了，RPC 服务相关的数据在注册中心没有了，现在只有 application - instance 这两个层级的数据。为了保证这部分缺少的 RPC 服务数据仍然能被 Consumer 端正确的感知，我们在 Consumer 和 Provider 间建立了一条单独的通信通道：Consumer 和 Provider 两两之间通过特定端口交换信息，我们把这种 Provider 自己主动暴露自身信息的行为认为是一种内省机制，因此从这个角度出发，我们把整个机制命名为：服务自省。 3 为什么需要服务自省？上面讲服务自省的大概原理的时候也提到了它给注册中心带来的几点不同，这几点不同体现在 Dubbo 框架侧（甚至整个微服务体系中），有以下优势： 与业界主流微服务模型对齐，比如 SpringCloud、Kubernetes Native Service等。 提升性能与可伸缩性。注册中心数据的重新组织（减少），能最大幅度的减轻注册中心的存储、推送压力，进而减少 Dubbo Consumer 侧的地址计算压力；集群规模也开始变得可预测、可评估（与 RPC 接口数量无关，只与实例部署规模相关）。 3.1 对齐主流微服务模型自动、透明的实例地址发现（负载均衡）是所有微服务框架需要解决的事情，这能让后端的部署结构对上游微服务透明，上游服务只需要从收到的地址列表中选取一个，发起调用就可以了。要实现以上目标，涉及两个关键点的自动同步： 实例地址，服务消费方需要知道地址以建立链接 RPC 方法定义，服务消费方需要知道 RPC 服务的具体定义，不论服务类型是 rest 或 rmi 等。 对于 RPC 实例间借助注册中心的数据同步，REST 定义了一套非常有意思的成熟度模型，感兴趣的朋友可以参考这里的链接 https://www.martinfowler.com/articles/richardsonMaturityModel.html， 按照文章中的 4 级成熟度定义，Dubbo 当前基于接口粒度的模型可以对应到 L4 级别。 接下来，我们看看 Dubbo、SpringCloud 以及 Kubernetes 分别是怎么围绕自动化的实例地址发现这个目标设计的。 1. Spring Cloud Spring Cloud 通过注册中心只同步了应用与实例地址，消费方可以基于实例地址与服务提供方建立链接，但是消费方对于如何发起 http 调用（SpringCloud 基于 rest 通信）一无所知，比如对方有哪些 http endpoint，需要传入哪些参数等。 RPC 服务这部分信息目前都是通过线下约定或离线的管理系统来协商的。这种架构的优缺点总结如下。优势：部署结构清晰、地址推送量小；缺点：地址订阅需要指定应用名， provider 应用变更（拆分）需消费端感知；RPC 调用无法全自动同步。 2. Dubbo Dubbo 通过注册中心同时同步了实例地址和 RPC 方法，因此其能实现 RPC 过程的自动同步，面向 RPC 编程、面向 RPC 治理，对后端应用的拆分消费端无感知，其缺点则是地址推送数量变大，和 RPC 方法成正比。 3. Dubbo + Kubernetes Dubbo 要支持 Kubernetes native service，相比之前自建注册中心的服务发现体系来说，在工作机制上主要有两点变化： 服务注册由平台接管，provider 不再需要关心服务注册 consumer 端服务发现将是 Dubbo 关注的重点，通过对接平台层的 API-Server、DNS 等，Dubbo client 可以通过一个 Service Name（通常对应到 Application Name）查询到一组 Endpoints（一组运行 provider 的 pod），通过将 Endpoints 映射到 Dubbo 内部地址列表，以驱动 Dubbo 内置的负载均衡机制工作。 Kubernetes Service 作为一个抽象概念，怎么映射到 Dubbo 是一个值得讨论的点 Service Name - &gt; Application Name，Dubbo 应用和 Kubernetes 服务一一对应，对于微服务运维和建设环节透明，与开发阶段解耦。 1234567891011apiVersion: v1kind: Servicemetadata: name: provider-app-namespec: selector:app: provider-app-name ports:- protocol: TCPport: targetPort: 9376 Service Name - &gt; Dubbo RPC Service，Kubernetes 要维护调度的服务与应用内建 RPC 服务绑定，维护的服务数量变多。 123456789101112131415161718192021222324252627282930---apiVersion: v1kind: Servicemetadata: name: rpc-service-1spec: selector: app: provider-app-name ports: ##...---apiVersion: v1kind: Servicemetadata: name: rpc-service-2spec: selector: app: provider-app-name ports: ##...---apiVersion: v1kind: Servicemetadata: name: rpc-service-Nspec: selector: app: provider-app-name ports: ##... 结合以上几种不同微服务框架模型的分析，我们可以发现，Dubbo 与 SpringCloud、Kubernetes 等不同产品在微服务的抽象定义上还是存在很大不同的。SpringCloud 和 Kubernetes 在微服务的模型抽象上还是比较接近的，两者基本都只关心实例地址的同步，如果我们去关心其他的一些服务框架产品，会发现它们绝大多数也是这么设计的； 即 REST 成熟度模型中的 L3 级别。 对比起来 Dubbo 则相对是比较特殊的存在，更多的是从 RPC 服务的粒度去设计的。 对应 REST 成熟度模型中的 L4 级别。 如我们上面针对每种模型做了详细的分析，每种模型都有其优势和不足。而我们最初决定 Dubbo 要做出改变，往其他的微服务发现模型上的对齐，是我们最早在确定 Dubbo 的云原生方案时，我们发现要让 Dubbo 去支持 Kubernetes Native Service，模型对齐是一个基础条件；另一点是来自用户侧对 Dubbo 场景化的一些工程实践的需求，得益于 Dubbo 对多注册、多协议能力的支持，使得 Dubbo 联通不同的微服务体系成为可能，而服务发现模型的不一致成为其中的一个障碍，这部分的场景描述请参见以下文章：https://www.atatech.org/articles/157719 3.2 更大规模的微服务集群 - 解决性能瓶颈这部分涉及到和注册中心、配置中心的交互，关于不同模型下注册中心数据的变化，之前原理部分我们简单分析过。为更直观的对比服务模型变更带来的推送效率提升，我们来通过一个示例看一下不同模型注册中心的对比： 图中左边是微服务框架的一个典型工作流程，Provider 和 Consumer 通过注册中心实现自动化的地址通知。其中，Provider 实例的信息如图中表格所示： 1应用 DEMO 包含三个接口 DemoService 1 2 3，当前实例的 ip 地址为 10.210.134.30。 对于 Spring Cloud 和 Kubernetes 模型，注册中心只会存储一条 DEMO - 10.210.134.30+metadata 的数据； 对于老的 Dubbo 模型，注册中心存储了三条接口粒度的数据，分别对应三个接口 DemoService 1 2 3，并且很多的址数据都是重复的； 可以总结出，基于应用粒度的模型所存储和推送的数据量是和应用、实例数成正比的，只有当我们的应用数增多或应用的实例数增长时，地址推送压力才会上涨。而对于基于接口粒度的模型，数据量是和接口数量正相关的，鉴于一个应用通常发布多个接口的现状，这个数量级本身比应用粒度是要乘以倍数的；另外一个关键点在于，接口粒度导致的集群规模评估的不透明，相对于实i例、应用增长都通常是在运维侧的规划之中，接口的定义更多的是业务侧的内部行为，往往可以绕过评估给集群带来压力。 以 Consumer 端服务订阅举例，根据我对社区部分 Dubbo 中大规模头部用户的粗略统计，根据受统计公司的实际场景，一个 Consumer 应用要消费（订阅）的 Provier 应用数量往往要超过 10 个，而具体到其要消费（订阅）的的接口数量则通常要达到 30 个，平均情况下 Consumer 订阅的 3 个接口来自同一个 Provider 应用，如此计算下来，如果以应用粒度为地址通知和选址基本单位，则平均地址推送和计算量将下降 60% 还要多，而在极端情况下，也就是当 Consumer 端消费的接口更多的来自同一个应用时，这个地址推送与内存消耗的占用将会进一步得到降低，甚至可以超过 80% 以上。 一个典型的几段场景即是 Dubbo 体系中的网关型应用，有些网关应用消费（订阅）达 100+ 应用，而消费（订阅）的服务有 1000+ ，平均有 10 个接口来自同一个应用，如果我们把地址推送和计算的粒度改为应用，则地址推送量从原来的 n 1000 变为 n 100，地址数量降低可达近 90%。 工作原理设计原则上面一节我们从服务模型及支撑大规模集群的角度分别给出了 Dubbo 往应用级服务发现靠拢的好处或原因，但这么做的同时接口粒度的服务治理能力还是要继续保留，这是 Dubbo 框架编程模型易用性、服务治理能力优势的基础。以下是我认为我们做服务模型迁移仍要坚持的设计原则 新的服务发现模型要实现对原有 Dubbo 消费端开发者的无感知迁移，即 Dubbo 继续面向 RPC 服务编程、面向 RPC 服务治理，做到对用户侧完全无感知。 建立 Consumer 与 Provider 间的自动化 RPC 服务元数据协调机制，解决传统微服务模型无法同步 RPC 级接口配置的缺点。 基本原理详解应用级服务发现作为一种新的服务发现机制，和以前 Dubbo 基于 RPC 服务粒度的服务发现在核心流程上基本上是一致的：即服务提供者往注册中心注册地址信息，服务消费者从注册中心拉取&amp;订阅地址信息。 这里主要的不同有以下两点： 注册中心数据以“应用 - 实例列表”格式组织，不再包含 RPC 服务信息 以下是每个 Instance metadata 的示例数据，总的原则是 metadata 只包含当前 instance 节点相关的信息，不涉及 RPC 服务粒度的信息。 总体信息概括如下：实例地址、实例各种环境标、metadata service 元数据、其他少量必要属性。 1234567891011121314151617181920{ &quot;name&quot;: &quot;provider-app-name&quot;, &quot;id&quot;: &quot;192.168.0.102:20880&quot;, &quot;address&quot;: &quot;192.168.0.102&quot;, &quot;port&quot;: 20880, &quot;sslPort&quot;: null, &quot;payload&quot;: { &quot;id&quot;: null, &quot;name&quot;: &quot;provider-app-name&quot;, &quot;metadata&quot;: { &quot;metadataService&quot;: &quot;{\\&quot;dubbo\\&quot;:{\\&quot;version\\&quot;:\\&quot;1.0.0\\&quot;,\\&quot;dubbo\\&quot;:\\&quot;2.0.2\\&quot;,\\&quot;release\\&quot;:\\&quot;2.7.5\\&quot;,\\&quot;port\\&quot;:\\&quot;20881\\&quot;}}&quot;, &quot;endpoints&quot;: &quot;[{\\&quot;port\\&quot;:20880,\\&quot;protocol\\&quot;:\\&quot;dubbo\\&quot;}]&quot;, &quot;storage-type&quot;: &quot;local&quot;, &quot;revision&quot;: &quot;6785535733750099598&quot;, } }, &quot;registrationTimeUTC&quot;: 1583461240877, &quot;serviceType&quot;: &quot;DYNAMIC&quot;, &quot;uriSpec&quot;: null} Client – Server 自行协商 RPC 方法信息在注册中心不再同步 RPC 服务信息后，服务自省在服务消费端和提供端之间建立了一条内置的 RPC 服务信息协商机制，这也是“服务自省”这个名字的由来。服务端实例会暴露一个预定义的 MetadataService RPC 服务，消费端通过调用 MetadataService 获取每个实例 RPC 方法相关的配置信息。 当前 MetadataService 返回的数据格式如下， 12345[ &quot;dubbo://192.168.0.102:20880/org.apache.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;deprecated=false&amp;dubbo=2.0.2&amp;dynamic=true&amp;generic=false&amp;interface=org.apache.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=9585&amp;release=2.7.5&amp;side=provider&amp;timestamp=1583469714314&quot;, &quot;dubbo://192.168.0.102:20880/org.apache.dubbo.demo.HelloService?anyhost=true&amp;application=demo-provider&amp;deprecated=false&amp;dubbo=2.0.2&amp;dynamic=true&amp;generic=false&amp;interface=org.apache.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=9585&amp;release=2.7.5&amp;side=provider&amp;timestamp=1583469714314&quot;, &quot;dubbo://192.168.0.102:20880/org.apache.dubbo.demo.WorldService?anyhost=true&amp;application=demo-provider&amp;deprecated=false&amp;dubbo=2.0.2&amp;dynamic=true&amp;generic=false&amp;interface=org.apache.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=9585&amp;release=2.7.5&amp;side=provider&amp;timestamp=1583469714314&quot;] 熟悉 Dubbo 基于 RPC 服务粒度的服务发现模型的开发者应该能看出来，服务自省机制机制将以前注册中心传递的 URL 一拆为二： 一部分和实例相关的数据继续保留在注册中心，如 ip、port、机器标识等。 另一部分和 RPC 方法相关的数据从注册中心移除，转而通过 MetadataService 暴露给消费端。 理想情况下是能达到数据按照实例、RPC 服务严格区分开来，但明显可以看到以上实现版本还存在一些数据冗余，有些也数据还未合理划分。尤其是 MetadataService 部分，其返回的数据还只是简单的 URL 列表组装，这些 URL其实是包含了全量的数据。 以下是服务自省的一个完整工作流程图，详细描述了服务注册、服务发现、MetadataService、RPC 调用间的协作流程。 服务提供者启动，首先解析应用定义的“普通服务”并依次注册为 RPC 服务，紧接着注册内建的 MetadataService 服务，最后打开 TCP 监听端口。 启动完成后，将实例信息注册到注册中心（仅限 ip、port 等实例相关数据），提供者启动完成。 服务消费者启动，首先依据其要“消费的 provider 应用名”到注册中心查询地址列表，并完成订阅（以实现后续地址变更自动通知）。 消费端拿到地址列表后，紧接着对 MetadataService 发起调用，返回结果中包含了所有应用定义的“普通服务”及其相关配置信息。 至此，消费者可以接收外部流量，并对提供者发起 Dubbo RPC 调用 在以上流程中，我们只考虑了一切顺利的情况，但在更详细的设计或编码实现中，我们还需要严格约定一些异常场景下的框架行为。比如，如果消费者 MetadataService 调用失败，则在重试知道成功之前，消费者将不可以接收外部流量。 服务自省中的关键机制元数据同步机制Client 与 Server 间在收到地址推送后的配置同步是服务自省的关键环节，目前针对元数据同步有两种具体的可选方案，分别是： 内建 MetadataService。 独立的元数据中心，通过中细化的元数据集群协调数据。 1. 内建 MetadataServiceMetadataService 通过标准的 Dubbo 协议暴露，根据查询条件，会将内存中符合条件的“普通服务”配置返回给消费者。这一步发生在消费端选址和调用前。 元数据中心复用 2.7 版本中引入的元数据中心，provider 实例启动后，会尝试将内部的 RPC 服务组织成元数据的格式到元数据中心，而 consumer 则在每次收到注册中心推送更新后，主动查询元数据中心。 注意 consumer 端查询元数据中心的时机，是等到注册中心的地址更新通知之后。也就是通过注册中心下发的数据，我们能明确的知道何时某个实例的元数据被更新了，此时才需要去查元数据中心。 RPC 服务 &lt; - &gt; 应用映射关系回顾上文讲到的注册中心关于“应用 - 实例列表”结构的数据组织形式，这个变动目前对开发者并不是完全透明的，业务开发侧会感知到查询/订阅地址列表的机制的变化。具体来说，相比以往我们基于 RPC 服务来检索地址，现在 consumer 需要通过指定 provider 应用名才能实现地址查询或订阅。 老的 Consumer 开发与配置示例： 12345&lt;!-- 框架直接通过 RPC Service 1/2/N 去注册中心查询或订阅地址列表 --&gt;&lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt;&lt;dubbo:reference interface=&quot;RPC Service 1&quot; /&gt;&lt;dubbo:reference interface=&quot;RPC Service 2&quot; /&gt;&lt;dubbo:reference interface=&quot;RPC Service N&quot; /&gt; 新的 Consumer 开发与配置示例： 12345&lt;!-- 框架需要通过额外的 provided-by=&quot;provider-app-x&quot; 才能在注册中心查询或订阅到地址列表 --&gt;&lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181?registry-type=service&quot;/&gt;&lt;dubbo:reference interface=&quot;RPC Service 1&quot; provided-by=&quot;provider-app-x&quot;/&gt;&lt;dubbo:reference interface=&quot;RPC Service 2&quot; provided-by=&quot;provider-app-x&quot; /&gt;&lt;dubbo:reference interface=&quot;RPC Service N&quot; provided-by=&quot;provider-app-y&quot; /&gt; 以上指定 provider 应用名的方式是 Spring Cloud 当前的做法，需要 consumer 端的开发者显示指定其要消费的 provider 应用。 以上问题的根源在于注册中心不知道任何 RPC 服务相关的信息，因此只能通过应用名来查询。 为了使整个开发流程对老的 Dubbo 用户更透明，同时避免指定 provider 对可扩展性带来的影响（参见下方说明），我们设计了一套 RPC 服务到应用名的映射关系，以尝试在 consumer 自动完成 RPC 服务到 provider 应用名的转换。 Dubbo 之所以选择建立一套“接口-应用”的映射关系，主要是考虑到 service - app 映射关系的不确定性。一个典型的场景即是应用/服务拆分，如上面提到的配置，PC Service 2 是定义于 provider-app-x 中的一个服务，未来它随时可能会被开发者分拆到另外一个新的应用如 provider-app-x-1 中，这个拆分要被所有的 PC Service 2 消费方感知到，并对应用进行修改升级，如改为，这样的升级成本不可否认还是挺高的。到底是 Dubbo 框架帮助开发者透明的解决这个问题，还是交由开发者自己去解决，当然这只是个策略选择问题，并且 Dubbo 2.7.5+ 版本目前是都提供了的。其实我个人更倾向于交由业务开发者通过组织上的约束来做，这样也可进一步降低 Dubbo 框架的复杂度，提升运行态的稳定性。 总结与展望应用级服务发现机制是 Dubbo 面向云原生走出的重要一步，它帮 Dubbo 打通了与其他微服务体系之间在地址发现层面的鸿沟，也成为 Dubbo 适配 Kubernetes Native Service 等基础设施的基础。我们期望 Dubbo 在新模型基础上，能继续保留在编程易用性、服务治理能力等方面强大的优势。但是我们也应该看到应用粒度的模型一方面带来了新的复杂性，需要我们继续去优化与增强；另一方面，除了地址存储与推送之外，应用粒度在帮助 Dubbo 选址层面也有进一步挖掘的潜力。","link":"/dubbo-app-pubsub/"},{"title":"Dubbo中的连接控制，你真的理解吗？","text":"前言这是一篇很久之前就想动笔写的文章，最近正好看到群里有小伙伴分享了 Dubbo 连接相关的文章，才又让我想起了这个话题。今天想跟大家聊的便是 Dubbo 中的连接控制这一话题。说到“连接控制”，可能有读者还没反应过来，但你对下面的配置可能不会感到陌生： 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; connections=&quot;10&quot; /&gt; 如果你还不了解 Dubbo 中连接控制的用法，可以参考官方文档：https://dubbo.apache.org/zh/docs/advanced/config-connections/ ，话说最近 Dubbo 官方文档来了一次大换血，好多熟悉的文档差点都没找到在哪儿 Orz。 众所周知，dubbo 协议通信默认是长连接，连接配置功能用于决定消费者与提供者建立的长连接数。但官方文档只给出了该功能的使用方法，却并没有说明什么时候应该配置连接控制，本文将主要围绕该话题进行探讨。 本文也会涉及长连接相关的一些知识点。 使用方式先来看一个 Dubbo 构建的简单 demo，启动一个消费者（192.168.4.226）和一个提供者（192.168.4.224），配置他们的直连。 消费者： 123&lt;dubbo:reference id=&quot;userService&quot; check=&quot;false&quot; interface=&quot;org.apache.dubbo.benchmark.service.UserService&quot; url=&quot;dubbo://192.168.4.224:20880&quot;/&gt; 提供者： 12&lt;dubbo:service interface=&quot;org.apache.dubbo.benchmark.service.UserService&quot; ref=&quot;userService&quot; /&gt;&lt;bean id=&quot;userService&quot; class=&quot;org.apache.dubbo.benchmark.service.UserServiceServerImpl&quot;/&gt; 长连接是看不见摸不着的东西，我们需要一个观测性工作来”看到“它。启动提供者和消费者之后，可以使用如下的命令查看 tcp 连接情况 Mac 下可使用：lsof -i:20880 Linux 下可使用：netstat -ano | grep 20880 提供者： 123[root ~]# netstat -ano | grep 20880tcp6 0 0 192.168.4.224:20880 :::* LISTEN off (0.00/0/0)tcp6 2502 0 192.168.4.224:20880 192.168.4.226:59100 ESTABLISHED off (0.00/0/0) 消费者： 12[root@ ~]# netstat -ano | grep 20880tcp6 320 720 192.168.4.226:59110 192.168.4.224:20880 ESTABLISHED on (0.00/0/0) 通过上述观察到的现象我们可以发现几个事实。 仅仅是启动了提供者和消费者，上述的 TCP 连接就已经存在了，要知道我并没有触发调用。也就是说，Dubbo 建连的默认策略是在地址发现时，而不是在调用时。当然，你也可以通过延迟加载 lazy=&quot;true&quot; 来修改这一行为，这样可以将建联延迟到调用时。 1234&lt;dubbo:reference id=&quot;userService&quot; check=&quot;false&quot; interface=&quot;org.apache.dubbo.benchmark.service.UserService&quot; url=&quot;dubbo://192.168.4.224:20880&quot; lazy=&quot;true&quot;/&gt; 除此之外，还可以发现消费者和提供者之间只有一条长连接，20880 是 Dubbo 提供者默认开放的端口，就跟 tomcat 默认开放的 8080 一个地位，而 59110 是消费者随机生成的一个端口。（我之前跟一些朋友交流过，发现很多人不知道消费者也是需要占用一个端口的） 而今天的主角”连接控制“便可以控制长连接的数量，例如我们可以进行如下的配置 1234&lt;dubbo:reference id=&quot;userService&quot; check=&quot;false&quot; interface=&quot;org.apache.dubbo.benchmark.service.UserService&quot; url=&quot;dubbo://192.168.4.224:20880&quot; connections=&quot;2&quot; /&gt; 再启动一次消费者，观察长连接情况 提供者： 1234[root@ ~]# netstat -ano | grep 20880tcp6 0 0 192.168.4.224:20880 :::* LISTEN off (0.00/0/0)tcp6 2508 96 192.168.4.224:20880 192.168.4.226:59436 ESTABLISHED on (0.00/0/0)tcp6 5016 256 192.168.4.224:20880 192.168.4.226:59434 ESTABLISHED on (0.00/0/0) 消费者： 123[root@ ~]# netstat -ano | grep 20880tcp6 0 2520 192.168.4.226:59436 192.168.4.224:20880 ESTABLISHED on (0.00/0/0)tcp6 48 1680 192.168.4.226:59434 192.168.4.224:20880 ESTABLISHED on (0.00/0/0) 可以看到，这里已经变成两条长连接了。 什么时候需要配置多条长连接现在我们知道了如何进行连接控制，但什么时候我们应该配置多少条长连接呢？这个时候我可以跟你说，具体视生产情况而定，但你如果你经常看我的公众号，肯定会知道这不是我的风格，我的风格是什么？benchmark！ 写作之前，我跟几个同事和网友对这个话题进行了简单的讨论，其实也没有什么定论，无非是对单连接和多连接吞吐量高低不同的论调。参考既往 Dubbo github 中的 issue，例如：https://github.com/apache/dubbo/pull/2457，我也参与了这个 pr 的讨论，讲道理，我是持怀疑态度的，我当时的观点是多连接不一定能够提升服务的吞吐量（还是挺保守的，没有这么绝对）。 那接下来，还是用 benchmark 来说话吧，测试工程还是我们的老朋友，使用 Dubbo 官方提供的 dubbo-benchmark 工程。 测试工程地址：https://github.com/apache/dubbo-benchmark.git 测试环境：2 台阿里云 Linux 4c8g ECS 测试工程在之前的文章介绍过，这里就不过多赘述了，测试方案也非常简单，两轮 benchmark，分别测试 connections=1 和 connections=2 时，观察测试方法的吞吐量。 说干就干，省略一堆测试步骤，直接给出测试结果。 connections=1 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 22265.286 ± 3060.319 ops/sClient.existUser thrpt 3 33129.331 ± 1488.404 ops/sClient.getUser thrpt 3 19916.133 ± 1745.249 ops/sClient.listUser thrpt 3 3523.905 ± 590.250 ops/s connections=2 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 31111.698 ± 3039.052 ops/sClient.existUser thrpt 3 42449.230 ± 2964.239 ops/sClient.getUser thrpt 3 30647.173 ± 2551.448 ops/sClient.listUser thrpt 3 6581.876 ± 469.831 ops/s 从测试结果来看，似乎单连接和多连接的差距是非常大的，近乎可以看做是 2 倍！看起来连接控制的效果真是好呀，那么事实真的如此吗？ 按照这种方案第一次测试下来之后，我也不太相信这个结果，因为我之前按照其他方式做过多连接的测试，并且我也参加过第三届中间件挑战赛，使得我对长连接的认知是：大多数时候，单连接往往能发挥出最优的性能。即使由于硬件原因，这个差距也不应该是两倍。怀着这样的疑问，我开始研究，是不是我的测试场景出了什么问题呢？ 发现测试方案的问题经过和闪电侠的讨论，他的一席话最终让我定位到了问题的所在。 不知道大家看完我和闪电侠的对话，有没有立刻定位到问题所在。 之前测试方案最大的问题便是没有控制好变量，殊不知：在连接数变化的同时，实际使用的 IO 线程数实际也发生了变化。 Dubbo 使用 Netty 来实现长连接通信，提到长连接和 IO 线程的关系，这里就要介绍到 Netty 的连接模型了。一言以蔽之，Netty 的设置 IO worker 线程和 channel 是一对多的绑定关系，即一个 channel 在建连之后，便会完全由一个 IO 线程来负责全部的 IO 操作。再来看看 Dubbo 是如何设置 NettyClient 和 NettyServer 的 worker 线程组的： 客户端 org.apache.dubbo.remoting.transport.netty4.NettyClient： 123456789101112private static final EventLoopGroup NIO_EVENT_LOOP_GROUP = eventLoopGroup(Constants.DEFAULT_IO_THREADS, &quot;NettyClientWorker&quot;);@Overrideprotected void doOpen() throws Throwable { final NettyClientHandler nettyClientHandler = new NettyClientHandler(getUrl(), this); bootstrap = new Bootstrap(); bootstrap.group(NIO_EVENT_LOOP_GROUP) .option(ChannelOption.SO_KEEPALIVE, true) .option(ChannelOption.TCP_NODELAY, true) .option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT); ...} Constants.DEFAULT_IO_THREADS 在 org.apache.dubbo.remoting.Constants 中被写死了 1int DEFAULT_IO_THREADS = Math.min(Runtime.getRuntime().availableProcessors() + 1, 32); 在我的 4c8g 的机器上，默认等于 5。 服务端 org.apache.dubbo.remoting.transport.netty4.NettyServer： 123456789101112131415161718protected void doOpen() throws Throwable { bootstrap = new ServerBootstrap(); bossGroup = NettyEventLoopFactory.eventLoopGroup(1, &quot;NettyServerBoss&quot;); workerGroup = NettyEventLoopFactory.eventLoopGroup( getUrl().getPositiveParameter(IO_THREADS_KEY, Constants.DEFAULT_IO_THREADS), &quot;NettyServerWorker&quot;); final NettyServerHandler nettyServerHandler = new NettyServerHandler(getUrl(), this); channels = nettyServerHandler.getChannels(); ServerBootstrap serverBootstrap = bootstrap.group(bossGroup, workerGroup) .channel(NettyEventLoopFactory.serverSocketChannelClass()); .option(ChannelOption.SO_REUSEADDR, Boolean.TRUE) .childOption(ChannelOption.TCP_NODELAY, Boolean.TRUE) .childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT); } 服务端倒是可以配置，例如我们可以通过 protocol 来控制服务端的 IO 线程数： 1&lt;dubbo:protocol name=&quot;dubbo&quot; host=&quot;${server.host}&quot; server=&quot;netty4&quot; port=&quot;${server.port}&quot; iothreads=&quot;5&quot;/&gt; 如果不设置，则跟客户端逻辑一致，是 core + 1 个线程。 好了，问题就在这儿，由于我并没有进行任何 IO 线程的设置，所以客户端和服务端都会默认开启 5 个 IO 线程。当 connections=1 时，Netty 会将 channel1 绑定到一个 IO 线程上，而当 connections=2 时，Netty 会将 channel1 和 channel2 按照顺序绑定到 NettyWorkerThread-1和 NettyWorkerThread-2 上，这样就会有两个 IO 线程在工作，这样的测试结果当然是不公平的。 这里需要考虑实际情况，在实际生产中，大多数时候都是分布式场景，连接数一定都是大于 IO 线程数的，所以基本不会出现测试场景中的 channel 数少于 IO 线程数的场景。 解决方案也很简单，我们需要控制变量，让 IO 线程数一致，仅仅观察连接数对吞吐量的影响。针对服务端，可以在 protocol 层配置 iothreads=1；针对客户端，由于源码被写死了，这里我只能通过修改源码的方式，重新本地打了一个包，使得客户端 IO 线程数也可以通过 -D 参数指定。 改造之后的，我们得到了如下的测试结果： 1 IO 线程 1 连接 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 22265.286 ± 3060.319 ops/sClient.existUser thrpt 3 33129.331 ± 1488.404 ops/sClient.getUser thrpt 3 19916.133 ± 1745.249 ops/sClient.listUser thrpt 3 3523.905 ± 590.250 ops/s 1 IO 线程 2 连接 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 21776.436 ± 1888.845 ops/sClient.existUser thrpt 3 31826.320 ± 1350.434 ops/sClient.getUser thrpt 3 19354.470 ± 369.486 ops/sClient.listUser thrpt 3 3506.714 ± 18.924 ops/s 可以发现，单纯提升连接数并不会提升服务的吞吐量，这样的测试结果也更加符合我认知的预期。 总结从上述测试的结果来看，一些配置参数并不是越大就代表了越好，类似的例子我也在多线程写文件等场景分析过，唯有理论分析+实际测试才能得出值得信服的结论。当然个人的测试，也可能会因为局部性关键信息的遗漏，导致误差，例如，如果我最终没有发现 IO 线程数和连接数之间的隐性关联，很容易就得出连接数和吞吐量成正比的错误结论了。当然，也不一定就代表本文最终的结论是靠谱的，说不定还是不够完善的，也欢迎大家留言，提出意见和建议。 最终回到最初的问题，我们什么时候应该配置 Dubbo 的连接控制呢？按照我个人的经验，大多数时候，生产环境下连接数是非常多的，你可以挑选一台线上的主机，通过 netstat -ano| grep 20880| wc -l 来大概统计下，一般是远超 IO 线程数的，没必要再多配置成倍的连接数，连接数和吞吐量并不是一个线性增长的关系。 Dubbo 框架有这个能力和大家真的需要用这个能力完全是两码事，我相信大多数读者应该已经过了技术新鲜感驱动项目的阶段了吧？如果有一天你需要控制连接数，去达到一定特殊的用途，你就会真心感叹，Dubbo 真是强呀，这个扩展点都有。 Dubbo 的连接控制真的完全没有用吗？也不尽然，我的测试场景还是非常有限的，可能在不同硬件上会跑出不一样的效果，例如我在第三届中间件性能挑战赛中，就是用 2 连接跑出了最好的成绩，并非单连接。 最后，你如果仅仅使用 Dubbo 去维系你们的微服务架构，大部分情况不需要关注到连接控制这个特性，多花点时间搬砖吧，就酱，我也去搬砖了。","link":"/dubbo-channel/"},{"title":"一文聊透 Dubbo 优雅停机","text":"1 前言一年之前，我曾经写过一篇《研究优雅停机时的一点思考》，主要介绍了 kill -9，kill -15 两个 Linux 指令的含义，并且针对性的聊到了 Spring Boot 应用如何正确的优雅停机，算是本文的前置文章，如果你对上述概念不甚了解，建议先去浏览一遍，再回头来看这篇文章。这篇文章将会以 Dubbo 为例，既聊架构设计，也聊源码，聊聊服务治理框架要真正实现优雅停机，需要注意哪些细节。 本文的写作思路是从 Dubbo 2.5.x 开始，围绕优雅停机这个优化点，一直追溯到最新的 2.7.x。先对 Dubbo 版本做一个简单的科普：2.7.x 和 2.6.x 是目前官方推荐使用的版本，其中 2.7.x 是捐献给 Apache 的版本，具备了很多新的特性，目前最新的 release 版本是 2.7.4，处于生产基本可用的状态；2.6.x 处于维护态，主要以 bugfix 为主，但经过了很多公司线上环境的验证，所以求稳的话，可以使用 2.6.x 分支最新的版本。至于 2.5.x，社区已经放弃了维护，并且 2.5.x 存在一定数量的 bug，本文介绍的 Dubbo 优雅停机特性便体现了这一点。 2 优雅停机的意义优雅停机一直是一个非常严谨的话题，但由于其仅仅存在于重启、下线这样的部署阶段，导致很多人忽视了它的重要性，但没有它，你永远不能得到一个完整的应用生命周期，永远会对系统的健壮性持怀疑态度。 同时，优雅停机又是一个庞大的话题 操作系统层面，提供了 kill -9 （SIGKILL）和 kill -15（SIGTERM） 两种停机策略 语言层面，Java 应用有 JVM shutdown hook 这样的概念 框架层面，Spring Boot 提供了 actuator 的下线 endpoint，提供了 ContextClosedEvent 事件 容器层面，Docker ：当执行 docker stop 命令时，容器内的进程会收到 SIGTERM 信号，那么 Docker Daemon 会在 10s 后，发出 SIGKILL 信号；K8S 在管理容器生命周期阶段中提供了 prestop 钩子方法 应用架构层面，不同架构存在不同的部署方案。单体式应用中，一般依靠 nginx 这样的负载均衡组件进行手动切流，逐步部署集群；微服务架构中，各个节点之间有复杂的调用关系，上述这种方案就显得不可靠了，需要有自动化的机制。 为避免该话题过度发散，本文的重点将会集中在框架和应用架构层面，探讨以 Dubbo 为代表的微服务架构在优雅停机上的最佳实践。Dubbo 的优雅下线主要依赖于注册中心组件，由其通知消费者摘除下线的节点，如下图所示： 上述的操作旨在让服务消费者避开已经下线的机器，但这样就算实现了优雅停机了吗？似乎还漏掉了一步，在应用停机时，可能还存在执行到了一半的任务，试想这样一个场景：一个 Dubbo 请求刚到达提供者，服务端正在处理请求，收到停机指令后，提供者直接停机，留给消费者的只会是一个没有处理完毕的超时请求。 结合上述的案例，我们总结出 Dubbo 优雅停机需要满足两点基本诉求： 服务消费者不应该请求到已经下线的服务提供者 在途请求需要处理完毕，不能被停机指令中断 优雅停机的意义：应用的重启、停机等操作，不影响业务的连续性。 3 优雅停机初始方案 — 2.5.x为了让读者对 Dubbo 的优雅停机有一个最基础的理解，我们首先研究下 Dubbo 2.5.x 的版本，这个版本实现优雅停机的方案相对简单，容易理解。 3.1 入口类：AbstractConfig123456789public abstract class AbstractConfig implements Serializable { static { Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() { public void run() { ProtocolConfig.destroyAll(); } }, &quot;DubboShutdownHook&quot;)); }} 在 AbstractConfig 的静态块中，Dubbo 注册了一个 shutdown hook，用于执行 Dubbo 预设的一些停机逻辑，继续跟进 ProtocolConfig.destroyAll() 。 3.2 ProtocolConfig12345678910111213141516171819202122232425public static void destroyAll() { if (!destroyed.compareAndSet(false, true)) { return; } AbstractRegistryFactory.destroyAll(); // ①注册中心注销 // Wait for registry notification try { Thread.sleep(ConfigUtils.getServerShutdownTimeout()); // ② sleep 等待 } catch (InterruptedException e) { logger.warn(&quot;Interrupted unexpectedly when waiting for registry notification during shutdown process!&quot;); } ExtensionLoader&lt;Protocol&gt; loader = ExtensionLoader.getExtensionLoader(Protocol.class); for (String protocolName : loader.getLoadedExtensions()) { try { Protocol protocol = loader.getLoadedExtension(protocolName); if (protocol != null) { protocol.destroy(); // ③协议/流程注销 } } catch (Throwable t) { logger.warn(t.getMessage(), t); } }} Dubbo 中的 Protocol 这个词不太能望文生义，它一般被翻译为”协议”，但我更习惯将它理解为“流程”，从 Protocol 接口的三个方法反而更加容易理解。 12345public interface Protocol { &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException; &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException; void destroy();} 它定义了暴露、订阅、注销这三个生命周期方法，所以不难理解为什么 Dubbo 会把 shutdown hook 触发后的注销方法定义在 ProtocolConfig 中了。 回到 ProtocolConfig 的源码中，我把 ProtocolConfig 中执行的优雅停机逻辑分成了三部分，其中第 1，2 部分和注册中心（Registry）相关，第 3 部分和协议/流程（Protocol）相关，分成下面的 3.3 和 3.4 两部分来介绍。 3.3 注册中心注销逻辑123456789101112131415161718public abstract class AbstractRegistryFactory implements RegistryFactory { public static void destroyAll() { LOCK.lock(); try { for (Registry registry : getRegistries()) { try { registry.destroy(); } catch (Throwable e) { LOGGER.error(e.getMessage(), e); } } REGISTRIES.clear(); } finally { // Release the lock LOCK.unlock(); } }} 这段代码对应了 3.2 小节 ProtocolConfig 源码的第 1 部分，代表了注册中心的注销逻辑，更深一层的源码不需要 debug 进去了，大致的逻辑就是删除掉注册中心中本节点对应的服务提供者地址。 123456// Wait for registry notificationtry { Thread.sleep(ConfigUtils.getServerShutdownTimeout());} catch (InterruptedException e) { logger.warn(&quot;Interrupted unexpectedly when waiting for registry notification during shutdown process!&quot;);} 这段代码对应了 3.2 小节 ProtocolConfig 源码的第 2 部分，ConfigUtils.getServerShutdownTimeout() 默认值是 10s，为什么需要在 shutdown hook 中等待 10s 呢？在注释中可以发现这段代码的端倪，原来是为了给服务消费者一点时间，确保等到注册中心的通知。10s 显然是一个经验值，这里也不妨和大家探讨一下，如何稳妥地设置这个值呢？ 设置的过短。由于注册中心通知消费者取消订阅某个地址是异步通知过去的，可能消费者还没收到通知，提供者这边就停机了，这就违背了我们的诉求 1：服务消费者不应该请求到已经下线的服务提供者。 设置的过长。这会导致发布时间变长，带来不必要的等待。 两个情况对比下，起码可以得出一个实践经验：如果拿捏不准等待时间，尽量设置一个宽松的一点的等待时间。 这个值主要取决三点因素： 集群规模的大小。如果只有几个服务，每个服务只有几个实例，那么再弱鸡的注册中心也能很快的下发通知。 注册中心的选型。以 Naocs 和 Zookeeper 为例，同等规模服务实例下 Nacos 在推送地址方面的能力远超 Zookeeper。 网络状况。服务提供者和服务消费者与注册中心的交互逻辑走的 TCP 通信，网络状况也会影响到推送时间。 所以需要根据实际部署场景测量出最合适的值。 3.4 协议/流程注销逻辑1234567891011ExtensionLoader&lt;Protocol&gt; loader = ExtensionLoader.getExtensionLoader(Protocol.class);for (String protocolName : loader.getLoadedExtensions()) { try { Protocol protocol = loader.getLoadedExtension(protocolName); if (protocol != null) { protocol.destroy(); } } catch (Throwable t) { logger.warn(t.getMessage(), t); }} 这段代码对应了 3.2 小节 ProtocolConfig 源码的第 3 部分，在运行时，loader.getLoadedExtension(protocolName) 这段代码会加载到两个协议 ：DubboProtocol 和 Injvm 。后者 Injvm 实在没啥好讲的，主要来分析一下 DubboProtocol 的逻辑。 DubboProtocol 实现了我们前面提到的 Protocol 接口，它的 destory 方法是我们重点要看的。 123456789101112131415161718192021222324252627public class DubboProtocol extends AbstractProtocol { public void destroy() { for (String key : new ArrayList&lt;String&gt;(serverMap.keySet())) { ExchangeServer server = serverMap.remove(key); if (server != null) { server.close(ConfigUtils.getServerShutdownTimeout()); } } for (String key : new ArrayList&lt;String&gt;(referenceClientMap.keySet())) { ExchangeClient client = referenceClientMap.remove(key); if (client != null) { client.close(ConfigUtils.getServerShutdownTimeout()); } } for (String key : new ArrayList&lt;String&gt;(ghostClientMap.keySet())) { ExchangeClient client = ghostClientMap.remove(key); if (client != null) { client.close(ConfigUtils.getServerShutdownTimeout()); } } stubServiceMethodsMap.clear(); super.destroy(); }} 主要分成了两部分注销逻辑：server 和 client，注意这里是先注销了服务提供者后，再注销了服务消费者，这样做是有意为之。在 RPC 调用中，经常是一个远程调用触发一个远程调用，所以在关闭一个节点时，应该先切断上游的流量，所以这里是先注销了服务提供者，这样从一定程度上，降低了后面服务消费者被调用到的可能性（当然，服务消费者也有可能被单独调用到）。由于 server 和 client 的流程类似，所以我只选取了 server 部分来分析具体的注销逻辑。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void close(final int timeout) { startClose(); if (timeout &gt; 0) { final long max = (long) timeout; final long start = System.currentTimeMillis(); if (getUrl().getParameter(Constants.CHANNEL_SEND_READONLYEVENT_KEY, true)) { // 如果注册中心有延迟，会立即受到readonly事件，下次不会再调用这台机器，当前已经调用的会处理完 sendChannelReadOnlyEvent(); } while (HeaderExchangeServer.this.isRunning() // ① &amp;&amp; System.currentTimeMillis() - start &lt; max) { try { Thread.sleep(10); } catch (InterruptedException e) { logger.warn(e.getMessage(), e); } } } doClose(); // ② server.close(timeout); // ③ } private boolean isRunning() { Collection&lt;Channel&gt; channels = getChannels(); for (Channel channel : channels) { if (DefaultFuture.hasFuture(channel)) { return true; } } return false; }private void doClose() { if (!closed.compareAndSet(false, true)) { return; } stopHeartbeatTimer(); try { scheduled.shutdown(); } catch (Throwable t) { logger.warn(t.getMessage(), t); } } 化繁为简，这里只挑选上面代码中打标的两个地方进行分析 判断服务端是否还在处理请求，在超时时间内一直等待到所有任务处理完毕 关闭心跳检测 关闭 NettyServer 特别需要关注第一点，正符合我们在一开始提出的优雅停机的诉求 2：“在途请求需要处理完毕，不能被停机指令中断”。 3.5 优雅停机初始方案总结上述介绍的几个类构成了 Dubbo 2.5.x 的优雅停机方案，简单做一下总结，Dubbo 的优雅停机逻辑时序如下： 123456789101112Registry 注销等待 -Ddubbo.service.shutdown.wait 秒，等待消费方收到下线通知Protocol 注销 DubboProtocol 注销 NettyServer 注销 等待处理中的请求完毕 停止发送心跳 关闭 Netty 相关资源 NettyClient 注销 停止发送心跳 等待处理中的请求完毕 关闭 Netty 相关资源 Dubbo 2.5.3 优雅停机的缺陷 如果你正在使用的 Dubbo 版本 &lt;= 2.5.3，一些并发问题和代码缺陷会导致你的应用不能很好的实现优雅停机功能，请尽快升级。 详情可以参考该 pull request 的变更：https://github.com/apache/dubbo/pull/568 4 Spring 容器下 Dubbo 的优雅停机上述的方案在不使用 Spring 时的确是无懈可击的，但由于现在大多数开发者选择使用 Spring 构建 Dubbo 应用，上述的方案会存在一些缺陷。 由于 Spring 框架本身也依赖于 shutdown hook 执行优雅停机，并且与 Dubbo 的优雅停机会并发执行，而 Dubbo 的一些 Bean 受 Spring 托管，当 Spring 容器优先关闭时，会导致 Dubbo 的优雅停机流程无法获取相关的 Bean，从而优雅停机失效。 Dubbo 开发者们迅速意识到了 shutdown hook 并发执行的问题，开始了一系列的补救措施。 4.1 增加 ShutdownHookListenerSpring 如此受欢迎的原因之一便是它的扩展点非常丰富，例如它提供了 ApplicationListener 接口，开发者可以实现这个接口监听到 Spring 容器的关闭事件，为解决 shutdown hook 并发执行的问题，在 Dubbo 2.6.3 中新增了 ShutdownHookListener 类，用作 Spring 容器下的关闭 Dubbo 应用的钩子。 123456789101112private static class ShutdownHookListener implements ApplicationListener { @Override public void onApplicationEvent(ApplicationEvent event) { if (event instanceof ContextClosedEvent) { // we call it anyway since dubbo shutdown hook make sure its destroyAll() is re-entrant. // pls. note we should not remove dubbo shutdown hook when spring framework is present, this is because // its shutdown hook may not be installed. DubboShutdownHook shutdownHook = DubboShutdownHook.getDubboShutdownHook(); shutdownHook.destroyAll(); } }} 当服务提供者 ServiceBean 和服务消费者 ReferenceBean 被初始化时，会触发该钩子被创建。 再来看看 AbstractConfig 中的代码，依旧保留了 JVM 的 shutdown hook 12345public abstract class AbstractConfig implements Serializable { static { Runtime.getRuntime().addShutdownHook(DubboShutdownHook.getDubboShutdownHook()); }} 也就是说，在 Spring 环境下会注册两个钩子，在 Non-Spring 环境下只会有一个钩子，但看到 2.6.x 的实现大家是否意识到了两个问题呢？ 两个钩子并发执行不会报错吗？ 为什么在 Spring 下不取消 JVM 的钩子，只保留 Spring 的钩子不就可以工作了吗？ 先解释第一个问题，这个按照我的理解，这段代码的 Commiter 可能认为只需要有一个 Spring 的钩子能正常注销就完事了，不需要考虑另外一个报不报错，因为都是独立的线程，不会有很大的影响。 再解释第二个问题，其实这个疑问的答案就藏在上面 ShutdownHookListener 代码的注释中，这段注释的意思是说：在 Spring 框架下不能直接移除原先的 JVM 钩子，因为 Spring 框架可能没有注册 ContextClosed 事件。啥意思呢？这里涉及到 Spring 框架生命周期的一个细节，我打算单独介绍一下。 4.2 Spring 的容器关闭事件详解在 Spring 中，我们可以使用至少三种方式来注册容器关闭时一些收尾工作： 使用 DisposableBean 接口 1234567public class TestDisposableBean implements DisposableBean { @Override public void destroy() throws Exception { System.out.println(&quot;== invoke DisposableBean ==&quot;); }} 使用 @PreDestroy 注解 12345678public class TestPreDestroy { @PreDestroy public void preDestroy(){ System.out.println(&quot;== invoke preDestroy ==&quot;); }} 使用 ApplicationListener 监听 ContextClosedEvent 12345678applicationContext.addApplicationListener(new ApplicationListener&lt;ApplicationEvent&gt;() { @Override public void onApplicationEvent(ApplicationEvent applicationEvent) { if (applicationEvent instanceof ContextClosedEvent) { System.out.println(&quot;== receive context closed event ==&quot;); } }}); 但需要注意的是，在使用 SpringBoot 内嵌 Tomcat 容器时，容器关闭钩子是自动被注册，但使用纯粹的 Spring 框架或者外部 Tomcat 容器，需要显式的调用 context.registerShutdownHook(); 接口进行注册 1234567891011ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring/beans.xml&quot;);context.start();context.registerShutdownHook();context.addApplicationListener(new ApplicationListener&lt;ApplicationEvent&gt;() { @Override public void onApplicationEvent(ApplicationEvent applicationEvent) { if (applicationEvent instanceof ContextClosedEvent) { System.out.println(&quot;== receive context closed event ==&quot;); } }}); 否则，上述三种回收方法都无法工作。我们来看看 registerShutdownHook() 都干了啥 123456789101112131415161718public abstract class AbstractApplicationContext extends DefaultResourceLoader implements ConfigurableApplicationContext, DisposableBean{ @Override public void registerShutdownHook() { if (this.shutdownHook == null) { // No shutdown hook registered yet. this.shutdownHook = new Thread() { @Override public void run() { synchronized (startupShutdownMonitor) { doClose(); } } }; Runtime.getRuntime().addShutdownHook(this.shutdownHook); // 重点！ } }} 其实也就是显式注册了一个属于 Spring 的钩子。这也解释上了 4.1 小节中，为什么有那段注释了，注册了事件不一定管用，还得保证 Spring 容器注册了它自己的钩子。 4.3 Dubbo 优雅停机中级方案总结第 4 节主要介绍了 Dubbo 开发者们在 Spring 环境下解决 Dubbo 优雅停机并发执行 shutdown hook 时的缺陷问题，但其实还不完善，因为在 Spring 环境下，如果没有显式注册 Spring 的 shutdown， 还是会存在缺陷的，准确的说，Dubbo 2.6.x 版本可以很好的在 Non-Spring、Spring Boot、Spring + ContextClosedEvent 环境下很好的工作。 5 Dubbo 2.7 最终方案12345678910public class SpringExtensionFactory implements ExtensionFactory { public static void addApplicationContext(ApplicationContext context) { CONTEXTS.add(context); if (context instanceof ConfigurableApplicationContext) { ((ConfigurableApplicationContext) context).registerShutdownHook(); DubboShutdownHook.getDubboShutdownHook().unregister(); } BeanFactoryUtils.addApplicationListener(context, SHUTDOWN_HOOK_LISTENER); }} 这段代码寥寥数行，却是经过了深思熟虑之后的产物，期间迭代了 3 个大版本，真是不容易。这段代码很好地解决了第 4 节提出的两个问题 担心两个钩子并发执行有问题？那就在可以注册 Spring 钩子的时候取消掉 JVM 的钩子。 担心当前 Spring 容器没有注册 Spring 钩子？那就显示调用 registerShutdownHook 进行注册。 其他细节方面的优化和 bugfix 我就不进行详细介绍了，可以见得实现一个优雅停机需要考虑的点非常之多。 6 总结优雅停机看似是一个不难的技术点，但在一个通用框架中，使用者的业务场景类型非常多，这会大大加剧整个代码实现的复杂度。 摸清楚整个 Dubbo 优雅停机演化的过程，也着实花费了我一番功夫，有很多实现需要 checkout 到非常古老的分支，同时翻阅了很多 issue、pull request 的讨论，最终才形成了这篇文章，虽然研究的过程是困难的，但获取到真相是让人喜悦的。 在开源产品的研发过程中，服务到每一个类型的用户真的是非常难的一件事，能做的是满足大部分用户。例如 2.6.x 在大多数环境下其实已经没问题了，在 2.7.x 中则是得到了更加的完善，但是我相信，在使用 Dubbo 的部分用户中，可能还是会存在优雅停机的问题，只不过还没有被发现。 商业化的思考：和开源产品一样，商业化产品的研发也同样是一个逐渐迭代的过程，需要数代开发者一起维护一份代码，使用者发现问题，开发者修复问题，这样的正反馈可以形成一个正反馈，促使产品更加优秀。 相关 pull request: 修复 2.5.3 bug 的 pr： https://github.com/apache/dubbo/pull/568 作者：@qinliujie 2.6.x Spring Shutdown Hook Enhancement: https://github.com/apache/dubbo/pull/1763 作者：@ralf0131 https://github.com/apache/dubbo/pull/1820 作者：@ralf0131 2.7.x Spring Shutdown Hook Enhancement: https://github.com/apache/dubbo/pull/3008/ 作者：@beiwei30","link":"/dubbo-gracefully-shutdown/"},{"title":"一文聊透 Dubbo 优雅上线","text":"1 前言在此文之前，我写过一篇 《一文聊透 Dubbo 优雅停机》，这篇文章算是一个续集，优雅停机和优雅上线两者都是微服务生命周期中，开发者必须关心的环节。 优雅上线还有很多称呼：「无损上线」，「延迟发布」，「延迟暴露」。它们的对立面自然是：「有损上线」，「直接发布」。 我最近写的「一文聊透 Dubbo xx」系列文章，都有一个特点，即当你不注重文章中实践，你的 Dubbo 应用依旧可以正常运行，但总归在某些场景 case 下，你的系统会出现问题。做不到优雅上线，你的系统将会出现：在应用刚启动时，就有流量进入，而此时应用尚未初始化完毕，导致调用失败，在集群规模较大时，影响会变得很明显。 2 方案一：延迟发布以 SpingBoot 下使用 Dubbo 为例，被 Dubbo 的 @Service 注解修饰的服务，会按照 Spring 中初始化 Bean 的顺序，串行执行发布逻辑。Dubbo 框架会完成一系列的操作： 创建远程调用的 Proxy 把代理对象注册到 ProviderConsumerRegTable，方便远程调用到来时寻找到对应的服务 向注册中心注册 一旦服务信息注册到注册中心，在消费者看来该服务就是可以被调用的。然而，此时可能出现一些数据库、缓存资源尚未加载完毕的场景，这取决于你的系统有没有对应的组件，它们何时加载完毕，也完全取决于你的业务。如果你担心你的系统存在这种隐患，可以尝试多次重启集群中的任意一台机器，查看调用方是否存在报错，如果有报错，一种可能性是没有实现优雅停机，一种可能性是没有实现优雅上线。 Dubbo 服务暴露的起点一般是以 Spring 容器启动完毕后发出的 ContextRefreshedEvent 事件为准，Dubbo 的 ServiceBean 实现了 ApplicationListener&lt;E extends ApplicationEvent&gt; 接口，用以接收这一容器刷新事件。 1234567public void onApplicationEvent(ContextRefreshedEvent event) { // 是否有延迟导出 &amp;&amp; 是否已导出 &amp;&amp; 是不是已被取消导出 if (isDelay() &amp;&amp; !isExported() &amp;&amp; !isUnexported()) { // 导出服务 export(); }} Dubbo 为服务提供了 delay 配置： 1&lt;dubbo:service delay=&quot;5000&quot; /&gt; 如上配置后，Dubbo 服务将会在 Spring 容器启动后 5s，再执行暴露逻辑。这里 delay 的时长，取决于你系统资源初始化的耗时，没有一个经验值。如果不配置改值，Dubbo 将会在收到 ContextRefreshedEvent 事件后，立即执行发布逻辑。 Dubbo 2.6.5 版本对服务延迟发布逻辑进行了细微的调整，将需要延迟暴露（delay &gt; 0）服务的倒计时动作推迟到了 Spring 初始化完成后进行。在此之前的版本的逻辑不太合理，如果想要让 2.6.5 之前的版本延迟到 Spring 初始化完成后，再暴露服务，可以这样配置：&lt;dubbo:service delay=”-1” /&gt; 本节参考 Dubbo 官方文档 延迟暴露：http://dubbo.apache.org/zh-cn/docs/user/demos/delay-publish.html 3 方案二：QOS 命令上线Dubbo 还为服务提供了另一个配置项： 1&lt;dubbo:service register=&quot;false&quot; /&gt; 该配置项配置后，服务将不会发布到注册中心，可能很多 Dubbo 用户不会注意到这个配置，它的作用恰恰是 QOS 指令使用的。 Dubbo 2.5.8 及以上的版本，还提供了一些在线运维命令。为了演示该命令，我们准备一个 GreetingService 的 demo： 123456789101112131415public class DubboProvider { public static void main(String[] args) throws Exception { ServiceConfig&lt;GreetingsService&gt; service = new ServiceConfig&lt;&gt;(); service.setApplication(new ApplicationConfig(&quot;dubbo-provider&quot;)); service.setRegistry(new RegistryConfig(&quot;nacos://127.0.0.1:8848&quot;)); service.setInterface(GreetingsService.class); service.setRef(new GreetingsServiceImpl()); service.setRegister(false); service.export(); System.out.println(&quot;dubbo service started&quot;); new CountDownLatch(1).await(); }} 注意我们配置了不发布：service.setRegister(false)，由于 QOS 配置是默认打开的，在本地的 22222 端口，可以进入 QOS 控制台。 123456789101112131415161718192021222324252627282930313233343536 krito git:(master) ✗ telnet localhost 22222Trying ::1...Connected to localhost.Escape character is '^]'. ___ __ __ ___ ___ ____ / _ \\ / / / // _ ) / _ ) / __ \\ / // // /_/ // _ |/ _ |/ /_/ / /____/ \\____//____//____/ \\____/ dubbo&gt;lsAs Provider side:+-------------------------------------+---+| Provider Service Name |PUB|+-------------------------------------+---+|com.alibaba.edas.api.GreetingsService| N |+-------------------------------------+---+As Consumer side:+---------------------+---+|Consumer Service Name|NUM|+---------------------+---+dubbo&gt;online OKdubbo&gt;lsAs Provider side:+-------------------------------------+---+| Provider Service Name |PUB|+-------------------------------------+---+|com.alibaba.edas.api.GreetingsService| Y |+-------------------------------------+---+As Consumer side:+---------------------+---+|Consumer Service Name|NUM|+---------------------+---+dubbo&gt; 如上图所示，我们首先查看到 com.alibaba.edas.api.GreetingsService 服务是未发布的，通过 online 命令手动将服务发布，再使用 ls 查看服务列表时，已经显示服务处于发布状态了。 除了使用 telnet，还可以通过 HTTP 访问： 1curl localhost:22222/online 小 tips：在使用 SpringBoot 注解式声明一个 Service 时，register 属性会失效，在 xml 或者 API 方式下声明则运行正常，怀疑是 dubbo-spring-boot-starter 的一个 bug。 大家在 SpringBoot 下使用 Dubbo 需要留意类似的问题，之前有过一些属性在 SpringBoot 注解中未解析或为提供注解配置的案例，在使用时需要注意。 本节参考 Dubbo 官方文档 服务配置说明：http://dubbo.apache.org/zh-cn/docs/user/references/xml/dubbo-service.html QOS：http://dubbo.apache.org/zh-cn/docs/user/references/qos.html 最佳实践本文介绍了两种 Dubbo 的机制： 方案一：延迟发布（delay=5000） 方案二：不发布 + QOS 指令发布（register=false） 想要实现优雅上线，可以采取适合你系统的方式。方案一延迟发布的优势在于实现简单，但具体 delay 多少秒，比较依赖系统维护者的经验。方案二使用 QOS 指令，一般依靠于发布系统，当发布系统检测到固定的资源加载完毕这样一个信号时，自动触发上线命令，更加灵活。 当你系统遇到应用启动时流量有所损失时，就应该考虑一下优雅上线的问题了，更多 Dubbo 使用的注意点，请持续关注公众号。 「技术分享」某种程度上，是让作者和读者，不那么孤独的东西。欢迎关注我的微信公众号：「Kirito的技术分享」","link":"/dubbo-gracefully-startup/"},{"title":"Dubbo 中的 http 协议","text":"太阳红彤彤，花儿五颜六色，各位读者朋友好，又来到了分享 Dubbo 知识点的时候了。说到 Dubbo 框架支持的协议，你的第一反应是什么？大概会有 Dubbo 默认支持的 dubbo 协议，以及老生常谈的由当当贡献给 Dubbo 的 rest 协议，或者是今天的主角 http。截止到目前，Dubbo 最新版本演进到了 2.7.3，已经支持了：dubbo，hessain，http，injvm，jsonrpc，memcached，native-thrift，thrift，redis，rest，rmi，webservice，xml 等协议，有些协议的使用方式还没有补全到官方文档中。原来 Dubbo 支持这么多协议，是不是有点出乎你的意料呢？ 这么多 RPC 协议，可能有人会产生如下的疑问：rest，jsonrpc，webservice 不都是依靠 http 通信吗？为什么还单独有一个 http 协议？先不急着回答这个问题，而是引出今天的话题，先来介绍下 Dubbo 框架中所谓的 http 协议。 Dubbo 中的 http 协议在 Dubbo 使用 http 协议和其他协议基本一样，只需要指定 protocol 即可。 1&lt;dubbo:protocol name=&quot;http&quot; port=&quot;8080&quot; server=&quot;jetty&quot; /&gt; server 属性可选值：jetty，tomcat，servlet。 配置过后，当服务消费者向服务提供者发起调用，底层便会使用标准的 http 协议进行通信。可以直接在 https://github.com/apache/dubbo-samples 中找到官方示例，其中的子模块：dubbo-samples-http 构建了一个 http 协议调用的例子。 为避免大家误解，特在此声明：本文中，所有的 http 协议特指的是 dubbo 中的 http 协议，并非那个大家耳熟能详的通用的 http 协议。 http 协议的底层原理从默认的 dubbo 协议改为 http 协议是非常简单的一件事，上面便是使用者视角所看到的全部的内容了，接下来我们将会探讨其底层实现原理。 翻看 Dubbo 的源码，找到 HttpProtocol 的实现，你可能会吃惊，基本就依靠 HttpProtocol 一个类，就实现了 http 协议 要知道实现自定义的 dubbo 协议，有近 30 个类！http 协议实现的如此简单，背后主要原因有两点： remoting 层使用 http 通信，不需要自定义编解码 借助了 Spring 提供的 HttpInvoker 封装了 refer 和 exporter 的逻辑 Spring 提供的 HttpInvoker 是何方神圣呢？的确是一个比较生僻的概念，但并不复杂，简单来说，就是使用 Java 序列化将对象转换成字节，通过 http 发送出去，在 server 端，Spring 能根据 url 映射，找到容器中对应的 Bean 反射调用的过程，没见识过它也不要紧，可以通过下面的示例快速掌握这一概念。 Spring HttpInvoker 本节内容可参见 Spring 文档：https://docs.spring.io/spring/docs/4.3.24.RELEASE/spring-framework-reference/htmlsingle/#remoting-httpinvoker-server 下面的示例将会展示如何使用 Spring 原生的 HttpInvoker 实现远程调用。 创建服务提供者1234567public class AccountServiceImpl implements AccountService { @Override public Account findById(int id) { Account account = new Account(id, new Date().toString()); return account; }} 123456789101112@BeanAccountService accountService(){ return new AccountServiceImpl();}@Bean(&quot;/AccountService&quot;)public HttpInvokerServiceExporter accountServiceExporter(AccountService accountService){ HttpInvokerServiceExporter exporter = new HttpInvokerServiceExporter(); exporter.setService(accountService); exporter.setServiceInterface(AccountService.class); return exporter;} 暴露服务的代码相当简单，需要注意两点： org.springframework.remoting.httpinvoker.HttpInvokerServiceExporter 是 Spring 封装的一个服务暴露器，它会以 serviceInterface 为公共接口，以 service 为实现类向外提供服务。 @Bean(“/AccountService”) 不仅仅指定了 IOC 容器中 bean 的名字，还充当了路径映射的功能，如果本地服务器暴露在 8080 端口，则示例服务的访问路径为 http://localhost:8080/AccountService 创建服务消费者12345678910@Configurationpublic class HttpProxyConfig { @Bean(&quot;accountServiceProxy&quot;) public HttpInvokerProxyFactoryBean accountServiceProxy(){ HttpInvokerProxyFactoryBean accountService = new HttpInvokerProxyFactoryBean(); accountService.setServiceInterface(AccountService.class); accountService.setServiceUrl(&quot;http://localhost:8080/AccountService&quot;); return accountService; }} 12345678@SpringBootApplicationpublic class HttpClientApp { public static void main(String[] args) { ConfigurableApplicationContext applicationContext = SpringApplication.run(HttpClientApp.class, args); AccountService accountService = applicationContext.getBean(AccountService.class); System.out.println(accountService.findById(10086)); }} 消费者端引用服务同样有两个注意点： org.springframework.remoting.httpinvoker.HttpInvokerProxyFactoryBean 是 Spring 封装的一个服务引用器，serviceInterface 指定了生成代理的接口，serviceUrl 指定了服务所在的地址，与之前配置的服务暴露者的路径需要对应。 HttpInvokerProxyFactoryBean 注册到容器之中时，会同时生成一个 AccountService 接口的代理类，由 Spring 封装远程调用的逻辑。 调用细节分析对于 Spring HttpInvoker 的底层实现，就没必要深究了，但大家肯定还是会好奇一些细节：dubbo 中的 http 报文体是怎么组织的？如何序列化对象的？ 我们使用 wireshark 可以抓取到客户端发送的请求以及服务端响应的报文。 追踪报文流，可以看到详细的请求和响应内容 从 ContentType: application/x-java-serialized-object 和报文 Body 部分的 ASCII 码可以看出，使用的是 Java Serialize 序列化。我们将 Body 部分导出成文件，使用 Java Serialize 反序列化响应，来验证一下它的庐山真面目： 使用 Java Serialize 可以正常反序列化报文，得到结果是 Spring 内置的包装类 RemoteInvocationResult，里面装饰着实际的业务返回结果。 http 协议的意义Dubbo 提供的众多协议有各自适用的场景，例如 dubbo://，dubbo 协议是默认的协议，自定义二进制协议；单个长连接节省资源；基于 tcp，架构于 netty 之上，性能还算可以；协议设计上没有足够的前瞻性，不适合做 service-mesh 谈不上多么优雅，但是好歹风风雨雨用了这么多年，周边也有不少配套组件例如 dubbo2.js, dubbo-go, dubbo-cpp，一定程度解决了多语言的问题。 webservice://,hession://,thrift:// 等协议，基本是为了适配已有协议的服务端 / 客户端，借助于 dubbo 框架的 api，可以使用其功能特性，意义不是特别大。 redis://,memcached:// 等协议，并非是暴露给用户配置的协议，一般是 dubbo 自用，在注册中心模块中会使用到相应的扩展 所有协议的具体使用场景和其特性，我可能会单独写文章来分析，而如今我们要思考的是 dubbo 提供 http 协议到底解决什么问题，什么场景下用户会考虑使用 dubbo 的 http 协议。 我个人认为 dubbo 现如今的 http 协议比较鸡肋，原生 http 通信的优势在于其通用性，基本所有语言都有配套的 http 客户端和服务端支持，但是 dubbo 的 http 协议却使用了 application/x-java-serialized-object 的格式来做为默认的 payload，使得其丧失了跨语言的优势。可能有读者会反驳：HttpInvoker 支持配置序列化格式，不能这么草率的诟病它。但其实我们所关注的恰恰是默认实现，正如 dubbo:// 协议也可以配置 fastjson 作为序列化方案，但是我们同样不认为 dubbo:// 协议是一个优秀的跨语言方案，理由是一样的。当然，评价一个应用层协议是否是优秀的，是否适合做 mesh 等等，需要多种方向去分析，这些我不在本文去分析。 说到底，本文花了一定的篇幅向大家介绍了 dubbo 的 http 协议，到头来却是想告诉你：这是一个比较鸡肋的协议，是不是有些失望呢？不要失望，dubbo 可能在 2.7.4 版本废弃现有的 http 协议，转而使用 jsonrpc 协议替代，其实也就是将 jsonrpc 协议换了个名字而已，而关于 jsonrpc 的细节，我将会在下一篇文章中介绍，届时，我也会分析，为什么 jsonrpc 比现有的 http 协议更适合戴上 http 协议的帽子，至于现有的 http 协议，我更倾向于称之为：spring-httpinvoker 协议。 总结，dubbo 现有 http 协议的意义是什么？如果你习惯于使用 Spring HttpInvoker，那或许现有的 http 协议还有一定的用处，但从 dubbo 交流群和 Spring 文档介绍其所花费的篇幅来看，它还是非常小众的。同时也可以让我们更好地认识协议发展的历史，知道一个协议为什么存在，为什么会被淘汰。 当然，我说了不算，最终还是要看 dubbo 社区的决策，如果你对这个迁移方案感兴趣，想要参与讨论，欢迎大家在 dubbo 社区的邮件列表中发表你的见解 Topic：[Proposal] replace the protocol=”http” with protocol=”jsonrpc” ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/dubbo-http-protocol/"},{"title":"Dubbo 的前世今生 &amp; Dubbo Meetup 南京","text":"Dubbo 的前世今生2011 年 10 月 27 日，阿里巴巴开源了自己的 SOA 服务化治理方案的核心框架 Dubbo，服务治理和 SOA 的设计理念开始逐渐在国内软件行业中落地，并被广泛应用。自开源后，许多非阿里系公司选择使用 Dubbo，其中既有当当网、网易考拉等互联网公司，也有中国人寿、青岛海尔等传统企业。 2012 年 10 月 23 日 Dubbo 2.5.3 发布后，在 Dubbo 开源将满一周年之际，阿里基本停止了对 Dubbo 的主要升级。 2013 年，2014 年，更新了 2 次 Dubbo 2.4 的维护版本，然后停止了所有维护工作。至此，Dubbo 对 Srping 的支持也停留在了 Spring 2.5.6 版本上。 阿里停止维护和升级 Dubbo 期间，当当网开始维护自己的 Dubbo 分支版本 Dubbox，新增支持了新版本的 Spring，支持了 Rest 协议等，并对外开源了 Dubbox。同时，网易考拉也维护了自己的独立分支 Dubbok，可惜并未对外开源。 2017 年 9 月 7 日，Dubbo 悄悄在 GitHub 发布了 2.5.4 版本。随后，又迅速发布了 2.5.5、2.5.6、2.5.7 等版本。在 10 月举行的云栖大会上，阿里宣布 Dubbo 被列入集团重点维护开源项目，这也就意味着 Dubbo 起死回生，开始重新进入快车道。 2018 年 1 月 8 日，Dubbo 2.6.0 版本发布，新版本将之前当当网开源的 Dubbox 进行了合并，实现了 Dubbo 版本的统一整合。 2018 年 2 月 9 日，Apache 基金会的邮件列表上发起了讨论是否接纳阿里的 Dubbo 项目进入 Apache 孵化器的投票。经过一周的投票，邮件列表显示，Dubbo 获得了 14 张赞成票，在无弃权和反对票的情况下，正式通过投票，顺利成为 Apache 基金会孵化项目。 自此，Dubbo 开始了两个长期维护的版本，Dubbo 2.6.x （包名：com.alibaba）稳定维护版本和 Dubbo 2.7.x （包名：org.apache）apache 孵化版本。 2018 ~ 2019 年，在此期间，Dubbo 发布了 4、5 个版本，并发布了 nodejs，python，go 等多语言的客户端。在此期间，Dubbo 社区相继在北京、上海、深圳、成都、杭州等地举办了开发者沙龙。 2019 年 1 月，2.7.0 release 版本发布，这个即将毕业的 apache 版本支持了丰富的新特性，全新的 Dubbo Ops 控制台。 报名 | Dubbo Meetup 南京 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/dubbo-meetup-nj/"},{"title":"一文聊透 Dubbo 元数据中心","text":"前言如果让你在本地构建一个 Dubbo 应用，你会需要额外搭建哪些中间件呢？如果没猜错的话，你的第一反应应该是注册中心，类 Dubbo 的大多数服务治理框架都有注册中心的概念。你可以部署一个 Zookeeper，或者一个 Nacos，看你的喜好。但在 Apache Dubbo 的 2.7 版本后，额外引入了两个中间件：元数据中心和配置中心。 在今年年初 Dubbo 2.7 刚发布时，我就写了一篇文章 《Dubbo 2.7 三大新特性详解》，介绍了包含元数据中心改造在内的三大新特性，但一些细节介绍没有详细呈现出来，在这篇文章中，我将会以 Dubbo 为例，跟大家一起探讨一下服务治理框架中元数据中心的意义与集成细节。 元数据中心介绍服务治理中的元数据（Metadata）指的是服务分组、服务版本、服务名、方法列表、方法参数列表、超时时间等，这些信息将会存储在元数据中心之中。与元数据平起平坐的一个概念是服务的注册信息，即：服务分组、服务版本、服务名、地址列表等，这些信息将会存储在注册中心中。稍微一对比可以发现，元数据中心和注册中心存储了一部分共同的服务信息，例如服务名。两者也有差异性，元数据中心还会存储方法列表即参数列表，注册中心存储了服务地址。上述的概述，体现出了元数据中心和注册中心在服务治理过程中，担任着不同的角色。为了有一个直观的对比，我整理出了下面的表格： 元数据 注册信息 职责 描述服务，定义服务的基本属性 存储地址列表 变化频繁度 基本不变 随着服务上下线而不断变更 数据量 大 小 数据交互/存储模型 消费者/提供者上报，控制台查询 PubSub 模型，提供者上报，消费者订阅 主要使用场景 服务测试、服务 MOCK 服务调用 可用性要求 元数据中心可用性要求不高，不影响主流程 注册中心可用性要求高，影响到服务调用的主流程 下面我会对每个对比点进行单独分析，以加深对元数据中心的理解。 职责在 Dubbo 2.7 版本之前，并没有元数据中心的概念，那时候注册信息和元数据都耦合在一起。Dubbo Provider 的服务配置有接近 30 个配置项，排除一部分注册中心服务治理需要的参数，很大一部分配置项仅仅是 Provider 自己使用，不需要透传给消费者；Dubbo Consumer 也有 20 多个配置项。在注册中心之中，服务消费者列表中只需要关注 application，version，group，ip，dubbo 版本等少量配置。这部分数据不需要进入注册中心，而只需要以 key-value 形式持久化存储在元数据中心即可。从职责来看，将不同职责的数据存储在对应的组件中，会使得逻辑更加清晰。 变化频繁度注册信息和元数据耦合在一起会导致注册中心数据量的膨胀，进而增大注册中心的网络开销，直接造成了服务地址推送慢等负面影响。服务上下线会随时发生，变化的其实是注册信息，元数据是相对不变的。 数据量由于元数据包含了服务的方法列表以及参数列表，这部分数据会导致元数据要比注册信息大很多。注册信息被设计得精简会直接直接影响到服务推送的 SLA。 数据交互/存储模型注册中心采用的是 PubSub 模型，这属于大家的共识，所以注册中心组件的选型一般都会要求其有 notify 的机制。而元数据中心并没有 notify 的诉求，一般只需要组件能够提供 key-value 的存储结构即可。 主要使用场景在服务治理中，注册中心充当了通讯录的职责，在复杂的分布式场景下，让消费者能找到提供者。而元数据中心存储的元数据，主要适用于服务测试、服务 MOCK 等场景，这些场景都对方法列表、参数列表有所诉求。在下面的小节中，我也会对使用场景进行更加详细的介绍。 可用性要求注册中心宕机或者网络不通会直接影响到服务的可用性，它影响了服务调用的主路径。但一般而言，元数据中心出现问题，不会影响到服务调用，它提供的能力是可被降级的。这也阐释了一点，为什么很多用户在 Dubbo 2.7 中没有配置元数据中心，也没有影响到正常的使用。元数据中心在服务治理中扮演的是锦上添花的一个角色。在组件选型时，我们一般也会对注册中心的可用性要求比较高，元数据中心则可以放宽要求。 元数据中心的价值小孩子才分对错，成年人只看利弊。额外引入一个元数据中心，必然带来运维成本、理解成本、迁移成本等问题，那么它具备怎样的价值，来说服大家选择它呢？上面我们介绍元数据中心时已经提到了服务测试、服务 MOCK 等场景，这一节我们重点探讨一下元数据中心的价值。 降低地址推送的时延由于注册中心采用的是 PubSub 模型，数据量的大小会直接影响到服务地址推送时间，不知道你有没有遇到过 No provider available 的报错呢？明明提供者已经启动了，但由于注册中心推送慢会导致很多问题，一方面会影响到服务的可用性，一方面也会增加排查问题的难度。 在一次杭州 Dubbo Meetup 中，网易考拉分享了他们对 Zookeeper 的改造，根源就是 推送量大 -&gt; 存储数据量大 -&gt; 网络传输量大 -&gt; 延迟严重 这一实际案例佐证了元数据改造并不是凭空产生的需求，而是切实解决了一个痛点。 服务测试 &amp; 服务 MOCK在 Dubbo 2.7 之前，虽然注册中心耦合存储了不少本应属于元数据的数据，但也漏掉了一部分元数据，例如服务的方法列表，参数列表。这些是服务测试和服务 MOCK 必备的数据，想要使用这些能力，就必须引入元数据中心。例如开源的 Dubbo Admin 就实现了服务测试功能，用户可以在控制台上对已经发布的服务提供者进行功能测试。可能你之前有过这样的疑惑：为什么只有 Dubbo 2.7 才支持了服务测试呢？啊哈，原因就是 Dubbo 2.7 才有了元数据中心的概念。当然，服务 MOCK 也是如此。 其他场景可以这么理解，任何依赖元数据的功能，都需要元数据中心的支持。其他场景还包括了网关应用获取元数据来进行泛化调用、服务自动化测试等等。再描述一个可能的场景，抛砖引玉。在一次南京 Dubbo Meetup 上，dubbo.js 的作者提及的一个场景，希望根据元数据自动生成 NodeJs 代码，以简化前端的开发量，也是元数据的作用之一。这里就需要发挥各位的想象力了 Dubbo 配置元数据中心目前 Dubbo 最新的版本为 2.7.4，目前支持的几种元数据中心可以从源码中得知（官方文档尚未更新）： 支持 consul、etcd、nacos、redis、zookeeper 这五种组件。 配置方式如下： 1dubbo.metadata-report.address=nacos://127.0.0.1:8848 元数据存储格式剖析前面我们介绍了元数据中心的由来以及价值，还是飘在天上的概念，这一节将会让概念落地。元数据是以怎么样一个格式存储的呢？ 以 DemoService 服务为例： 12&lt;dubbo:service interface=&quot;com.alibaba.dubbo.demo.DemoService&quot; ref=&quot;demoService&quot; executes=&quot;4500&quot; retries=&quot;7&quot; owner=&quot;kirito&quot;/&gt;&lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot; /&gt; 首先观察在 Dubbo 2.6.x 中，注册中心如何存储这个服务的信息： 123456789dubbo://30.5.120.185:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;bean.name=com.alibaba.dubbo.demo.DemoService&amp;dubbo=2.0.2&amp;executes=4500&amp;generic=false&amp;owner=kirito&amp;pid=84228&amp;retries=7&amp;side=provider&amp;timestamp=1552965771067 例如 bean.name 和 owner 这些属性，肯定是没必要注册上来的。 接着，我们在 Dubbo 2.7 中使用最佳实践，为 registry 配置 simplified=true： 123&lt;dubbo:service interface=&quot;com.alibaba.dubbo.demo.DemoService&quot; ref=&quot;demoService&quot; executes=&quot;4500&quot; retries=&quot;7&quot; owner=&quot;kirito&quot;/&gt;&lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot; simplified=&quot;true&quot; /&gt;&lt;dubbo:metadata-report address=&quot;nacos://127.0.0.1:8848&quot;/&gt; 之后再观察注册中心的数据，已经变得相当精简了： 12345dubbo://30.5.120.185:20880/org.apache.dubbo.demo.api.DemoService?application=demo-provider&amp;dubbo=2.0.2&amp;release=2.7.0&amp;timestamp=1552975501873 被精简省略的数据不代表没有用了，而是转移到了元数据中心之中，我们观察一下此时元数据中心中的数据： 最佳实践元数据中心是服务治理中的一个关键组件，但对于大多数用户来说还是一个比较新的概念，我整理了一些我认为的最佳实践，分享给大家。 从 Dubbo 2.6 迁移到 Dubbo 2.7 时，可以采取三步走的策略来平滑迁移元数据。第一步：Dubbo 2.6 + 注册中心，第二步：Dubbo 2.7 + 注册中心 + 元数据中心，第三步：Dubbo 2.7 + 注册中心（simplified=true） + 元数据中心。在未来 Dubbo 的升级版本中，registry 的 simplified 默认值将会变成 true，目前是 false，预留给用户一个升级的时间。 应用在启动时，会发布一次元数据，在此之后会有定时器，一天同步一次元数据，以上报那些运行时生成的 Bean，目前用户不可以配置元数据上报的周期，但可以通过 -Dcycle.report 关闭这一定时器。 元数据中心推荐选型：Nacos 和 Redis。 Dubbo 2.7 还有很多有意思的特性，如果你对 Dubbo 有什么感兴趣的问题，欢迎在文末或者后台进行留言，后面我会继续更新 Dubbo 系列的文章。","link":"/dubbo-metadata/"},{"title":"平滑迁移 Dubbo 服务的思考","text":"前言近日，有报道称在 HashCorp 的商业软件试用协议上发现，旗下所有商业产品禁止在中国境内使用、部署、安装，这其中就包含了 Terraform, Consul, Vagrant 等众多知名软件，其中 Consul 是一个在微服务领域的开源软件，可以用于做注册发现、配置管理等场景。 该新闻在国内发酵后，有人在 Twitter上咨询了HashCorp 公司的创始人，得到的回复是影响的软件仅限于 Vault 这款加密软件，目前 HashCorp 公司的官方网站上已经更新了相关的条款，明确了受影响的产品仅限 Vault 这一款产品。 Consul 开源版是否收到影响？上面的条款里只提到了商业软件，那么开源的 Consul 是否受到影响呢？在 Github 的 Consul 仓库上，可以得知项目的 license 是 Mozilla Public License 2.0 ，这款许可证在 Apache 官网上是 Category B , 属于 Weak Copy Left 许可，那么它有哪些特点呢？ 任何可以使用，复制，修改，重新分发该代码，包括商业目的使用。 如果修改了 MPL 协议许可下的源码，再重新发布这部分源码的话，必须保留原来 MPL 许可证不得更换。 如果基于该项目衍生出更大的项目，那么这部分工作可以使用新许可证的方式进行分发，只要没有修改原来 MPL 许可下的代码。（这也是为什么 Apache 项目的分发的源码中可以包含 MPL 协议下二进制文件的原因） 可以看到，MPL 通常被认为是介于 Apache License 和 GPL/LGPL 之间的一个折中方案。相对于 Apache License，MPL 2.0 要求修改了源码必须保持相同协议；相对于 GPL/LGPL, MPL 2.0 可以商用，同时衍生的作品在一定条件下也可以更换许可证类型。 总体来看的话，开源版 Consul 无论是私用还是商用都是不受限制的。但这也可能是一个警钟，如果对 Consul 还是有所顾忌的话，如何替代掉它呢？ 在微服务领域，Consul 主要被用来做充当注册中心和配置中心，例如 Dubbo 和 SpringCloud 都有对应的支持。本文便以这个事为一个引子，介绍如何平滑地迁移 Dubbo 服务，达到替换注册中心的效果。 平滑迁移服务的定义和意义如果 Dubbo 应用已经部署到生产环境并处于正常运行状态中，此时想将应用的注册中心替换，那么在迁移过程中，保证业务的平稳运行不中断一定是第一要义。我们将保证应用运行不中断，并最终达成注册中心替换的过程称为平滑迁移。可以类比为给飞行中的飞机替换引擎，在项目升级、框架调整等很多时候，现状和终态之间往往都有一个过度方案。 平滑迁移可以避免终态方案一次性上线后出现和原有方案的不兼容性，规避了整体回归的风险 没有哪个互联网公司可以承担的起：“自 xx 至 xx，系统维护一小时，期间服务将无法提供，请广大用户谅解” 这种停机升级方案。 平滑迁移过程说到注册中心迁移，可能很多人第一时间都能想到双注册双订阅这种方案 双注册和双订阅迁移方案是指在应用迁移时同时接入两个注册中心（原有注册中心和新注册中心）以保证已迁移的应用和未迁移的应用之间的相互调用。 以 Consul 迁移到 Nacos 为例： 在迁移态下，一共有两种应用类型：未迁移应用，迁移中应用。我们所说的双注册双订阅都是指的【迁移中应用】。明白下面几个点，平滑迁移的过程一下子就清晰了： 【未迁移应用】不做任何改动 为了让【未迁移应用】调用到【迁移中应用】，要求【迁移中应用】不仅要将数据写到 Nacos，还要写回旧的 Consul，这是双注册 为了让【迁移中应用】调用到【未迁移应用】，要求【迁移中应用】不仅要订阅 Nacos 的数据，还要监听旧的 Consul，这是双订阅 当所有应用变成【迁移中应用】时，旧的 Consul 就可以光荣下岗了，至此平滑迁移完成。 在这个过程中，还可以灵活的变换一些规则，例如在迁移中后期，大部分应用在 Nacos 中已经有服务了，可以切换双订阅为单订阅，以验证迁移情况。并且在真实场景下，还会并存配置中心、元数据中心的迁移，过程会更加复杂。 Dubbo 平滑迁移方案 – 多注册中心Dubbo 多注册中心配置文档地址：http://dubbo.apache.org/zh-cn/docs/user/demos/multi-registry.html 本文的完整代码示例将会在文末提供，其中 Consul 注册中心搭建在本地，而 Nacos 注册中心使用的是阿里云的云产品：微服务引擎 MSE，其可以提供托管的 Nacos/Zookeeper/Eureka 等集群。 Dubbo 支持多注册中心的配置，这就为我们平滑迁移提供了很多的便利性。在使用 dubbo-spring-boot-starter 时，只需要增加如下的配置，即可配置多注册中心： 12345dubbo.registries.first.protocol=consuldubbo.registries.first.address=localhost:8500dubbo.registries.second.protocol=nacosdubbo.registries.second.address=mse-kirito-p.nacos-ans.mse.aliyuncs.com:8848 在 Consul 控制台可以看到服务已经注册成功： 在 MSE 控制台可以看到 Nacos 服务也已经注册成功 并且，服务调用一切正常。你可能回想：前面讲了一堆，你告诉我改了两行配置就是平滑迁移了？我还是得好好纠正下这种想法，改代码从来都是最轻松的事，难的是在迁移中，时刻观察业务状况，确保服务不因为迁移有损。除此之外，还需要注意的是，Dubbo 自带的多注册中心方案因为框架实现的问题，存在一定的缺陷。 Dubbo 多注册中心的缺陷在 Dubbo 的实现中，多个注册中心的地址是隔离的，地址不会融合。也就是说，当消费者如下配置后： 12345dubbo.registries.first.protocol=consuldubbo.registries.first.address=localhost:8500dubbo.registries.second.protocol=nacosdubbo.registries.second.address=mse-kirito-p.nacos-ans.mse.aliyuncs.com:8848 会永远优先从 Consul 中读取服务地址，除非 Consul 中没有服务，才会尝试从 Nacos 中读取，顺序取决于配置文件中注册中心声明的先后。这可能不符合大多数人对多注册中心的直观认知，但没办法，Dubbo 就是这么设计的，我也尝试猜想了几个这么设计的可能性： 多个注册中心没有感知到对方存在的必要，所以只能串行读取多个注册中心 Dubbo 本身模型不支持注册中心聚合，除非专门搞一个 AggregationRegistry 代理多个注册中心实现 多个注册地址的 equals 方案难以确定，官方没有给出契约规范，即 ip 和 port 相同就可以认为同一个地址吗？ Dubbo 的多注册中心的设计并不只是为了适配平滑迁移方案，其他场景可能恰恰希望使用这种串行读取的策略 为了让读者有一个直观的感受，我用文末的 demo 进行了测试，让服务提供者 A1（端口号 12346） 只注册到 Nacos，服务提供者 A2（端口号为 12345） 只注册到 Consul，消费者 B 双订阅 Nacos 和 Consul。如下图所示，在测试初期，可以发现，稳定调用到 A1；期间，我手动 kill 了 A1，图中也清晰地打印出了一条地址下线通知，之后稳定调用到 A2。 这样的缺陷，会导致我们在平滑迁移过程中无法对未迁移应用和迁移中应用进行充分的测试。 Dubbo 平滑迁移方案 – 注册中心聚合注册中心聚合这个词其实是我自己想的，因为 Dubbo 官方文档并没有直接给出这种方案，而是由阿里云的微服务商业化 EDAS 团队提供的开源实现（ps，没错，就是我所在的团队啦）。其基本思路就是前文提到的，聚合多个注册中心的地址。使用方式也同样简单 引入依赖： 123456&lt;dependency&gt; &lt;groupId&gt;com.alibaba.edas&lt;/groupId&gt; &lt;artifactId&gt;edas-dubbo-migration-bom&lt;/artifactId&gt; &lt;version&gt;2.6.5.1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt;&lt;/dependency&gt; 增加配置： 在 application.properties 中添加注册中心的地址。 1dubbo.registry.address = edas-migration://30.5.124.15:9999?service-registry=consul://localhost:8500,nacos://mse-kirito-p.nacos-ans.mse.aliyuncs.com:8848&amp;reference-registry=consul://localhost:8500,nacos://mse-kirito-p.nacos-ans.mse.aliyuncs.com:8848 说明 如果是非 Spring Boot 应用，在 dubbo.properties 或者对应的 Spring 配置文件中配置。 ```edas-migration://30.5.124.15:9999 12345 多注册中心的头部信息。可以不做更改，ip 和 port 可以任意填写，主要是为了兼容 Dubbo 对 ip 和 port 的校验。启动时，如果日志级别是 WARN 及以下，可能会抛一个 WARN 的日志，可以忽略。- ``` service-registry 服务注册的注册中心地址。写入多个注册中心地址。每个注册中心都是标准的 Dubbo 注册中心格式；多个用,分隔。 reference-registry 服务订阅的注册中心地址。每个注册中心都是标准的 Dubbo 注册中心格式；多个用,分隔。 验证该方案： 已经变成了随机调用，解决了多注册中心的缺陷。 迁移完成后，建议删除原注册中心的配置和迁移过程专用的依赖edas-dubbo-migration-bom，在业务量较小的时间分批重启应用。edas-dubbo-migration-bom 是一个迁移专用的依赖，虽然长期使用对您业务的稳定性没有影响，但其并不会跟随 Dubbo 的版本进行升级，为避免今后框架升级过程中出现兼容问题，推荐您在迁移完毕后清理掉，然后在业务量较小的时间分批重启应用。 说明：edas-dubbo-migration-bom 目前的 release 版本只支持 Dubbo 2.6，我在文末的代码中提供了 2.7 的支持，预计很快两个版本都会贡献给 Dubbo 开源社区。 彩蛋：阿里云微服务引擎 MSE 重磅升级，上线微服务治理中心微服务治理中心是一个面向开源微服务框架微服务中心，通过 Java Agent 技术使得您的应用无需修改任何代码和配置，即可享有阿里云提供的微服务治理能力。 已经上线的功能包含 服务查询、无损下线、服务鉴权、离群实例摘除、标签路由。 微服务治理中心具有如下优势：功能强大，覆盖和增强了开源的治理功能，还提供差异化的功能。零成本接入，支持近五年的 Spring Cloud 和 Dubbo 版本，无需修改任何代码和配置。易被集成，阿里云容器服务用户只需在应用市场安装 微服务中心对应的 pilot ，并修改部署时的配置即可接入。 微服务中心尤其适合以下场景 解决应用发布时影响业务的问题。如果您的应用在发布新版本的时候，此应用的服务消费者仍旧调用已经下线的节点，出现业务有损，数据不一致的情况。这时候您需要使用 微服务治理中心，微服务治理中心提供的无损下线功能能够实现服务消费者及时感知服务提供者下线情况，保持业务连续无损。容器服务 K8s 集群的应用在接入 微服务治理中心后，您无需再额外对应用进行任何配置、也无需在 MSC 控制台进行任何操作，即可实现 Dubbo 和 Spring Cloud 流量的无损下线。 满足应用调用中权限控制的需求。当您的某个微服务应用有权限控制要求，不希望其它所有应用都能调用。比如优惠券部门的优惠券查询接口是默认内部的部门都是可以调用的，但是优惠券发放接口只允许特定的部门的应用才可以调用。这时候您需要使用 微服务治理中心，微服务治理中心提供的服务鉴权功能，既能够对整个应用做一些权限控制，也能对应用中的某个接口和 URL 进行权限控制，满足您不同场景下的权限控制需求 解决不健康实例影响业务对问题，当节点出现 Full GC、网络分区、机器异常等问题时，这种情况下会导致调用此应用的流量出现异常、影响业务。但是运维人员又很难及时发现问题，且无法判断应该采取何种措施，如重启或者单纯地等待应用恢复。这时候您需要使用微服务治理中心，微服务治理中心 提供的离群实例摘除功能，能够根据您配置的规则自动摘服务调用列表中不健康的应用实例，以免异常的节点影响您的业务。同时还能自动地探测实例是否恢复并恢复流量，以及将实例异常信息触发监控报警，保护您的业务，提升稳定性。 附录本文测试代码地址：https://github.com/lexburner/dubbo-migration 「技术分享」某种程度上，是让作者和读者，不那么孤独的东西。欢迎关注我的微信公众号：「Kirito的技术分享」","link":"/dubbo-migration/"},{"title":"Dubbo 基础教程：使用 Nacos 实现服务注册与发现","text":"什么是 NacosNacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。Nacos 帮助您更敏捷和容易地构建、交付和管理微服务平台。Nacos 是构建以“服务”为中心的现代应用架构 (例如微服务范式、云原生范式) 的服务基础设施。 在接下里的教程中，将使用 Nacos 作为微服务架构中的注册中心，替代 ZooKeeper 传统方案。 安装 Nacos下载地址：https://github.com/alibaba/nacos/releases本文版本：1.4.1 下载完成之后，解压。根据不同平台，执行不同命令，启动单机版 Nacos 服务： Linux/Unix/Mac：sh startup.sh -m standalone Windows：cmd startup.cmd -m standalone startup.sh 脚本位于 Nacos 解压后的 bin 目录下。 启动完成之后，访问：http://127.0.0.1:8848/nacos/，使用默认的用户名和密码：nacos/nacos 可以进入 Nacos 的服务管理页面。 构建 Dubbo 应用接入 Nacos 注册中心在完成了 Nacos 安装和启动之后，下面我们就可以编写两个应用（服务提供者与服务消费者）来验证服务的注册与发现了。 定义接口契约第一步：创建一个 maven 项目，命名为：dubbo-nacos-api。 123&lt;artifactId&gt;dubbo-nacos-api&lt;/artifactId&gt;&lt;groupId&gt;moe.cnkirito&lt;/groupId&gt;&lt;version&gt;1.0&lt;/version&gt; 第二步：定义服务提供者和服务消费者公用的 Java 接口 123public interface HelloService { String hello(String name);} Dubbo 的服务提供者和服务消费者一般会共同引用相同的接口，凭借接口达成调用的契约。 服务提供者第一步：创建一个 Dubbo 应用，命名为：dubbo-nacos-provider。 第二步：编辑 pom.xml，加入必要的依赖配置： 12345678910111213141516171819202122232425262728293031323334353637&lt;artifactId&gt;dubbo-nacos-provider&lt;/artifactId&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.7.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt; &lt;artifactId&gt;nacos-client&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;dubbo-nacos-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; dubbo-spring-boot-starter：Dubbo 应用可以使用 api 配置、xml 配置、SpringBoot 自动配置，推荐使用 dubbo-spring-boot-starter 提供的自动装配机制构建 Dubbo 应用。 nacos-client：Nacos 提供的 Java 客户端，一般需要显式指定版本，推荐使用和 nacos-server 配套的客户端版本，以确保没有兼容性问题 dubbo-nacos-api：接口契约 第三步：创建应用并定义服务提供者 123456@SpringBootApplicationpublic class DubboProvider { public static void main(String[] args) { SpringApplication.run(DubboProvider.class, args); }} 123456789@DubboService(version = &quot;1.0.0&quot;, group = &quot;DUBBO&quot;)public class HelloServiceImpl implements HelloService { @Override public String hello(String name) { return &quot;hello &quot; + name; }} 内容非常简单，@DubboService 注解是高版本 Dubbo 定义的新注解，用于服务提供者的暴露。一般我们定义 Dubbo 提供者时倾向于明确指定 version 和 group，而不是留空，Dubbo 会根据 interfaceName、version、group 的三元组唯一确定一个服务。 第四步：配置 Dubbo 服务提供者，定义 application.yaml： 1234567891011121314151617server: port: 8080dubbo: scan: base-packages: moe.cnkirito.demo application: name: dubbo-nacos-provider protocol: name: dubbo port: 20880 registry: address: nacos://127.0.0.1:8848 config-center: address: nacos://127.0.0.1:8848 metadata-report: address: nacos://127.0.0.1:8848 dubbo.scan.base-packages：配置 @DubboService 等 Dubbo 注解的包扫描路径 dubbo.application.name：Dubbo 的应用名，建议配置，Dubbo 越来越推崇应用级别的服务治理。 dubbo.protocol.name 和 dubbo.protocol.port：Dubbo 的协议配置，默认值为 dubbo 和 20880，这里配置出来主要是为了提醒大家，Dubbo 服务提供者会占用掉 dubbo.protocol.port 配置的端口号，当一个主机上启动多个服务提供者时，除了需要修改 server.port 外还需要修改 dubbo.protocol.port 的值 dubbo.registry.address 、dubbo.config-center.address 和 dubbo.metadata-report.address：Dubbo 注册中心、配置中心、元数据中心的配置地址，同时指向 Naocs。关于三个中心的介绍可以参考《Dubbo2.7 三大新特性详解》。 第五步：启动应用 启动之后，在日志中观察到如下的日志输出，则代表服务发布成功 1[DUBBO] Register: dubbo://192.168.0.105:20880/moe.cnkirito.api.HelloService?anyhost=true&amp;application=dubbo-nacos-provider&amp;deprecated=false&amp;dubbo=2.0.2&amp;dynamic=true&amp;generic=false&amp;group=DUBBO&amp;interface=moe.cnkirito.api.HelloService&amp;metadata-type=remote&amp;methods=hello&amp;pid=3885&amp;release=2.7.8&amp;revision=1.0.0&amp;side=provider&amp;timestamp=1610790598864&amp;version=1.0.0, dubbo version: 2.7.8, current host: 192.168.0.105 我们可以访问 Nacos 的管理页面 http://127.0.0.1:8848/nacos/ 来查看服务列表，此时可以看到如下内容： 点击详情，可以查看实例级别的信息 服务消费者接下来实现一个服务消费者来消费上面的服务 第一步：创建一个 Dubbo 应用，命名为：dubbo-nacos-consumer 第二步：编辑 pom.xml 中的依赖内容，与上面服务提供者内容一致 第三步：创建应用并实现服务消费者 1234567891011121314151617@SpringBootApplication@RestControllerpublic class DubboConsumer { @DubboReference(version = &quot;1.0.0&quot;, group = &quot;DUBBO&quot;) private HelloService helloService; public static void main(String[] args) { SpringApplication.run(DubboConsumer.class, args); } @RequestMapping(&quot;/hello&quot;) public String hello(String name) { return helloService.hello(name); }} @DubboReference 与 @DubboService 与成对出现，用于配置服务消费者。需要指定和服务提供者相同的 version 和 group。 第四步：配置 Dubbo 服务消费者，定义 application.yaml： 1234567891011121314server: port: 8081dubbo: scan: base-packages: moe.cnkirito.demo application: name: dubbo-nacos-consumer registry: address: nacos://127.0.0.1:8848 config-center: address: nacos://127.0.0.1:8848 metadata-report: address: nacos://127.0.0.1:8848 和服务提供者配置的差异主要在于这里不用配置 protocol 暴露端口号了，因为消费者不会占用一个端口。但在实际开发中，一个业务应用往往既是服务提供者又是服务消费者，所以往往都需要配置 protocol。 第五步：启动应用发起调用测试 关键日志如下，收到了服务端的地址推送，消费者即可拿着该地址进行调用 12021-01-16 18:13:04.714 INFO 4811 --- [ncesChangeEvent] o.a.dubbo.registry.nacos.NacosRegistry : [DUBBO] Notify urls for subscribe url consumer://192.168.0.105/moe.cnkirito.api.HelloService?application=dubbo-nacos-consumer&amp;category=providers,configurators,routers&amp;dubbo=2.0.2&amp;group=DUBBO&amp;init=false&amp;interface=moe.cnkirito.api.HelloService&amp;metadata-type=remote&amp;methods=hello&amp;pid=4811&amp;qos.enable=false&amp;release=2.7.8&amp;revision=1.0.0&amp;side=consumer&amp;sticky=false&amp;timestamp=1610791940776&amp;version=1.0.0 我们可以访问 Nacos 的管理页面 http://127.0.0.1:8848/nacos/ 来查看服务消费者列表，此时可以看到如下内容： 执行调用 12$curl &quot;localhost:8081/hello?name=kirito&quot;hello kirito 常见错误 Caused by: java.lang.NoClassDefFoundError: org/apache/commons/lang3/StringUtils Dubbo 源码依赖了 common-lang3，如果项目中没有引入过该依赖，需要手动加上该依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.9&lt;/version&gt;&lt;/dependency&gt; 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。","link":"/dubbo-nacos-registry/"},{"title":"Dubbo 稳定性案例：Nacos 注册中心可用性问题复盘","text":"问题描述上周四晚刚回到家，就接到了软负载同学的电话，说是客户线上出了故障，我一听”故障“两个字，立马追问是什么情况，经过整理，还原出线上问题的原貌： 客户使用了 Dubbo，注册中心使用的是 Nacos，在下午开始不断有调用报错，查看日志，发现了 Nacos 心跳请求返回 502 122019-11-15 03:02:41.973 [com.alibaba.nacos.client.naming454] -ERROR [com.alibaba.nacos.naming.beat.sender] request xx.xx.xx.xx failed.com.alibaba.nacos.api.exception.NacosException: failed to req API: xx.xx.xx.xx:8848/nacos/v1/ns/instance/beat. code:502 msg: 此时还没有大范围的报错。随后，用户对部分机器进行了重启，开始出现大规模的 Nacos 连接不上的报错，并且调用开始出现大量 no provider 的报错。 问题分析Nacos 出现心跳报错，一般会有两种可能： 用户机器出现问题，如网络不通 Nacos Server 宕机 但由于是大面积报错，所以很快定位到是 Nacos Server 本身出了问题：由于磁盘老旧导致 IO 效率急剧下降，Nacos Server 无法响应客户端的请求，客户端直接接收到 502 错误响应。这个事件本身并不复杂，是一起注册中心磁盘故障引发的血案，但从这起事件，却可以窥探到很多高可用的问题，下面来跟大家一起聊聊这当中的细节。 问题复现Dubbo 版本：2.7.4 Nacos 版本：1.1.4 复现目标：在本地模拟 Nacos Server 宕机，检查 Dubbo 的调用是否会受到影响。 复现步骤： 本地启动 Nacos Server、Provider、Consumer，触发 Consumer 调用 Provider kill -9 Nacos Server，模拟 Nacos Server 宕机，触发 Consumer 调用 Provider 重启 Consumer，触发 Consumer 调用 Provider 期望： 3 个步骤均可以调用成功 实际结果: 1、2 调用成功，3 调用失败 问题成功复现，重启 Consumer 之后，没有调用成功，客户恰好遇到了这个问题。大家可能对这其中的细节还是有一些疑问，我设想了一些疑惑点，来和大家一起进行探讨。 为什么 Nacos 宕机后，仍然可以调用成功我们都知道，一般聊到 Dubbo，有三个角色是必须要聊到的：服务提供者、服务消费者、注册中心。他们的关系不用我赘述，可以从下面的连通性列表得到一个比较全面的认识： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 服务提供者向注册中心注册其提供的服务，此时间不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，此时间包含网络开销 注册中心，服务提供者，服务消费者三者之间均为长连接 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心可选的，服务消费者可以直连服务提供者 重点关注倒数第二条，Dubbo 其实在内存中缓存了一份提供者列表，这样可以方便地在每次调用时，直接从本地内存拿地址做负载均衡，而不避免每次调用都访问注册中心。只有当服务提供者节点发生上下线时，才会推送到本地，进行更新。所以，Nacos 宕机后，Dubbo 仍然可以调用成功。 Nacos 宕机不影响服务调用，为什么日志中仍然有调用报错宕机期间，已有的服务提供者节点可能突然下线，但由于注册中心无法通知给消费者，所以客户端调用到下线的 IP 就会出现报错。 对于此类问题，Dubbo 也可以进行兜底 Dubbo 会在连接级别进行心跳检测，当 channel 本身不可用时，即使没有注册中心通知，也会对其进行断连，并设置定时器，当该连接恢复后，再恢复其可用性 在阿里云商业版的 Dubbo – EDAS 中，提供了「离群摘除」功能，可以在调用层面即时摘除部分有问题的节点，保证服务的可用性。 为什么期望 Consumer 重启之后，调用成功Nacos Server 宕机后，Consumer 依旧可以调用成功，这个大家应该都比较清楚。但是为什么期望 Consumer 重启之后，依旧调用成功，有些人可能就会有疑问了，注册中心都宕机了，重启之后一定连不上，理应调用失败，怎么会期望成功呢？这就要涉及到 Nacos 的本地缓存了。 Nacos 本地缓存的作用：当应用与服务注册中心发生网络分区或服务注册中心完全宕机后，应用进行了重启操作，内存里没有数据，此时应用可以通过读取本地缓存文件的数据来获取到最后一次订阅到的内容。 例如在 Dubbo 应用中定义了如下服务： 1&lt;dubbo:service interface=&quot;com.alibaba.edas.xml.DemoService&quot; group=&quot;DUBBO&quot; version=&quot;1.0.0&quot; ref=&quot;demoService&quot; /&gt; 可以在本机的 /home/${user}/​nacos/naming/ 下看到各个命名空间发布的所有服务的信息，其内容格式如下： 1{&quot;metadata&quot;:{},&quot;dom&quot;:&quot;DEFAULT_GROUP@@providers:com.alibaba.edas.xml.DemoService:1.0.0:DUBBO&quot;,&quot;cacheMillis&quot;:10000,&quot;useSpecifiedURL&quot;:false,&quot;hosts&quot;:[{&quot;valid&quot;:true,&quot;marked&quot;:false,&quot;metadata&quot;:{&quot;side&quot;:&quot;provider&quot;,&quot;methods&quot;:&quot;sayHello&quot;,&quot;release&quot;:&quot;2.7.4&quot;,&quot;deprecated&quot;:&quot;false&quot;,&quot;dubbo&quot;:&quot;2.0.2&quot;,&quot;pid&quot;:&quot;5275&quot;,&quot;interface&quot;:&quot;com.alibaba.edas.xml.DemoService&quot;,&quot;version&quot;:&quot;1.0.0&quot;,&quot;generic&quot;:&quot;false&quot;,&quot;revision&quot;:&quot;1.0.0&quot;,&quot;path&quot;:&quot;com.alibaba.edas.xml.DemoService&quot;,&quot;protocol&quot;:&quot;dubbo&quot;,&quot;dynamic&quot;:&quot;true&quot;,&quot;category&quot;:&quot;providers&quot;,&quot;anyhost&quot;:&quot;true&quot;,&quot;bean.name&quot;:&quot;com.alibaba.edas.xml.DemoService&quot;,&quot;group&quot;:&quot;DUBBO&quot;,&quot;timestamp&quot;:&quot;1575355563302&quot;},&quot;instanceId&quot;:&quot;30.5.122.3#20880#DEFAULT#DEFAULT_GROUP@@providers:com.alibaba.edas.xml.DemoService:1.0.0:DUBBO&quot;,&quot;port&quot;:20880,&quot;healthy&quot;:true,&quot;ip&quot;:&quot;30.5.122.3&quot;,&quot;clusterName&quot;:&quot;DEFAULT&quot;,&quot;weight&quot;:1.0,&quot;ephemeral&quot;:true,&quot;serviceName&quot;:&quot;DEFAULT_GROUP@@providers:com.alibaba.edas.xml.DemoService:1.0.0:DUBBO&quot;,&quot;enabled&quot;:true}],&quot;name&quot;:&quot;DEFAULT_GROUP@@providers:com.alibaba.edas.xml.DemoService:1.0.0:DUBBO&quot;,&quot;checksum&quot;:&quot;69c4eb7e03c03d4b18df129829a486a&quot;,&quot;lastRefTime&quot;:1575355563862,&quot;env&quot;:&quot;&quot;,&quot;clusters&quot;:&quot;&quot;} 为什么期望重启后调用成功？因为经过检查，发现线上出现问题的机器上，缓存文件一切正常。虽然 Nacos Server 宕机了，本地的缓存文件依旧可以作为一个兜底，所以期望调用成功。 为什么 Consumer 重启后，没有按照预期加载本地缓存文件缓存文件正常，问题只有可能出现在读取缓存文件的逻辑上。 可能是 nacos-client 出了问题 可能是 Dubbo 的 nacos-registry 出了问题 一番排查，在 Nacos 研发的协助下，找到了 naocs-client 的一个参数： namingLoadCacheAtStart ，该配置参数控制启动时是否加载缓存文件，默认值为 false。也就是说，使用 nacos-client，默认是不会加载本地缓存文件的。终于定位到线上问题的原因了：需要手动开启加载本地缓存，才能让 Nacos 加载本地缓存文件。 该参数设置为 true 和 false 的利弊： 设置为 true，认为可用性 &amp; 稳定性优先，宁愿接受可能出错的数据，也不能因为没有数据导致调用完全出错 设置为 false，则认为 Server 的可用性较高，更能够接受没有数据，也不能接受错误的数据 无论是 true 还是 false，都是对一些极端情况的兜底，而不是常态。对于注册发现场景，设置成 true，可能更合适一点，这样可以利用 Nacos 的本地缓存文件做一个兜底。 Dubbo 传递注册中心参数Dubbo 中使用统一 URL 模型进行参数的传递，当我们需要在配置文件传递注册中心相关的配置参数时，可以通过键值对的形式进行拼接，当我们想要在 Dubbo 中开启加载注册中心缓存的开关时，可以如下配置： 1&lt;dubbo:registry address=&quot;nacos://127.0.0.1:8848?namingLoadCacheAtStart=true&quot;/&gt; 遗憾的是，最新版本的 Dubbo 只传递了部分参数给 Nacos Server，即使用户配置了 namingLoadCacheAtStart 也不会被服务端识别，进而无法加载本地缓存。我在本地修改了 Dubbo 2.7.5-SNAPSHOT，传递上述参数后，可以使得 1、2、3 三个阶段都调用成功，证明了 namingLoadCacheAtStart 的确可以使得 Dubbo 加载本地缓存文件。该问题将会在 Dubbo 2.7.5 得到修复，届时 Dubbo 中使用 Nacos 的稳定性将会得到提升。 问题总结该线上问题反映出了 Nacos 注册中心可用性对 Dubbo 应用的影响，以及系统在某个组件宕机时，整体系统需要进行的一些兜底逻辑，不至于因为某个组件导致整个系统的瘫痪。 总结下现有代码的缺陷以及一些最佳实践： Dubbo 传递注册中心参数给 Nacos 时，只能够识别部分参数，这会导致用户的部分配置失效，在接下来的版本会进行修复。 nacos-client 加载本地缓存文件的开关等影响到系统稳定性的参数最好设计成 -D 启动参数，或者环境变量参数，这样方便发现问题，及时止血。例如此次的事件，有缺陷的 Dubbo 代码仅仅依赖于参数的传递，无法加载本地缓存文件，而如果有 -D 参数，可以强行开始加载缓存，大大降低了问题的影响面。 namingLoadCacheAtStart 是否默认开启，还需要根据场景具体确定，但 nacos-server 宕机等极端场景下，开启该参数，可以尽可能地降低问题的影响面。顺带一提，Nacos 本身还提供了一个本地灾备文件，与本地缓存文件有一些差异，有兴趣的朋友也可以去了解一下。","link":"/dubbo-nacos-stability/"},{"title":"使用 JMeter 进行 Dubbo 性能测试","text":"1 前言说道性能测试工具，你会立刻联想到哪一个？ab（ApacheBench）、JMeter、LoadRunner、wrk…可以说市面上的压测工具实在是五花八门。那如果再问一句，对 Dubbo 进行性能压测，你会 pick 哪一个？可能大多数人就懵逼了。可以发现，大多数的压测工具对开放的协议支持地比较好，例如：HTTP 协议，但对于 Dubbo 框架的私有协议：dubbo，它们都显得力不从心了。 如果不从通用的压测工具上解决 Dubbo 的压测需求问题，可以自己写 Dubbo 客户端，自己统计汇总结果，但总归不够优雅，再加上很多开发同学没有丰富的测试经验，很容易出现一些偏差。说到底，还是压测工具靠谱，于是便引出了本文的主角 —— **jmeter-plugins-for-apache-dubbo**。这是一款由 Dubbo 社区 Commiter – 凝雨 同学开发的 JMeter 插件，可以非常轻松地对 Dubbo 实现性能测试。 2 JMeter 介绍在开始压测 Dubbo 之前，先简单介绍一下这款开源的性能测试工具 —— JMeter。JMeter 是 Apache 组织基于 Java 开发的一款性能测试工具。它最初被设计用于 Web 应用测试，但后来扩展到其他测试领域，并可以在 Windows、Mac、Linux 环境下安装使用。JMeter 还提供了图形界面，这使得编写测试用例变得非常简单，具有易学和易操作的特点。 JMeter 官网：http://jmeter.apache.org/download_jmeter.cgi 2.1 安装 JMeter截止本文发布，官方的最新版本为：apache-jmeter-5.1.1.zip , 下载后直接解压即可。 在 ${JMETER_HOME}/bin 下找到启动脚本，可以打开图形化界面 Mac/Linux 用户可以直接使用 jmeter 可执行文件，或者 jmeter.sh 启动脚本 Windows 用户可以使用 jmeter.bat 启动脚本 2.2 命令行提示信息启动过程中会有一段命令行日志输出： 12345678================================================================================Don't use GUI mode for load testing !, only for Test creation and Test debugging.For load testing, use CLI Mode (was NON GUI): jmeter -n -t [jmx file] -l [results file] -e -o [Path to web report folder]&amp; increase Java Heap to meet your test requirements: Modify current env variable HEAP=&quot;-Xms1g -Xmx1g -XX:MaxMetaspaceSize=256m&quot; in the jmeter batch fileCheck : https://jmeter.apache.org/usermanual/best-practices.html================================================================================ 注意到第一行的提示，GUI 仅仅能够用于调试和创建测试计划，实际的性能测试需要使用命令行工具进行。 jmeter -n -t [jmx file] -l [results file] -e -o [Path to web report folder] 【jmx file】：使用 GUI 创建的测试计划文件，后缀名为 .jmx 【results file】：测试结果文本文件输出路径 【Path to web report folder】：测试报告输出路径，JMeter 的强大之处，可以生成图文并茂的测试报告 2.3 GUI 界面展示 上图所示为 JMeter 的主界面。官方提供了国际化支持，通过 【Options】-&gt;【Choose Language】可以将界面语言变更为简体中文。 3 JMeter 压测 HTTP本节以 JMeter 压测 HTTP 为引子，介绍 JMeter 的使用方式，让没有使用过 JMeter 的读者对这款工具有一个较为直观的感受。 3.1 创建线程组在“测试计划”上右键 【添加】–&gt;【线程（用户）】–&gt;【线程组】。 给线程组起一个名字，方便记忆。 线程数：决定了由多少线程并发压测 Ramp-Up：代表了 JMeter 创建所有线程所需要的时间，如图所示则代表每 0.1s 创建一个线程 循环次数：在运行所设置的次数之后，压测将会终止。如果想要运行固定时长的压测，可以设置为：永远，并在下面的调度器中指定持续时间 3.2 增加 HTTP 取样器在刚刚创建的线程组上右键 【添加】–&gt;【取样器】–&gt;【HTTP 请求】。 为 HTTP 取样器配置上压测地址和必要的参数 3.3 添加察看结果树在刚刚创建的线程组上右键 【添加】–&gt;【监听器】–&gt;【察看结果树】。 只有添加了【察看结果树】才能让我们看到 GUI 中测试的结果。 3.4 准备 HTTP Server使用 SpringBoot 可以快速构建一个 RestController，其暴露了 localhost:8080/queryOrder/{orderNo} 做为压测入口 123456789101112@RestControllerpublic class OrderRestController { @Autowired OrderService orderService; @RequestMapping(&quot;/queryOrder/{orderNo}&quot;) public OrderDTO queryOrder(@PathVariable(&quot;orderNo&quot;) long orderNo) { return orderService.queryOrder(orderNo); }} 被压测的服务 OrderService ： 1234567891011@Componentpublic class OrderService { public OrderDTO queryOrder(long orderNo) { OrderDTO orderDTO = new OrderDTO(); orderDTO.setOrderNo(orderNo); orderDTO.setTotalPrice(new BigDecimal(ThreadLocalRandom.current().nextDouble(100.0D))); orderDTO.setBody(new byte[1000]); return orderDTO; }} 3.5 验证结果 在刚刚创建的线程组上右键 【验证】，执行单次验证，可以用来测试与服务端的连通性。在【察看结果树】选项卡中可以看到【响应数据】已经正常返回了。 3.6 执行测试计划还记得之前启动 GUI 时控制台曾经提示过我们，GUI 只负责创建测试计划并验证，不能用于执行实际的并发压测。在 GUI 中准备就绪之后，我们可以在【文件】-&gt;【保存测试计划为】中将测试计划另存为 rest-order-thread-group.jmx 测试文件，以便我们在命令行进行压测： 1jmeter -n -t ./rest-order-thread-group.jmx -l ./result.txt -e -o ./webreport 下图展示了最终生成的测试报告，主要汇总了执行次数、响应时间、吞吐量、网络传输速率。 在实际的测试报告中，还有更加详细的维度可以展示，上述只是展示了汇总信息。 4 JMeter 压测 DubboJMeter 默认并不支持私有的 dubbo 协议，但其优秀的扩展机制使得只需要添加插件，就可以完成 Dubbo 压测，这一节也是本文重点介绍的部分。 4.1 安装 jmeter-plugins-for-apache-dubbo 插件地址：https://github.com/thubbo/jmeter-plugins-for-apache-dubbo 目前该插件支持对最新版本的 Dubbo 进行压测，推荐的安装方式： 克隆项目：git clone https://github.com/thubbo/jmeter-plugins-for-apache-dubbo.git 打包项目，构建 JMeter 插件：mvn clean install ，得到：jmeter-plugins-dubbo-2.7.3-jar-with-dependencies.jar 将插件添加到 ${JMETER_HOME}\\lib\\ext 4.2 增加 Dubbo 取样器之前的小结已经介绍了如何添加线程组和 HTTP 取样器，现在想要对 Dubbo 应用进行性能测试，可以直接复用之前的线程组配置，在线程组上右键 【添加】–&gt;【取样器】–&gt;【Dubbo Sample】。 创建 Dubbo 取样器之后，可以对其进行配置 4.3 准备 Dubbo Provider复用 HTTP 取样器时的 OrderService 1234567891011@Servicepublic class OrderDubboProvider implements OrderApi { @Autowired OrderService orderService; @Override public OrderDTO queryOrder(long orderNo) { return orderService.queryOrder(orderNo); }} 配置 application.properties，注册服务到 Zookeeper 注册中心: 1234dubbo.scan.basePackages=com.alibaba.edas.benchmarkdubbo.application.name=dubbo-provider-demodubbo.registry.address=zookeeper://127.0.0.1:2181dubbo.protocol.port=20880 4.4 验证结果在 JMeter 中配置好 Dubbo 服务所连接的注册中心，接着通过 Get Provider List 可以获取到服务提供者列表，以供压测选择。在线程组上右键 【验证】，执行单次验证，可以用来测试与服务端的连通性。在【察看结果树】选项卡中可以看到【响应数据】可以正常执行 Dubbo 调用了。 4.5 执行测试计划可以将 Dubbo 取样器和 HTTP 取样器包含在同一个测试计划中一起执行，同时进行了 Dubbo 接口与 Rest 接口的性能对比。在命令行进行压测： 1jmeter -n -t ./rest-order-thread-group.jmx -l ./result.txt -e -o ./webreport 下图展示了最终生成的测试报告： Dubbo 接口与 Rest 接口所封装的业务接口均为 OrderService，所以压测上的差距直接体现出了 Dubbo 和 Rest 的差距。从报告对比上来看，Dubbo 接口的平均 RT 远低于 Rest 接口。 5 总结本文从零到一介绍了使用 JMeter 压测 HTTP 的方法，让读者熟悉 JMeter 的使用方式，并着重介绍了使用 jmeter-plugins-for-apache-dubbo 插件压测 Dubbo 的方法。 由于 JMeter Plugin 的限制，目前 Dubbo 的压测请求是通过泛化调用进行发送的，会有一定程度的性能下降，所以在实际评估 Dubbo 接口性能时，接口实际性能会比压测结果更加乐观。","link":"/dubbo-perf-benchmark/"},{"title":"【Dubbo3.0 新特性】集成 RSocket, 新增响应式支持","text":"响应式编程响应式编程现在是现在一个很热的话题。响应式编程让开发者更方便地编写高性能的异步代码，关于响应式编程更详细的信息可以参考 http://reactivex.io/ 。很可惜，在之前很长一段时间里，Dubbo 并不支持响应式编程，简单来说，Dubbo 不支持在 rpc 调用时，使用 Mono/Flux 这种流对象（reactive-stream 中流的概念 )，给用户使用带来了不便。 RSocket 是一个支持 reactive-stream 语义的开源网络通信协议，它将 reactive 语义的复杂逻辑封装了起来，使得上层可以方便实现网络程序。RSocket 详细资料：http://rsocket.io/。 Dubbo 在 3.0.0-SNAPSHOT 版本里基于 RSocket 对响应式编程提供了支持，用户可以在请求参数和返回值里使用 Mono 和 Flux 类型的对象。下面我们给出使用范例，源码可以在文末获取。 Dubbo RSocket 初体验服务接口1234public interface DemoService { Mono&lt;String&gt; requestMonoWithMonoArg(Mono&lt;String&gt; m1, Mono&lt;String&gt; m2); Flux&lt;String&gt; requestFluxWithFluxArg(Flux&lt;String&gt; f1, Flux&lt;String&gt; f2);} 12345&lt;dependency&gt; &lt;groupId&gt;io.projectreactor&lt;/groupId&gt; &lt;artifactId&gt;reactor-core&lt;/artifactId&gt; &lt;version&gt;3.2.3-RELEASE&lt;/version&gt;&lt;/dependency&gt; 在服务定义层，引入了 Mono，Flux 等 reactor 的概念，所以需要添加 reactor-core 的依赖。 服务提供者123456789101112131415161718192021public class DemoServiceImpl implements DemoService { @Override public Mono&lt;String&gt; requestMonoWithMonoArg(Mono&lt;String&gt; m1, Mono&lt;String&gt; m2) { return m1.zipWith(m2, new BiFunction&lt;String, String, String&gt;() { @Override public String apply(String s, String s2) { return s+&quot; &quot;+s2; } }); } @Override public Flux&lt;String&gt; requestFluxWithFluxArg(Flux&lt;String&gt; f1, Flux&lt;String&gt; f2) { return f1.zipWith(f2, new BiFunction&lt;String, String, String&gt;() { @Override public String apply(String s, String s2) { return s+&quot; &quot;+s2; } }); }} 除了常规的 Dubbo 必须依赖之外，还需要添加 dubbo-rsocket 的扩展 12345//... other dubbo moudle&lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-rpc-rsocket&lt;/artifactId&gt;&lt;/dependency&gt; 配置并启动服务端，注意协议名字填写 rsocket： 12345678910111213141516171819202122&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://dubbo.apache.org/schema/Dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;!-- provider's application name, used for tracing dependency relationship --&gt; &lt;dubbo:application name=&quot;demo-provider&quot;/&gt; &lt;!-- use registry center to export service --&gt; &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt; &lt;!-- use Dubbo protocol to export service on port 20890 --&gt; &lt;dubbo:protocol name=&quot;rsocket&quot; port=&quot;20890&quot;/&gt; &lt;!-- service implementation, as same as regular local bean --&gt; &lt;bean id=&quot;demoService&quot; class=&quot;org.apache.dubbo.samples.basic.impl.DemoServiceImpl&quot;/&gt; &lt;!-- declare the service interface to be exported --&gt; &lt;dubbo:service interface=&quot;org.apache.dubbo.samples.basic.api.DemoService&quot; ref=&quot;demoService&quot;/&gt;&lt;/beans&gt; 服务提供者的 bootstrap： 123456789public class RsocketProvider { public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[]{&quot;spring/rsocket-provider.xml&quot;}); context.start(); System.in.read(); // press any key to exit }} 服务消费者然后配置并启动消费者消费者如下, 注意协议名填写 rsocket： 123456789101112131415161718&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/Dubbo&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;!-- consumer's application name, used for tracing dependency relationship (not a matching criterion), don't set it same as provider --&gt; &lt;dubbo:application name=&quot;demo-consumer&quot;/&gt; &lt;!-- use registry center to discover service --&gt; &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt; &lt;!-- generate proxy for the remote service, then demoService can be used in the same way as the local regular interface --&gt; &lt;dubbo:reference id=&quot;demoService&quot; check=&quot;true&quot; interface=&quot;org.apache.dubbo.samples.basic.api.DemoService&quot;/&gt;&lt;/beans&gt; 12345678910111213141516171819202122232425262728293031public class RsocketConsumer { public static void main(String[] args) { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[]{&quot;spring/rsocket-consumer.xml&quot;}); context.start(); DemoService demoService = (DemoService) context.getBean(&quot;demoService&quot;); // get remote service proxy while (true) { try { Mono&lt;String&gt; monoResult = demoService.requestMonoWithMonoArg(Mono.just(&quot;A&quot;), Mono.just(&quot;B&quot;)); monoResult.doOnNext(new Consumer&lt;String&gt;() { @Override public void accept(String s) { System.out.println(s); } }).block(); Flux&lt;String&gt; fluxResult = demoService.requestFluxWithFluxArg(Flux.just(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), Flux.just(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;)); fluxResult.doOnNext(new Consumer&lt;String&gt;() { @Override public void accept(String s) { System.out.println(s); } }).blockLast(); } catch (Throwable throwable) { throwable.printStackTrace(); } } }} 可以看到配置上除了协议名使用 rsocket 以外其他并没有特殊之处。 实现原理以前用户并不能在参数或者返回值里使用 Mono/Flux 这种流对象（reactive-stream 里的流的概念）。因为流对象自带异步属性，当业务把流对象作为参数或者返回值传递给框架之后，框架并不能将流对象正确的进行序列化。 Dubbo 基于 RSocket 提供了 reactive 支持。RSocket 将 reactive 语义的复杂逻辑封装起来了，给上层提供了简洁的抽象如下： 1234567Mono&lt;Void&gt; fireAndForget(Payload payload);Mono&lt;Payload&gt; requestResponse(Payload payload);Flux&lt;Payload&gt; requestStream(Payload payload);Flux&lt;Payload&gt; requestChannel(Publisher&lt;Payload&gt; payloads); 从客户端视角看，框架建立连接之后，只需要将请求信息编码到 Payload 里，然后通过 requestStream 方法即可向服务端发起请求。 从服务端视角看，RSocket 收到请求之后，会调用我们实现的 requestStream 方法，我们从 Payload 里解码得到请求信息之后，调用业务方法，然后拿到 Flux 类型的返回值即可。 需要注意的是业务返回值一般是 Flux&lt;BizDO&gt;，而 RSocket 要求的是 Flux&lt;Payload&gt;，所以我们需要通过 map operator 拦截业务数据，将 BizDO 编码为 Payload 才可以递交给 RSocket。而 RSocket 会负责数据的传输和 reactive 语义的实现。 结语Dubbo 2.7 相比 Dubbo 2.6 提供了 CompletableFuture 的异步化支持，在 Dubbo 3.0 又继续拥抱了 Reactive，不断对新特性的探索，无疑是增加了使用者的信心。RSocket 这一框架 / 协议，如今在国内外也是比较火的一个概念，它提供了丰富的 Reactive 语义以及多语言的支持，使得服务治理框架可以很快地借助它实现 Reactive 语义。有了响应式编程支持，业务可以更加方便的实现异步逻辑。 本篇文章对 Dubbo RSocket 进行了一个简单的介绍，对 Reactive、RSocket 感兴趣的同学也可以浏览下 Dubbo 3.0 源码对 RSocket 的封装。 相关链接： [1] 文中源码：https://github.com/apache/incubator-dubbo-samples/tree/3.x/dubbo-samples-rsocket [2] Dubbo 3.x 开发分支：https://github.com/apache/incubator-Dubbo/tree/3.x-dev","link":"/dubbo-rsocket/"},{"title":"Dubbo 支持的几个主流序列化框架评测","text":"前言今天要聊的技术是序列化，这不是我第一次写序列化相关的文章了，今天动笔之前，我还特地去博客翻了下我博客早期的一篇序列化文章（如下图），竟然都过去 4 年了。 为什么又想聊序列化了呢？因为最近的工作用到了序列化相关的内容，其次，这几年 Dubbo 也发生了翻天覆地的变化，其中 Dubbo 3.0 主推的 Tripple 协议，更是打着下一代 RPC 通信协议的旗号，有取代 Dubbo 协议的势头。而 Tripple 协议使用的便是 Protobuf 序列化方案。 另外，Dubbo 社区也专门搞了一个序列化压测的项目：https://github.com/apache/dubbo-benchmark.git ，本文也将围绕这个项目，从性能维度展开对 Dubbo 支持的各个序列化框架的讨论。 当我们聊序列化的时候，我们关注什么？最近几年，各种新的高效序列化方式层出不穷，最典型的包括： 专门针对 Java 语言的：JDK 序列化、Kryo、FST 跨语言的：Protostuff，ProtoBuf，Thrift，Avro，MsgPack 等等 为什么开源社区涌现了这么多的序列化框架，Dubbo 也扩展了这么多的序列化实现呢？主要还是为了满足不同的需求。 序列化框架的选择主要有以下几个方面： 跨语言。是否只能用于 java 间序列化 / 反序列化，是否跨语言，跨平台。 性能。分为空间开销和时间开销。序列化后的数据一般用于存储或网络传输，其大小是很重要的一个参数；解析的时间也影响了序列化协议的选择，如今的系统都在追求极致的性能。 兼容性。系统升级不可避免，某一实体的属性变更，会不会导致反序列化异常，也应该纳入序列化协议的考量范围。 和 CAP 理论有点类似，目前市面上很少有一款序列化框架能够同时在三个方面做到突出，例如 Hessian2 在兼容性方面的表现十分优秀，性能也尚可，Dubbo 便使用了其作为默认序列化实现，而性能方面它其实是不如 Kryo 和 FST 的，在跨语言这一层面，它表现的也远不如 ProtoBuf，JSON。 其实反过来想想，要是有一个序列化方案既是跨语言的，又有超高的性能，又有很好的兼容性，那不早就成为分布式领域的标准了？其他框架早就被干趴了。 大多数时候，我们是挑选自己关注的点，找到合适的框架，满足我们的诉求，这才导致了序列化框架百花齐放的局面。 性能测试很多序列化框架都宣称自己是“高性能”的，光他们说不行呀，我还是比较笃信“benchmark everything”的箴言，这样得出的结论，更能让我对各个技术有自己的认知，避免人云亦云，避免被不是很权威的博文误导。 怎么做性能测试呢？例如像这样？ 123long start = System.currentTimeMillis();measure();System.out.println(System.currentTimeMillis()-start); 貌似不太高大上，但又说不上有什么问题。如果你这么想，那我推荐你了解下 JMH 基准测试框架，我之前写过的一篇文章《JAVA 拾遗 — JMH 与 8 个测试陷阱》推荐你先阅读以下。 事实上，Dubbo 社区的贡献者们早就搭建了一个比较完备的 Dubbo 序列化基础测试工程：https://github.com/apache/dubbo-benchmark.git。 你只要具备基本的 JMH 和 Dubbo 的知识，就可以测试出在 Dubbo 场景下各个序列化框架的表现。 我这里也准备了一份我测试的报告，供读者们参考。如果大家准备自行测试，不建议在个人 windows/mac 上 benchmark，结论可能会不准确。我使用了两台阿里云的 ECS 来进行测试，测试环境：Aliyun Linux，4c8g，启动脚本： 1java -server -Xmx2g -Xms2g -XX:MaxDirectMemorySize=1g -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/home/admin/ 为啥选择这个配置？我手上正好有两台这样的资源，没有特殊的设置~，况且从启动脚本就可以看出来，压测程序不会占用太多资源，我都没用满。 测试工程介绍： 123456789public interface UserService { public boolean existUser(String email); public boolean createUser(User user); public User getUser(long id); public Page&lt;User&gt; listUser(int pageNo);} 一个 UserService 接口对业务应用中的 CRUD 操作。server 端以不同的序列化方案提供该服务，client 使用 JMH 进行多轮压测。 12345678910111213141516171819202122232425262728293031@Benchmark @BenchmarkMode({Mode.Throughput }) @OutputTimeUnit(TimeUnit.SECONDS) @Override public boolean existUser() throws Exception { // ... } @Benchmark @BenchmarkMode({Mode.Throughput}) @OutputTimeUnit(TimeUnit.SECONDS) @Override public boolean createUser() throws Exception { // ... } @Benchmark @BenchmarkMode({Mode.Throughput}) @OutputTimeUnit(TimeUnit.SECONDS) @Override public User getUser() throws Exception { // ... } @Benchmark @BenchmarkMode({Mode.Throughput}) @OutputTimeUnit(TimeUnit.SECONDS) @Override public Page&lt;User&gt; listUser() throws Exception { // ... } 整体的 benchmark 框架结构如上，详细的实现，可以参考源码。我这里只选择的一个评测指标 Throughput，即吞吐量。 省略一系列压测过程，直接给出结果： Kryo 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 20913.339 ± 3948.207 ops/sClient.existUser thrpt 3 31669.871 ± 1582.723 ops/sClient.getUser thrpt 3 29706.647 ± 3278.029 ops/sClient.listUser thrpt 3 17234.979 ± 1818.964 ops/s Fst 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 15438.865 ± 4396.911 ops/sClient.existUser thrpt 3 25197.331 ± 12116.109 ops/sClient.getUser thrpt 3 21723.626 ± 7441.582 ops/sClient.listUser thrpt 3 15768.321 ± 11684.183 ops/s Hessian2 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 22948.875 ± 2005.721 ops/sClient.existUser thrpt 3 34735.122 ± 1477.339 ops/sClient.getUser thrpt 3 20679.921 ± 999.129 ops/sClient.listUser thrpt 3 3590.129 ± 673.889 ops/s FastJson 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 26269.487 ± 1667.895 ops/sClient.existUser thrpt 3 29468.687 ± 5152.944 ops/sClient.getUser thrpt 3 25204.239 ± 4326.485 ops/sClient.listUser thrpt 3 9823.574 ± 2087.110 ops/s Tripple 12345Benchmark Mode Cnt Score Error UnitsClient.createUser thrpt 3 19721.871 ± 5121.444 ops/sClient.existUser thrpt 3 35350.031 ± 20801.169 ops/sClient.getUser thrpt 3 20841.078 ± 8583.225 ops/sClient.listUser thrpt 3 4655.687 ± 207.503 ops/s 怎么看到这个测试结果呢？createUser、existUser、getUser 这几个方法测试下来，效果是参差不齐的，不能完全得出哪个框架性能最优，我的推测是因为序列化的数据量比较简单，量也不大，就是一个简单的 User 对象；而 listUser 的实现是返回了一个较大的 List&lt;User&gt; ，可以发现，Kryo 和 Fst 序列化的确表现优秀，处于第一梯队；令我意外的是 FastJson 竟然比 Hessian 还要优秀，位列第二梯队；Tripple（背后是 ProtoBuf）和 Hessian2 位列第三梯队。 当然，这样的结论一定受限于 benchmark 的模型，测试用例中模拟的 CRUD 也不一定完全贴近业务场景，毕竟业务是复杂的。 怎么样，这样的结果是不是也符合你的预期呢？ Dubbo 序列化二三事最后，聊聊你可能知道也可能不知道的一些序列化知识。 hession-liteDubbo 使用的 Hessian2 其实并不是原生的 Hessian2 方案。注意看源码中的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;hessian-lite&lt;/artifactId&gt;&lt;/dependency&gt; 最早是阿里开源的 hessian-lite，后来随着 Dubbo 贡献给了 Apache，该项目也一并进入了 Apache，github 地址：https://github.com/apache/dubbo-hessian-lite。相比原生 Hessian2，Dubbo 独立了一个仓库致力于在 RPC 场景下，发挥出更高的性能以及满足一些定制化的需求。 在 IO 线程中进行序列化Dubbo 客户端在高版本中默认是在业务线程中进行序列化的，而不是 IO 线程，你可以通过 decode.in.io 控制序列化与哪个线程绑定 12345&lt;dubbo:reference id=&quot;userService&quot; check=&quot;false&quot; interface=&quot;org.apache.dubbo.benchmark.service.UserService&quot; url=&quot;dubbo://${server.host}:${server.port}&quot;&gt; &lt;dubbo:parameter key=&quot;decode.in.io&quot; value=&quot;true&quot; /&gt;&lt;/dubbo:reference&gt; 在 benchmark 时，我发现 IO 线程中进行序列化，性能会更好，这可能和序列化本身是一个耗费 CPU 的操作，多线程无法加速反而会导致更多的竞争有关。 SerializationOptimizer某些序列化实现，例如 Kryo 和 Fst 可以通过显示注册序列化的类来进行加速，如果想利用该特性来提升序列化性能，可以实现 org.apache.dubbo.common.serialize.support.SerializationOptimizer 接口。一个示例： 123456public class SerializationOptimizerImpl implements SerializationOptimizer { @Override public Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses() { return Arrays.asList(User.class, Page.class, UserService.class); }} 按照大多数人的习惯，可能会觉得这很麻烦，估计很少有用户这么用。注意客户端和服务端需要同时开启这一优化。 别忘了在 protocol 上配置指定这一优化器： 1&lt;dubbo:protocol name=&quot;dubbo&quot; host=&quot;${server.host}&quot; server=&quot;netty4&quot; port=&quot;${server.port}&quot; serialization=&quot;kryo&quot; optimizer=&quot;org.apache.dubbo.benchmark.serialize.SerializationOptimizerImpl&quot;/&gt; 序列化方式由服务端指定一般而言，Dubbo 框架使用的协议（默认是 dubbo）和序列化方式（默认是 hessian2）是由服务端指定的，不需要在消费端指定。因为服务端是服务的提供者，拥有对服务的定义权，消费者在订阅服务收到服务地址通知时，服务地址会包含序列化的实现方式，Dubbo 以这样的契约方式从而实现 consumer 和 provider 的协同通信。 在大多数业务应用，应用可能既是服务 A 的提供者，同时也是服务 B 的消费者，所以建议在架构决策者层面协商固定出统一的协议，如果没有特殊需求，保持默认值即可。 但如果应用仅仅作为消费者，而又想指定序列化协议或者优化器（某些特殊场景），注意这时候配置 protolcol 是不生效的，因为没有服务提供者是不会触发 protocol 的配置流程的。可以像下面这样指定消费者的配置： 12345&lt;dubbo:reference id=&quot;userService&quot; check=&quot;false&quot; interface=&quot;org.apache.dubbo.benchmark.service.UserService&quot; url=&quot;dubbo://${server.host}:${server.port}?optimizer=org.apache.dubbo.benchmark.serialize.SerializationOptimizerImpl&amp;amp;serialization=kryo&quot;&gt; &lt;dubbo:parameter key=&quot;decode.in.io&quot; value=&quot;true&quot; /&gt;&lt;/dubbo:reference&gt; &amp;amp; 代表 &amp;，避免 xml 中的转义问题 总结借 Dubbo 中各个序列化框架的实现，本文探讨了选择序列化框架时我们的关注点，并探讨了各个序列化实现在 Dubbo 中具体的性能表现， 给出了详细的测试报告，同时，也给出了一些序列化的小技巧，如果在 Dubbo 中修改默认的序列化行为，你可能需要关注这些细节。 最后再借 Dubbo3 支持的 Tripple 协议来聊一下技术发展趋势的问题。我们知道 json 能替代 xml 作为众多前后端开发者耳熟能详的一个技术，并不是因为其性能如何如何，而是在于其恰如其分的解决了大家的问题。一个技术能否流行，也是如此，一定在于其帮助用户解决了痛点。至于解决了什么问题，在各个历史发展阶段又是不同的，曾经，Dubbo2.x 凭借着其丰富的扩展能力，强大的性能，活跃度高的社区等优势帮助用户解决一系列的难题，也获得了非常多用户的亲来；现在，Dubbo3.x 提出的应用级服务发现、统一治理规则、Tripple 协议，也是在尝试解决云原生时代下的难题，如多语言，适配云原生基础设施等，追赶时代，帮助用户。","link":"/dubbo-serialize-talk/"},{"title":"Dubbo 中的 URL 统一模型","text":"定义在不谈及 dubbo 时，我们大多数人对 URL 这个概念并不会感到陌生。统一资源定位器 (RFC1738――Uniform Resource Locators (URL)）应该是最广为人知的一个 RFC 规范，它的定义也非常简单 因特网上的可用资源可以用简单字符串来表示，该文档就是描述了这种字符串的语法和语义。而这些字符串则被称为：“统一资源定位器”（URL） ** 一个标准的 URL 格式 ** 至多可以包含如下的几个部分 1protocol://username:password@host:port/path?key=value&amp;key=value ** 一些典型 URL** 123http://www.facebook.com/friends?param1=value1&amp;param2=value2https://username:password@10.20.130.230:8080/list?version=1.0.0ftp://username:password@192.168.1.7:21/1/read.txt 当然，也有一些 ** 不太符合常规的 URL**，也被归类到了 URL 之中 1234567891011121314151617181920192.168.1.3:20880url protocol = null, url host = 192.168.1.3, port = 20880, url path = nullfile:///home/user1/router.js?type=scripturl protocol = file, url host = null, url path = home/user1/router.jsfile://home/user1/router.js?type=script&lt;br&gt;url protocol = file, url host = home, url path = user1/router.jsfile:///D:/1/router.js?type=scripturl protocol = file, url host = null, url path = D:/1/router.jsfile:/D:/1/router.js?type=script同上 file:///D:/1/router.js?type=script/home/user1/router.js?type=scripturl protocol = null, url host = null, url path = home/user1/router.jshome/user1/router.js?type=scripturl protocol = null, url host = home, url path = user1/router.js Dubbo 中的 URL在 dubbo 中，也使用了类似的 URL，主要用于在各个扩展点之间传递数据，组成此 URL 对象的具体参数如下: protocol：一般是 dubbo 中的各种协议 如：dubbo thrift http zk username/password：用户名 / 密码 host/port：主机 / 端口 path：接口名称 parameters：参数键值对 12345678910111213141516171819202122public URL(String protocol, String username, String password, String host, int port, String path, Map&lt;String, String&gt; parameters) { if ((username == null || username.length() == 0) &amp;&amp; password != null &amp;&amp; password.length()&gt; 0) { throw new IllegalArgumentException(&quot;Invalid url, password without username!&quot;); } this.protocol = protocol; this.username = username; this.password = password; this.host = host; this.port = (port &lt; 0 ? 0 : port); this.path = path; // trim the beginning &quot;/&quot; while(path != null &amp;&amp; path.startsWith(&quot;/&quot;)) { path = path.substring(1); } if (parameters == null) { parameters = new HashMap&lt;String, String&gt;(); } else { parameters = new HashMap&lt;String, String&gt;(parameters); } this.parameters = Collections.unmodifiableMap(parameters);} 可以看出，dubbo 认为 protocol，username，passwored，host，port，path 是主要的 URL 参数，其他键值对村房子啊 parameters 之中。 ** 一些典型的 Dubbo URL** 12345678dubbo://192.168.1.6:20880/moe.cnkirito.sample.HelloService?timeout=3000描述一个 dubbo 协议的服务zookeeper://127.0.0.1:2181/org.apache.dubbo.registry.RegistryService?application=demo-consumer&amp;dubbo=2.0.2&amp;interface=org.apache.dubbo.registry.RegistryService&amp;pid=1214&amp;qos.port=33333&amp;timestamp=1545721981946描述一个 zookeeper 注册中心consumer://30.5.120.217/org.apache.dubbo.demo.DemoService?application=demo-consumer&amp;category=consumers&amp;check=false&amp;dubbo=2.0.2&amp;interface=org.apache.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=1209&amp;qos.port=33333&amp;side=consumer&amp;timestamp=1545721827784描述一个消费者 可以说，任意的一个领域中的一个实现都可以认为是一类 URL，dubbo 使用 URL 来统一描述了元数据，配置信息，贯穿在整个框架之中。 URL 相关的生命周期解析服务基于 dubbo.jar 内的 META-INF/spring.handlers 配置，Spring 在遇到 dubbo 名称空间时，会回调 DubboNamespaceHandler。 所有 dubbo 的标签，都统一用 DubboBeanDefinitionParser 进行解析，基于一对一属性映射，将 XML 标签解析为 Bean 对象。 在 ServiceConfig.export() 或 ReferenceConfig.get() 初始化时，将 Bean 对象转换 URL 格式，所有 Bean 属性转成 URL 的参数。 然后将 URL 传给协议扩展点，基于扩展点自适应机制，根据 URL 的协议头，进行不同协议的服务暴露或引用。 暴露服务1. 只暴露服务端口： 在没有注册中心，直接暴露提供者的情况下，ServiceConfig 解析出的 URL 的格式为：dubbo://service-host/com.foo.FooService?version=1.0.0。 基于扩展点自适应机制，通过 URL 的 dubbo:// 协议头识别，直接调用 DubboProtocol 的 export() 方法，打开服务端口。 2. 向注册中心暴露服务： 在有注册中心，需要注册提供者地址的情况下，ServiceConfig 解析出的 URL 的格式为: registry://registry-host/org.apache.dubbo.registry.RegistryService?export=URL.encode(&quot;dubbo://service-host/com.foo.FooService?version=1.0.0&quot;)， 基于扩展点自适应机制，通过 URL 的 registry:// 协议头识别，就会调用 RegistryProtocol 的 export() 方法，将 export 参数中的提供者 URL，先注册到注册中心。 再重新传给 Protocol 扩展点进行暴露： dubbo://service-host/com.foo.FooService?version=1.0.0，然后基于扩展点自适应机制，通过提供者 URL 的 dubbo:// 协议头识别，就会调用 DubboProtocol 的 export() 方法，打开服务端口。 引用服务1. 直连引用服务： 在没有注册中心，直连提供者的情况下，ReferenceConfig 解析出的 URL 的格式为：dubbo://service-host/com.foo.FooService?version=1.0.0。 基于扩展点自适应机制，通过 URL 的 dubbo:// 协议头识别，直接调用 DubboProtocol 的 refer() 方法，返回提供者引用。 2. 从注册中心发现引用服务： 在有注册中心，通过注册中心发现提供者地址的情况下，ReferenceConfig 解析出的 URL 的格式为：registry://registry-host/org.apache.dubbo.registry.RegistryService?refer=URL.encode(&quot;consumer://consumer-host/com.foo.FooService?version=1.0.0&quot;)。 基于扩展点自适应机制，通过 URL 的 registry:// 协议头识别，就会调用 RegistryProtocol 的 refer() 方法，基于 refer 参数中的条件，查询提供者 URL，如： dubbo://service-host/com.foo.FooService?version=1.0.0。 基于扩展点自适应机制，通过提供者 URL 的 dubbo:// 协议头识别，就会调用 DubboProtocol 的 refer() 方法，得到提供者引用。 然后 RegistryProtocol 将多个提供者引用，通过 Cluster 扩展点，伪装成单个提供者引用返回。 URL 统一模型的意义对于 dubbo 中的 URL，有人理解为配置总线，有人理解为统一配置模型，说法虽然不同，但都是在表达一个意思，这样的 URL 在 dubbo 中被当做是 公共契约，所有扩展点参数都包含 URL 参数，URL 作为上下文信息贯穿整个扩展点设计体系。 在没有 URL 之前，只能以字符串传递参数，不停的解析和拼装，导致相同类型的接口，参数时而 Map, 时而 Parameters 类包装： 12export(String url) createExporter(String host, int port, Parameters params) 使用 URL 一致性模型： 12export(URL url) createExporter(URL url) 在最新的 dubbo 代码中，我们可以看到大量使用 URL 来进行上下文之间信息的传递，这样的好处是显而易见的： 使得代码编写者和阅读者能够将一系列的参数联系起来，进而形成规范，使得代码易写，易读。 可扩展性强，URL 相当于参数的集合 (相当于一个 Map)，他所表达的含义比单个参数更丰富，当我们在扩展代码时，可以将新的参数追加到 URL 之中，而不需要改变入参，返参的结构。 统一模型，它位于 org.apache.dubbo.common 包中，各个扩展模块都可以使用它作为参数的表达形式，简化了概念，降低了代码的理解成本。 如果你能够理解 final 契约和 restful 契约，那我相信你会很好地理解 URL 契约。契约的好处我还是啰嗦一句：大家都这么做，就形成了默契，沟通是一件很麻烦的事，统一 URL 模型可以省去很多沟通成本，这边是 URL 统一模型存在的意义。","link":"/dubbo-url/"},{"title":"Dubbo2.7 三大新特性详解","text":"1 背景介绍自 2017 年 7 月阿里重启 Dubbo 开源，到目前为止 github star 数，contributor 数都有了非常大的提升。2018 年 2 月 9 日阿里决定将 Dubbo 项目贡献给 Apache，经过一周的投票，顺利成为了 Apache 的孵化项目，也就是大家现在看到的 Incubator Dubbo。预计在 2019 年 4 月，Dubbo 可以达成毕业，成为 Apache 的顶级项目。 2 分支介绍 Dubbo 目前有如图所示的 5 个分支，其中 2.7.1-release 只是一个临时分支，忽略不计，对其他 4 个分支进行介绍。 2.5.x 近期已经通过投票，Dubbo 社区即将停止对其的维护。 2.6.x 为长期支持的版本，也是 Dubbo 贡献给 Apache 之前的版本，其包名前缀为：com.alibaba，JDK 版本对应 1.6。 3.x-dev 是前瞻性的版本，对 Dubbo 进行一些高级特性的补充，如支持 rx 特性。 master 为长期支持的版本，版本号为 2.7.x，也是 Dubbo 贡献给 Apache 的开发版本，其包名前缀为：org.apache，JDK 版本对应 1.8。 如果想要研究 Dubbo 的源码，建议直接浏览 master 分支。 3 Dubbo 2.7 新特性Dubbo 2.7.x 作为 Apache 的孵化版本，除了代码优化之外，还新增了许多重磅的新特性，本文将会介绍其中最典型的三个新特性： 异步化改造 三大中心改造 服务治理增强 4 异步化改造4.1 几种调用方式 在远程方法调用中，大致可以分为这 4 种调用方式。oneway 指的是客户端发送消息后，不需要接受响应。对于那些不关心服务端响应的请求，比较适合使用 oneway 通信。 注意，void hello() 方法在远程方法调用中，不属于 oneway 调用，虽然 void 方法表达了不关心返回值的语义，但在 RPC 层面，仍然需要做通信层的响应。 sync 是最常用的通信方式，也是默认的通信方法。 future 和 callback 都属于异步调用的范畴，他们的区别是：在接收响应时，future.get() 会导致线程的阻塞;callback 通常会设置一个回调线程，当接收到响应时，自动执行，不会对当前线程造成阻塞。 4.2 Dubbo 2.6 异步化异步化的优势在于客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小。介绍 2.7 中的异步化改造之前，先回顾一下如何在 2.6 中使用 Dubbo 异步化的能力。 将同步接口声明成 async=true 1&lt;dubbo:reference id=&quot;asyncService&quot; interface=&quot;org.apache.dubbo.demo.api.AsyncService&quot; async=&quot;true&quot;/&gt; 123public interface AsyncService { String sayHello(String name);} 通过上下文类获取 future 123AsyncService.sayHello(&quot;Han Meimei&quot;);Future&lt;String&gt; fooFuture = RpcContext.getContext().getFuture();fooFuture.get(); 可以看出，这样的使用方式，不太符合异步编程的习惯，竟然需要从一个上下文类中获取到 Future。如果同时进行多个异步调用，使用不当很容易造成上下文污染。而且，Future 并不支持 callback 的调用方式。这些弊端在 Dubbo 2.7 中得到了改进。 4.3 Dubbo 2.7 异步化 无需配置中特殊声明，显示声明异步接口即可 123456public interface AsyncService { String sayHello(String name); default CompletableFuture&lt;String&gt; sayHiAsync(String name) { return CompletableFuture.completedFuture(sayHello(name)); }} 使用 callback 方式处理返回值 12345678CompletableFuture&lt;String&gt; future = asyncService.sayHiAsync(&quot;Han MeiMei&quot;);future.whenComplete((retValue, exception) -&gt; { if (exception == null) { System.out.println(retValue); } else { exception.printStackTrace(); }}); Dubbo 2.7 中使用了 JDK1.8 提供的 CompletableFuture 原生接口对自身的异步化做了改进。CompletableFuture 可以支持 future 和 callback 两种调用方式，用户可以根据自己的喜好和场景选择使用，非常灵活。 4.4 异步化设计 FAQQ：如果 RPC 接口只定义了同步接口，有办法使用异步调用吗？ A：2.6 中的异步调用唯一的优势在于，不需要在接口层面做改造，又可以进行异步调用，这种方式仍然在 2.7 中保留；使用 Dubbo 官方提供的 compiler hacker，编译期自动重写同步方法，请 在此 讨论和跟进具体进展。 Q：关于异步接口的设计问题，为何不提供编译插件，根据原接口，自动编译出一个 XxxAsync 接口？ A：Dubbo 2.7 采用采用过这种设计，但接口的膨胀会导致服务类的增量发布，而且接口名的变化会影响服务治理的一些相关逻辑，改为方法添加 Async 后缀相对影响范围较小。 Q：Dubbo 分为了客户端异步和服务端异步，刚刚你介绍的是客户端异步，为什么不提服务端异步呢？ A：Dubbo 2.7 新增了服务端异步的支持，但实际上，Dubbo 的业务线程池模型，本身就可以理解为异步调用，个人认为服务端异步的特性较为鸡肋。 5 三大中心改造三大中心指的：注册中心，元数据中心，配置中心。 在 2.7 之前的版本，Dubbo 只配备了注册中心，主流使用的注册中心为 zookeeper。新增加了元数据中心和配置中心，自然是为了解决对应的痛点，下面我们来详细阐释三大中心改造的原因。 5.1 元数据改造元数据是什么？元数据定义为描述数据的数据，在服务治理中，例如服务接口名，重试次数，版本号等等都可以理解为元数据。在 2.7 之前，元数据一股脑丢在了注册中心之中，这造成了一系列的问题： ** 推送量大 -&gt; 存储数据量大 -&gt; 网络传输量大 -&gt; 延迟严重 ** 生产者端注册 30+ 参数，有接近一半是不需要作为注册中心进行传递；消费者端注册 25+ 参数，只有个别需要传递给注册中心。有了以上的理论分析，Dubbo 2.7 进行了大刀阔斧的改动，只将真正属于服务治理的数据发布到注册中心之中，大大降低了注册中心的负荷。 同时，将全量的元数据发布到另外的组件中：元数据中心。元数据中心目前支持 redis（推荐），zookeeper。这也为 Dubbo 2.7 全新的 Dubbo Admin 做了准备，关于新版的 Dubbo Admin，我将会后续准备一篇独立的文章进行介绍。 示例：使用 zookeeper 作为元数据中心 1&lt;dubbo:metadata-report address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt; 5.2 Dubbo 2.6 元数据1234567891011121314dubbo://30.5.120.185:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;bean.name=com.alibaba.dubbo.demo.DemoService&amp;dubbo=2.0.2&amp;executes=4500&amp;generic=false&amp;owner=kirito&amp;pid=84228&amp;retries=7&amp;side=provider&amp;timestamp=1552965771067 从本地的 zookeeper 中取出一条服务数据，通过解码之后，可以看出，的确有很多参数是不必要。 5.3 Dubbo 2.7 元数据在 2.7 中，如果不进行额外的配置，zookeeper 中的数据格式仍然会和 Dubbo 2.6 保持一致，这主要是为了保证兼容性，让 Dubbo 2.6 的客户端可以调用 Dubbo 2.7 的服务端。如果整体迁移到 2.7，则可以为注册中心开启简化配置的参数： 1&lt;dubbo:registry address=“zookeeper://127.0.0.1:2181” simplified=&quot;true&quot;/&gt; Dubbo 将会只上传那些必要的服务治理数据，一个简化过后的数据如下所示： 12345dubbo://30.5.120.185:20880/org.apache.dubbo.demo.api.DemoService?application=demo-provider&amp;dubbo=2.0.2&amp;release=2.7.0&amp;timestamp=1552975501873 对于那些非必要的服务信息，仍然全量存储在元数据中心之中： 元数据中心的数据可以被用于服务测试，服务 MOCK 等功能。目前注册中心配置中 simplified 的默认值为 false，因为考虑到了迁移的兼容问题，在后续迭代中，默认值将会改为 true。 5.4 配置中心支持衡量配置中心的必要性往往从三个角度出发： 分布式配置统一管理 动态变更推送 安全性 Spring Cloud Config, Apollo, Nacos 等分布式配置中心组件都对上述功能有不同程度的支持。在 2.7 之前的版本中，在 zookeeper 中设置了部分节点：configurators，routers，用于管理部分配置和路由信息，它们可以理解为 Dubbo 配置中心的雏形。在 2.7 中，Dubbo 正式支持了配置中心，目前支持的几种注册中心 Zookeeper，Apollo，Nacos（2.7.1-release 支持）。 在 Dubbo 中，配置中心主要承担了两个作用 外部化配置。启动配置的集中式存储 服务治理。服务治理规则的存储与通知 示例：使用 Zookeeper 作为配置中心 1&lt;dubbo:config-center address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt; 引入配置中心后，需要注意配置项的覆盖问题，优先级如图所示 6 服务治理增强我更倾向于将 Dubbo 当做一个服务治理框架，而不仅仅是一个 RPC 框架。在 2.7 中，Dubbo 对其服务治理能力进行了增强，增加了标签路由的能力，并抽象出了应用路由和服务路由的概念。在最后一个特性介绍中，着重对标签路由 TagRouter 进行探讨。 在服务治理中，路由层和负载均衡层的对比。区别 1，Router：m 选 n，LoadBalance：n 选 1；区别 2，路由往往是叠加使用的，负载均衡只能配置一种。 在很长的一段时间内，Dubbo 社区经常有人提的一个问题是：Dubbo 如何实现流量隔离和灰度发布，直到 2.7 提供了标签路由，用户可以使用这个功能，来实现上述的需求。 标签路由提供了这样一个能力，当调用链路为 A -&gt; B -&gt; C -&gt; D 时，用户给请求打标，最典型的打标方式可以借助 attachment（他可以在分布式调用中传递下去），调用会优先请求那些匹配的服务端，如 A -&gt; B，C -&gt; D，由于集群中未部署 C 节点，则会降级到普通节点。 打标方式会收到集成系统差异的影响，从而导致很大的差异，所以 Dubbo 只提供了 RpcContext.getContext().setAttachment() 这样的基础接口，用户可以使用 SPI 扩展，或者 server filter 的扩展，对测试流量进行打标，引导进入隔离环境 / 灰度环境。 新版的 Dubbo Admin 提供了标签路由的配置项： Dubbo 用户可以在自己系统的基础上对标签路由进行二次扩展，或者借鉴标签路由的设计，实现自己系统的流量隔离，灰度发布。 7 总结本文介绍了 Dubbo 2.7 比较重要的三大新特性：异步化改造，三大中心改造，服务治理增强。Dubbo 2.7 还包含了很多功能优化、特性升级，可以在项目源码的 CHANGES.md 中浏览全部的改动点。最后提供一份 Dubbo 2.7 的升级文档：2.7 迁移文档，欢迎体验。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/dubbo27-features/"},{"title":"天池中间件大赛 dubboMesh 优化总结（qps 从 1000 到 6850）","text":"天池中间件大赛的初赛在今早终于正式结束了，公众号停更了一个月，主要原因就是博主的空余时间几乎全花在这个比赛上，第一赛季结束，做下参赛总结，总的来说，收获不小。 先说结果，最终榜单排名是第 15 名（除去前排大佬的两个小号，加上作弊的第一名，勉强能算是第 12 名），说实话是挺满意的成绩。这篇文章主要是分享给以下读者：比赛中使用了 netty 却没有达到理想 qps 的朋友，netty 刚入门的朋友，对 dubbo mesh 感兴趣的朋友。 在比赛之前我个人对 netty 的认识也仅仅停留在了解的层面，在之前解读 RPC 原理的系列文章中涉及到 netty 传输时曾了解过一二，基本可以算零基础使用 netty 参赛，所以我会更多地站在一个小白的视角来阐述自己的优化历程，一步步地提高 qps，也不会绕开那些自己踩过的坑以及负优化。另一方面，由于自己对 netty 的理解并不是很深，所以文中如果出现错误，敬请谅解，欢迎指正。 Dubbo Mesh 是什么？为了照顾那些不太了解这次比赛内容的读者，我先花少量的篇幅介绍下这次阿里举办的天池中间件大赛到底比的是个什么东西，那就不得不先介绍下 Dubbo Mesh 这个概念。 如果你用过 dubbo，并且对 service mesh 有所了解，那么一定可以秒懂 Dubbo Mesh 是为了解决什么问题。说白了，dubbo 原先是为了 java 语言而准备的，没有考虑到跨语言的问题，这意味着 nodejs，python，go 要想无缝使用 dubbo 服务，要么借助于各自语言的 dubbo 客户端，例如：node-dubbo-client，python-dubbo-client，go-dubbo-client；要么就是借助于 service mesh 的解决方案，让 dubbo 自己提供跨语言的解决方案，来屏蔽不同语言的处理细节，于是乎，dubbo 生态的跨语言 service mesh 解决方案就被命名为了 dubbo mesh。一图胜千言： 在原先的 dubbo 生态下，只有 consumer，provider，注册中心的概念。dubbo mesh 生态下为每个服务（每个 consumer，provider 实例）启动一个 agent，服务间不再进行直接的通信，而是经由各自的 agent 完成交互，并且服务的注册发现也由 agent 完成。图中红色的 agent 便是这次比赛的核心，选手们可以选择合适的语言来实现 agent，最终比拼高并发下各自 agent 实现的 qps，qps 即最终排名的依据。 赛题剖析这次比赛的主要考察点在于高并发下网络通信模型的实现，可以涵盖以下几个关键点：reactor 模型，负载均衡，线程，锁，io 通信，阻塞与非阻塞，零拷贝，序列化，http/tcp/udp 与自定义协议，批处理，垃圾回收，服务注册发现等。它们对最终程序的 qps 起着或大或小的影响，对它们的理解越深，越能够编写出高性能的 dubbo mesh 方案。 语言的选择，初赛结束后的感受，大家主要还是在 java，c++，go 中进行了抉择。语言的选择考虑到了诸多的因素，通用性，轻量级，性能，代码量和 qps 的性价比，选手的习惯等等。虽然前几名貌似都是 c++，但总体来说，排名 top 10 之外，绝不会是因为语言特性在从中阻挠。c++ 选手高性能的背后，可能是牺牲了 600 多行代码在自己维护一个 etcd-lib（比赛限制使用 etcd，但据使用 c++ 的选手说，c++ 没有提供 etcd 的 lib）；且这次比赛提供了预热环节，java 党也露出了欣慰的笑容。java 的主流框架还是在 nio，akka，netty 之间的抉择，netty 应该是众多 java 选手中较为青睐的，博主也选择了 netty 作为 dubbo mesh 的实现；go 的协程和网络库也是两把利器，并不比 java 弱，加上其进程轻量级的特性，也作为了一个选择。 官方提供了一个 qps 并不是很高的 demo，来方便选手们理解题意，可以说是非常贴心了，来回顾一下最简易的 dubbo mesh 实现： 如上图所示，是整个初始 dubbo mesh 的架构图，其中 consumer 和 provider 以灰色表示，因为选手是不能修改其实现的，绿色部分的 agent 是可以由选手们自由发挥的部分。比赛中 consumer，consumer-agent 为 单个实例，provider、provider-agent 分别启动了三个性能不一的实例：small，medium，large，这点我没有在图中表示出来，大家自行脑补。所以所有选手都需要完成以下几件事： consumer-agent 需要启动一个 http 服务器，接收来自 consumer 的 http 请求 consumer-agent 需要转发该 http 请求给 provider-agent，并且由于 provider-agent 有多个实例，所以需要做负载均衡。consumer-agent 与 provider-agent 之间如何通信可以自由发挥。 provider-agent 拿到 consumer-agent 的请求之后，需要组装成 dubbo 协议， 使用 tcp 与 provider 完成通信。 这样一个跨语言的简易 dubbo mesh 便呈现在大家面前了，从 consumer 发出的 http 协议，最终成功调用到了使用 java 语言编写的 dubbo 服务。这中间如何优化，如何使用各种黑科技成就了一场非常有趣的比赛。博主所有的优化都不是一蹴而就的，都是一天天的提交试出来的，所以恰好可以使用时间线顺序叙述自己的改造历程。 优化历程Qps 1000 到 2500 (CA 与 PA 使用异步 http 通信) 官方提供的 demo 直接跑通了整个通信流程，省去了我们大量的时间，初始版本评测可以达到 1000+ 的 qps，所以 1000 可以作为 baseline 给大家提供参考。demo 中 consumer 使用 asyncHttpClient 发送异步的 http 请求， consumer-agent 使用了 springmvc 支持的 servlet3.0 特性；而 consumer-agent 到 provider-agent 之间的通信却使用了同步 http，所以 C 到 CA 这一环节相比 CA 到 PA 这一环节性能是要强很多的。改造起来也很简单，参照 C 到 CA 的设计，直接将 CA 到 PA 也替换成异步 http，qps 可以直接到达 2500。 主要得益于 async-http-client 提供的异步 http-client，以及 servlet3.0 提供的非阻塞 api。 12345&lt;dependency&gt; &lt;groupId&gt;org.asynchttpclient&lt;/groupId&gt; &lt;artifactId&gt;async-http-client&lt;/artifactId&gt; &lt;version&gt;2.4.7&lt;/version&gt;&lt;/dependency&gt; 123456// 非阻塞发送 http 请求ListenableFuture&lt;org.asynchttpclient.Response&gt; responseFuture = asyncHttpClient.executeRequest(request);// 非阻塞返回 http 响应@RequestMapping(value = &quot;/invoke&quot;)public DeferredResult&lt;ResponseEntity&gt; invoke(){} Qps 2500 到 2800 (负载均衡优化为加权轮询) demo 中提供的负载均衡算法是随机算法，在 small-pa，medium-pa，large-pa 中随机选择一个访问，每个服务的性能不一样，响应时间自然也不同，随机负载均衡算法存在严重的不稳定性，无法按需分配请求，所以成了自然而然的第二个改造点。 优化为加权轮询算法，这一块的实现参考了 motan（weibo 开源的 rpc 框架）的实现，详见 com.alibaba.dubbo.performance.demo.agent.cluster.loadbalance.WeightRoundRobinLoadBalance(文末贴 git 地址)。 在启动脚本中配置权重信息，伴随 pa 启动注册服务地址到 etcd 时，顺带将权重信息一并注册到 etcd 中，ca 拉取服务列表时即可获取到负载比例。 123456large:-Dlb.weight=3medium:-Dlb.weight=2small:-Dlb.weight=1 预热赛时最高并发为 256 连接，这样的比例可以充分发挥每个 pa 的性能。 Qps 2800 到 3500 (future-&gt;callback) c 到 ca 以及 ca 到 pa 此时尽管是 http 通信，但已经实现了非阻塞的特性（请求不会阻塞 io 线程），但 dubbo mesh 的 demo 中 pa 到 p 的这一通信环节还是使用的 future.get + countDownLatch 的阻塞方式，一旦整个环节出现了锁和阻塞，qps 必然上不去。关于几种获取结果的方式，也是老生常谈的话题： future 方式在调用过程中不会阻塞线程，但获取结果是会阻塞线程，provider 固定 sleep 了 50 ms，所以获取 future 结果依旧是一个耗时的过程，加上这种模型一般会使用锁来等待，性能会造成明显的下降。替换成 callback 的好处是，io 线程专注于 io 事件，降低了线程数，这和 netty 的 io 模型也是非常契合的。 12Promise&lt;Integer&gt; agentResponsePromise = new DefaultPromise&lt;&gt;(ctx.executor());agentResponsePromise.addListener(); netty 为此提供了默认的 Promise 的抽象，以及 DefaultPromise 的默认实现，我们可以 out-of-box 的使用 callback 特性。在 netty 的入站 handler 的 channelRead 事件中创建 promise，拿到 requestId，建立 requestId 和 promise 的映射；在出站 handler 的 channelRead 事件中拿到返回的 requestId，查到 promise，调用 done 方法，便完成了非阻塞的请求响应。可参考： 入站 handler ConsumerAgentHttpServerHandler 和 和出站 handler ConsumerAgentClientHandler 的实现。 Qps 3500 到 4200 (http 通信替换为 tcp 通信) ca 到 pa 的通信原本是异步 http 的通信方式，完全可以参考 pa 到 p 的异步 tcp 通信进行改造。自定义 agent 之间的通信协议也非常容易，考虑到 tcp 粘包的问题，使用定长头 + 字节数组来作为自定义协议是一个较为常用的做法。这里踩过一个坑，原本想使用 protoBuffer 来作为自定义协议，netty 也很友好的提供了基于 protoBuffer 协议的编解码器，只需要编写好 DubboMeshProto.proto 文件即可： 123456789101112message AgentRequest { int64 requestId = 1; string interfaceName = 2; string method = 3; string parameterTypesString = 4; string parameter = 5;}message AgentResponse { int64 requestId = 1; bytes hash = 2;} protoBuffer 在实际使用中的优势是毋庸置疑的，其可以尽可能的压缩字节，减少 io 码流。在正式赛之前一直用的好好的，但后来的 512 并发下通过 jprofile 发现，DubboMeshProto 的 getSerializedSize ,getDescriptorForType 等方法存在不必要的耗时，对于这次比赛中如此简单的数据结构而言 protoBuffer 并不是那么优秀。最终还是采取了定长头 + 字节数组的自定义协议。参考：com.alibaba.dubbo.performance.demo.agent.protocol.simple.SimpleDecoder http 通信既然换了，干脆一换到底，ca 的 springmvc 服务器也可以使用 netty 实现，这样更加有利于实现 ca 整体的 reactive。使用 netty 实现 http 服务器很简单，使用 netty 提供的默认编码解码器即可。 12345678910public class ConsumerAgentHttpServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; { @Override public void initChannel(SocketChannel ch) { ChannelPipeline p = ch.pipeline(); p.addLast(&quot;encoder&quot;, new HttpResponseEncoder()); p.addLast(&quot;decoder&quot;, new HttpRequestDecoder()); p.addLast(&quot;aggregator&quot;, new HttpObjectAggregator(10 * 1024 * 1024)); p.addLast(new ConsumerAgentHttpServerHandler()); }} http 服务器的实现也踩了一个坑，解码 http request 请求时没注意好 ByteBuf 的释放，导致 qps 跌倒了 2000+，反而不如 springmvc 的实现。在队友 @闪电侠的帮助下成功定位到了内存泄露的问题。 123456789101112131415public static Map&lt;String, String&gt; parse(FullHttpRequest req) { Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); // 是 POST 请求 HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(new DefaultHttpDataFactory(false), req); List&lt;InterfaceHttpData&gt; postList = decoder.getBodyHttpDatas(); for (InterfaceHttpData data : postList) { if (data.getHttpDataType() == InterfaceHttpData.HttpDataType.Attribute) { MemoryAttribute attribute = (MemoryAttribute) data; params.put(attribute.getName(), attribute.getValue()); } } // resolve memory leak decoder.destroy(); return params;} 在正式赛后发现还有更快的 decode 方式，不需要借助于上述的 HttpPostRequestDecoder，而是改用 QueryStringDecoder： 123456789101112131415public static Map&lt;String, String&gt; fastParse(FullHttpRequest httpRequest) { String content = httpRequest.content().toString(StandardCharsets.UTF_8); QueryStringDecoder qs = new QueryStringDecoder(content, StandardCharsets.UTF_8, false); Map&lt;String, List&lt;String&gt;&gt; parameters = qs.parameters(); String interfaceName = parameters.get(&quot;interface&quot;).get(0); String method = parameters.get(&quot;method&quot;).get(0); String parameterTypesString = parameters.get(&quot;parameterTypesString&quot;).get(0); String parameter = parameters.get(&quot;parameter&quot;).get(0); Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); params.put(&quot;interface&quot;, interfaceName); params.put(&quot;method&quot;, method); params.put(&quot;parameterTypesString&quot;, parameterTypesString); params.put(&quot;parameter&quot;, parameter); return params;} 节省篇幅，直接在这儿将之后的优化贴出来，后续不再对这个优化赘述了。 Qps 4200 到 4400 (netty 复用 eventLoop) 这个优化点来自于比赛认识的一位好友 @半杯水，由于没有使用过 netty，比赛期间恶补了一下 netty 的线程模型，得知了 netty 可以从客户端引导 channel，从而复用 eventLoop。不了解 netty 的朋友可以把 eventLoop 理解为 io 线程，如果入站的 io 线程和 出站的 io 线程使用相同的线程，可以减少不必要的上下文切换，这一点在 256 并发下可能还不明显，只有 200 多 qps 的差距，但在 512 下尤为明显。复用 eventLoop 在《netty 实战》中是一个专门的章节，篇幅虽然不多，但非常清晰地向读者阐释了如何复用 eventLoop（注意复用同时存在于 ca 和 pa 中）。 1234567891011121314// 入站服务端的 eventLoopGroupprivate EventLoopGroup workerGroup;// 为出站客户端预先创建好的 channelprivate void initThreadBoundClient(EventLoopGroup workerGroup) { for (EventExecutor eventExecutor : eventLoopGroup) { if (eventExecutor instanceof EventLoop) { ConsumerAgentClient consumerAgentClient = new ConsumerAgentClient((EventLoop) eventExecutor); consumerAgentClient.init(); ConsumerAgentClient.put(eventExecutor, consumerAgentClient); } }} 使用入站服务端的 eventLoopGroup 为出站客户端预先创建好 channel，这样可以达到复用 eventLoop 的目的。并且此时还有一个伴随的优化点，就是将存储 Map&lt;requestId,Promise&gt; 的数据结构，从 concurrentHashMap 替换为了 ThreadLocal , 因为入站线程和出站线程都是相同的线程，省去一个 concurrentHashMap 可以进一步降低锁的竞争。 到了这一步，整体架构已经清晰了，c-&gt;ca，ca-&gt;pa，pa-&gt;p 都实现了异步非阻塞的 reactor 模型，qps 在 256 并发下，也达到了 4400 qps。 正式赛 512 连接带来的新格局上述这份代码在预热赛 256 并发下表现尚可，但正式赛为了体现出大家的差距，将最高并发数直接提升了一倍，但 qps 却并没有得到很好的提升，卡在了 5400 qps。和 256 连接下同样 4400 的朋友交流过后，发现我们之间的差距主要体现在 ca 和 pa 的 io 线程数，以及 pa 到 p 的连接数上。5400 qps 显然低于我的预期，为了降低连接数，我修改了原来 provider-agent 的设计。从以下优化开始，是正式赛 512 连接下的优化，预热赛只有 256 连接。 Qps 5400 到 5800 (降低连接数) 对 netty 中 channel 的优化搜了很多文章，依旧不是很确定连接数到底是不是影响我代码的关键因素，在和小伙伴沟通之后实在找不到 qps 卡在 5400 的原因，于是乎抱着试试的心态修改了下 provider-agent 的设计，采用了和 consumer-agent 一样的设计，预先拿到 provder-agent 入站服务器的 woker 线程组，创建出站请求的 channel，将原来的 4 个线程，4 个 channel 降低到了 1 个线程，一个 channel。其他方面未做任何改动，qps 顺利达到了 5800。 理论上来说，channel 数应该不至于成为性能的瓶颈，可能和 provider dubbo 的线程池策略有关，最终得出的经验就是：在 server 中合理的在 io 事件处理能力的承受范围内，使用尽可能少的连接数和线程数，可以提升 qps，减少不必要的线程切换。顺带一提（此时 ca 的线程数为 4，入站连接为 http 连接，最高为 512 连接，出站连接由于和线程绑定，又需要做负载均衡，所以为$$线程数 pa 数 =43=12$$这个阶段，还存在另一个问题，由于 provider 线程数固定为 200 个线程，如果 large-pa 继续分配 3/1+2+3=0.5 即 50% 的请求，很容易出现 provider 线程池饱满的异常，所以调整了加权值为 1：2：2。限制加权负载均衡的不再仅仅是机器性能，还要考虑到 provider 的连接处理能力。 Qps 5800 到 6100 (Epoll 替换 Nio) 依旧感谢 @半杯水的提醒，由于评测环境使用了 linux 作为评测环境，所以可以使用 netty 自己封装的 EpollSocketChannel 来代替 NioSocketChannel，这个提升远超我的想象，直接帮助我突破了 6000 的关卡。 12345private EventLoopGroup bossGroup = Epoll.isAvailable()? new EpollEventLoopGroup(1) : new NioEventLoopGroup(1);private EventLoopGroup workerGroup = Epoll.isAvailable()? new EpollEventLoopGroup(2) : new NioEventLoopGroup(2);bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup) .channel(Epoll.isAvailable() ? EpollServerSocketChannel.class : NioServerSocketChannel.class) 本地调试由于我是 mac 环境，没法使用 Epoll，所以加了如上的判断。 NioServerSocketChannel 使用了 jdk 的 nio，其会根据操作系统选择使用不同的 io 模型，在 linux 下同样是 epoll，但默认是 level-triggered ，而 netty 自己封装的 EpollSocketChannel 默认是 edge-triggered。 我原先以为是 et 和 lt 的差距导致了 qps 如此大的悬殊，但后续优化 Epoll 参数时发现 EpollSocketChannel 也可以配置为 level-triggered，qps 并没有下降，在比赛的特殊条件下，个人猜想并不是这两种触发方式带来的差距，而仅仅是 netty 自己封装 epoll 带来的优化。 1234// 默认bootstrap.option(EpollChannelOption.EPOLL_MODE, EpollMode.EDGE_TRIGGERED);// 可修改触发方式bootstrap.option(EpollChannelOption.EPOLL_MODE, EpollMode.LEVEL_TRIGGERED); Qps 6100 到 6300 (agent 自定义协议优化) agent 之间的自定义协议我之前已经介绍过了，由于一开始我使用了 protoBuf，发现了性能问题，就是在这儿发现的。在 512 下 protoBuf 的问题尤为明显，最终为了保险起见，以及为了和我后面的一个优化兼容，最终替换为了自定义协议—Simple 协议，这一点优化之前提到了，不在过多介绍。 Qps 6300 到 6500 (参数调优与 zero-copy) 这一段优化来自于和 @折袖 - 许华建 的交流，非常感谢。又是一个对 netty 不太了解而没注意的优化点： 关闭 netty 的内存泄露检测： 1-Dio.netty.leakDetectionLevel=disabled netty 会在运行期定期抽取 1% 的 ByteBuf 进行内存泄露的检测，关闭这个参数后，可以获得性能的提升。 开启 quick_ack： 1bootstrap.option(EpollChannelOption.TCP_QUICKACK, java.lang.Boolean.TRUE) tcp 相比 udp ，一个区别便是为了可靠传输而进行的 ack，netty 为 Epoll 提供了这个参数，可以进行 quick ack，具体原理没来及研究。 开启 TCP_NODELAY 1serverBootstrap.childOption(ChannelOption.TCP_NODELAY, true) 这个优化可能大多数人都知道，放在这儿一起罗列出来。网上搜到了一篇阿里毕玄的 rpc 优化文章，提到高并发下 ChannelOption.TCP_NODELAY=false 可能更好，但实测之后发现并不会。 其他调优的参数可能都是玄学了，对最终的 qps 影响微乎其微。参数调优并不能体现太多的技巧，但对结果产生的影响却是很可观的。 在这个阶段还同时进行了一个优化，和参数调优一起进行的，所以不知道哪个影响更大一些。demo 中 dubbo 协议编码没有做到 zero-copy，这无形中增加了一份数据从内核态到用户态的拷贝；自定义协议之间同样存在这个问题，在 dubbo mesh 的实践过程中应该尽可能做到：能用 ByteBuf 的地方就不要用其他对象，ByteBuf 提供的 slice 和 CompositeByteBuf 都可以很方便的实现 zero-copy。 Qps 6500 到 6600 (自定义 http 协议编解码) 看着榜单上的人 qps 逐渐上升，而自己依旧停留在 6500，于是乎动了歪心思，GTMD 的通用性，自己解析 http 协议得了，不要 netty 提供的 http 编解码器，不需要比 HttpPostRequestDecoder 更快的 QueryStringDecoder，就一个偏向于固定的 http 请求，实现自定义解析非常简单。 123456POST / HTTP/1.1\\r\\ncontent-length: 560\\r\\ncontent-type: application/x-www-form-urlencoded\\r\\nhost: 127.0.0.1:20000\\r\\n\\r\\ninterface=com.alibaba.dubbo.performance.demo.provider.IHelloService&amp;method=hash&amp;parameterTypesString=Ljava%32lang%32String;&amp;parameter=xxxxx http 文本协议本身还是稍微有点复杂的，所以 netty 的实现考虑到通用性，必然不如我们自己解析来得快，具体的粘包过程就不叙述了，有点 hack 的倾向。 同理，response 也自己解析： 123456HTTP/1.1 200 OK\\r\\nConnection: keep-alive\\r\\nContent-Type: text/plain;charset=UTF-8\\r\\nContent-Length: 6\\r\\n\\r\\n123456 Qps 6600 到 6700 (去除对象) 继续丧心病狂，不考虑通用性，把之前所有的中间对象都省略，encode 和 decode 尽一切可能压缩到 handler 中去处理，这样的代码看起来非常难受，存在不少地方的 hardcoding。但效果是存在的，ygc 的次数降低了不少，全程使用 ByteBuf 和 byte[] 来进行数据交互。这个优化点同样存在存在 hack 倾向，不过多赘述。 Qps 6700 到 6850 (批量 flush，批量 decode) 事实上到了 6700 有时候还是需要看运气的，从群里的吐槽现象就可以发现，512 下的网路 io 非常抖，不清楚是机器的问题还是高并发下的固有现象，6700 的代码都能抖到 5000 分。所以 6700 升 6850 的过程比较曲折，而且很不稳定，提交 20 次一共就上过两次 6800+。 所做的优化是来自队友 @闪电侠的批量 flush 类，一次传输的字节数可以提升，使得网络 io 次数可以降低，原理可以简单理解为：netty 中 write 10 次，flush 1 次。一共实现了两个版本的批量 flush。一个版本是根据同一个 channel write 的次数积累，最终触发 flush；另一个版本是根据一次 eventLoop 结束才强制 flush。经过很多测试，由于环境抖动太厉害，这两者没测出多少差距。 123456789handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel ch) { ch.pipeline() .addLast(new SimpleDecoder()) .addLast(new BatchFlushHandler(false)) .addLast(new ConsumerAgentClientHandler()); }}); 批量 decode 的思想来自于蚂蚁金服的 rpc 框架 sofa-bolt 中提供的一个抽象类：AbstractBatchDecoder Netty 提供了一个方便的解码工具类 ByteToMessageDecoder ，如图上半部分所示，这个类具备 accumulate 批量解包能力，可以尽可能的从 socket 里读取字节，然后同步调用 decode 方法，解码出业务对象，并组成一个 List 。最后再循环遍历该 List ，依次提交到 ChannelPipeline 进行处理。此处我们做了一个细小的改动，如图下半部分所示，即将提交的内容从单个 command ，改为整个 List 一起提交，如此能减少 pipeline 的执行次数，同时提升吞吐量。这个模式在低并发场景，并没有什么优势，而在高并发场景下对提升吞吐量有不小的性能提升。 值得指出的一点：这个对于 dubbo mesh 复用 eventLoop 的特殊场景下的优化效果其实是存疑的，但我的最好成绩的确是使用了 AbstractBatchDecoder 之后跑出来的。我曾经单独将 ByteToMessageDecoder 和 AbstractBatchDecoder 拉出跑了一次分，的确是后者 qps 更高。 总结其实在 qps 6500 时，整体代码还是挺漂亮的，至少感觉能拿的出手给别人看。但最后为了性能，加上时间比较赶，不少地方都进行了 hardcoding，而实际能投入生产使用的代码必然要求通用性和扩展性，赛后有空会整理出两个分支：一个 highest-qps 追求性能，另一个分支保留下通用性。这次比赛从一个 netty 小白，最终学到了不少的知识点，还是收获很大的，最后感谢一下比赛中给过我指导的各位老哥。 最高 qps 分支：highest-qps 考虑通用性的分支（适合 netty 入门）：master https://code.aliyun.com/250577914/agent-demo.git 最后帮队友 @闪电侠推广下他的 netty 视频教程，比赛中两个比较难的优化点，都是由他进行的改造。imooc.com 搜索 Netty，可以获取 netty 源码分析视频。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/dubboMesh/"},{"title":"【千米网】从跨语言调用到 dubbo2.js","text":"dubbo2.js 是 千米网 贡献给 dubbo 社区的一款 nodejs dubbo 客户端，它提供了 nodejs 对原生 dubbo 协议的支持，使得 nodejs 和 java 这两种异构语言的 rpc 调用变得便捷，高效。 微服务跨语言调用微服务架构已成为目前互联网架构的趋势，关于微服务的讨论，几乎占据了各种技术大会的绝大多数版面。国内使用最多的服务治理框架非阿里开源的 dubbo 莫属，千米网也选择了 dubbo 作为微服务治理框架。另一方面，和大多数互联网公司一样，千米的开发语言是多样的，大多数后端业务由 java 支撑，而每个业务线有各自开发语言的选择权，便出现了 nodejs，python，go 多语言调用的问题。 跨语言调用是一个很大的话题，也是一个很有挑战的技术活，目前业界经常被提及的解决方案有如下几种，不妨拿出来老生常谈一番： spring cloud。spring cloud 提供了一整套微服务开发组件，它主要面向 java 开发，但由于其使用的协议是基于 restful 风格的 http 协议，这使得其天然具备跨语言能力，异构语言只需要提供 http 客户端，便可以实现跨语言调用。 service mesh。号称下一代微服务框架的 service mesh，其解决跨语言问题的核心在于 SideCar ，SideCar 在 service mesh 的发展过程中概念不断的迁移，但本质都是完成了一件事：处理服务间通信，负责实现请求的可靠传递。 motan。motan 是新浪微博开源的一款跨语言服务治理框架，在其早期版本中仅支持 motan-java，随着版本演进，在目前最新版本 (1.1.0) 中，提供了 motan-go，motan-php，motan-openresty 等跨语言特性。类似于 service mesh 中的 SideCar，motan 借助于 motan-go 作为 agent 完成协议的转发，并且依赖于定制协议：motan2，实现跨语言调用。 当我们再聊跨语言调用时我们在聊什么？纵观上述几个较为通用，成熟的解决方案，可以得出结论：解决跨语言调用的思路无非是两种： 寻找一个通用的协议 使用 agent 完成协议的适配 如果一个新型的团队面临技术选型，我认为上述的方案都可以纳入参考，可考虑到遗留系统的兼容性问题 旧系统的迁移成本 这也关键的选型因素。我们做出的第一个尝试，便是在 RPC 协议上下功夫。 通用协议的跨语言支持**springmvc 的美好时代 ** 在没有实现真正的跨语言调用之前，想要实现“跨语言”大多数方案是使用 http 协议做一层转换，最常见的手段莫过于借助 springmvc 提供的 controller/restController，间接调用 dubbo provider。这种方案的优势和劣势显而易见 优势是简单，是最通俗的解决方案。 劣势是使得调用链路变长，tcp 通信之上又多了一层 http 通信；开发体验差，为了将 rpc 接口暴露出去，需要额外编写一份 controller 层的代码。 ** 通用协议的支持 ** 事实上，大多数服务治理框架都支持多种协议，dubbo 框架除默认的 dubbo 协议之外，还有当当网扩展的 rest 协议和千米网扩展的 json-rpc 协议可供选择。这两者都是通用的跨语言协议。 rest 协议为满足 JAX-RS 2.0 标准规范，在开发过程中引入了 @Path，@POST，@GET 等注解，习惯于编写传统 rpc 接口的人可能不太习惯 rest 风格的 rpc 接口。一方面这样会影响开发体验，另一方面，独树一帜的接口风格使得它与其他协议不太兼容，旧接口的共生和迁移都无法实现。如果没有遗留系统，rest 协议无疑是跨语言方案最简易的实现，绝大多数语言支持 rest 协议。 和 rest 协议类似，json-rpc 的实现也是文本序列化 &amp;http 协议。dubbox 在 restful 接口上已经做出了尝试，但是 rest 架构和 dubbo 原有的 rpc 架构是有区别的，rest 架构需要对资源 (Resources) 进行定义， 需要用到 http 协议的基本操作 GET、POST、PUT、DELETE。在我们看来，restful 更合适互联网系统之间的调用，而 rpc 更适合一个系统内的调用。使用 json-rpc 协议使得旧接口得以兼顾，开发习惯仍旧保留，同时获得了跨语言的能力。 千米网在早期实践中采用了 json-rpc 作为 dubbo 的跨语言协议实现，并开源了基于 json-rpc 协议下的 python 客户端 dubbo-client-py 和 node 客户端 dubbo-node-client，使用 python 和 nodejs 的小伙伴可以借助于它们直接调用 dubbo-provider-java 提供的 rpc 服务。系统中大多数 java 服务之间的互相调用还是以 dubbo 协议为主，考虑到新旧协议的适配，在不影响原有服务的基础上，我们配置了双协议。 12&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt;&lt;dubbo:protocol name=&quot;jsonrpc&quot; port=&quot;8080&quot; /&gt; dubbo 协议主要支持 java 间的相互调用，适配老接口；json-rpc 协议主要支持异构语言的调用。 定制协议的跨语言支持微服务框架所谓的协议 (protocol) 可以简单理解为：报文格式和序列化方案。服务治理框架一般都提供了众多的协议配置项供使用者选择，除去上述两种通用协议，还存在一些定制化的协议，如 dubbo 框架的默认协议：dubbo 协议以及 motan 框架提供的跨语言协议：motan2。 motan2 协议的跨语言支持 motan2 协议被设计用来满足跨语言的需求主要体现在两个细节中—MetaData 和 motan-go。在最初的 motan 协议中，协议报文仅由 Header+Body 组成，这样导致 path，param，group 等存储在 Body 中的数据需要反序列得到，这对异构语言来说是很不友好的，所以在 motan2 中修改了协议的组成；weibo 开源了 motan-go ，motan-php ，motan-openresty , 并借助于 motan-go 充当了 agent 这一翻译官的角色，使用 simple 序列化方案来序列化协议报文的 Body 部分（simple 序列化是一种较弱的序列化方案）。 仔细揣摩下可以发现这么做和双协议的配置区别并不是大，只不过这里的 agent 是隐式存在的，与主服务共生。明显的区别在于 agent 方案中异构语言并不直接交互。 dubbo 协议的跨语言支持dubbo 协议设计之初只考虑到了常规的 rpc 调用场景，它并不是为跨语言而设计，但跨语言支持从来不是只有支持、不支持两种选择，而是要按难易程度来划分。是的，dubbo 协议的跨语言调用可能并不好做，但并非无法实现。千米网便实现了这一点，nodejs 构建的前端业务是异构语言的主战场，最终实现了 dubbo2.js，打通了 nodejs 和原生 dubbo 协议。作为本文第二部分的核心内容，重点介绍下我们使用 dubbo2.js 干了什么事。 Dubbo 协议报文格式 dubbo 协议报文消息头详解： magic：类似 java 字节码文件里的魔数，用来判断是不是 dubbo 协议的数据包。魔数是常量 0xdabb flag：标志位, 一共 8 个地址位。低四位用来表示消息体数据用的序列化工具的类型（默认 hessian），高四位中，第一位为 1 表示是 request 请求，第二位为 1 表示双向传输（即有返回 response），第三位为 1 表示是心跳 ping 事件。 status：状态位, 设置请求响应状态，dubbo 定义了一些响应的类型。具体类型见 com.alibaba.dubbo.remoting.exchange.Response invoke id：消息 id, long 类型。每一个请求的唯一识别 id（由于采用异步通讯的方式，用来把请求 request 和返回的 response 对应上） body length：消息体 body 长度, int 类型，即记录 Body Content 有多少个字节 body content：请求参数，响应参数的抽象序列化之后存储于此。 协议报文最终都会变成字节，使用 tcp 传输，任何语言只要支持网络模块，有类似 Socket 之类的封装，那么通信就不成问题。那，跨语言难在哪儿？以其他语言调用 java 来说，主要有两个难点： 异构语言如何表示 java 中的数据类型，特别是动态语言，可能不存在严格的数据类型 序列化方案如何做到跨语言 dubbo2.js 解决方案上面我们分析出了两个难点，dubbo2.js 解决这两个问题的关键依赖于两个类库：js-to-java ，hessian.js 。js-to-java 使得 nodejs 具备 java 对象的表达能力，而 hessian.js 提供了序列化能力。借助于 nodejs 的 socket ，复刻一套 dubbo 协议的报文格式，最终便实现了 nodejs 对 java-dubbo-provider 的调用。 dubbo2.js 快速入门为了让对 dubbo2.js 感兴趣的读者有一个直观的体验，本节呈现一个快速入门示例，让你体会到使用 dubbo2.js 调用 dubbo 服务是一件多么轻松的事。 创建 dubbo-java-provider 后端 dubbo 服务使用 java 来提供，这服务大多数的业务场景。首先定义服务接口： 123456public interface DemoProvider { String sayHello(String name); String echo() ; void test(); UserResponse getUserInfo(UserRequest request);} 其次，实现服务： 1234567891011121314151617181920212223242526public class DemoProviderImpl implements DemoProvider { public String sayHello(String name) { System.out.println(&quot;[&quot; + new SimpleDateFormat(&quot;HH:mm:ss&quot;).format(new Date()) + &quot;] Hello&quot; + name + &quot;, request from consumer:&quot; + RpcContext.getContext().getRemoteAddress()); return &quot;Hello&quot; + name + &quot;, response form provider:&quot; + RpcContext.getContext().getLocalAddress(); } @Override public String echo() { System.out.println(&quot;receive....&quot;); return &quot;pang&quot;; } @Override public void test() { System.out.println(&quot;test&quot;); } @Override public UserResponse getUserInfo(UserRequest request) { System.out.println(request); UserResponse response = new UserResponse(); response.setStatus(&quot;ok&quot;); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put(&quot;id&quot;, &quot;1&quot;); map.put(&quot;name&quot;, &quot;test&quot;); response.setInfo(map); return response; }} 暴露服务： 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd&quot;&gt; &lt;!-- 提供方应用信息，用于计算依赖关系 --&gt; &lt;dubbo:application name=&quot;demo-provider&quot;/&gt; &lt;dubbo:registry protocol=&quot;zookeeper&quot; address=&quot;localhost:2181&quot;/&gt; &lt;!-- 用 dubbo 协议在 20880 端口暴露服务 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot;/&gt; &lt;!-- 和本地 bean 一样实现服务 --&gt; &lt;bean id=&quot;demoProvider&quot; class=&quot;com.alibaba.dubbo.demo.provider.DemoProviderImpl&quot;/&gt; &lt;!-- 声明需要暴露的服务接口 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.dubbo.demo.DemoProvider&quot; ref=&quot;demoProvider&quot; version=&quot;1.0.0&quot;/&gt;&lt;/beans&gt; 我们完成了服务端的所有配置，启动启动类即可在本地注册一个 dubbo 服务。 1234567public class Provider { public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[]{&quot;META-INF/spring/dubbo-demo-provider.xml&quot;}); context.start(); System.in.read(); }} 实现 nodejs 的 dubbo 客户端 安装 dubbo2.js： 1npm install dubbo2.js --save 配置 dubboConfig.ts： 1234567891011121314151617181920212223242526272829303132333435363738import {Dubbo, java, TDubboCallResult} from 'dubbo2.js'const dubbo = new Dubbo({ application: {name: 'demo-provider'}, register: 'localhost:2181', dubboVersion: '2.0.0', interfaces: [ 'com.alibaba.dubbo.demo.DemoProvider', ],});interface IDemoService { sayHello(name: string): TDubboCallResult&lt;string&gt;;}export const demoService = dubbo.proxyService&lt;IDemoService&gt;({ dubboInterface: 'com.alibaba.dubbo.demo.DemoProvider', version: '1.0.0', methods: { sayHello(name: string) { return [java.String(name)]; }, echo(){}, test(){}, getUserInfo() { return [ java.combine('com.alibaba.dubbo.demo.UserRequest', { id: 1, name: 'nodejs', email: 'node@qianmi.com', }), ]; }, },}); 使用 typescript 可以带来更好的开发体验。 编写调用类 main.ts： 12345import {demoService} from './dubboConfig'demoService.sayHello('kirito').then(({res,err})=&gt;{ console.log(res)}); 执行调用 Debug 模式启动 nodejs 客户端： 1DEBUG=dubbo* ts-node main.ts 查看运行结果： 1Hello kirito, response form provider: 172.19.6.151:20880 congratulation！ dubbo2.js 特性 支持 zookeeper 注册中心 支持原生 dubbo 协议 支持服务直连 全链路跟踪 dubbo 接口自动生成 MORE DETAILS本文中的示例代码，提供在此处，https://github.com/lexburner/Dubbojs-Learning 。如果你对 dubbo 协议不慎了解，想要理解它的工作原理，项目中提供了一个子 moudle — java-socket-consumer，使用面向过程的思路实现了 java-socket-consumer，完成了原生 socket 发送 dubbo 协议报文，完成方法调用，并获取响应的全流程。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/dubbojs-in-qianmi/"},{"title":"EDAS 微服务治理解密","text":"前言2020 是多事的一年，新冠状性病毒的肆虐，其次是自己也生了一场病，希望随着天气暖和起来，一起都能变得更好。 前一段时间真的很忙，一直没有抽出时间，也没有什么思路给大家分享优质的文章，今天这篇文章很久之前就想写了，抓住这次假期的尾巴，总结一下我最近这一年的工作。 EDAS 是什么有很多读者问我在阿里是做什么的，我一般会回答：“我在阿里主要负责 Dubbo、HSF、SpringCloud Alibaba 这几个微服务框架研发和商业化相关的工作”，这样回答，如果对方是 Java 开发，一般都能知道个大概。熟悉阿里云的朋友会了解到阿里云上有很多的 PaaS 产品，我主要就是负责开发 “企业级分布式应用服务 EDAS” 这款产品；不熟悉阿里云的朋友，会对阿里云上一堆产品、一堆名词感到奇怪，什么 EDAS、MSE、ACM、OAM… 我开始接触 EDAS 时，也是吐槽了一下这次名词，但随着逐渐了解，也就觉得这些云产品像 Redis、DB、MQ 这些东西一样亲切了。这篇文章将围绕 EDAS 这款阿里云云产品进行介绍。 企业级分布式应用服务 EDAS（Enterprise Distributed Application Service）是一个应用托管和微服务管理的 PaaS 平台，提供应用开发、部署、监控、运维等全栈式解决方案，同时支持 Spring Cloud、Apache Dubbo（以下简称 Dubbo ）、HSF 等微服务运行环境，助力您的各类应用轻松上云。 可以从简介中提取出 EDAS 的核心能力：应用开发、应用部署、应用监控、应用运维。我这一篇文章自然是没办法介绍整个 PaaS 平台的能力，而且说实话，应用部署和应用监控等等能力也都是我同事们开发的，我真的一无所知，想介绍都难，所以在设计到细节时，我会主要介绍 EDAS 和微服务治理相关的那部分能力。 为什么用 EDAS 上图中，我将微服务框架的不同部署方式和云计算进行了绑定，以方便下面的介绍。 “种子”用户最容易理解了，例如使用开源的 Apache Dubbo 作为微服务框架，部署在自建机房的机器上，通常以中小公司为主，会有专门的运维搭建操作系统，设计网络拓扑，这也是云计算之前大多数公司的玩法。 但大家都知道，在现实生活中，“种子”变成“小麦”一般是农民伯伯的活，应该很少有人会因为想吃一个馒头，而去播撒一波种子的。例如很多互联网创业公司肯定接受不了这种玩法，想要尽快落地产品，第一步一定不是搞一堆机器，于是 IaaS 成了他们的救星，很多云计算公司提供了云服务器（ECS），解决了基础硬件问题，剩下的东西研发工程师都能搞定，所以“我有一个很好的 idea，就差一个程序员了”就变得流行了起来。在 IaaS 阶段，可以用 ECS 部署数据库，部署 Redis，部署 Dubbo 节点，机器出了问题可以提工单给云厂商，大大减少了运维成本，可以让研发人员更加专注在业务开发上。 很多 Dubbo 用户都是在“种子”和“小麦”阶段，还有人跟我说：我们公司在腾讯云上使用 Dubbo。老铁没毛病，开源框架没有强制跟任何云厂商绑定，Pivotal 的 SpringCloud 和 Alibaba 主导的 Dubbo 都是如此。可是，那我又有什么理由去进阶为“面粉”阶段的用户呢？我从 Dubbo 微信交流群的一些讨论去尝试回答这个问题： 请教大家一个问题，开源的 Dubbo 控制台有一个 xx 的问题，大家有遇到吗？ 请教一下大家，Dubbo 的限流你们是怎么做的？ 有没有人做过 Dubbo 的全链路跟踪的？ Dubbo 的灰度发布有人在生产实践过吗？ Dubbo 的分布式事务怎么做？ Dubbo 的网关怎么做？ … 以 Dubbo 为例，大家可以看到一些端倪，开源框架也会存在一些问题。例如开源框架优先考虑的是功能的普适性，对例如灰度发布这种定制化的需求支持较少，且功能越高端，越难以支持。再例如 Dubbo 的全链路跟踪，基本上是每次 Dubbo Meetup 之后收集到的高频问题。有一些公司有基础架构团队，虽然使用的是开源 Dubbo，但他们有能力去进行改造，例如当当、考拉都拉了自己的分支在进行迭代，但我相信，他们自己的扩展也一定是优先适配各自公司的场景。还有一类问题，开源产品也会存在 bug，有时候自己定位出了 bug 提给社区，还得等其他人 review &amp; merge request，必然会有时间成本，这也是很多公司自己拉分支自己维护的重要原因。说了这么多，其实都是在说“小麦”的缺点，“面粉”阶段又是如何解决这些问题的呢？ 我在图中将 EDAS 这款阿里云的产品定位在了“面粉”阶段，主要原因便是： IaaS 层解决的问题，它都解决了，甚至做的更好。EDAS 甚至不需要用户去购买机器，可以进行诸如：弹性伸缩，限流降级，优雅上下线等应用生命周期的管理 在微服务侧，它支持 Dubbo、SpringCloud、HSF 等主流的微服务框架，并且对微服务能力进行了增强。有很多商业化的特性，例如：白屏化的服务治理界面、分布式链路追踪、灰度发布、离群摘除、服务限流、注册中心的高可用…等等，并且随着后续的迭代，会继续支持分布式事务、网关等特性 背靠阿里云，有专门的团队保障微服务的稳定性，让专业的人做专业的事 我认为”面粉“是一个恰到好处的阶段，你可以烹饪成面包、面条、馒头，却又不会束缚你的产品形态。 EDAS 支持微服务的发展史在阿里云的角度，如何吸引用户使用阿里云的服务呢？阿里云的服务实在太多了，还是来看微服务这个点。在 N 年前，EDAS 刚刚公测时，EDAS 主打的微服务框架是 HSF（High Speed Framework），熟悉它的朋友知道 HSF 是阿里集团内部使用的自研微服务框架，在历年双 11 支持整个阿里各个产品的平稳运行，有了双 11 这样量级的考验和阿里的背书，EDAS 支持 HSF 也就成了理所当然的事。这时候玩法是这样的： 如果是 Dubbo 既存用户想要上云使用 EDAS，就必须要改用 HSF 框架改写自己的业务代码，这个迁移成本是很高的。那如果是全新的业务使用 HSF，是不是就没有这个问题了呢？也不尽然。站在开发的角度，开源 Dubbo 的社区非常活跃，文档详细，遇到问题时，搜索引擎也基本能提供解决方案。但 HSF 不同，它是一款闭源的框架，即使是阿里内部用户，可能对其也是一知半解，遇到问题，只能借助于工单、答疑群解决，看不到源码是原罪。尽管 HSF 非常稳定，但用户把自己的代码托管在一个黑匣子中，多多少少会有所顾忌。 很多云平台都有类似的问题，业务上云往往意味着迁移技术栈，这个成本可想而知。 随着阿里重启了 Dubbo 的开源，EDAS 开始支持了开源 Dubbo，这一下子解决了很多上面提到的问题。对于那些处于”种子“、”小麦“阶段的 Dubbo 用户而言，使用 EDAS 不需要修改任何一行代码，即可获得 EDAS 承诺的诸多增强能力。 紧接着是 SpringCloud Alibaba 的开源，EDAS 也提供了 100% 的兼容。至此，Dubbo 和 SpringCloud 应用上云，EDAS 都能够 cover。 开源与商业化与云原生从刚刚的发展史可以发现云厂商的一个玩法，Dubbo、SpringCloud Alibaba 最早都是由阿里主导开源的，在商业化云产品上都得到了优先的支持，不仅阿里这一家这么玩，华为开源了 ServiceComb，蚂蚁开源了 Sofa Stack，腾讯开源了 Tars，这几家都有各自的云平台，基本都会让用户迁移至自家的微服务框架。开源一方面是培养了用户习惯，积累影响力，一方面也是在为商业化铺路。 对接开源 SDK 逐渐成为了云厂商们的共识，只不过这里的开源 SDK 现如今没有统一，各家都有各自的玩法。不过这已经是一个很大的进步了，因为开源不会绑架用户，哪一天这家云厂商用的不爽了，随时迁走。相比使用云厂商提供的 SDK 来说，使用开源 SDK 这个理念不知道先进到哪里去了。 说道这里，不免会有人提出云原生的概念，开源 SDK 实在太多了，如果只有一套大家都遵守的规范就好了，云原生大概就是在做这么一件事。俗话说的好，三等码农写代码，二等码农写框架，一等码农定规范。但目前来说，还没有一统江湖的”云原生开源 SDK“，但已经有人再按照这样的思路再推进了，有兴趣的可以去了解下阿里提出的 OAM。 EDAS 微服务治理的难点前面我们介绍到 EDAS 不仅仅是作为一个跑微服务框架的运行时容器，还为微服务框架提供了很多增强能力。科普完发展史之后，下面对微服务增强的细节进行介绍，但在此之前，为了不让读者知其然而不知其所以然，我先梳理下 EDAS 微服务治理的难点。 难点 1EDAS 支持的微服务框架种类多，目前支持三种微服务框架：Dubbo、SpringCloud、HSF 难点 2微服务框架版本多： Dubbo 支持 2.5.x，2.6.x，2.7.x SpringCloud 支持 D 以上的版本 难点 3例如传统的服务查询，需要访问注册中心，但用户使用的注册中心种类多 Zookeeper Nacos Eureka 难点 4部署形态多，EDAS 支持两种部署形态 ECS Jar 包部署 K8s 镜像部署 难点 5需要考虑存量用户的迁移问题和改造成本 总结EDAS 微服务增强的实现方式必须要考虑以上众多因素，在不得不进行 trade off 时，也应该尽可能解决较多的痛点，避免用户进行过多的改造。 EDAS 微服务增强实现用户部署在 EDAS 中的代码使用的是开源的 SDK，EDAS 又承诺用户不需要改动代码，那是如何做到微服务能力增强的呢？满足这个要求的正是 JDK 提供的 Instrument 机制，熟悉 pinpoint 和 skywalking 等分布式链路追踪框架的读者应该对这个技术不会感到陌生。很久之前我曾经写过一篇文章对它进行过介绍：JAVA 拾遗 – Instrument 机制。 借助一个 Demo，通过 Instrument 来实现无侵入式的 AOP。 MicroServiceTransformer 12345678910public class MicroServiceTransformer implements ClassFileTransformer { @Override public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) throws IllegalClassFormatException { if (&quot;org/apache/dubbo/config/ReferenceConfig&quot;.equals(className)) { System.out.println(&quot;microservice improve&quot;); } return null; }} 对类进行装配的第一步是编写一个 GreetingTransformer 类，其继承自：java.lang.instrument.ClassFileTransformer。 MicroServiceAgent 除了上述的 Transformer，我们还需要有一个容器去加载它。 12345public class MicroService { public static void premain(String options, Instrumentation ins) { ins.addTransformer(new MicroServiceTransformer()); }} MicroServiceAgent 便是最终用户的 Jar 运行时挂载的 agent。具体如何加载 agent ，可以参考上述的文章链接，有完整的 demo 和教程。这里主要是为了引出装配机制。 EDAS 的微服务增强逻辑便如同 AOP 一样，利用无侵入式的挂载，以达到增强的逻辑，这个过程需要 agent 对不同版本进行逐一的适配，从而实现服务查询、灰度发布、分布式链路跟踪、离群摘除等能力。 总结本文其实也是一个主观视角，从一个对 EDAS 一无所知的角度，侧重于 EDAS 的微服务能力介绍了一遍 EDAS。EDAS 本身是一个商业化的云产品，但结合开源我们可以从它的演进历史看到一些软件架构演进的规律，这对于我们把握今后的技术发展趋势也有一定的指导意义。 别的也不多了，我们 Dubbo、SpringCloud 商业化团队招人，对微服务、中间件感兴趣的同学欢迎私聊我，一起干有意思的事。私聊或者投简历~ 微信号：xiayimiaoshenghua","link":"/edas-microservice/"},{"title":"简单了解 RPC 实现原理","text":"时下很多企业应用更新换代到分布式，一篇文章了解什么是 RPC。原作者梁飞，在此记录下他非常简洁的 rpc 实现思路。 核心框架类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (&quot;Confidential * Information&quot;). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.framework;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.net.ServerSocket;import java.net.Socket;/** * RpcFramework * * @author william.liangf */public class RpcFramework { /** * 暴露服务 * * @param service 服务实现 * @param port 服务端口 * @throws Exception */ public static void export(final Object service, int port) throws Exception { if (service == null) throw new IllegalArgumentException(&quot;service instance == null&quot;); if (port &lt;= 0 || port &gt; 65535) throw new IllegalArgumentException(&quot;Invalid port&quot; + port); System.out.println(&quot;Export service&quot; + service.getClass().getName()+ &quot;on port&quot; + port); ServerSocket server = new ServerSocket(port); for(;;) { try { final Socket socket = server.accept(); new Thread(new Runnable() { @Override public void run() { try { try { ObjectInputStream input = new ObjectInputStream(socket.getInputStream()); try { String methodName = input.readUTF(); Class&lt;?&gt;[] parameterTypes = (Class&lt;?&gt;[])input.readObject(); Object[] arguments = (Object[])input.readObject(); ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream()); try { Method method = service.getClass().getMethod(methodName, parameterTypes); Object result = method.invoke(service, arguments); output.writeObject(result); } catch (Throwable t) { output.writeObject(t); } finally { output.close(); } } finally { input.close(); } } finally { socket.close(); } } catch (Exception e) { e.printStackTrace(); } } }).start(); } catch (Exception e) { e.printStackTrace(); } } } /** * 引用服务 * * @param &lt;T&gt; 接口泛型 * @param interfaceClass 接口类型 * @param host 服务器主机名 * @param port 服务器端口 * @return 远程服务 * @throws Exception */ @SuppressWarnings(&quot;unchecked&quot;) public static &lt;T&gt; T refer(final Class&lt;T&gt; interfaceClass, final String host, final int port) throws Exception { if (interfaceClass == null) throw new IllegalArgumentException(&quot;Interface class == null&quot;); if (! interfaceClass.isInterface()) throw new IllegalArgumentException(&quot;The&quot; + interfaceClass.getName() + &quot;must be interface class!&quot;); if (host == null || host.length() == 0) throw new IllegalArgumentException(&quot;Host == null!&quot;); if (port &lt;= 0 || port &gt; 65535) throw new IllegalArgumentException(&quot;Invalid port&quot; + port); System.out.println(&quot;Get remote service&quot; + interfaceClass.getName() + &quot;from server&quot; + host + &quot;:&quot; + port); return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class&lt;?&gt;[] {interfaceClass}, new InvocationHandler() { public Object invoke(Object proxy, Method method, Object[] arguments) throws Throwable { Socket socket = new Socket(host, port); try { ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream()); try { output.writeUTF(method.getName()); output.writeObject(method.getParameterTypes()); output.writeObject(arguments); ObjectInputStream input = new ObjectInputStream(socket.getInputStream()); try { Object result = input.readObject(); if (result instanceof Throwable) { throw (Throwable) result; } return result; } finally { input.close(); } } finally { output.close(); } } finally { socket.close(); } } }); }} 定义服务接口12345678910111213141516171819/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (&quot;Confidential * Information&quot;). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;/** * HelloService * * @author william.liangf */public interface HelloService { String hello(String name);} 实现服务123456789101112131415161718192021/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (&quot;Confidential * Information&quot;). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;/** * HelloServiceImpl * * @author william.liangf */public class HelloServiceImpl implements HelloService { public String hello(String name) { return &quot;Hello&quot; + name; }} 暴露服务123456789101112131415161718192021222324/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (&quot;Confidential * Information&quot;). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;import com.alibaba.study.rpc.framework.RpcFramework;/** * RpcProvider * * @author william.liangf */public class RpcProvider { public static void main(String[] args) throws Exception { HelloService service = new HelloServiceImpl(); RpcFramework.export(service, 1234); }} 引用服务12345678910111213141516171819202122232425262728/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (&quot;Confidential * Information&quot;). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;import com.alibaba.study.rpc.framework.RpcFramework;/** * RpcConsumer * * @author william.liangf */public class RpcConsumer { public static void main(String[] args) throws Exception { HelloService service = RpcFramework.refer(HelloService.class, &quot;127.0.0.1&quot;, 1234); for (int i = 0; i &lt; Integer.MAX_VALUE; i ++) { String hello = service.hello(&quot;World&quot; + i); System.out.println(hello); Thread.sleep(1000); } } } 总结这个简单的例子的实现思路是使用阻塞的 socket IO 流来进行 server 和 client 的通信，也就是 rpc 应用中服务提供方和服务消费方。并且是端对端的，用端口号来直接进行通信。方法的远程调用使用的是 jdk 的动态代理，参数的序列化也是使用的最简单的 objectStream。 真实的 rpc 框架会对上面的实现方式进行替换，采用更快更稳定，更高可用易扩展，更适宜分布式场景的中间件，技术来替换。例如使用 netty 的 nio 特性达到非阻塞的通信，使用 zookeeper 统一管理服务注册与发现，解决了端对端不灵活的劣势。代理方式有 cglib 字节码技术。序列化方式有 hession2，fastjson 等等。不过梁飞大大的博客使用原生的 jdk api 就展现给各位读者一个生动形象的 rpc demo，实在是强。rpc 框架解决的不仅仅是技术层面的实现，还考虑到了 rpc 调用中的诸多问题，重试机制，超时配置… 这些就需要去了解成熟的 rpc 框架是如果考虑这些问题的了。 推荐一个轻量级的 rpc 框架：motan。weibo 团队在 github 开源的一个 rpc 框架，有相应的文档，用起来感觉比 dubbo 要轻量级，易上手。","link":"/easy-know-rpc/"},{"title":"EDAS 让 Spring Cloud Gateway 生产可用的二三策","text":"Spring Cloud Gateway 是 Spring Cloud 微服务生态下的网关组件，一直以来备受 Java 社区的用户关注，很多企业选择使用其作为微服务网关或者业务网关。在阿里云上，也不乏有很多网关类型的产品供用户使用，例如 API Gateway 和 MSE Higress，使用 PaaS 化的方式提供网关能力，用户不再需要关注网关的实现，直接获得开箱即用的能力。在从前，用户只能选择自建 Spring Cloud Gateway，或者购买云产品，而今天介绍的 EDAS 增强 Spring Cloud Gateway 的新姿势，给用户提供了一个新的选择。 让 Spring Cloud Gateway 生产可用开源 Spring Cloud Gateway 存在一些让企业级用户担忧的因素，包括内存泄漏问题，以及路由设计问题，EDAS 根据云服务总线 CSB 多年沉淀下来的 Spring Cloud Gateway 使用经验，对诸多已经存在的问题进行了治理，对诸多的风险因素也进行了规避，彻底打消用户使用 Spring Cloud Gateway 技术侧的顾虑。 内存泄漏问题，该问题来自于 CSB 的生产实践，Spring Cloud Gateway 底层依赖 netty 进行 IO 通信，熟悉 netty 的人应当知道其有一个读写缓冲的设计，如果通信内容较小，一般会命中 chunked buffer，而通信内容较大时，例如文件上传，则会触发内存的新分配，而 Spring Cloud Gateway 在对接 netty 时存在逻辑缺陷，会导致新分配的池化内存无法完全回收，导致堆外内存泄漏。并且这块堆外内存时 netty 使用 unsafe 自行分配的，通过常规的 JVM 工具还无法观测，非常隐蔽。 EDAS 建议为 Spring Cloud Gateway 应用增加启动参数 -Dio.netty.allocator.type=unpooled，使得请求未命中 chunked buffer 时，分配的临时内存不进行池化，规避内存泄漏问题。-Dio.netty.allocator.type=unpooled 不会导致性能下降，只有大报文才会触发该内存的分配，而网关的最佳实践应该是不允许文件上传这类需求，加上该参数是为了应对非主流场景的一个兜底行为。 开源 Spring Cloud Gateway 并未提供路由配置校验能力，当路由配置出错时，可能会带来灾难性的后果，例如在配置路由时，误将 POST 写成了 PEST： predicates: Method=PEST，可能会导致网关中所有路由失效，爆炸半径极大。 EDAS 建议为 Spring Cloud Gateway 应用配置 spring.cloud.gateway.fail-on-route-definition-error: false ，降低爆炸半径。通过 EDAS 创建的路由，将会经过校验，确保路由的格式正确，提前规避问题。 以上只是 EDAS 增强 Spring Cloud Gateway 方案的部分案例，EDAS 围绕性能、安全、稳定性等方面，全面为用户的网关保驾护航，让用户彻底回归到业务本身。 围绕让 Spring Cloud Gateway 生产可用这个基本话题，让用户在云上放心的使用 Spring Cloud Gateway，EDAS 推出了一个新的功能，使用无侵入式的方式增强 Spring Cloud Gateway。 功能介绍众所周知，在 EDAS 中部署的 Java 应用都会挂载一个 Java Agent，通过 Java Agent 技术，EDAS 提供了丰富的微服务治理以及可观测性的能力，此次介绍的 Spring Cloud Gateway 增强能力，同样通过该 Java Agent 实现。 EDAS 增强 Spring Cloud Gateway 带来的最直观的变化便是提供了一个白屏控制台，方便用户进行操作，同时提供了诸多的增强能力： 动态配置 自定义插件 路由调试 限流降级 可观测性增强 为了方便用户有一个直观的了解，本文以一个快速入门开始进行介绍 部署 Spring Cloud Gateway 用户可以将已有的 Spring Cloud Gateway 打包成 jar 包或者镜像，在 EDAS 中进行部署，或者也可以使用 EDAS 提供 Demo 部署包进行部署。如上图所示，EDAS 新增支持了 Spring Cloud Gateway 应用的 Demo 部署包，在该部署包中，事先配置好了一个 Nacos 注册中心，会自动连接到当前部署的微服务空间，并未配置任何路由，因为接下来将会进行动态路由配置的演示，所以无需事先在配置文件中配置。整个部署过程和部署一个普通的微服务应用没有任何差异。 创建路由并测试 EDAS 会识别到 Spring Cloud Gateway 应用的特征，并在菜单栏中动态增加应用网关的菜单。在快速入门中，示例创建了两条路由，分别是 http:// 格式的直接请求场景和 lb:// 格式的服务发现场景。为方便测试，可以在应用总览中为该网关应用配置一个公网的 SLB，通过 curl 请求测试： 1234567891011121314151617181920212223242526272829~ curl 121.xx.xx.xx/httpbin/get{ &quot;args&quot;: {}, &quot;headers&quot;: { &quot;Aaa&quot;: &quot;ccc&quot;, &quot;Accept&quot;: &quot;*/*&quot;, &quot;Content-Length&quot;: &quot;0&quot;, &quot;Eagleeye-Ip&quot;: &quot;192.168.2.1&quot;, &quot;Eagleeye-Pappname&quot;: &quot;5ae05114-bc80-4a32-9048-209b3a93d723&quot;, &quot;Eagleeye-Prpc&quot;: &quot;/httpbin/get&quot;, &quot;Eagleeye-Pspanid&quot;: &quot;-7254661991881594415&quot;, &quot;Eagleeye-Root-App&quot;: &quot;5ae05114-bc80-4a32-9048-209b3a93d723&quot;, &quot;Eagleeye-Rpcid&quot;: &quot;0.2.1&quot;, &quot;Eagleeye-Sampled&quot;: &quot;s0&quot;, &quot;Eagleeye-Spanid&quot;: &quot;-1207596966212570593&quot;, &quot;Eagleeye-Traceid&quot;: &quot;eac0a8020116974429411421021d0001&quot;, &quot;Eagleeye-Userdata&quot;: &quot;__microservice_match_result__=[]&quot;, &quot;Forwarded&quot;: &quot;proto=http;host=121.xx.xx.xx;for=\\&quot;140.xx.xx.xx\\&quot;&quot;, &quot;Gfs.Scg.Ip&quot;: &quot;192.168.2.1&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Name&quot;: &quot;kirito,kirito&quot;, &quot;User-Agent&quot;: &quot;curl/7.64.1&quot;, &quot;X-Amzn-Trace-Id&quot;: &quot;Root=1-652cec7d-50f852f622c546f20f8997fe&quot;, &quot;X-Forwarded-Host&quot;: &quot;121.xx.xx.xx&quot;, &quot;X-Forwarded-Prefix&quot;: &quot;/httpbin&quot; }, &quot;origin&quot;: &quot;121.xx.xx.xx, 47.xx.xx.xx&quot;, &quot;url&quot;: &quot;http://121.xx.xx.xx/get&quot;} 网关成功转发了请求，至此路由测试完毕。 方案优势以下情况，均可以考虑使用 EDAS Spring Cloud Gateway 增强方案 已经在使用 Spring Cloud Gateway 网关存在较强的业务定制需求，例如企业级用户/权限体系对接 Java 技术栈主导，希望对网关组件有自主掌控力 网关后端服务使用 Spring Cloud 技术栈 EDAS 提供的 Spring Cloud Gateway 增强方案解耦了网关的业务属性和中间件属性，用户可以专注于在 Spring Cloud Gateway 开源的基础上进行二次开发，注入复杂的业务逻辑，而将网关的功能（动态配置、限流降级等）、安全、性能等中间件属性交给 EDAS。 对于已经在使用 Spring Cloud Gateway 的用户，当 Spring Cloud Gateway 应用被 EDAS 托管后，无需改动任何代码，即可以在保留原本扩展点的同时，获得诸多的增强能力。今后依旧可以在应用基础上继续进行二次开发，使网关应用获得和业务应用一样的开发体验。传统的 PaaS 化网关在自定义扩展的支持上，一般要求用户去适配网关自身的规范，使用不熟悉的语言或者插件机制，存在一定的学习成本和风险。 如果用户目前没有网关，考虑新增一个网关，正在进行网关方案的调研，则需要针对自身的业务场景进行充分的考虑。如果符合 Java 技术栈、Spring Cloud 微服务体系等关键词，那么同样可以优先考虑该方案。 相比较阿里云上同类型的 PaaS 网关产品，他们同样有各自的使用场景，例如 API Gateway 可以实现精细化的 API 管理，MSE Higress 可以作为三合一的网关，也可以作为 K8s Ingress 的实现。可以根据自身需求来决定网关方案，EDAS 增强 Spring Cloud Gateway 的方案为用户新添了一个选型。 相比开源 Spring Cloud Gateway，EDAS Agent 增强方案在 100% 兼容开源功能的基础上，进行了以下能力的增强。 能力增强动态配置能力 EDAS 为 Spring Cloud Gateway 的路由（Route）和插件（Gateway Filter）提供了动态配置能力，以白屏化的形式呈现，方便用户进行配置。 如果 Spring Cloud Gateway 项目中已经配置了路由，例如配置在 application.yml 中，同时又在 EDAS 控制台中进行了配置，这些路由最终会合并成一份路由集合。需要注意的是 EDAS 控制台中只会展示由 EDAS 发布的路由配置，不会展示 application.yml 中的配置，但实际上这两份路由都会生效。使用该方案时，建议用户通过配置导入&amp;导出的方式将配置迁移至 EDAS 控制台，方便统一管理。 配置导入&amp;导出 路由和全局插件均支持通过 Yaml 创建，Yaml 的格式遵循开源 Spring Cloud Gateway 的 schema 规范，以下是两个配置示例： 12345678910111213141516171819202122232425262728293031spring: cloud: gateway: routes: # 1. 利用域名进行路由匹配，且后端是固定 HTTP URL 的场景 - id: r-demo predicates: # 只有域名为 demo.com 的请求才会匹配上该路由 - Host=demo.com filters: # 该插件在转发请求时，在请求头中添加 Header 键值对 - AddRequestHeader=a,b # uri 里填写后端 HTTP URL uri: http://demo.com # order 代表路由的优先级，值越小，优先级越高 order: 1000 # 2. 利用路径前缀进行路由匹配，且后端是微服务的场景 - id: r-demo-2 predicates: # 请求路径以 /demo-2 开头，才会匹配上该路由 - Path=/demo-2/** filters: # 该插件确保请求在转发至后端服务时，会移除掉 /demo-2 的前缀 - StripPrefix=1 # 后端为微服务时，uri应该以 lb:// 开头，并填写服务名 uri: lb://service-provider # 可以为路由添加元数据，以在插件中使用 metadata: ccc: ddd eee: 10 order: 1000 路由 r-demo 是一个通过域名进行路由的配置示例，后端服务对应到了一个直接请求的地址，路由 r-demo-2 是一个通过路径前缀匹配路由的配置示例，配置了 StripPrefix 插件，使得在转发到后端时移除用于匹配的前缀，后端服务则是以 lb 开头，表明是服务发现发现场景。 同时也支持批量查看路由的 Yaml 定义： Yaml 创建和查看的设计，是为了尽可能地对齐到开源 Spring Cloud Gateway 的规范，如果用户是 Spring Cloud Gateway 开源的资深用户，这会保留用户原有的使用体验。 同时，借助于该功能，可以实现多套网关的配置同步，例如一批路由在测试环境验证完毕，需要迁移至生产网关，只需要将测试环境的路由选中导出，再导入至生产网关即可。 也可以借助于该功能，将用户本地配置文件中的路由导入至 EDAS，完全由 EDAS 管理，EDAS 提供的动态配置能力使用起来会更加方便。 插件交互Spring Cloud Gateway 提供了非常丰富的插件（GatewayFilter）机制，允许配置在路由和全局级别，EDAS 在此基础上提升了插件的易用性。 Spring Cloud Gateway 原生的插件配置采用的是精简配置的方式，对于一些不太常用的插件，很难直观地去判断如何添加参数，在 EDAS 中则没有这样的烦恼，EDAS 会将插件的解释、参数是否必填、参数含义、参数个数进行拆解，避免误用。 插件参考： 插件名 描述 AddRequestHeader 添加请求头。 AddRequestParameter 添加请求参数。 AddResponseHeader 添加响应头。 SetRequestHeader 修改请求头。 SetResponseHeader 修改响应头。 SetStatus 修改响应码。 SetPath 修改请求路径。 MapRequestHeader 请求头参数映射。 PrefixPath 为请求路径添加前缀。 StripPrefix 删除请求路径前缀。 RemoveRequestHeader 删除请求头。 RemoveResponseHeader 删除响应头。 RemoveRequestParameter 删除请求参数。 DedupeResponseHeader 删除响应的重复头。 PreserveHostHeader 保留请求的域名属性。 RedirectTo 重定向。 RequestSize 请求大小限制。 RequestHeaderSize 请求头大小限制。 RewritePath 重写请求路径。 RewriteResponseHeader 重写响应头。 需要注意的一点是，这些插件是允许重复添加的，但部分插件只建议配置一次，例如 StripPrefix、SetPath 等等，否则会出现未知的表现。 快速测试 针对于 Spring Cloud Gateway 应用，EDAS 会列举出控制台中的路由路径，供用户进行路由测试，借助于快速测试的能力，可以在路由配置完毕后快速进行验证，从而判断配置是否正确。 可观测开源 Spring Cloud Gateway 并未配备网关应有的 accessLog，EDAS 补齐了这部分必备能力，任何经过网关的请求，都会打印在 /home/admin/.opt/ArmsAgent/logs/scg-access.log 路径下，用户可以在应用详情的日志中心中进行查看： 用户可以选择将这份数据采集至 SLS 或者自定义的日志中心，用作监控。 access.log日志格式说明 编号 说明 字段名 内容示例 1 日志记录时间 dateTime 2023-06-19 16:06:53 966 2 请求 trace id traceId 0ab32f9f15293956139457176d485a 3 客户端IP clientIp 127.0.0.1 4 请求方法 method GET 5 请求路径 path /httpbin/get 6 请求数据大小 requestSize 122 7 请求开始时间 startTime 1667381534546 8 匹配上的路由ID routeId sc-A 路由对应的URI routeUri http://httpbin.org:80 、lb://sc-A 9 后端调用开始时间 backendStartTime 1667381534546 10 后端请求方法 backendMethod GET 11 后端请求URL backendUrl httpbin.org/get 13 后端请求体大小 backendRequestSize 122 14 后端响应码 backendStatusCode 200 15 后端响应体大小 backendResponseSize 433 16 后端调用结束时间 backendEndTIme 1667381534560 17 后端调用耗时 backendRt 14 18 请求响应码 statusCode 200 19 请求响应体大小 responseSize 433 20 调用是否成功 status SUCCESS/FAILURE 21 错误信息 errorMsg 成功时：-失败时打印具体信息，例如：Service Unavailable 22 请求结束时间 endTime 1667381534565 23 请求总耗时 rt 19 后续规划EDAS 增强 Spring Cloud Gateway 方案在后续还会提供更多的能力，丰富网关生态，目前规划中的能力包括： 丰富插件生态，新增鉴权、限流降级、跨域插件 跨微服务空间访问微服务 支持 Metrics 指标，提供网关资源监控和业务监控 单机 QOS 排查能力 欢迎用户使用反馈，与我们进行交流，钉钉交流群：23197114。 如需体验，可参考用户文档：https://help.aliyun.com/zh/edas/user-guide/spring-cloud-gateway-application-routing","link":"/edas-scg-agent/"},{"title":"浅析 Spring 中的事件驱动机制","text":"今天来简单地聊聊事件驱动，其实写这篇文章挺令我挺苦恼的，因为事件驱动这个名词，我没有找到很好的定性解释，担心自己的表述有误，而说到事件驱动可能立刻联想到如此众多的概念：观察者模式，发布订阅模式，消息队列 MQ，消息驱动，事件，EventSourcing… 为了不产生歧义，笔者把自己所了解的这些模棱两可的概念都列了出来，再开始今天的分享。 在设计模式中，观察者模式可以算得上是一个非常经典的行为型设计模式，猫叫了，主人醒了，老鼠跑了，这一经典的例子，是事件驱动模型在设计层面的体现。 另一模式，发布订阅模式往往被人们等同于观察者模式，但我的理解是两者唯一区别，是发布订阅模式需要有一个调度中心，而观察者模式不需要，例如观察者的列表可以直接由被观察者维护。不过两者即使被混用，互相替代，通常不影响表达。 MQ，中间件级别的消息队列（e.g. ActiveMQ,RabbitMQ），可以认为是发布订阅模式的一个具体体现。事件驱动 -&gt; 发布订阅 -&gt;MQ，从抽象到具体。 java 和 spring 中都拥有 Event 的抽象，分别代表了语言级别和三方框架级别对事件的支持。 EventSourcing 这个概念就要关联到领域驱动设计，DDD 对事件驱动也是非常地青睐，领域对象的状态完全是由事件驱动来控制，由其衍生出了 CQRS 架构，具体实现框架有 AxonFramework。 Nginx 可以作为高性能的应用服务器（e.g. openResty），以及 Nodejs 事件驱动的特性，这些也是都是事件驱动的体现。 本文涵盖的内容主要是前面 4 点。 Spring 对 Event 的支持Spring 的文档对 Event 的支持翻译之后描述如下： ApplicationContext 通过 ApplicationEvent 类和 ApplicationListener 接口进行事件处理。 如果将实现 ApplicationListener 接口的 bean 注入到上下文中，则每次使用 ApplicationContext 发布 ApplicationEvent 时，都会通知该 bean。 本质上，这是标准的观察者设计模式。 而在 spring4.2 之后，提供了注解式的支持，我们可以使用任意的 java 对象配合注解达到同样的效果，首先来看看不适用注解如何在 Spring 中使用事件驱动机制。 定义业务需求：用户注册后，系统需要给用户发送邮件告知用户注册成功，需要给用户初始化积分；隐含的设计需求，用户注册后，后续需求可能会添加其他操作，如再发送一条短信等等，希望程序具有扩展性，以及符合开闭原则。 如果不使用事件驱动，代码可能会像这样子： 1234567891011121314151617public class UserService { @Autowired EmailService emailService; @Autowired ScoreService scoreService; @Autowired OtherService otherService; public void register(String name) { System.out.println(&quot;用户：&quot; + name + &quot;已注册！&quot;); emailService.sendEmail(name); scoreService.initScore(name); otherService.execute(name); } } 要说有什么毛病，其实也不算有，因为可能大多数人在开发中都会这么写，喜欢写同步代码。但这么写，实际上并不是特别的符合隐含的设计需求，假设增加更多的注册项 service，我们需要修改 register 的方法，并且让 UserService 注入对应的 Service。而实际上，register 并不关心这些“额外”的操作，如何将这些多余的代码抽取出去呢？便可以使用 Spring 提供的 Event 机制。 定义用户注册事件1234567public class UserRegisterEvent extends ApplicationEvent{ public UserRegisterEvent(String name) { //name 即 source super(name); }} ApplicationEvent 是由 Spring 提供的所有 Event 类的基类，为了简单起见，注册事件只传递了 name（可以复杂的对象，但注意要了解清楚序列化机制）。 定义用户注册服务 (事件发布者)123456789101112131415@Service // &lt;1&gt;public class UserService implements ApplicationEventPublisherAware { // &lt;2&gt; public void register(String name) { System.out.println(&quot;用户：&quot; + name + &quot;已注册！&quot;); applicationEventPublisher.publishEvent(new UserRegisterEvent(name));// &lt;3&gt; } private ApplicationEventPublisher applicationEventPublisher; // &lt;2&gt; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) { // &lt;2&gt; this.applicationEventPublisher = applicationEventPublisher; }} &lt;1&gt; 服务必须交给 Spring 容器托管 &lt;2&gt; ApplicationEventPublisherAware 是由 Spring 提供的用于为 Service 注入 ApplicationEventPublisher 事件发布器的接口，使用这个接口，我们自己的 Service 就拥有了发布事件的能力。 &lt;3&gt; 用户注册后，不再是显示调用其他的业务 Service，而是发布一个用户注册事件。 定义邮件服务，积分服务，其他服务 (事件订阅者)12345678@Service // &lt;1&gt;public class EmailService implements ApplicationListener&lt;UserRegisterEvent&gt; { // &lt;2&gt; @Override public void onApplicationEvent(UserRegisterEvent userRegisterEvent) { System.out.println(&quot;邮件服务接到通知，给&quot; + userRegisterEvent.getSource() + &quot;发送邮件...&quot;);// &lt;3&gt; }} &lt;1&gt; 事件订阅者的服务同样需要托管于 Spring 容器 &lt;2&gt; ApplicationListener&lt;E extends ApplicationEvent&gt; 接口是由 Spring 提供的事件订阅者必须实现的接口，我们一般把该 Service 关心的事件类型作为泛型传入。 &lt;3&gt; 处理事件，通过 event.getSource() 即可拿到事件的具体内容，在本例中便是用户的姓名。 其他两个 Service，也同样编写，实际的业务操作仅仅是打印一句内容即可，篇幅限制，这里省略。 编写启动类1234567891011121314151617@SpringBootApplication@RestControllerpublic class EventDemoApp { public static void main(String[] args) { SpringApplication.run(EventDemoApp.class, args); } @Autowired UserService userService; @RequestMapping(&quot;/register&quot;) public String register(){ userService.register(&quot;kirito&quot;); return &quot;success&quot;; }} 当我们调用 userService.register(“kirito”); 方法时，控制台打印信息如下： 他们的顺序是无序的，如果需要控制顺序，需要重写 order 接口，这点不做介绍。其次，我们完成了用户注册和其他服务的解耦，这也是事件驱动的最大特性之一，如果需要在用户注册时完成其他操作，只需要再添加相应的事件订阅者即可。 Spring 对 Event 的注解支持上述的几个接口已经非常清爽了，如果习惯使用注解，Spring 也提供了，不再需要显示实现 注解式的事件发布者123456789101112@Servicepublic class UserService { public void register(String name) { System.out.println(&quot;用户：&quot; + name + &quot;已注册！&quot;); applicationEventPublisher.publishEvent(new UserRegisterEvent(name)); } @Autowired private ApplicationEventPublisher applicationEventPublisher;} Spring4.2 之后，ApplicationEventPublisher 自动被注入到容器中，采用 Autowired 即可获取。 注解式的事件订阅者12345678@Servicepublic class EmailService { @EventListener public void listenUserRegisterEvent(UserRegisterEvent userRegisterEvent) { System.out.println(&quot;邮件服务接到通知，给&quot; + userRegisterEvent.getSource() + &quot;发送邮件...&quot;); }} @EventListener 注解完成了 ApplicationListener&lt;E extends ApplicationEvent&gt; 接口的使命。 更多的特性可以参考 SpringFramework 的文档。 Spring 中事件的应用在以往阅读 Spring 源码的经验中，接触了不少使用事件的地方，大概列了以下几个，加深以下印象： Spring Security 中使用 AuthenticationEventPublisher 处理用户认证成功，认证失败的消息处理。 1234567public interface AuthenticationEventPublisher { void publishAuthenticationSuccess(Authentication authentication); void publishAuthenticationFailure(AuthenticationException exception, Authentication authentication);} Hibernate 中持久化对象属性的修改是如何被框架得知的？正是采用了一系列持久化相关的事件，如 DefaultSaveEventListener，DefaultUpdateEventListener, 事件非常多，有兴趣可以去 org.hibernate.event 包下查看。 Spring Cloud Zuul 中刷新路由信息使用到的 ZuulRefreshListener 1234567891011121314private static class ZuulRefreshListener implements ApplicationListener&lt;ApplicationEvent&gt; { ... public void onApplicationEvent(ApplicationEvent event) { if(!(event instanceof ContextRefreshedEvent) &amp;&amp; !(event instanceof RefreshScopeRefreshedEvent) &amp;&amp; !(event instanceof RoutesRefreshedEvent)) { if(event instanceof HeartbeatEvent &amp;&amp; this.heartbeatMonitor.update(((HeartbeatEvent)event).getValue())) { this.zuulHandlerMapping.setDirty(true); } } else { this.zuulHandlerMapping.setDirty(true); } } } Spring 容器生命周期相关的一些默认 Event 1ContextRefreshedEvent,ContextStartedEvent,ContextStoppedEvent,ContextClosedEvent,RequestHandledEvent 。。。其实吧，非常多。。。 总结本文暂时只介绍了 Spring 中的一些简单的事件驱动机制，相信如果之后再看到 Event，Publisher，EventListener 一类的单词后缀时，也能立刻和事件机制联系上了。再阅读 Spring 源码时，如果发现出现了某个 Event，但由于不是同步调用，所以很容易被忽视，我一般习惯下意识的去寻找有没有提供默认的 Listener，这样不至于漏掉一些“隐藏”的特性。下一篇文章打算聊一聊分布式场景下，事件驱动使用的注意点。 公众号刚刚创立，如果觉得文章不错，希望能分享到您的朋友圈，如果对文章有什么想法和建议，可以与我沟通。","link":"/event-1/"},{"title":"浅析分布式下的事件驱动机制（PubSub 模式）","text":"上一篇文章《浅析 Spring 中的事件驱动机制》简单介绍了 Spring 对事件的支持。Event 的整个生命周期，从 publisher 发出，经过 applicationContext 容器通知到 EventListener，都是发生在单个 Spring 容器中，而在分布式场景下，有些时候一个事件的产生，可能需要被多个实例响应，本文主要介绍分布式场景下的事件驱动机制，由于使用了 Redis，ActiveMQ，也可以换一个名词来理解：分布式下的发布订阅模式。 JMS 规范在日常项目开发中，我们或多或少的发现一些包一些类位于 java 或 javax 中，他们主要提供抽象类，接口，提供了一种规范，如 JPA，JSR，JNDI，JTA，JMS，他们是由 java 指定的标准规范，一流企业做标准、二流企业做品牌、三流企业做产品，虽然有点调侃的意味，但也可以见得它的重要意义。而 JMS 就是 java 在消息服务上指定的标准 The Java Message Service (JMS) API is a messaging standard that allows application components based on the Java Platform Enterprise Edition (Java EE) to create, send, receive, and read messages. It enables distributed communication that is loosely coupled, reliable, and asynchronous. JMS（JAVA Message Service,java 消息服务）API 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 消息中间件有非常多的实现，如 ActiveMQ，RabbitMQ，RocketMQ，而他们同一遵循的接口规范，便是 JMS。在下文中即将出现的 ConnectionFactory，Destination，Connection，Session，MessageListener，Topic，Queue 等等名词，都是 JMS 核心的接口，由于本文的初衷并不是讲解 MQ&amp;JMS，所以这些机制暂且跳过。 定义分布式事件需求在上一个项目中，我们对接了外网的 http 接口，而安全性的保障则是交给 OAuth2 来完成，作为 OAuth2 的客户端，我们需要获取服务端返回的 token，而 token 接口的获取次数每个月是有限制的，于是我们选择使用 Redis 来保存，定时刷新。由于每次发起请求时都要携带 token，为了更高的性能减少一次 redis io，我们在 TokenService 中使用了本地变量缓存 token。于是形成如下的 token 获取机制： 这个图并不复杂，只是为了方便描述需求：首先去本地变量中加载 token，若 token==null，则去 Redis 加载，若 Redis 未命中（token 过期了），则最终调用外部的 http 接口获取实时的 token，同时存入 redis 中和本地变量中。 这个需求设计到这样一个问题：大多数情况下是单个实例中发现 redis 中的 token 为空，而它需要同时获取最新 token，并通知其他的实例也去加载最新的 token，这个时候事件广播就可以派上用场了。 由于 token 缓存在了 Redis 中，我们首先介绍 Redis 的发布订阅机制。 Redis 中的 Pub 与 Subredis 不仅仅具备缓存的功能，它还拥有一个 channel 机制，我们可以使用 Redis 来进行发布订阅。上述的 token 流程我们简化一下，省略保存到 redis 的那一环，直接介绍如何通知其他应用刷新 token。 引入依赖和配置1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 12345spring: redis: database: 0 host: localhost port: 6379 定义 TokenService1234567891011121314151617181920@Servicepublic class TokenService { @Autowired StringRedisTemplate redisTemplate; public void getToken(String username) { // &lt;1&gt; String token = UUID.randomUUID().toString(); // 模拟 http 接口使用用户名和密码获取 token System.out.println(username + &quot;成功获取 token ...&quot; + token); // 发送 token 刷新广播 System.out.println(&quot;广播 token 刷新事件 ...&quot;); redisTemplate.convertAndSend(RedisPubSubConfig.tokenChannel, token); } public void refreshTokenListener(String token) { // &lt;2&gt; System.out.println(&quot;接到 token 刷新事件，刷新 token :&quot; + token); }} &lt;1&gt; 模拟获取 token 的方法，获取 token 的同时发送广播。 &lt;2&gt; 用于接收其他应用发送过来的广播消息。 配置 RedisMessageListenerContainer在 Spring 应用中 Event 是由 Spring 容器管理的，而在 Redis 的消息机制中，Event 是由 RedisMessageListenerContainer 管理的。我们为 token 配置一个 channel，用于刷新 token： 123456789101112131415161718192021222324252627@Configurationpublic class RedisPubSubConfig { public final static String tokenChannel = &quot;tokenChannel&quot;; @Bean RedisMessageListenerContainer redisMessageListenerContainer(RedisConnectionFactory redisConnectionFactory) { RedisMessageListenerContainer redisMessageListenerContainer = new RedisMessageListenerContainer();// &lt;1&gt; redisMessageListenerContainer.setConnectionFactory(redisConnectionFactory); redisMessageListenerContainer.addMessageListener(tokenRefreshListener(), new ChannelTopic(tokenChannel)); // &lt;2&gt; return redisMessageListenerContainer; } @Autowired TokenService tokenService; MessageListener tokenRefreshListener() { return new MessageListener() { @Override public void onMessage(Message message, byte[] pattern) { byte[] bytes = message.getBody(); // &lt;3&gt; tokenService.refreshTokenListener(new String(bytes)); } }; }} &lt;1&gt; RedisMessageListenerContainer 用于管理所有的 redis 相关的发布与订阅 &lt;2&gt; 为 Redis 容器注册特定的订阅者，在本例中使用 tokenRefreshListener 监听 tokenChannel 频道，当收到消息通知时，会自动调用 onMessage 方法。 &lt;3&gt; 使用 message.getBody() 可以获取消息的具体内容，在本例中即 token 测试结果同样的这个应用，我们在 8080,8081,8082 启动三个，在 8080 中，我们调用 tokenService.getToken(“kirito”);(注意必须要连接到 redis 的同一个 database) 在三个控制台中我们得到了如下的结果： 8080： 123kirito 成功获取 token ...5d4d2a48-934f-450d-8806-e6095b172286广播 token 刷新事件 ...接到 token 刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 8081： 1接到 token 刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 8082： 1接到 token 刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 可以发现其他系统的确收到了通知。 ActiveMQ 中的 Pub 与 SubRedis 中的发布订阅其实在真正的企业开发中并不是很常用，如果涉及到一致性要求较高的需求，专业的消息中间件可以更好地为我们提供服务。下面介绍一下 ActiveMQ 如何实现发布订阅。 ActiveMQ 为我们提供很好的监控页面，延时队列，消息 ACK，事务，持久化等等机制，且拥有较高的吞吐量，是企业架构中不可或缺的一个重要中间件。 引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt;&lt;/dependency&gt; 12345678spring: activemq: in-memory: false # &lt;1&gt; broker-url: tcp://127.0.0.1:61616 user: admin password: admin jms: pub-sub-domain: true # &lt;2&gt; &lt;1&gt; springboot 的自动配置会帮我们启动一个内存中的消息队列，引入 spring-boot-starter-activemq 倚赖时需要特别注意这一点，本例连接本机的 ActiveMQ。 &lt;2&gt; springboot 默认不支持 PubSub 模式，需要手动开启。 定义 TokenService123456789101112131415161718192021222324252627282930@Servicepublic class TokenService { @Autowired JmsTemplate jmsTemplate; // &lt;1&gt; @Autowired Topic tokenTopic; // &lt;3&gt; public void getToken(String username) { String token = UUID.randomUUID().toString(); // 模拟 http 接口使用用户名和密码获取 token System.out.println(username + &quot;成功获取 token ...&quot; + token); // 发送 token 刷新广播 System.out.println(&quot;广播 token 刷新事件 ...&quot;); try { Message message = new ActiveMQMessage(); message.setStringProperty(&quot;token&quot;, token); jmsTemplate.convertAndSend(tokenTopic, message);// &lt;1&gt; } catch (Exception e) { throw new RuntimeException(e); } } @JmsListener(destination = ActivemqPubSubConfig.tokenTopic) // &lt;2&gt; public void refreshTokenListener(Message message) throws Exception { System.out.println(&quot;接到 token 刷新事件，刷新 token :&quot; + message.getStringProperty(&quot;token&quot;)); }} &lt;1&gt; 使用模板设计模式的好处体现了出来，再前面的 RedisTemplate 中我们也是使用同样的 template.convertAndSend() 发送消息 &lt;2&gt; JmsListener 对应于 EventListener，接收来自 ActiveMQ 中 tokenTopic 的消息通知 &lt;3&gt; tokenTopic 定义在下面的 config 中 配置 ActiveMQ 的 topic123456789101112@Configurationpublic class ActivemqPubSubConfig { public final static String tokenTopic = &quot;tokenTopic&quot;; @Bean Topic tokenTopic(){ return new ActiveMQTopic(ActivemqPubSubConfig.tokenTopic); }} 非常简单的配置，因为 ActiveMQAutoConfiguration 已经帮我们做了相当多的配置，我们只需要顶一个 topic 即可使用 ActiveMQ 的功能。 查看 ActiveMQ 的监控端省略了发送消息的过程，实际上可以得到和 Redis PubSub 一样的效果。来看一下 ActiveMQ 自带的监控端，在发送消息后，发生了什么变化，访问本地端口 http://localhost:8161/admin ，可以看到消息被消费了。 总结本文介绍了 Redis，ActiveMQ 的 PubSub 特性，这是我理解的分布式场景下的事件驱动的使用。事件驱动是一种思想，PubSub 是一种模式，Redis，ActiveMQ 是一种应用，落到实处，便可以是本文介绍的 token 这个小小的业务实现。但是注意，使用 Redis，ActiveMQ 理解事件驱动可以，但是不能等同事件驱动，事件驱动还有很多其他场景下体现，笔者功力不够，无法一一介绍，怕人误解，特此强调一下。","link":"/event-2/"},{"title":"从 Feign 使用注意点到 RESTFUL 接口设计规范","text":"最近项目中大量使用了 Spring Cloud Feign 来对接 http 接口，踩了不少坑，也产生了一些对 RESTFUL 接口设计的想法，特此一篇记录下。 [TOC] SpringMVC 的请求参数绑定机制了解 Feign 历史的朋友会知道，Feign 本身是 Netflix 的产品，Spring Cloud Feign 是在原生 Feign 的基础上进行了封装，引入了大量的 SpringMVC 注解支持，这一方面使得其更容易被广大的 Spring 使用者开箱即用，但也产生了不小的混淆作用。所以在使用 Spring Cloud Feign 之前，笔者先介绍一下 SpringMVC 的一个入参机制。预设一个 RestController，在本地的 8080 端口启动一个应用，用于接收 http 请求。 1234567891011@RestControllerpublic class BookController {&lt;!-- more --&gt; @RequestMapping(value = &quot;/hello&quot;) // &lt;1&gt; public String hello(String name) { // &lt;2&gt; return &quot;hello&quot; + name; }} 这个接口写起来非常简单，但实际 springmvc 做了非常多的兼容，使得这个接口可以接受多种请求方式。 &lt;1&gt; RequestMapping 代表映射的路径，使用 GET,POST,PUT,DELETE 方式都可以映射到该端点。 &lt;2&gt; SpringMVC 中常用的请求参数注解有（@RequestParam,@RequestBody,@PathVariable）等。name 被默认当做 @RequestParam。形参 String name 由框架使用字节码技术获取 name 这个名称，自动检测请求参数中 key 值为 name 的参数，也可以使用 @RequestParam(“name”) 覆盖变量本身的名称。当我们在 url 中携带 name 参数或者 form 表单中携带 name 参数时，会被获取到。 12345POST /hello HTTP/1.1Host: localhost:8080Content-Type: application/x-www-form-urlencodedname=formParam 或 12GET /hello?name=queryString HTTP/1.1Host: localhost:8080 Feign 的请求参数绑定机制上述的 SpringMVC 参数绑定机制，大家应该都是非常熟悉的，但这一切在 Feign 中有些许的不同。 我们来看一个非常简单的，但是实际上错误的接口写法： 12345678// 注意：错误的接口写法@FeignClient(&quot;book&quot;)public interface BookApi { @RequestMapping(value = &quot;/hello&quot;,method = RequestMethod.GET) String hello(String name);} 配置请求地址： 1234567ribbon: eureka: enabled: falsebook: ribbon: listOfServers: http://localhost:8080 我们按照写 SpringMVC 的 RestController 的习惯写了一个 FeignClient，按照我们的一开始的想法，由于指定了请求方式是 GET，那么 name 应该会作为 QueryString 拼接到 Url 中吧？发出一个这样的 GET 请求： 12GET /hello?name=xxx HTTP/1.1Host: localhost:8080 而实际上，RestController 并没有接收到，我们在 RestController 一侧的应用中获得了一些提示： 并没有按照期望使用 GET 方式发送请求，而是 POST 方式 name 参数没有被封装，获得了一个 null 值 查看文档发现，如果不加默认的注解，Feign 则会对参数默认加上 @RequestBody 注解，而 RequestBody 一定是包含在请求体中的，GET 方式无法包含。所以上述两个现象得到了解释。Feign 在 GET 请求包含 RequestBody 时强制转成了 POST 请求，而不是报错。 理解清楚了这个机制我们就可以在开发 Feign 接口避免很多坑。而解决上述这个问题也很简单 在 Feign 接口中为 name 添加 @RequestParam(“name”) 注解，name 必须指定，Feign 的请求参数不会利用 SpringMVC 字节码的机制自动给定一个默认的名称。 由于 Feign 默认使用 @RequestBody，也可以改造 RestController，使用 @RequestBody 接收。但是，请求参数通常是多个，推荐使用上述的 @RequestParam，而 @RequestBody 一般只用于传递对象。 Feign 绑定复合参数指定请求参数的类型与请求方式，上述问题的出现实际上是由于在没有理清楚 Feign 内部机制的前提下想当然的和 SpringMVC 进行了类比。同样，在使用对象作为参数时，也需要注意这样的问题。 对于这样的接口 1234567891011121314151617@FeignClient(&quot;book&quot;)public interface BookApi { @RequestMapping(value = &quot;/book&quot;,method = RequestMethod.POST) Book book(@RequestBody Book book); // &lt;1&gt; @RequestMapping(value = &quot;/book&quot;,method = RequestMethod.POST) Book book(@RequestParam(&quot;id&quot;) String id,@RequestParam(&quot;name&quot;) String name); // &lt;2&gt; @RequestMapping(value = &quot;/book&quot;,method = RequestMethod.POST) Book book(@RequestParam Map map); // &lt;3&gt; // 错误的写法 @RequestMapping(value = &quot;/book&quot;,method = RequestMethod.POST) Book book(@RequestParam Book book); // &lt;4&gt;} &lt;1&gt; 使用 @RequestBody 传递对象是最常用的方式。 &lt;2&gt; 如果参数并不是很多，可以平铺开使用 @RequestParam &lt;3&gt; 使用 Map，这也是完全可以的，但不太符合面向对象的思想，不能从代码立刻看出该接口需要什么样的参数。 &lt;4&gt; 错误的用法，Feign 没有提供这样的机制自动转换实体为 Map。 Feign 中使用 @PathVariable 与 RESTFUL 规范这涉及到一个如何设计 RESTFUL 接口的话题，我们知道在自从 RESTFUL 在 2000 年初被提出来之后，就不乏文章提到资源，契约规范，CRUD 对应增删改查操作等等。下面笔者从两个实际的接口来聊聊自己的看法。 根据 id 查找用户接口： 1234567@FeignClient(&quot;user&quot;)public interface UserApi { @RequestMapping(value = &quot;/user/{userId}&quot;,method = RequestMethod.GET) String findById(@PathVariable(&quot;id&quot;) String userId);} 这应该是没有争议的，注意前面强调的，@PathVariable(“id”) 括号中的 id 不可以忘记。那如果是“根据邮箱查找用户呢”? 很有可能下意识的写出这样的接口： 1234567@FeignClient(&quot;user&quot;)public interface UserApi { @RequestMapping(value = &quot;/user/{email}&quot;,method = RequestMethod.GET) String findByEmail(@PathVariable(&quot;email&quot;) String email);} 首先看看 Feign 的问题。email 中通常包含’.‘这个特殊字符，如果在路径中包含，会出现意想不到的结果。我不想探讨如何去解决它（实际上可以使用 {email:.+} 的方式), 因为我觉得这不符合设计。 再谈谈规范的问题。这两个接口是否是相似的，email 是否应该被放到 path 中？这就要聊到 RESTFUL 的初衷，为什么 userId 这个属性被普遍认为适合出现在 RESTFUL 路径中，因为 id 本身起到了资源定位的作用，他是资源的标记。而 email 不同，它可能是唯一的，但更多的，它是资源的属性，所以，笔者认为不应该在路径中出现非定位性的动态参数。而是把 email 作为 @RequestParam 参数。 RESUFTL 结构化查询笔者成功的从 Feign 的话题过度到了 RESTFUL 接口的设计问题，也导致了本文的篇幅变长了，不过也不打算再开一片文章谈了。 再考虑一个接口设计，查询某一个月某个用户的订单，可能还会携带分页参数，这时候参数变得很多，按照传统的设计，这应该是一个查询操作，也就是与 GET 请求对应，那是不是意味着应当将这些参数拼接到 url 后呢？再思考 Feign，正如本文的第二段所述，是不支持 GET 请求携带实体类的，这让我们设计陷入了两难的境地。而实际上参考一些 DSL 语言的设计如 elasticSearch，也是使用 POST JSON 的方式来进行查询的，所以在实际项目中，笔者并不是特别青睐 CRUD 与四种请求方式对应的这种所谓的 RESTFUL 规范，如果说设计 RESTFUL 应该遵循什么规范，那大概是另一些名词，如契约规范和领域驱动设计。 1234567@FeignClient(&quot;order&quot;)public interface BookApi { @RequestMapping(value = &quot;/order/history&quot;,method = RequestMethod.POST) Page&lt;List&lt;Orders&gt;&gt; queryOrderHistory(@RequestBody QueryVO queryVO);} RESTFUL 行为限定在实际接口设计中，我遇到了这样的需求，用户模块的接口需要支持修改用户密码，修改用户邮箱，修改用户姓名，而笔者之前阅读过一篇文章，也是讲舍弃 CRUD 而是用领域驱动设计来规范 RESTFUL 接口的定义，与项目中我的想法不谋而合。看似这三个属性是同一个实体类的三个属性，完全可以如下设计： 1234567@FeignClient(&quot;user&quot;)public interface UserApi { @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.POST) User update(@RequestBody User user);} 但实际上，如果再考虑多一层，就应该产生这样的思考：这三个功能所需要的权限一致吗？真的应该将他们放到一个接口中吗？实际上，笔者并不希望接口调用方传递一个实体，因为这样的行为是不可控的，完全不知道它到底是修改了什么属性，如果真的要限制行为，还需要在 User 中添加一个操作类型的字段，然后在接口实现方加以校验，这太麻烦了。而实际上，笔者觉得规范的设计应当如下： 12345678910111213@FeignClient(&quot;user&quot;)public interface UserApi { @RequestMapping(value = &quot;/user/{userId}/password/update&quot;,method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updatePassword(@PathVariable(&quot;userId) String userId,@RequestParam(&quot;password&quot;) password); @RequestMapping(value = &quot;/user/{userId}/email/update&quot;,method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateEmail(@PathVariable(&quot;userId) String userId,@RequestParam(&quot;email&quot;) String email); @RequestMapping(value = &quot;/user/{userId}/username/update&quot;,method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateUsername(@PathVariable(&quot;userId) String userId,@RequestParam(&quot;username&quot;) String username);} 一般意义上 RESTFUL 接口不应该出现动词，这里的 update 并不是一个动作，而是标记着操作的类型，因为针对某个属性可能出现的操作类型可能会有很多，所以我习惯加上一个 update 后缀，明确表达想要进行的操作，而不是仅仅依赖于 GET，POST，PUT，DELETE。实际上，修改操作推荐使用的请求方式应当是 PUT，这点笔者的理解是，已经使用 update 标记了行为，实际开发中不习惯使用 PUT。 password，email，username 都是 user 的属性，而 userId 是 user 的识别符号，所以 userId 以 PathVariable 的形式出现在 url 中，而三个属性出现在 ReqeustParam 中。 顺带谈谈逻辑删除，如果一个需求是删除用户的常用地址，这个 api 的操作类型，我通常也不会设计为 DELETE 请求，而是同样使用 delete 来标记操作行为 12@RequestMapping(value = &quot;/user/{userId}/address/{addressId}/delete&quot;,method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateEmail(@PathVariable(&quot;userId&quot;) String userId,@PathVariable(&quot;userId&quot;) String email); 总结本文从 Feign 的使用注意点，聊到了 RESTFUL 接口的设计问题，其实是一个互相补充的行为。接口设计需要载体，所以我以 Feign 的接口风格谈了谈自己对 RESTFUL 设计的理解，而 Feign 中一些坑点，也正是我想要规范 RESTFUL 设计的出发点。如有对 RESTFUL 设计不同的理解，欢迎与我沟通。","link":"/feign-1/"},{"title":"文件 IO 操作的一些最佳实践","text":"背景已经过去的中间件性能挑战赛，和正在进行中的 第一届 PolarDB 数据性能大赛 都涉及到了文件操作，合理地设计架构以及正确地压榨机器的读写性能成了比赛中获取较好成绩的关键。正在参赛的我收到了几位公众号读者朋友的反馈，他们大多表达出了这样的烦恼：“对比赛很感兴趣，但不知道怎么入门”，“能跑出成绩，但相比前排的选手，成绩相差 10 倍有余”…为了能让更多的读者参与到之后相类似的比赛中来，我简单整理一些文件 IO 操作的最佳实践，而不涉及整体系统的架构设计，希望通过这篇文章的介绍，让你能够欢快地参与到之后类似的性能挑战赛之中来。 知识点梳理本文主要关注的 Java 相关的文件操作，理解它们需要一些前置条件，比如 PageCache，Mmap(内存映射)，DirectByteBuffer(堆外缓存)，顺序读写，随机读写… 不一定需要完全理解，但至少知道它们是个啥，因为本文将会主要围绕这些知识点来展开描述。 初识 FileChannel 和 MMAP首先，文件 IO 类型的比赛最重要的一点，就是选择好读写文件的方式，那 JAVA 中文件 IO 有多少种呢？原生的读写方式大概可以被分为三种：普通 IO，FileChannel(文件通道)，MMAP(内存映射)。区分他们也很简单，例如 FileWriter,FileReader 存在于 java.io 包中，他们属于普通 IO；FileChannel 存在于 java.nio 包中，属于 NIO 的一种，但是注意 NIO 并不一定意味着非阻塞，这里的 FileChannel 就是阻塞的；较为特殊的是后者 MMAP，它是由 FileChannel 调用 map 方法衍生出来的一种特殊读写文件的方式，被称之为内存映射。 使用 FIleChannel 的方式： 1FileChannel fileChannel = new RandomAccessFile(new File(&quot;db.data&quot;), &quot;rw&quot;).getChannel(); 获取 MMAP 的方式： 1MappedByteBuffer mappedByteBuffer = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, filechannel.size(); MappedByteBuffer 便是 JAVA 中 MMAP 的操作类。 面向于字节传输的传统 IO 方式遭到了我们的唾弃，我们重点探讨 FileChannel 和 MMAP 这两种读写方式的区别。 FileChannel 读写123456789101112131415// 写byte[] data = new byte[4096];long position = 1024L;// 指定 position 写入 4kb 的数据fileChannel.write(ByteBuffer.wrap(data), position);// 从当前文件指针的位置写入 4kb 的数据fileChannel.write(ByteBuffer.wrap(data));// 读ByteBuffer buffer = ByteBuffer.allocate(4096);long position = 1024L;// 指定 position 读取 4kb 的数据fileChannel.read(buffer,position)；// 从当前文件指针的位置读取 4kb 的数据fileChannel.read(buffer); FileChannel 大多数时候是和 ByteBuffer 这个类打交道，你可以将它理解为一个 byte[] 的封装类，提供了丰富的 API 去操作字节，不了解的同学可以去熟悉下它的 API。值得一提的是，write 和 read 方法均是 ** 线程安全 ** 的，FileChannel 内部通过一把 private final Object positionLock = new Object(); 锁来控制并发。 FileChannel 为什么比普通 IO 要快呢？这么说可能不严谨，因为你要用对它，FileChannel 只有在一次写入 4kb 的整数倍时，才能发挥出实际的性能，这得益于 FileChannel 采用了 ByteBuffer 这样的内存缓冲区，让我们可以非常精准的控制写盘的大小，这是普通 IO 无法实现的。4kb 一定快吗？也不严谨，这主要取决你机器的磁盘结构，并且受到操作系统，文件系统，CPU 的影响，例如中间件性能挑战赛时的那块盘，一次至少写入 64kb 才能发挥出最高的 IOPS。 然而 PolarDB 这块盘就完全不一样了，可谓是异常彪悍，具体是如何的表现由于比赛仍在进行中，不予深究，但凭借着 benchmark everyting 的技巧，我们完全可以测出来。 另外一点，成就了 FileChannel 的高效，介绍这点之前，我想做一个提问：FileChannel 是直接把 ByteBuffer 中的数据写入到磁盘吗？思考几秒…答案是：NO。ByteBuffer 中的数据和磁盘中的数据还隔了一层，这一层便是 PageCache，是用户内存和磁盘之间的一层缓存。我们都知道磁盘 IO 和内存 IO 的速度可是相差了好几个数量级。我们可以认为 filechannel.write 写入 PageCache 便是完成了落盘操作，但实际上，操作系统最终帮我们完成了 PageCache 到磁盘的最终写入，理解了这个概念，你就应该能够理解 FileChannel 为什么提供了一个 force() 方法，用于通知操作系统进行及时的刷盘。 同理，当我们使用 FileChannel 进行读操作时，同样经历了：磁盘 -&gt;PageCache-&gt; 用户内存这三个阶段，对于日常使用者而言，你可以忽略掉 PageCache，但作为挑战者参赛，PageCache 在调优过程中是万万不能忽视的，关于读操作这里不做过多的介绍，我们再下面的小结中还会再次提及，这里当做是引出 PageCache 的概念。 MMAP 读写12345678910111213141516171819// 写byte[] data = new byte[4];int position = 8;// 从当前 mmap 指针的位置写入 4b 的数据mappedByteBuffer.put(data);// 指定 position 写入 4b 的数据MappedByteBuffer subBuffer = mappedByteBuffer.slice();subBuffer.position(position);subBuffer.put(data);// 读byte[] data = new byte[4];int position = 8;// 从当前 mmap 指针的位置读取 4b 的数据mappedByteBuffer.get(data)；// 指定 position 读取 4b 的数据MappedByteBuffer subBuffer = mappedByteBuffer.slice();subBuffer.position(position);subBuffer.get(data); FileChannel 已经足够强大了，MappedByteBuffer 还能玩出什么花来呢？请容许我卖个关子先，先介绍一下 MappedByteBuffer 的使用注意点。 当我们执行 fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1.5 * 1024 * 1024 * 1024); 之后，观察一下磁盘上的变化，会立刻获得一个 1.5G 的文件，但此时文件的内容全部是 0（字节 0）。这符合 MMAP 的中文描述：内存映射文件，我们之后对内存中 MappedByteBuffer 做的任何操作，都会被最终映射到文件之中， mmap 把文件映射到用户空间里的虚拟内存，省去了从内核缓冲区复制到用户空间的过程，文件中的位置在虚拟内存中有了对应的地址，可以像操作内存一样操作这个文件，相当于已经把整个文件放入内存，但在真正使用到这些数据前却不会消耗物理内存，也不会有读写磁盘的操作，只有真正使用这些数据时，也就是图像准备渲染在屏幕上时，虚拟内存管理系统 VMS 才根据缺页加载的机制从磁盘加载对应的数据块到物理内存进行渲染。这样的文件读写文件方式少了数据从内核缓存到用户空间的拷贝，效率很高 看了稍微官方一点的描述，你可能对 MMAP 有了些许的好奇，有这么厉害的黑科技存在的话，还有 FileChannel 存在的意义吗！并且网上很多文章都在说，MMAP 操作大文件性能比 FileChannel 搞出一个数量级！然而，通过我比赛的认识，MMAP 并非是文件 IO 的银弹，它只有在 ** 一次写入很小量数据的场景 ** 下才能表现出比 FileChannel 稍微优异的性能。紧接着我还要告诉你一些令你沮丧的事，至少在 JAVA 中使用 MappedByteBuffer 是一件非常麻烦并且痛苦的事，主要表现为三点： MMAP 使用时必须实现指定好内存映射的大小，并且一次 map 的大小限制在 1.5G 左右，重复 map 又会带来虚拟内存的回收、重新分配的问题，对于文件不确定大小的情形实在是太不友好了。 MMAP 使用的是虚拟内存，和 PageCache 一样是由操作系统来控制刷盘的，虽然可以通过 force() 来手动控制，但这个时间把握不好，在小内存场景下会很令人头疼。 MMAP 的回收问题，当 MappedByteBuffer 不再需要时，可以手动释放占用的虚拟内存，但…方式非常的诡异。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static void clean(MappedByteBuffer mappedByteBuffer) { ByteBuffer buffer = mappedByteBuffer; if (buffer == null || !buffer.isDirect() || buffer.capacity()== 0) return; invoke(invoke(viewed(buffer), &quot;cleaner&quot;), &quot;clean&quot;);}private static Object invoke(final Object target, final String methodName, final Class&lt;?&gt;... args) { return AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() { public Object run() { try { Method method = method(target, methodName, args); method.setAccessible(true); return method.invoke(target); } catch (Exception e) { throw new IllegalStateException(e); } } });}private static Method method(Object target, String methodName, Class&lt;?&gt;[] args) throws NoSuchMethodException { try { return target.getClass().getMethod(methodName, args); } catch (NoSuchMethodException e) { return target.getClass().getDeclaredMethod(methodName, args); }}private static ByteBuffer viewed(ByteBuffer buffer) { String methodName = &quot;viewedBuffer&quot;; Method[] methods = buffer.getClass().getMethods(); for (int i = 0; i &lt; methods.length; i++) { if (methods[i].getName().equals(&quot;attachment&quot;)) { methodName = &quot;attachment&quot;; break; } } ByteBuffer viewedBuffer = (ByteBuffer) invoke(buffer, methodName); if (viewedBuffer == null) return buffer; else return viewed(viewedBuffer);} 对的，你没看错，这么长的代码仅仅是为了干回收 MappedByteBuffer 这一件事。 所以我建议，优先使用 FileChannel 去完成初始代码的提交，在必须使用小数据量 (例如几个字节) 刷盘的场景下，再换成 MMAP 的实现，其他场景 FileChannel 完全可以 cover(前提是你理解怎么合理使用 FileChannel)。至于 MMAP 为什么在一次写入少量数据的场景下表现的比 FileChannel 优异，我还没有查到理论根据，如果你有相关的线索，欢迎留言。理论分析下，FileChannel 同样是写入内存，但是在写入小数据量时，MMAP 表现的更加优秀，所以在索引数据落盘时，大多数情况应该选择使用 MMAP。至于 MMAP 分配的虚拟内存是否就是真正的 PageCache 这一点，我觉得可以近似理解成 PageCache。 顺序读比随机读快，顺序写比随机写快无论你是机械硬盘还是 SSD，这个结论都是一定成立的，虽然背后的原因不太一样，我们今天不讨论机械硬盘这种古老的存储介质，重点 foucs 在 SSD 上，来看看在它之上进行的随机读写为什么比顺序读写要慢。即使各个 SSD 和文件系统的构成具有差异性，但我们今天的分析同样具备参考价值。 首先，什么是顺序读，什么是随机读，什么是顺序写，什么是随机写？可能我们刚接触文件 IO 操作时并不会有这样的疑惑，但写着写着，自己都开始怀疑自己的理解了，不知道你有没有经历过这样类似的阶段，反正我有一段时间的确怀疑过。那么，先来看看两段代码： 写入方式一：64 个线程，用户自己使用一个 atomic 变量记录写入指针的位置，并发写入 12345678ExecutorService executor = Executors.newFixedThreadPool(64);AtomicLong wrotePosition = new AtomicLong(0);for(int i=0;i&lt;1024;i++){ final int index = i; executor.execute(()-&gt;{ fileChannel.write(ByteBuffer.wrap(new byte[4*1024]),wrote.getAndAdd(4*1024)); })} 写入方式二：给 write 加了锁，保证了同步。 123456789101112ExecutorService executor = Executors.newFixedThreadPool(64);AtomicLong wrotePosition = new AtomicLong(0);for(int i=0;i&lt;1024;i++){ final int index = i; executor.execute(()-&gt;{ write(new byte[4*1024]); })}public synchronized void write(byte[] data){ fileChannel.write(ByteBuffer.wrap(new byte[4*1024]),wrote.getAndAdd(4*1024));} 答案是方式二才算顺序写，顺序读也是同理。对于文件操作，加锁并不是一件非常可怕的事，不敢同步 write/read 才可怕！有人会问：FileChannel 内部不是已经有 positionLock 保证写入的线程安全了吗，为什么还要自己加同步？为什么这样会快？我用大白话来回答的话就是多线程并发 write 并且不加同步，会导致文件空洞，它的执行次序可能是 时序 1：thread1 write position[0~4096) 时序 2：thread3 write position[8194~12288) 时序 3：thread2 write position[4096~8194) 所以并不是完全的“顺序写”。不过你也别担心加锁会导致性能下降，我们会在下面的小结介绍一个优化：通过文件分片来减少多线程读写时锁的冲突。 在来分析原理，顺序读为什么会比随机读要快？顺序写为什么比随机写要快？这两个对比其实都是一个东西在起作用：PageCache，前面我们已经提到了，它是位于 application buffer(用户内存) 和 disk file(磁盘) 之间的一层缓存。 以顺序读为例，当用户发起一个 fileChannel.read(4kb) 之后，实际发生了两件事 操作系统从磁盘加载了 16kb 进入 PageCache，这被称为预读 操作通从 PageCache 拷贝 4kb 进入用户内存 最终我们在用户内存访问到了 4kb，为什么顺序读快？很容量想到，当用户继续访问接下来的 [4kb,16kb] 的磁盘内容时，便是直接从 PageCache 去访问了。试想一下，当需要访问 16kb 的磁盘内容时，是发生 4 次磁盘 IO 快，还是发生 1 次磁盘 IO+4 次内存 IO 快呢？答案是显而易见的，这一切都是 PageCache 带来的优化。 深度思考：当内存吃紧时，PageCache 的分配会受影响吗？PageCache 的大小如何确定，是固定的 16kb 吗？我可以监控 PageCache 的命中情况吗？ PageCache 会在哪些场景失效，如果失效了，我们又要哪些补救方式呢？ 我进行简单的自问自答，背后的逻辑还需要读者去推敲： 当内存吃紧时，PageCache 的预读会受到影响，实测，并没有搜到到文献支持 PageCache 是动态调整的，可以通过 linux 的系统参数进行调整，默认是占据总内存的 20% https://github.com/brendangregg/perf-tools github 上一款工具可以监控 PageCache 这是很有意思的一个优化点，如果用 PageCache 做缓存不可控，不妨自己做预读如何呢？ 顺序写的原理和顺序读一致，都是收到了 PageCache 的影响，留给读者自己推敲一下。 直接内存 (堆外) VS 堆内内存前面 FileChannel 的示例代码中已经使用到了堆内内存： ByteBuffer.allocate(4 * 1024)，ByteBuffer 提供了另外的方式让我们可以分配堆外内存 ： ByteBuffer.allocateDirect(4 * 1024)。这就引来的一系列的问题，我什么时候应该使用堆内内存，什么时候应该使用直接内存？ 我不花太多笔墨去阐述了，直接上对比： 堆内内存 堆外内存 ** 底层实现 ** 数组，JVM 内存 unsafe.allocateMemory(size) 返回直接内存 ** 分配大小限制 ** -Xms-Xmx 配置的 JVM 内存相关，并且数组的大小有限制，在做测试时发现，当 JVM free memory 大于 1.5G 时，ByteBuffer.allocate(900M) 时会报错 可以通过 -XX:MaxDirectMemorySize 参数从 JVM 层面去限制，同时受到机器虚拟内存（说物理内存不太准确）的限制 ** 垃圾回收 ** 不必多说 当 DirectByteBuffer 不再被使用时，会出发内部 cleaner 的钩子，保险起见，可以考虑手动回收：((DirectBuffer) buffer).cleaner().clean(); ** 内存复制 ** 堆内内存 -&gt; 堆外内存 -&gt; pageCache 堆外内存 -&gt; pageCache 关于堆内内存和堆外内存的一些最佳实践： 当需要申请大块的内存时，堆内内存会受到限制，只能分配堆外内存。 堆外内存适用于生命周期中等或较长的对象。(如果是生命周期较短的对象，在 YGC 的时候就被回收了，就不存在大内存且生命周期较长的对象在 FGC 对应用造成的性能影响)。 堆内内存刷盘的过程中，还需要复制一份到堆外内存，这部分内容可以在 FileChannel 的实现源码中看到细节，至于 Jdk 为什么需要这么做，可以参考我的另外一篇文章：《一文探讨堆外内存的监控与回收》 同时，还可以使用池 + 堆外内存 的组合方式，来对生命周期较短，但涉及到 I/O 操作的对象进行堆外内存的再使用 (Netty 中就使用了该方式)。在比赛中，尽量不要出现在频繁 new byte[] ，创建内存区域再回收也是一笔不小的开销，使用 ThreadLocal&lt;ByteBuffer&gt; 和 ThreadLocal&lt;byte[]&gt; 往往会给你带来意外的惊喜 ~ 创建堆外内存的消耗要大于创建堆内内存的消耗，所以当分配了堆外内存之后，尽可能复用它。 黑魔法：UNSAFE123456789101112public class UnsafeUtil { public static final Unsafe UNSAFE; static { try { Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); field.setAccessible(true); UNSAFE = (Unsafe) field.get(null); } catch (Exception e) { throw new RuntimeException(e); } }} 我们可以使用 UNSAFE 这个黑魔法实现很多无法想象的事，我这里就稍微介绍一两点吧。 实现直接内存与内存的拷贝： 1234ByteBuffer buffer = ByteBuffer.allocateDirect(4 * 1024 * 1024);long addresses = ((DirectBuffer) buffer).address();byte[] data = new byte[4 * 1024 * 1024];UNSAFE.copyMemory(data, 16, null, addresses, 4 * 1024 * 1024); copyMemory 方法可以实现内存之间的拷贝，无论是堆内和堆外，12 个参数是 source 方，34 是 target 方，第 5 个参数是 copy 的大小。如果是堆内的字节数组，则传递数组的首地址和 16 这个固定的 ARRAY_BYTE_BASE_OFFSET 偏移常量；如果是堆外内存，则传递 null 和直接内存的偏移量，可以通过 ((DirectBuffer) buffer).address() 拿到。为什么不直接拷贝，而要借助 UNSAFE？当然是因为它快啊！少年！另外补充：MappedByteBuffer 也可以使用 UNSAFE 来 copy 从而达到写盘 / 读盘的效果哦。 至于 UNSAFE 还有那些黑科技，可以专门去了解下，我这里就不过多赘述了。 文件分区前面已经提到了顺序读写时我们需要对 write，read 加锁，并且我一再强调的一点是：加锁并不可怕，文件 IO 操作并没有那么依赖多线程。但是加锁之后的顺序读写必然无法打满磁盘 IO，如今系统强劲的 CPU 总不能不压榨吧？我们可以采用文件分区的方式来达到一举两得的效果：既满足了顺序读写，又减少了锁的冲突。 那么问题又来了，分多少合适呢？文件多了，锁冲突变降低了；文件太多了，碎片化太过严重，单个文件的值太少，缓存也就不容易命中，这样的 trade off 如何平衡？没有理论答案，benchmark everything~ Direct IO 最后我们来探讨一下之前从没提到的一种 IO 方式，Direct IO，什么，Java 还有这东西？博主你骗我？之前怎么告诉我只有三种 IO 方式！别急着骂我，严谨来说，这并不是 JAVA 原生支持的方式，但可以通过 JNA/JNI 调用 native 方法做到。从上图我们可以看到 ：Direct IO 绕过了 PageCache，但我们前面说到过，PageCache 可是个好东西啊，干嘛不用他呢？再仔细推敲一下，还真有一些场景下，Direct IO 可以发挥作用，没错，那就是我们前面没怎么提到的：** 随机读 **。当使用 fileChannel.read() 这类会触发 PageCache 预读的 IO 方式时，我们其实并不希望操作系统帮我们干太多事，除非真的踩了狗屎运，随机读都能命中 PageCache，但几率可想而知。Direct IO 虽然被 Linus 无脑喷过，但在随机读的场景下，依旧存在其价值，减少了 Block IO Layed（近似理解为磁盘） 到 Page Cache 的 overhead。 话说回来，Java 怎么用 Direct IO 呢？有没有什么限制呢？前面说过，Java 目前原生并不支持，但也有好心人封装好了 Java 的 JNA 库，实现了 Java 的 Direct IO，github 地址：https://github.com/smacke/jaydio 12345678int bufferSize = 20 * 1024 * 1024;DirectRandomAccessFile directFile = new DirectRandomAccessFile(new File(&quot;dio.data&quot;), &quot;rw&quot;, bufferSize);for(int i= 0;i&lt; bufferSize / 4096;i++){ byte[] buffer = new byte[4 * 1024]; directFile.read(buffer); directFile.readFully(buffer);}directFile.close(); 但需要注意的是，** 只有 Linux 系统才支持 DIO**! 所以，少年，是时候上手装一台 linux 了。值得一提的是，据说在 Jdk10 发布之后，Direct IO 将会得到原生的支持，让我们拭目以待吧！ 总结以上均是个人的实践积累而来的经验，有部分结论没有找到文献的支撑，所以如有错误，欢迎指正。关于 PolarDB 数据性能大赛的比赛分析，等复赛结束后我会专门另起一篇文章，分析下具体如何使用这些优化点，当然这些小技巧其实很多人都知道，决定最后成绩的还是整体设计的架构，以及对文件 IO，操作系统，文件系统，CPU 和语言特性的理解。虽然 JAVA 搞这种性能挑战赛并不吃香，但依旧是乐趣无穷，希望这些文件 IO 的知识能够帮助你，等下次比赛时看到你的身影 ~ ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/file-io-best-practise/"},{"title":"文件 IO 中如何保证掉电不丢失数据","text":"前言好久没有分享文件 IO 的小技巧了，依稀记得上次分享还是在上次。 第二届云原生编程挑战赛正在火热进行中，Kirito 也在做《针对冷热读写场景的RocketMQ存储系统设计》这个题目，不过参与的是内部赛道，没法跟外部的小伙伴们一起排名了。 众所周知，存储设计离不开文件 IO，将数据存储到文件中进行持久化，是大多数消息队列、数据库系统的常规操作。在比赛中，为了更贴近实际的生产场景，往往也会引入正确性检测阶段，以避免让选手设计一些仅仅支持内存行为的代码逻辑。试想一下，RocketMQ 或者 Mysql 在宕机之后因为索引丢失，而导致数据无法查询，这该是多么可怕的一件事！ 正确性检测要求我们写入的数据能够被查询出来，没有丢失，按照我个人的参赛经验，通常分为三种级别 进程正常退出或者进程被 kill -15 中断 进程被 kill -9 中断 系统掉电 第一个级别，进程正常退出或者进程被 kill -15 中断，该场景没有什么好讲的，一般评测程序会留出 destroy 、close 等回调接口，用于显式关闭，或者在 Java 中使用 JVM 提供的 ShutdownHook 监听 -15 信号，这是最简单的一种场景，一般不需要考虑数据一致性的问题。在实际生产中，对应我们优雅退出、手动关机的流程。 第二个级别，进程被 kill -9 中断。这意味着，我们使用内存去聚合一些数据可能是受限的，但我们仍然可以利用操作系统的一些特性，例如 PageCache 去做缓存。毕竟进程挂了，机器可没挂。在实际生产中，对应我们遇到一些内存溢出、FullGC 重启进程等暴力退出程序的场景。 第三个级别，系统掉电。这也是我这篇文章的主角，同时也是数据一致性要求最高的级别。系统掉电意味着我们甚至连 PageCache 都不能直接利用，必须严格保证数据落到磁盘当中。在实际生产中，对应主机宕机，机房断电等场景。 可以发现，任何一个级别，都有他们实际应用的场景，越是一致性要求高的级别，通常性能就越差，能够利用的手段也越少，系统也就越难设计。 而这次比赛的正确性描述 写入若干条数据。 重启机器 再读出来，必须严格等于之前写入的数据 其中的重启机器环节，恰恰是模拟的掉电。 如何理解数据不丢失在介绍 Java 文件 IO 中保证掉电不丢失的手段之前，我还需要做一个概念的介绍，这样方便我们更好的理解文章后续的观点。 很多同学可能有疑惑，如果一个数据写到一半，发生了掉电，那评测程序怎么知道这条数据落盘了没有呢？评测程序会不会读取这条数据呢？其实，对于”执行到一半“这种逻辑，谁都没有办法保证，正如系统真正掉电时，他可不会跟你商量。所以，在一般的评测中，去验证选手的数据一致性时，通常采取的做法是：当一个方法同步返回时，就应该认为这个数据落盘了，即使返回后立刻断电，也应该可以在重启之后，查询到这条数据。 这符合我们在实际开发/生产场景的认知： 对于同步方法，其实隐含了 ack 的契约，即拿到返回值的那一瞬间，认为对方处理完毕了。 对于异步方法，我们才需要增加回调或者轮询 ack 的机制。 Java 文件 IO 保障掉电不丢数据在《文件 IO 操作的一些最佳实践》一文中，我其实已经介绍了，Java 中无非就一个 FileChannel 是最常用的文件操作类。 FileChannel 的 write 方法看似是一个同步方法，将内存数据写入了磁盘，但其实它和磁盘之间还隔着一层 PageCache。 尽管操作系统可能很快就将 PageCache 刷入到了磁盘，但这个过程仍然是一个异步的过程。就以这次比赛而言，如果你仅仅数据写入到 PageCache 就不管不问了，肯定是无法通过正确性检测的。 解决方法也很简单，调用 FileChannel#force(boolean meta) 方法即可，该方法会强制操作系统将 PageCache 刷盘。 force 的入参是一个 boolean 值，代表是否将元数据也刷盘，这块网上资料比较少，我也没有详细的依据。按照我个人的理解，元数据包含了大小和时间戳信息，可能会影响文件的实际长度，所以 force(true) 可能更稳妥一些。 结合第二节中介绍的内容，我们只需要保证在每次写入操作返回之前，调用 force，即可实现掉电数据不丢失的效果。 那么，代价是什么呢？意味着我们完全丧失了操作系统给文件 IO 设置的一道缓存。在没有缓存又没有 4kb 对齐的情况下，写入放大问题将会非常明显。 这里用一份数据说话，根据官方给出的数据，这次评测使用的 SSD 吞吐可达到 320MiB/s，而我实测在不经过优化的场景下使用 force，仅仅能达到 50 Mib/s，直接会导致评测超时。 force 是掉电的拯救者，也可能是性能的毁灭者。 force 下可能的优化方案在实际场景中，消息的生产者可能会同步地连续地发送多条消息，也有可能会有多个生产者一起在发送消息，尽管消息的投递是同步的，但我们仍然可以在多个不同生产者的消息之间做一些文章，在保证 force 的同时，减少写入放大的问题。 鉴于比赛还在进行中，我就不过多聊详细设计了，懂的应该看到上面这段话都懂了，还算是比较基础的优化。我在优化过后，可以保证在 force 的前提下，将吞吐量从 50 Mib/s 提升到 275 Mib/s，尽管离理论值还是有所差距，但已经足够出一个 baseline 了。 RocketMQ 中的实际应用以 RocketMQ 为例，聊聊其是如何保障数据不丢失的。RocketMQ 在 Broker 侧保障数据不丢失主要有两种机制： RocketMQ 支持配置同步双写，保障消息在主节点之外，还在一个从节点有备份 RocketMQ 支持同步刷盘策略，即本文介绍的 FileChannel#force(boolean meta) 方案 今天对文件 IO 的理解有没有多一点点呢，如果你愿意多花点时间阅读这篇文章的话，你就会发现多花了点时间。对了，这次比赛，我有拉一个小群，欢迎对比赛感兴趣的同学加我微信 xiayimiaoshenghua 进群交流，或者留言讨论哦。","link":"/filechannel_force/"},{"title":"聊聊 IT 行业应届生求职","text":"前言回首大三下的暑假，那时候刚开始出来找实习，如今已经即将进入大四下学期，恍惚间，已经过去了 8，9 个月。写这篇文章的初衷就是想结合自己的经验给即将要出来找工作的应届生一些建议，想当初自己刚出来时，也得到过热心学长的教导，权当一种传递吧。 个人经历坐标上海，目前在一家 IT 软件公司从事电子商务，金融保险类的网站开发，主要使用的语言是 JAVA。从任职的 3-4 个月起，开始担任项目小组长协同项目经理进行开发。期间由于技术总监常驻广州的原因，我兼任了上海分部这一块的面试工作，主要负责技术部分的面试（TMD 工资却没涨 T__T）。所以对广大来面试者的水平，以及公司想要的人才都有了更深的了解；有了面试经验后，一些观念也有了转变。 面试杂谈大四肯定很多人想出来找实习，但是又完全没有任何经验，这就很尴尬了，我先来说一些一定要注意的点。 不要乱投简历，现在互联网上有很多培训机构，中介机构，打着招聘的牌子，背后却干着培训的勾当。通常是对一些基础不太好的同学进行技术面试，对他们的信心造成碾压，而后，提出培训后入职的建议。通常这类公司就是通过这种手段去拉人培训，招人根本不是初衷。所以，要问清楚公司的情况，有必要面试之前先去百度搜一搜公司的基本情况和评价。 紧接着上面那点，可以通过一些业界信誉比较高的 app 或者网站去筛选公司。如 BOSS 直聘，拉钩，51job，前程无忧… 特别是前面两个，是专门给程序员招聘使用的，针对性很强，对自己能力有了解的同学也可以量力而行，挑选适合自己的岗位。 投简历之前搞清楚公司的性质。IT 行业目前大方向就分为两类：软件公司，互联网公司。我当初刚进公司的时候甲方乙方都搞不清楚，大家可能一下子也不知道这两种公司性质有什么区别。可以参照知乎这个问题的讨论 https://www.zhihu.com/question/20274106/answer/40996303，简单来说同样的能力：软件公司轻松，钱少；互联网公司累，钱多。软件公司中又有外企，民营，国资等划分，工作性质又分为外包，自营… 外包又分为人力外包和项目外包… 互联网公司一说，大家肯定都知道 BAT，京东，谷歌… 还有一个层面的划分就是，软件公司大多提供的是服务，互联网公司通常都有自己的产品，不过这么说不够严谨，权当个参考吧。 下面说一说这么多公司，怎么挑选适合自己的岗位。有很多的参考项，个人的能力，期望的工作地点以及地域的工资水准，未来的职业规划，房价，~~ 对象 ~~，水土气候，人脉等等诸多因素。本人是干 java 的，所以就以 java 求职来做例子，其他职业，专业请结合自己的专业知识做好对比即可。全部以上海为准，上海的起薪大概是 2.8K 左右，这叫基本工资，其他城市，例如无锡，苏州，大概在 2.3k 左右，视经济发展程度而定，先有个大概了解。 下面来看看具体招聘需求A 类： Java 6K-12K职位描述 人品过硬。愿意追随项目长期发展。有能力。 有阅历。 有学历。符合 PSD 原则，即出身贫寒、渴望成功、聪明机智。 不需要我吐槽了吧，这种明明是招技术岗，却对技术没有要求的，估计能骗一些小白去面试，只有技术一无所知，才会退而去要求人品，试想一下，你啥都不会，也只能要求你人品过关了。 B 类： 职位描述 任职要求： 大专或以上学历，计算机相关专业，1-3 年以上软件开发经验； 熟练掌握 Java 开发技术，j2ee 平台的核心技术的原理：jsp、ajax、servlet，jdbc 等； 熟练掌握一种主流数据库：MySQL/sql,server/oracle/DB2，熟悉一种应用服务器的配置：tomcat/jboss/weblogic/websphere； 熟悉和理解 Java 开发各层次框架，如 struts、spring、hiberate 等，掌握基本 Web 前台技术； 热爱开发工作，具备良好的程序开发驾驭能力，需求分析把握能力； 好的沟通和解能力，善于团队合作，逻辑思维强，能够独立思考。 此文我是想写给应届生的，1-3 年的工作经验没那么恐怖，大多数情况下，你的能力够了，公司不会跟你较真，用年限压你，所以看到自己技术水平能够达到，资历却不符合的岗位，也可以尝试着投一投。这类公司其实已经算是对技术有了要求了，而且技术细节都明确了出来，但是，看到只对 jsp，servlet 这些技术有所要求，明眼人都知道，这是在招初级开发，了解一点框架，懂计算机基础，这样的新手，公司还是可以接受的，上海这边针对可以独立开发的应届生，或者培训班出来可以直接上手的非科班生：软件公司，实习开价大概在 4-5k，转正开价大概在 7-8k；互联网公司实习大概在 5-6k，转正开价 9-10k 起步。985/211 或者能力不错能够入职的高校生，在互联网名企的开价，就以阿里为例，我了解到的情况大概是 12k14 or 1216。这里都是说一个上海地区价格，不适用与全国。北京的情况是 IT 非常发达，很多互联网公司都在北京，而上海，深圳，广州其次，注意，上海是金融之都，并非 IT 之都。 C 类 ： Java 工程师 13K-21K任职资格 大学本科或以上学历，计算机相关专业； 熟练掌握 core java 以及主流 java 框架， 熟悉 HTML5、CSS3、JAVASCRIPT、JQUERY 等前端技术； 熟练掌握面向对象的设计原则，熟悉 JAVA 设计模式，具备一定的系统架构设计能力； 熟悉常用的互联网相关技术产品和中间件，例如 redis，elasticSearch，activeMq，Dubbo 等； 能够带领开发小组独立完成产品功能的模块设计和研发； 熟悉面向服务的开发，有大型互联网项目的开发 / 设计经验优先； 较强的上进心和求知欲，善于学习和运用新知识，善于沟通和逻辑表达，有强烈的团队意识和执行力。 没找到特别适合本科生的描述，简单概括下这类公司，按照招聘要求来说吧。对计算机专业做要求，说明希望应聘者的专业素质有所保障，懂得基本的操作系统原理，数据结构，编译原理… 因为这些都是本科期间必学的。对 core java 有掌握，说明是要招 java 岗位，基础必须牢固。前端知识有所了解，说明要懂得如果跟前端人员交互，不是完全的服务端开发设计模式和架构，说明不是要招只能够写增删改查的业务人员，更希望是那种能驱动团队的人才一系列中间件的要求说明企业比较正规，跟的上互联网的步伐，通常这类公司的技术总监是比较厉害的，发展前景不错dubbo 一出来，说明该公司还是搞得分布式框架，微服务架构，对程序员的要求更上了一个档次 综合来看，具备以上素质的人当然配得上高一点的工资。 简历简历不要弄虚作假，什么东西是自己做的，什么东西不是自己做的，面试官一句话就能问出来。我面试过的很多人把自己的项目技能写的天花乱坠，随便问一个东西，都不能说个所以然出来，你还写了干嘛，徒增尴尬。 简历不要写与应聘岗位相差太大的描述，如果写了，也要能自圆其说，为什么体现出了自己的才能。我看过一个应聘 JAVA 后端的“人才”写着有普通话证书，来，我现场让你说一段绕口令？还有诸如“参加 XXX 比赛，虽然没得奖，但是自己得到了锻炼”之类的话，真的有必要写在简历上面吗？ 真是没得写的，可以说一说自己大学里面参加的活动体现出怎么样的能力，自己的优异表现，学分绩点，专业课程知识等等。要是实在一无可写… 算了，那还是写普通话证书吧。 有项目经验，比赛经历，专业技能证书，英语考级证书的务必要写上（排名分先后）。都是应届生吗，注意一些技巧，如果你其他方面很突出，但是英语不行，只过了 4 级，那就别写英语 4 级了，因为会暴露你没有过 6 级。用其他证书掩盖过去。这不是欺骗，而是扬长避短。 简历得体大方，模板到处有，关于应届生求职简历的事，可以到知乎好好看看。 公司的诉求普通公司找人，一是看人的基础水平符不符合岗位需求，二是看人的素质符不符合团队的理念，再者就是追求一个性价比。 不是说你能力够了我就要招你，有些时候，公司就是要招基础的业务人员，你技术太厉害，要价太高，完全没必要招你。一个公司的垂直分层，必然是金字塔结构。所以讲究一个对号入座，搞清楚自己的能力，搞清楚自己想要什么样的一份岗位，投简历之前好好看看岗位的描述，公司的诉求。 我面了前前后后也快 30 多个人了，有很多培训班出来的非科班生，很多应届或者一年经验的人，985/211 也有，工作了 12 年的人也有，说实话，能力也就这样，能力很强的人要么出国了，要么内推进了名企，我就只能从我接触到的这些人，说出一些看法。资历在我看来不是很重要，仅仅作为一个参考的位面，好几个工作了 3-4 年的人我感觉好不如咱们应届生，不追求技术的突破，一直干着增删改查操作，问一些 JAVA 基础性的知识又一无所知，要价有得太低，体现出对自己的不自信，有得太高，不清楚自己的定位，入职率很低。再加上现在公司都是对分布式架构的开发，需要的从业者的素质越来越高。整个互联网的趋势也是如此，没有什么人是突然就变得很厉害的，我司技术总监拥有着这么厉害的技术，在我所知也是靠着毕业后依旧数年如一日的对技术的热忱追求。所以，特别是 IT 互联网行业，更希望找到的，是有一颗学习的心，具备终身学习能力的人，以应对日新月异的互联网技术变更。 最后大多数人还是需要有自己的思考，此文谨代表个人看法供大家参考。","link":"/fresh-seek-job/"},{"title":"JAVA 拾遗 --Future 模式与 Promise 模式","text":"写这篇文章的动机，是缘起于微信闲聊群的一场讨论，粗略整理下，主要涉及了以下几个具体的问题： 同步，异步，阻塞，非阻塞的关联及区别。 JAVA 中有 callback 调用吗？ jdk 包中的 Future 怎么用？ Future 模式和 Promise 模式是包含的关系，还是交集的关系，还是没有关系？ 带着上面这些疑问，来看看我到底要拾遗些啥。 浅析同步，异步，阻塞，非阻塞这几个概念一直困扰着我，说实话我现在依旧不能从一个很深的层次去和一个小白解释，这几个概念到底有什么区别。本节我不掺杂自己的描述，主要列出几个我学习过程中认为不错的点，分享给大家，以供诸位理解。 翻看知乎高赞答案，『怎样理解阻塞非阻塞与同步异步的区别？』 文章从『消息通信机制』和『程序在等待调用结果时的状态』两个方面来区分这两组概念，并举例说明了理解他们的方式。但我相信很多人会有跟我一样的感觉，例子看的时候都觉得自己懂了，但要从理论上的层面去解释，又会觉得词穷。以至于一探讨到这四个概念，大家都开始了举例子大会。 正确理解这四个概念，有很多前置条件，比如得框定上下文，Linux 中的 network IO 具有“同步，异步，阻塞，非阻塞”这些概念，而 JAVA 相关框架以及原生 jdk 也涉及这些概念（比如 socket，netty），他们具有很多的相似性，但概念又不尽相同，这也是导致这几个概念难以被理解的原因。从 Linux 层面来理解这几个概念的区别，我也找到一篇不错的文章：『IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）』 如果想要从 JAVA 的角度来理解这四个概念，就必须对 IO 模型有所了解，首先明晰如下的概念：Java 对于 IO 的封装分为 BIO、NIO 和 AIO。Java 目前并不支持异步 IO，BIO 对应的是阻塞同步 IO，NIO 和 AIO 对应的都是非阻塞同步 IO。特别是最后一点有不少文章会曲解，认为 AIO 是异步 IO。细致的讲解可以参考张亮大神的这篇文章：https://mp.weixin.qq.com/s/uDgueoMIEjl-HCE_fcSmSw 同步调用模式我个人认为，一味地想要搞清楚上述这四个知识点，对我们理解方法调用模式并不会有太大帮助。我们来看看下面的这个比较简单的例子。 1234567891011121314151617181920212223public class SyncDemo { public static void main(String[] args) throws InterruptedException { long l = System.currentTimeMillis(); int i = syncCalculate(); System.out.println(&quot;计算结果:&quot; + i); System.out.println(&quot;主线程运算耗时:&quot; + (System.currentTimeMillis() - l)+ &quot;ms&quot;); } // 最常用的同步调用 static int syncCalculate() { System.out.println(&quot;执行耗时操作...&quot;); timeConsumingOperation(); return 100; } static void timeConsumingOperation() { try { Thread.sleep(3000); } catch (Exception e) { e.printStackTrace(); } }} 控制台输出： 1执行耗时操作... 计算结果:100 主线程运算耗时:3000 ms 同步调用模式是我们最最最常用的方式，如果是业务开发，几乎 99% 的方法是同步方法。再回到一开始的纠结点：是同步调用还是异步调用，毫无疑问是同步调用；是阻塞还是非阻塞？其实压根就不涉及到这个问题，说是阻塞也没毛病，syncCalculate 方法阻塞了主线程，但我们通常不会讨论这里是阻塞还是非阻塞。 Future 模式上述的例子是较为简单的引子，本节将会介绍 JAVA 中的 Future 模式。上述的 syncCalculate 方法是一个耗时的操作，为了优化性能，我们可以考虑使用 Future 模式。Future 模式相当于一个占位符，代表一个操作的未来的结果，其简单的概念不在本文中介绍，直接给出总结：Future 模式可以细分为将来式和回调式两种模式。 Future 模式 – 将来式 1 12345678910111213141516171819202122232425public class FutureDemo1 { public static void main(String[] args) throws ExecutionException, InterruptedException { long l = System.currentTimeMillis(); ExecutorService executorService = Executors.newSingleThreadExecutor(); Future&lt;Integer&gt; future = executorService.submit(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { System.out.println(&quot;执行耗时操作...&quot;); timeConsumingOperation(); return 100; } }); //&lt;1&gt; // 其他耗时操作..&lt;3&gt; System.out.println(&quot;计算结果:&quot; + future.get());//&lt;2&gt; System.out.println(&quot;主线程运算耗时:&quot; + (System.currentTimeMillis() - l)+ &quot;ms&quot;); } static void timeConsumingOperation() { try { Thread.sleep(3000); } catch (Exception e) { e.printStackTrace(); } }} 控制台输出： 1执行耗时操作... 计算结果:100 主线程运算耗时:3007 ms &lt;1&gt; 将回调接口交给线程池去执行，这一步是非阻塞的，返回了一个运算结果的占位符 –future。 &lt;2&gt; 在这一步中我们调用了 future 的 get 方法，那么如果 future 的计算还未完成，主线程将会被这一步阻塞。 &lt;3&gt; 我们观察一下控制台的输出，发现依旧耗费 3s 来完成这次耗时操作，并没有比同步调用方式快。但是提交任务（非阻塞）和获取结果（阻塞）之间我们可以进行一些额外的操作，而这将形成一个并行执行的效果。 我们会发现如果 future 提交给线程池执行之后立刻 get()，其实执行效率并不会变高，反而由于线程的开销会比同步调用更慢。这种将来式的 future 适用多个耗时操作并发执行的场景。 Future 模式 – 将来式 2 除了这个阻塞式的 get() 获取结果，jdk 的 future 还提供了非阻塞式的方式用来获取 future 的结果。查看 jdk 中 Future 的定义： 1234567891011public interface Future&lt;V&gt; { boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;} 其中可以通过轮询 isDone()方法来达到非阻塞式获取结果的效果。但个人认为与阻塞式的 get() 并没有什么差异，实际项目中也没有需要使用非阻塞式的场景。 **Future 模式 – 回调式 ** 写过前端 ajax 代码的朋友对 callback 的写法并不会陌生，而 Future 模式的第二种用法便是回调。很不幸的事，jdk 实现的 Future 并没有实现 callback,addListener 这样的方法，想要在 JAVA 中体验到 callback 的特性，得引入一些额外的框架。 ** 回调式实现一 –Netty** Netty 除了是一个高性能的网络通信框架之外，还对 jdk 的 Future 做了扩展，翻看其文档 http://netty.io/wiki/using-as-a-generic-library.html#wiki-h2-5 可以发现其扩展了一个 listener 接口。 引入 Netty 的 maven 依赖 12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.22.Final&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930public class NettyFutureDemo { public static void main(String[] args) throws InterruptedException { long l = System.currentTimeMillis(); EventExecutorGroup group = new DefaultEventExecutorGroup(4); Future&lt;Integer&gt; f = group.submit(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { System.out.println(&quot;执行耗时操作...&quot;); timeConsumingOperation(); return 100; } }); f.addListener(new FutureListener&lt;Object&gt;() { @Override public void operationComplete(Future&lt;Object&gt; objectFuture) throws Exception { System.out.println(&quot;计算结果:：&quot; + objectFuture.get()); } }); System.out.println(&quot;主线程运算耗时:&quot; + (System.currentTimeMillis() - l)+ &quot;ms&quot;); new CountDownLatch(1).await(); } static void timeConsumingOperation() { try { Thread.sleep(3000); } catch (Exception e) { e.printStackTrace(); } }} 控制台输出： 1主线程运算耗时:329 ms 执行耗时操作... 计算结果:：100 结果分析：使用了 addListener 这样的方法为一个 future 结果添加回调，从而达到“当耗时操作完成后，自行触发钩子去执行打印操作”的效果。细心的朋友会发现，主线程只耗费了不到 1s 的时间，整个过程没有被耗时操作阻塞，这才是异步编程的推荐方式：回调。 ** 回调式实现二 –Guava** 不仅仅 Netty 想到了这一点，google 提供的扩展包 Guava 也为回调式的 Future 提供了实现，其核心接口为 引入 Guava 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;21.0&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223242526272829303132public class GuavaFutureDemo { public static void main(String[] args) throws InterruptedException { long l = System.currentTimeMillis(); ListeningExecutorService service = MoreExecutors.listeningDecorator(Executors.newSingleThreadExecutor()); ListenableFuture&lt;Integer&gt; future = service.submit(new Callable&lt;Integer&gt;() { public Integer call() throws Exception { System.out.println(&quot;执行耗时操作...&quot;); timeConsumingOperation(); return 100; } });//&lt;1&gt; Futures.addCallback(future, new FutureCallback&lt;Integer&gt;() { public void onSuccess(Integer result) { System.out.println(&quot;计算结果:&quot; + result); } public void onFailure(Throwable throwable) { System.out.println(&quot;异步处理失败,e=&quot; + throwable); } });//&lt;2&gt; System.out.println(&quot;主线程运算耗时:&quot; + (System.currentTimeMillis() - l)+ &quot;ms&quot;); new CountDownLatch(1).await(); } static void timeConsumingOperation() { try { Thread.sleep(3000); } catch (Exception e) { e.printStackTrace(); } }} 控制台输出： 1执行耗时操作... 主线程运算耗时:65 ms 计算结果:100 结果分析：几乎和 Netty 的异步回调效果一样，在这儿顺便补充一下之前我自己学习时的一个疑惑：我一直会担心一个问题，由于 &lt;1&gt; 处的执行是异步，会不会存在一种特殊情况，即 future 结果已经计算好了，但 &lt;2&gt; 操作添加监听器还未执行完成，会导致接收不到回调。实际上后来翻阅了一些资料，这么写是没问题的，无论是在何时 addListener，都可以接收到异步回调。 由 Callback Hell 引出 Promise 模式同样的如果你对 ES6 有所接触，就不会对 Promise 这个模式感到陌生，如果你对前端不熟悉，也不要紧，我们先来看看回调地狱（Callback Hell）是个什么概念。 回调是一种我们推崇的异步调用方式，但也会遇到问题，也就是回调的嵌套。当需要多个异步回调一起书写时，就会出现下面的代码 (以 js 为例): 123456789asyncFunc1(opt, (...args1) =&gt; { asyncFunc2(opt, (...args2) =&gt; { asyncFunc3(opt, (...args3) =&gt; { asyncFunc4(opt, (...args4) =&gt; { // some operation }); }); });}); 虽然在 JAVA 业务代码中很少出现回调的多层嵌套（至少我目前的业务没有接触过），但总归是个问题，这样的代码不易读，嵌套太深修改也麻烦。于是 ES6 提出了 Promise 模式来解决回调地狱的问题。由于我的博客主要还是面向于 JAVA 读者，就不介绍 JavaScript 中的 Promise 用法了。可能就会有人想问：java 中存在 Promise 模式吗？答案是肯定的。 前面提到了 Netty 和 Guava 的扩展都提供了 addListener 这样的接口，用于处理 Callback 调用，但其实 jdk1.8 已经提供了一种更为高级的回调方式：CompletableFuture。首先尝试用 CompletableFuture 来解决回调的问题。 1234567891011121314151617181920212223public class CompletableFutureDemo { public static void main(String[] args) throws InterruptedException { long l = System.currentTimeMillis(); CompletableFuture&lt;Integer&gt; completableFuture = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;执行耗时操作...&quot;); timeConsumingOperation(); return 100; }); completableFuture.whenComplete((result, e) -&gt; { System.out.println(&quot;结果：&quot; + result); }); System.out.println(&quot;主线程运算耗时:&quot; + (System.currentTimeMillis() - l)+ &quot;ms&quot;); new CountDownLatch(1).await(); } static void timeConsumingOperation() { try { Thread.sleep(3000); } catch (Exception e) { e.printStackTrace(); } }} 控制台输出： 1执行耗时操作... 主线程运算耗时:55 ms 结果：100 可以发现耗时操作没有占用主线程的时间片，达到了异步调用的效果。我们也不需要引入任何第三方的依赖，这都是依赖于 java.util.concurrent.CompletableFuture 的出现。CompletableFuture 提供了近 50 多个方法，大大便捷了 java 多线程操作，和异步调用的写法。 使用 CompletableFuture 解决回调地狱问题： 123456789101112131415161718192021222324252627282930public class CompletableFutureDemo { public static void main(String[] args) throws InterruptedException { long l = System.currentTimeMillis(); CompletableFuture&lt;Integer&gt; completableFuture = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;在回调中执行耗时操作...&quot;); timeConsumingOperation(); return 100; }); completableFuture = completableFuture.thenCompose(i -&gt; { return CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;在回调的回调中执行耗时操作...&quot;); timeConsumingOperation(); return i + 100; }); });//&lt;1&gt; completableFuture.whenComplete((result, e) -&gt; { System.out.println(&quot;计算结果:&quot; + result); }); System.out.println(&quot;主线程运算耗时:&quot; + (System.currentTimeMillis() - l)+ &quot;ms&quot;); new CountDownLatch(1).await(); } static void timeConsumingOperation() { try { Thread.sleep(3000); } catch (Exception e) { e.printStackTrace(); } }} 控制台输出： 1在回调中执行耗时操作... 主线程运算耗时:63 ms 在回调的回调中执行耗时操作... 计算结果:200 &lt;1&gt; 使用 thenCompose 或者 thenComposeAsync 等方法可以实现回调的回调，且写出来的方法易于维护。 总结同步，异步，阻塞，非阻塞的理解需要花费很大的精力，从 IO 模型和内核进行深入地理解，才能分清区别。在日常开发中往往没必要过于纠结到底是何种调用，但得对调用的特性有所了解，比如是否占用主线程的时间片，出现异常怎么捕获，超时怎么解决等等（后面这些本文未介绍）。 Future 有两种模式：将来式和回调式。而回调式会出现回调地狱的问题，由此衍生出了 Promise 模式来解决这个问题。这才是 Future 模式和 Promise 模式的相关性。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/future-and-promise/"},{"title":"研究优雅停机时的一点思考","text":"开头先废话几句，有段时间没有更新博客了，除了公司项目比较忙之外，还有个原因就是开始思考如何更好地写作。远的来说，我从大一便开始在 CSDN 上写博客，回头看那时的文笔还很稚嫩，一心想着反正只有自己看，所以更多的是随性发挥，随意吐槽，内容也很简陋：刷完一道算法题记录下解题思路，用 JAVA 写完一个 demo 之后，记录下配置步骤。近的来看，工作之后开始维护自己的博客站点: www.cnkirito.moe 也会同步更新自己公众号。相比圈子里其他前辈来说，读者会少很多，但毕竟有人看，每次动笔之前便会开始思考一些事。除了给自己的学习经历做一个归档，还多了一些顾虑：会不会把知识点写错？会不会误人子弟？自己的理解会不会比较片面，不够深刻？等等等等。但自己的心路历程真的发生了一些改变。在我还是个小白的时候，学习技术：第一个想法是百度，搜别人的博客，一步步跟着别人后面配置，把 demo run 起来。而现在，遇到问题的第一思路变成了：源码 debug，官方文档。我便开始思考官方文档和博客的区别，官方文档的优势除了更加全面之外，还有就是：“它只教你怎么做”，对于一个有经验有阅历的程序员来说，这反而是好事，这可以让你有自己的思考。而博客则不一样，如果这个博主特别爱 BB，便会产生很多废话（就像本文的第一段），它会有很多作者自己思考的产物，一方面它比官方文档更容易出错，更容易片面，一方面它比官方文档更容易启发人，特别是读到触动到我的好文时，会抑制不住内心的喜悦想要加到作者的好友，这便是共情。我之后的文章也会朝着这些点去努力：不避重就轻，多思考不想当然，求精。 最近瞥了一眼项目的重启脚本，发现运维一直在使用 kill -9 &lt;pid&gt; 的方式重启 springboot embedded tomcat，其实大家几乎一致认为：kill -9 &lt;pid&gt; 的方式比较暴力，但究竟会带来什么问题却很少有人能分析出个头绪。这篇文章主要记录下自己的思考过程。 kill -9 和 kill -15 有什么区别？在以前，我们发布 WEB 应用通常的步骤是将代码打成 war 包，然后丢到一个配置好了应用容器（如 Tomcat，Weblogic）的 Linux 机器上，这时候我们想要启动 / 关闭应用，方式很简单，运行其中的启动 / 关闭脚本即可。而 springboot 提供了另一种方式，将整个应用连同内置的 tomcat 服务器一起打包，这无疑给发布应用带来了很大的便捷性，与之而来也产生了一个问题：如何关闭 springboot 应用呢？一个显而易见的做法便是，根据应用名找到进程 id，杀死进程 id 即可达到关闭应用的效果。 上述的场景描述引出了我的疑问：怎么优雅地杀死一个 springboot 应用进程呢？这里仅仅以最常用的 Linux 操作系统为例，在 Linux 中 kill 指令负责杀死进程，其后可以紧跟一个数字，代表 ** 信号编号 **(Signal)，执行 kill -l 指令，可以一览所有的信号编号。 12xu@ntzyz-qcloud ~ % kill -l HUP INT QUIT ILL TRAP ABRT BUS FPE KILL USR1 SEGV USR2 PIPE ALRM TERM STKFLT CHLD CONT STOP TSTP TTIN TTOU URG XCPU XFSZ VTALRM PROF WINCH POLL PWR SYS 本文主要介绍下第 9 个信号编码 KILL，以及第 15 个信号编号 TERM 。 先简单理解下这两者的区别：kill -9 pid 可以理解为操作系统从内核级别强行杀死某个进程，kill -15 pid 则可以理解为发送一个通知，告知应用主动关闭。这么对比还是有点抽象，那我们就从应用的表现来看看，这两个命令杀死应用到底有啥区别。 ** 代码准备 ** 由于笔者 springboot 接触较多，所以以一个简易的 springboot 应用为例展开讨论，添加如下代码。 1 增加一个实现了 DisposableBean 接口的类 1234567@Componentpublic class TestDisposableBean implements DisposableBean{ @Override public void destroy() throws Exception { System.out.println(&quot;测试 Bean 已销毁 ...&quot;); }} 2 增加 JVM 关闭时的钩子 1234567891011121314@SpringBootApplication@RestControllerpublic class TestShutdownApplication implements DisposableBean { public static void main(String[] args) { SpringApplication.run(TestShutdownApplication.class, args); Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() { @Override public void run() { System.out.println(&quot;执行 ShutdownHook ...&quot;); } })); }} ** 测试步骤 ** 执行 java -jar test-shutdown-1.0.jar 将应用运行起来 测试 kill -9 pid，kill -15 pid，ctrl + c 后输出日志内容 ** 测试结果 ** kill -15 pid &amp; ctrl + c，效果一样，输出结果如下 123452018-01-14 16:55:32.424 INFO 8762 --- [Thread-3] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2cdf8d8a: startup date [Sun Jan 14 16:55:24 UTC 2018]; root of context hierarchy2018-01-14 16:55:32.432 INFO 8762 --- [Thread-3] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown执行 ShutdownHook ...测试 Bean 已销毁 ...java -jar test-shutdown-1.0.jar 7.46s user 0.30s system 80% cpu 9.674 total kill -9 pid，没有输出任何应用日志 12[1] 8802 killed java -jar test-shutdown-1.0.jarjava -jar test-shutdown-1.0.jar 7.74s user 0.25s system 41% cpu 19.272 total 可以发现，kill -9 pid 是给应用杀了个措手不及，没有留给应用任何反应的机会。而反观 kill -15 pid，则比较优雅，先是由 AnnotationConfigEmbeddedWebApplicationContext （一个 ApplicationContext 的实现类）收到了通知，紧接着执行了测试代码中的 Shutdown Hook，最后执行了 DisposableBean#destory() 方法。孰优孰劣，立判高下。 一般我们会在应用关闭时处理一下“善后”的逻辑，比如 关闭 socket 链接 清理临时文件 发送消息通知给订阅方，告知自己下线 将自己将要被销毁的消息通知给子进程 各种资源的释放 等等 而 kill -9 pid 则是直接模拟了一次系统宕机，系统断电，这对于应用来说太不友好了，不要用收割机来修剪花盆里的花。取而代之，便是使用 kill -15 pid 来代替。如果在某次实际操作中发现：kill -15 pid 无法关闭应用，则可以考虑使用内核级别的 kill -9 pid ，但请事后务必排查出是什么原因导致 kill -15 pid 无法关闭。 springboot 如何处理 -15 TERM Signal上面解释过了，使用 kill -15 pid 的方式可以比较优雅的关闭 springboot 应用，我们可能有以下的疑惑： springboot/spring 是如何响应这一关闭行为的呢？是先关闭了 tomcat，紧接着退出 JVM，还是相反的次序？它们又是如何互相关联的？ 尝试从日志开始着手分析，AnnotationConfigEmbeddedWebApplicationContext 打印出了 Closing 的行为，直接去源码中一探究竟，最终在其父类 AbstractApplicationContext 中找到了关键的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Overridepublic void registerShutdownHook() { if (this.shutdownHook == null) { this.shutdownHook = new Thread() { @Override public void run() { synchronized (startupShutdownMonitor) { doClose(); } } }; Runtime.getRuntime().addShutdownHook(this.shutdownHook); }}@Overridepublic void close() { synchronized (this.startupShutdownMonitor) { doClose(); if (this.shutdownHook != null) { Runtime.getRuntime().removeShutdownHook(this.shutdownHook); } }}protected void doClose() { if (this.active.get() &amp;&amp; this.closed.compareAndSet(false, true)) { LiveBeansView.unregisterApplicationContext(this); // 发布应用内的关闭事件 publishEvent(new ContextClosedEvent(this)); // Stop all Lifecycle beans, to avoid delays during individual destruction. if (this.lifecycleProcessor != null) { this.lifecycleProcessor.onClose(); } // spring 的 BeanFactory 可能会缓存单例的 Bean destroyBeans(); // 关闭应用上下文 &amp;BeanFactory closeBeanFactory(); // 执行子类的关闭逻辑 onClose(); this.active.set(false); }} 为了方便排版以及便于理解，我去除了源码中的部分异常处理代码，并添加了相关的注释。在容器初始化时，ApplicationContext 便已经注册了一个 Shutdown Hook，这个钩子调用了 Close()方法，于是当我们执行 kill -15 pid 时，JVM 接收到关闭指令，触发了这个 Shutdown Hook，进而由 Close() 方法去处理一些善后手段。具体的善后手段有哪些，则完全依赖于 ApplicationContext 的 doClose() 逻辑，包括了注释中提及的销毁缓存单例对象，发布 close 事件，关闭应用上下文等等，特别的，当 ApplicationContext 的实现类是 AnnotationConfigEmbeddedWebApplicationContext 时，还会处理一些 tomcat/jetty 一类内置应用服务器关闭的逻辑。 窥见了 springboot 内部的这些细节，更加应该了解到优雅关闭应用的必要性。JAVA 和 C 都提供了对 Signal 的封装，我们也可以手动捕获操作系统的这些 Signal，在此不做过多介绍，有兴趣的朋友可以自己尝试捕获下。 还有其他优雅关闭应用的方式吗？spring-boot-starter-actuator 模块提供了一个 restful 接口，用于优雅停机。 ** 添加依赖 ** 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; ** 添加配置 ** 1234#启用 shutdownendpoints.shutdown.enabled=true#禁用密码验证endpoints.shutdown.sensitive=false 生产中请注意该端口需要设置权限，如配合 spring-security 使用。 执行 curl -X POST host:port/shutdown 指令，关闭成功便可以获得如下的返回： 1{&quot;message&quot;:&quot;Shutting down, bye...&quot;} 虽然 springboot 提供了这样的方式，但按我目前的了解，没见到有人用这种方式停机，kill -15 pid 的方式达到的效果与此相同，将其列于此处只是为了方案的完整性。 如何销毁作为成员变量的线程池？尽管 JVM 关闭时会帮我们回收一定的资源，但一些服务如果大量使用异步回调，定时任务，处理不当很有可能会导致业务出现问题，在这其中，线程池如何关闭是一个比较典型的问题。 123456789101112@Servicepublic class SomeService { ExecutorService executorService = Executors.newFixedThreadPool(10); public void concurrentExecute() { executorService.execute(new Runnable() { @Override public void run() { System.out.println(&quot;executed...&quot;); } }); }} 我们需要想办法在应用关闭时（JVM 关闭，容器停止运行），关闭线程池。 初始方案：什么都不做。在一般情况下，这不会有什么大问题，因为 JVM 关闭，会释放之，但显然没有做到本文一直在强调的两个字，没错 —- 优雅。 方法一的弊端在于线程池中提交的任务以及阻塞队列中未执行的任务变得极其不可控，接收到停机指令后是立刻退出？还是等待任务执行完成？抑或是等待一定时间任务还没执行完成则关闭？ 方案改进： 发现初始方案的劣势后，我立刻想到了使用 DisposableBean 接口，像这样： 1234567891011121314151617181920@Servicepublic class SomeService implements DisposableBean{ ExecutorService executorService = Executors.newFixedThreadPool(10); public void concurrentExecute() { executorService.execute(new Runnable() { @Override public void run() { System.out.println(&quot;executed...&quot;); } }); } @Override public void destroy() throws Exception { executorService.shutdownNow(); //executorService.shutdown(); }} 紧接着问题又来了，是 shutdown 还是 shutdownNow 呢？这两个方法还是经常被误用的，简单对比这两个方法。 ThreadPoolExecutor 在 shutdown 之后会变成 SHUTDOWN 状态，无法接受新的任务，随后等待正在执行的任务执行完成。意味着，shutdown 只是发出一个命令，至于有没有关闭还是得看线程自己。 ThreadPoolExecutor 对于 shutdownNow 的处理则不太一样，方法执行之后变成 STOP 状态，并对执行中的线程调用 Thread.interrupt() 方法（但如果线程未处理中断，则不会有任何事发生），所以并不代表“立刻关闭”。 查看 shutdown 和 shutdownNow 的 java doc，会发现如下的提示： shutdown()：Initiates an orderly shutdown in which previously submitted tasks are executed, but no new tasks will be accepted.Invocation has no additional effect if already shut down.This method does not wait for previously submitted tasks to complete execution.Use {@link #awaitTermination awaitTermination} to do that. shutdownNow()：Attempts to stop all actively executing tasks, halts the processing of waiting tasks, and returns a list of the tasks that were awaiting execution. These tasks are drained (removed) from the task queue upon return from this method.This method does not wait for actively executing tasks to terminate. Use {@link #awaitTermination awaitTermination} to do that.There are no guarantees beyond best-effort attempts to stop processing actively executing tasks. This implementation cancels tasks via {@link Thread#interrupt}, so any task that fails to respond to interrupts may never terminate. 两者都提示我们需要额外执行 awaitTermination 方法，仅仅执行 shutdown/shutdownNow 是不够的。 最终方案：参考 spring 中线程池的回收策略，我们得到了最终的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738public abstract class ExecutorConfigurationSupport extends CustomizableThreadFactory implements DisposableBean{ @Override public void destroy() { shutdown(); } /** * Perform a shutdown on the underlying ExecutorService. * @see java.util.concurrent.ExecutorService#shutdown() * @see java.util.concurrent.ExecutorService#shutdownNow() * @see #awaitTerminationIfNecessary() */ public void shutdown() { if (this.waitForTasksToCompleteOnShutdown) { this.executor.shutdown(); } else { this.executor.shutdownNow(); } awaitTerminationIfNecessary(); } /** * Wait for the executor to terminate, according to the value of the * {@link #setAwaitTerminationSeconds &quot;awaitTerminationSeconds&quot;} property. */ private void awaitTerminationIfNecessary() { if (this.awaitTerminationSeconds &gt; 0) { try { this.executor.awaitTermination(this.awaitTerminationSeconds, TimeUnit.SECONDS)); } catch (InterruptedException ex) { Thread.currentThread().interrupt(); } } }} 保留了注释，去除了一些日志代码，一个优雅关闭线程池的方案呈现在我们的眼前。 1 通过 waitForTasksToCompleteOnShutdown 标志来控制是想立刻终止所有任务，还是等待任务执行完成后退出。 2 executor.awaitTermination(this.awaitTerminationSeconds, TimeUnit.SECONDS)); 控制等待的时间，防止任务无限期的运行（前面已经强调过了，即使是 shutdownNow 也不能保证线程一定停止运行）。 更多需要我们的思考的优雅停机策略在我们分析 RPC 原理的系列文章里面曾经提到，服务治理框架一般会考虑到优雅停机的问题。通常的做法是事先隔断流量，接着关闭应用。常见的做法是将服务节点从注册中心摘除，订阅者接收通知，移除节点，从而优雅停机；涉及到数据库操作，则可以使用事务的 ACID 特性来保证即使 crash 停机也能保证不出现异常数据，正常下线则更不用说了；又比如消息队列可以依靠 ACK 机制 + 消息持久化，或者是事务消息保障；定时任务较多的服务，处理下线则特别需要注意优雅停机的问题，因为这是一个长时间运行的服务，比其他情况更容易受停机问题的影响，可以使用幂等和标志位的方式来设计定时任务… 事务和 ACK 这类特性的支持，即使是宕机，停电，kill -9 pid 等情况，也可以使服务尽量可靠；而同样需要我们思考的还有 kill -15 pid，正常下线等情况下的停机策略。最后再补充下整理这个问题时，自己对 jvm shutdown hook 的一些理解。 When the virtual machine begins its shutdown sequence it will start all registered shutdown hooks in some unspecified order and let them run concurrently. When all the hooks have finished it will then run all uninvoked finalizers if finalization-on-exit has been enabled. Finally, the virtual machine will halt. shutdown hook 会保证 JVM 一直运行，知道 hook 终止 (terminated)。这也启示我们，如果接收到 kill -15 pid 命令时，执行阻塞操作，可以做到等待任务执行完成之后再关闭 JVM。同时，也解释了一些应用执行 kill -15 pid 无法退出的问题，没错，中断被阻塞了。 参考资料 [1] https://stackoverflow.com/questions/2921945/useful-example-of-a-shutdown-hook-in-java [2] spring 源码 [3] jdk 文档","link":"/gracefully-shutdown/"},{"title":"Guava Cache 使用小结","text":"闲聊话说原创文章已经断更 2 个月了，倒也不是因为忙，主要还是懒。但是也感觉可以拿出来跟大家分享的技术点越来越少了，一方面主要是最近在从事一些“内部项目”的研发，纵使我很想分享，也没法搬到公众号 &amp; 博客上来；一方面是一些我并不是很擅长的技术点，在我还是新手时，我敢于去写，而有了一定工作年限之后，反而有些包袱了，我的读者会不会介意呢？思来想去，我回忆起了写作的初心，不就是为了记录自己的学习过程吗？于是乎，我还是按照我之前的文风记录下了此文，以避免成为一名断更的博主。 以下是正文。 前言“缓存”一直是我们程序员聊的最多的那一类技术点，诸如 Redis、Encache、Guava Cache，你至少会听说过一个。需要承认的是，无论是面试八股文的风气，还是实际使用的频繁度，Redis 分布式缓存的确是当下最为流行的缓存技术，但同时，从我个人的项目经验来看，本地缓存也是非常常用的一个技术点。 分析 Redis 缓存的文章很多，例如 Redis 雪崩、Redis 过期机制等等，诸如此类的公众号标题不鲜出现在我朋友圈的 timeline 中，但是分析本地缓存的文章在我的映像中很少。 在最近的项目中，有一位新人同事使用了 Guava Cache 来对一个 RPC 接口的响应进行缓存，我在 review 其代码时恰好发现了一个不太合理的写法，遂有此文。 本文将会介绍 Guava Cache 的一些常用操作：基础 API 使用，过期策略，刷新策略。并且按照我的写作习惯，会附带上实际开发中的一些总结。需要事先说明的是，我没有阅读过 Guava Cache 的源码，对其的介绍仅仅是一些使用经验或者最佳实践，不会有过多深入的解析。 先简单介绍一下 Guava Cache，它是 Google 封装的基础工具包 guava 中的一个内存缓存模块，它主要提供了以下能力： 封装了缓存与数据源交互的流程，使得开发更关注于业务操作 提供线程安全的存取操作（可以类比 ConcurrentHashMap） 提供常用的缓存过期策略，缓存刷新策略 提供缓存命中率的监控 基础使用使用一个示例介绍 Guava Cache 的基础使用方法 – 缓存大小写转换的返回值。 12345678910111213141516171819private String fetchValueFromServer(String key) { return key.toUpperCase();}@Testpublic void whenCacheMiss_thenFetchValueFromServer() throws ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); assertEquals(0, cache.size()); assertEquals(&quot;HELLO&quot;, cache.getUnchecked(&quot;hello&quot;)); assertEquals(&quot;HELLO&quot;, cache.get(&quot;hello&quot;)); assertEquals(1, cache.size());} 使用 Guava Cache 的好处已经跃然于纸上了，它解耦了缓存存取与业务操作。CacheLoader 的 load 方法可以理解为从数据源加载原始数据的入口，当调用 LoadingCache 的 getUnchecked 或者 get方法时，Guava Cache 行为如下： 缓存未命中时，同步调用 load 接口，加载进缓存，返回缓存值 缓存命中，直接返回缓存值 多线程缓存未命中时，A 线程 load 时，会阻塞 B 线程的请求，直到缓存加载完毕 注意到，Guava 提供了两个 getUnchecked 或者 get 加载方法，没有太大的区别，无论使用哪一个，都需要注意，数据源无论是 RPC 接口的返回值还是数据库，都要考虑访问超时或者失败的情况，做好异常处理。 预加载缓存预加载缓存的常见使用场景： 老生常谈的秒杀场景，事先缓存预热，将热点商品加入缓存； 系统重启过后，事先加载好缓存，避免真实请求击穿缓存 Guava Cache 提供了 put 和 putAll 方法 123456789101112131415@Testpublic void whenPreloadCache_thenPut() { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); String key = &quot;kirito&quot;; cache.put(key,fetchValueFromServer(key)); assertEquals(1, cache.size());} 操作和 HashMap 一模一样。 这里有一个误区，而那位新人同事恰好踩到了，也是我写这篇文章的初衷，请务必仅在预加载缓存这个场景使用 put，其他任何场景都应该使用 load 去触发加载缓存。看下面这个反面示例： 123456789101112131415161718192021// 注意这是一个反面示例@Testpublic void wrong_usage_whenCacheMiss_thenPut() throws ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return &quot;&quot;; } }); String key = &quot;kirito&quot;; String cacheValue = cache.get(key); if (&quot;&quot;.equals(cacheValue)) { cacheValue = fetchValueFromServer(key); cache.put(key, cacheValue); } cache.put(key, cacheValue); assertEquals(1, cache.size());} 这样的写法，在 load 方法中设置了一个空值，后续通过手动 put + get 的方式使用缓存，这种习惯更像是在操作一个 HashMap，但并不推荐在 Cache 中使用。在前面介绍过 get 配合 load 是由 Guava Cache 去保障了线程安全，保障多个线程访问缓存时，第一个请求加载缓存的同时，阻塞后续请求，这样的 HashMap 用法既不优雅，在极端情况下还会引发缓存击穿、线程安全等问题。 请务必仅仅将 put 方法用作预加载缓存场景。 缓存过期前面的介绍使用起来依旧没有脱离 ConcurrentHashMap 的范畴，Cache 与其的第一个区别在“缓存过期”这个场景可以被体现出来。本节介绍 Guava 一些常见的缓存过期行为及策略。 缓存固定数量的值123456789101112131415161718@Testpublic void whenReachMaxSize_thenEviction() throws ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().maximumSize(3).build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); cache.get(&quot;one&quot;); cache.get(&quot;two&quot;); cache.get(&quot;three&quot;); cache.get(&quot;four&quot;); assertEquals(3, cache.size()); assertNull(cache.getIfPresent(&quot;one&quot;)); assertEquals(&quot;FOUR&quot;, cache.getIfPresent(&quot;four&quot;));} 使用 ConcurrentHashMap 做缓存的一个最大的问题，便是我们没有简易有效的手段阻止其无限增长，而 Guava Cache 可以通过初始化 LoadingCache 的过程，配置 maximumSize ，以确保缓存内容不导致你的系统出现 OOM。 值得注意的是，我这里的测试用例使用的是除了 get 、getUnchecked 外的第三种获取缓存的方式，如字面意思描述的那样，getIfPresent 在缓存不存在时，并不会触发 load 方法加载数据源。 LRU 过期策略依旧沿用上述的示例，我们在设置容量为 3 时，仅获悉 LoadingCache 可以存储 3 个值，却并未得知第 4 个值存入后，哪一个旧值需要淘汰，为新值腾出空位。实际上，Guava Cache 默认采取了 LRU 缓存淘汰策略。Least Recently Used 即最近最少使用，这个算法你可能没有实现过，但一定会听说过，在 Guava Cache 中 Used 的语义代表任意一次访问，例如 put、get。继续看下面的示例。 1234567891011121314151617181920@Testpublic void whenReachMaxSize_thenEviction() throws ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().maximumSize(3).build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); cache.get(&quot;one&quot;); cache.get(&quot;two&quot;); cache.get(&quot;three&quot;); // access one cache.get(&quot;one&quot;); cache.get(&quot;four&quot;); assertEquals(3, cache.size()); assertNull(cache.getIfPresent(&quot;two&quot;)); assertEquals(&quot;ONE&quot;, cache.getIfPresent(&quot;one&quot;));} 注意此示例与上一节示例的区别：第四次 get 访问 one 后，two 变成了最久未被使用的值，当第四个值 four 存入后，淘汰的对象变成了 two，而不再是 one 了。 缓存固定时间为缓存设置过期时间，也是区分 HashMap 和 Cache 的一个重要特性。Guava Cache 提供了expireAfterAccess、 expireAfterWrite 的方案，为 LoadingCache 中的缓存值设置过期时间。 1234567891011121314151617181920@Testpublic void whenEntryIdle_thenEviction() throws InterruptedException, ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().expireAfterAccess(1, TimeUnit.SECONDS).build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); cache.get(&quot;kirito&quot;); assertEquals(1, cache.size()); cache.get(&quot;kirito&quot;); Thread.sleep(2000); assertNull(cache.getIfPresent(&quot;kirito&quot;));} 缓存失效1234567891011121314151617@Testpublic void whenInvalidate_thenGetNull() throws ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder() .build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); String name = cache.get(&quot;kirito&quot;); assertEquals(&quot;KIRITO&quot;, name); cache.invalidate(&quot;kirito&quot;); assertNull(cache.getIfPresent(&quot;kirito&quot;));} 使用 void invalidate(Object key) 移除单个缓存，使用 void invalidateAll() 移除所有缓存。 缓存刷新缓存刷新的常用于使用数据源的新值覆盖缓存旧值，Guava Cache 提供了两类刷新机制：手动刷新和定时刷新。 手动刷新1cache.refresh(&quot;kirito&quot;); refresh 方法将会触发 load 逻辑，尝试从数据源加载缓存。 需要注意点的是，refresh 方法并不会阻塞 get 方法，所以在 refresh 期间，旧的缓存值依旧会被访问到，直到 load 完毕，看下面的示例。 123456789101112131415161718192021222324252627282930313233@Testpublic void whenCacheRefresh_thenLoad() throws InterruptedException, ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().expireAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) throws InterruptedException { Thread.sleep(2000); return key + ThreadLocalRandom.current().nextInt(100); } }); String oldValue = cache.get(&quot;kirito&quot;); new Thread(() -&gt; { cache.refresh(&quot;kirito&quot;); }).start(); // make sure another refresh thread is scheduling Thread.sleep(500); String val1 = cache.get(&quot;kirito&quot;); assertEquals(oldValue, val1); // make sure refresh cache Thread.sleep(2000); String val2 = cache.get(&quot;kirito&quot;); assertNotEquals(oldValue, val2);} 其实任何情况下，缓存值都有可能和数据源出现不一致，业务层面需要做好访问到旧值的容错逻辑。 自动刷新12345678910111213141516@Testpublic void whenTTL_thenRefresh() throws ExecutionException, InterruptedException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().refreshAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return key + ThreadLocalRandom.current().nextInt(100); } }); String first = cache.get(&quot;kirito&quot;); Thread.sleep(1000); String second = cache.get(&quot;kirito&quot;); assertNotEquals(first, second);} 和上节的 refresh 机制一样，refreshAfterWrite 同样不会阻塞 get 线程，依旧有访问旧值的可能性。 缓存命中统计Guava Cache 默认情况不会对命中情况进行统计，需要在构建 CacheBuilder 时显式配置 recordStats。 1234567891011121314151617181920212223@Testpublic void whenRecordStats_thenPrint() throws ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().maximumSize(100).recordStats().build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); cache.get(&quot;one&quot;); cache.get(&quot;two&quot;); cache.get(&quot;three&quot;); cache.get(&quot;four&quot;); cache.get(&quot;one&quot;); cache.get(&quot;four&quot;); CacheStats stats = cache.stats(); System.out.println(stats);}---CacheStats{hitCount=2, missCount=4, loadSuccessCount=4, loadExceptionCount=0, totalLoadTime=1184001, evictionCount=0} 缓存移除的通知机制在一些业务场景中，我们希望对缓存失效进行一些监测，或者是针对失效的缓存做一些回调处理，就可以使用 RemovalNotification 机制。 1234567891011121314151617181920@Testpublic void whenRemoval_thenNotify() throws ExecutionException { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().maximumSize(3) .removalListener( cacheItem -&gt; System.out.println(cacheItem + &quot; is removed, cause by &quot; + cacheItem.getCause())) .build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) { return fetchValueFromServer(key); } }); cache.get(&quot;one&quot;); cache.get(&quot;two&quot;); cache.get(&quot;three&quot;); cache.get(&quot;four&quot;);}---one=ONE is removed, cause by SIZE removalListener 可以给 LoadingCache 增加一个回调处理器，RemovalNotification 实例包含了缓存的键值对以及移除原因。 Weak Keys &amp; Soft ValuesJava 基础中的弱引用和软引用的概念相信大家都学习过，这里先给大家复习一下 软引用：如果一个对象只具有软引用，则内存空间充足时，垃圾回收器就不会回收它；如果内存空间不足，就会回收这些对象。只要垃圾回收器没有回收它，该对象就可以被程序使用 弱引用：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。 在 Guava Cache 中，CacheBuilder 提供了 weakKeys、weakValues、softValues 三种方法，将缓存的键值对与 JVM 垃圾回收机制产生关联。 该操作可能有它适用的场景，例如最大限度的使用 JVM 内存做缓存，但依赖 GC 清理，性能可想而知会比较低。总之我是不会依赖 JVM 的机制来清理缓存的，所以这个特性我不敢使用，线上还是稳定性第一。 如果需要设置清理策略，可以参考缓存过期小结中的介绍固定数量和固定时间两个方案，结合使用确保使用缓存获得高性能的同时，不把内存打挂。 总结本文介绍了 Guava Cache 一些常用的 API 、用法示例，以及需要警惕的一些使用误区。 在选择使用 Guava 时，我一般会结合实际使用场景，做出以下的考虑： 为什么不用 Redis？ 如果本地缓存能够解决，我不希望额外引入一个中间件。 如果保证缓存和数据源数据的一致性？ 一种情况，我会在数据要求敏感度不高的场景使用缓存，所以短暂的不一致可以忍受；另外一些情况，我会在设置定期刷新缓存以及手动刷新缓存的机制。举个例子，页面上有一个显示应用 developer 列表的功能，而本地仅存储了应用名，developer 列表是通过一个 RPC 接口查询获取的，而由于对方的限制，该接口 qps 承受能力非常低，便可以考虑缓存 developer 列表，并配置 maximumSize 以及 expireAfterAccess。如果有用户在 developer 数据源中新增了数据，导致了数据不一致，页面也可以设置一个同步按钮，让用户去主动 refresh；或者，如果判断当前用户不在 developer 列表，也可以程序 refresh 一次。总之非常灵活，使用 Guava Cache 的 API 可以满足大多数业务场景的缓存需求。 为什么是 Guava Cache，它的性能怎么样？ 我现在主要是出于稳定性考虑，项目一直在使用 Guava Cache。据说有比 Guava Cache 快的本地缓存，但那点性能我的系统不是特别关心。","link":"/guava-cache/"},{"title":"Kirito 杭州买房记 | 纯小白向杭州购房攻略","text":"2021 年刚开年，解决了人生的一个大事，没错，就像标题里透露的那样，我在杭州买房啦。第一次有在杭州买房这个念头，还要说回 2020 年 6 月，当时和朋友们聚餐时偶然聊到了房子的话题，意外地发现竟然只有自己还没有在杭州买房，其中不乏有 96 年如此年轻的小伙子，自那次聚餐之后，我便开始关注起了杭州的楼市。 先说结果吧，我参与摇号的盘是在杭州市余杭区未来科技城的【天空之城】，摇号结果： 这是个什么概念呢，一共 6181 户参与摇号，房子一共有 1014 套，大概有 1/6 的人算是摇中，而我是摇中的人里面第 2 顺位选房的。记得等待摇号结果的那个周日中午，等待结果通知的那最后一个小时，真的比高考都要紧张，所幸不负期许，人品可以说非常爆炸了，希望摇号这件事没有花光我今年所有运气。 预计在大年初七~初十的时间点还需要办理预审并交付首付，再往后还要办理按揭、网签等流程，还需要忙活一段时间才算尘埃落定。趁着过年这段时间比较空闲，所幸记录一下自己在杭州楼市的经历，这样也给那些刚来到杭州，想要定居于此的小白们一些参考。 杭州楼市印象 如果我是一个记者，我就去跟拍一下摇号江湖的沉浮的人儿 一户来杭三年漂泊的大龄单身青年刚刚结婚成为无房户 一个忙着给家里老人办户口来倒腾名额的主妇 一个只能被认定为有房户（其实没房）绝望的 25 岁年轻人…… 在这个秋天，在杭州魔幻的楼市里沉浮的经历。 – 阿里购房交流群内群友的感慨 记忆拉回去年的 6 月份，那时我就是一个购房纯小白，我不清楚为什么那么多人热衷于讨论杭州的房地产，不清楚杭州政府出台的关于购房的政策，不清楚新房和二手房的区别，甚至不清楚原来在杭州买房子竟然还需要摇号。而现在，我已经对选房摇号流程烂熟于心，并且接触了贷款并了解了利率上浮、组合贷等相关知识。 聊聊我的三次摇号经历吧 楼盘名 楼盘位置 时间 摇号结果 心情 富力中心 余杭区未来科技城 2020年6月下旬 3000 多号，彻底没戏 第一次摇号，虽然没摇中，但也没感觉 天空之城二期 余杭区未来科技城 2020年9月下旬 583 号，轮候 失落，差了 60 多号 天空之城三期 余杭区未来科技城 2021年2月上旬 2 号，摇中 boom！ 秉持着自住买房的原则，我基本只在余杭区附近 10 公里内选择了新盘进行摇号，没有往远的地方例如滨江区等地方考虑，也没有往特别贵的地方例如西湖区考虑。 这三次摇号经历，心情都是不一样的。第一次摇富力中心没中时，真的一点感觉都没有，心想着，反正机会还多的是；第二次摇天空之城二期时，已经明显感觉到杭州政府的购房政策开始发生变化了，具体哪些政策变化了我下面会详细介绍，限制了一部分人参与摇号，选房时由于差了 60 多号，虽然到我时还有房可选，但总价远超了我的预算，只能作罢；第三次摇天空之城三期时，政策又发生了变化，增加了 5 年限售的政策，所幸总算是摇到了。每一波新盘摇号，都有新的变化。 如果用一个词语描述我对杭州楼市的印象，【魔幻】二字一点不为过。 杭州购房全流程 我下面的内容偏记录向，基本是对上图的诠释，如果有兴趣听我啰嗦几句，可以继续往下看。 杭州楼市政策2018 年，我从南京来到了杭州，对于一个打工人来说，原本以为只是换一个地方打工罢了，但随着工作的稳定，并且我的女朋友也在杭州，于是便决定在杭州买房定居了，第一个念头当然就是落户了。我原户口并不是上海、北京这种稀有户口，换个城市落户对我而言没有什么损失，再加上杭州的落户政策是相对宽松的，所以在 2020 年初，我便把户口迁到了杭州。虽然目前在杭州交完 2 年社保也是能买房的，但政策这个东西说不好哪一天就变了，为了保险起见，户口迁的早总归不是坏事。 我在 2020 年 6 月才算正式开始了解杭州的楼市。刚开始啥都不懂，追着几个杭州买过房的朋友刨根问底，在这里也向他们表示下感谢。第一个摆在我面前的选项便是：新房（便宜需要摇号） or 二手房（贵不用摇号），那时也是我第一次感受到房地产的魅力，因为在杭州楼市流传着这么一句俗语：“买到新房，就是赚钱”，一方面杭州政府对新房会进行房价调控，直接导致倒挂周边二手房能够达到上万一平米，另一方面杭州目前有大量的新盘在建，杭州房地产市场正处于火热阶段，导致新盘市场极其火热。但这个事情并非只有利于杭州本地人或者打算在杭州定居的年轻人，凡是有利益的地方，就会吸引资本进场，一时间来杭州炒房的人变多了，一些红盘动辄出现万人摇的局面，直接让我们这些原本就不富裕的刚需家庭更是雪上加霜。 杭州政府当然也不会希望炒房客把房子抢去了，所以每隔一段时间都会出一些新政，核心目的就是为了贯彻“房住不炒”的思想。更早的不谈，就说说我这几次摇号经历的杭州楼市新政吧。 2020 年 6 月下旬，我的杭州第一次摇号，贡献给了富力中心这个盘，还算是政策宽松的时候：有杭州户口就能参与摇号。结果很惨烈，没有摇中。 2020 年 9 月下旬，参与了天空之城二期的摇号，已经明显感觉到了政策的变化，开始强调无房家庭这个概念了。无房家庭主要是两类人群，一类是字面意思结婚之后没有房子的家庭，一类是 30 岁以上单身的人士，就这样误杀了一类刚毕业没几年还没结婚的 30 岁以下年轻人。我周围有一些朋友在新政出来之后立马去领了证，这样才得以保住摇号资格。除了无房家庭，还有一类可以摇号的人群：人才，如果满足一定条件，是可以获得人才认证的，有了这个身份，也可以参与摇号。摇号结果是比较可惜的，天空之城二期房源数量较少，只有 520 套，我的号码是 583 套，正式选房时，只剩下一些 600 多万的一楼可选，当时的顾虑主要是预算不够，于是选择了弃选。 2021 年 2 月上旬，参与了天空之城三期的摇号，而这次政策又有了较大的变化，我作为小白，能感受到比较大的两个点是 红盘分流。原先我同时关注了天空之城，紫璋台，中梁沐宸院这三个楼盘的，他们都满足我对自住宅的选房标准，如果一个个摇，我可以摇三次，但杭州市政府为了避免出现万人摇的局面，对于很多楼盘都是同一时间发了预售证，购房者只能选择一个楼盘进行报名。 5 年限售。对于红盘，中签概率低于 10% 的楼盘增加了 5 年限售的限制，避免了投资客入场。 可能也是因为这些政策的原因，加上运气的成分，我终于在第三次摇号的时候摇中了。我是幸运的，一个同期摇中的朋友是摇了 20 多次才上车的。 关于杭州楼市政策，我想给同为小白想买房的读者分享下我的看法，参考上海和深圳的楼市政策，例如上海的积分落户政策，深圳的购房限制，在逐渐火热的杭州楼市，未来可能也会出现同样的局面，趁早落户比较稳妥。杭州楼市政策在每一波开盘潮来临之后，几乎都会出现调整，需要时刻关注对个人的影响。 选房在老家住的房子都是父母那一辈人辛苦打拼来的，自己住着也挺习惯，没有想过选房有哪些标准，等到了自己想要买房时，才发现自己对于选房真是一无所知。 也有人问我，杭州房子现在多少钱一平？这当然也很难回答，因为同一时间段不同的开发商，不同的地段，不同的配套等因素必然会导致不同的房价，群众眼睛是雪亮的，一分钱一分货，从摇号人数就开始看出大家的倾向性。 我整理了一些关键的参考指标，算是这几个月对杭州购房的一些总结。 开发商/物业开发商的资质决定了楼盘质量，需要重点关注。不同开发商专注的楼盘档次不同（高端改善、刚需改善、刚需），高端盘自然不是我讨论涉及到的内容，我在网上搜集了这么一张排行，对于刚需和刚改盘有一定参考意义。 开发商和物业理论上来说应当是两家公司，但似乎很多开发商和物业都从属于一个集团，例如万科房地产和万科物业、绿城房地产和绿城物业，品牌开发商通过成立物业公司也是对业主长期进行品牌维护的一种方式。有一些小开放商甚至会出现跑路的问题，这样的例子也不是没有出现过。 地理位置因为我工作靠近余杭区 EFC 欧美金融城，我女朋友在余杭区西溪，都是在未来科技城板块，综合考虑的话，未来科技城肯定是首选，其次周边 10 km 以内的楼盘都可以摇号。 整个杭州楼市我也就只关注了这么些板块，例如最热的当数未来科技城和亚运村。前者是因为阿里巴巴的带动，以及近几年头条、OPPO、VIVO、富士康等公司陆续进驻，坐拥高新科技产业园区，会为后面房屋增值以及整个板块的发展带来较高的增速；后者自然不用多说，杭州要举办亚运会，虽说你可能会吐槽说亚运会和房价有啥关系，但上一次人们这么吐槽是 G20 峰会，的确带动了杭州楼市的一波价格上涨。 其次，选择楼盘时，也需要关注附近是否有高架、污水厂、垃圾处理厂、发电站等不利因素，还记得我来杭第一个月租房又换房，就是因为受不了楼下垃圾车每天 8 点准时把我吵醒，买房是更为长期的一件事，自然得需要更加慎重。 有一些朋友讲究风水，会比较在意楼盘附近是否有运河和公墓，前者会带走运势，后者总感觉瘆得慌。 房屋性质 真正的房屋性质分类有很多，分类方式也很严谨，但是对于小白来说，我觉得只需要搞懂几个常见的概念就行了，一图胜千言，上图是我从贝壳找房上的二手房信息中截取出来的图片，最左下角是阿里巴巴总部西溪园区，我挑选出了它周边的几个小区，就比较有代表性，可以从价格上明显对比出普通住宅、公寓、回迁房的区别。虽然楼盘之间仍然有其他很多因素影响着它们的价格，但房屋性质显然是众多因素中影响比重最大的一个。 当人们在讨论买房时，99.9% 的情况讨论的是普通住宅，公寓和回迁房等其他房屋性质的房产均有不同程度的不利因素，例如产权、落户、学区等问题，这里就不一一列举了。 学校学区无疑是牵动所有购房者心的一个话题，提到学区房，几乎所有人一致的反应都是：贵。之前，我对学区房一直有误解，认为有的房子买了之后，孩子是没学上的，只有学区房才有学上。不知道有没有小白跟我有一样的误解，事实上，只要是住宅性质的房子（非公寓），你的孩子就一定有学上，只不过是学校的好坏罢了。那些所谓的学区房，学区自然是划分了好的小学和初中。 对于新房，有的楼盘会包含幼儿园、小学，甚至初中（这里指的是建筑，例如有教学楼和操场），但只有交房后，才能确定是哪个具体的教育集团或者是公办学校来接管（这里指的是师资力量）。对于小孩已经快要上学的家庭，新盘就不会是一个好的选择了，基本都流向了昂贵的二手学区房。 至于新房学区能否划分到好的学区，这个完全看人品，有一定赌的成分。但我了解到有的小区没有划分到重点小学的学区，家长去教育局闹的，真的是楼市体现出生活百态。 另外一个不得不面对的现实问题便是，余杭区的高中和主城区的高中学籍是不通的。这意味着如果在余杭区读完小学初中，中考成绩再好，也没法就读主城区的知名高中，这就使得很多在余杭工作的家长，不得不跑到西湖区购置“学区房”。这让我想到了北京西二旗那批程序员的孩子，硬生生把回龙观考成“学区房”的励志故事，但毕竟是小概率事件，有条件的家长还是会更加青睐西湖区。 再说说我个人吧，毕竟考虑下一代这件事还不是我的当务之急，还是安心搞定第一套力所能及的房子吧。至于 3~4 年后，学籍政策是否有变化，这谁也说不准。 交通杭州是一座交通规划远赶不上城市发展的城市，我从所在的余杭区想进西湖区，大多数情况会选择打的，而上海给我的感觉是地铁能到达任何一个地方。利好的地方在于，最近 5 号线增加了延线，14 号线也在规划中了，最明显的感受便是目前我租的房子附近，正在修建地铁站，道路也进行了拓宽，正在往好的方向发展。 交通对于楼盘的意义重大，不用我赘述。在现如今的城市发展中，交通约等于地铁，很多开发商在宣传楼盘时，都会标榜自己是地铁房，例如我摇中的楼盘就是打着“地铁万科天空之城”这样的旗号宣传的。如果当下没有地铁，也需要关注下，是否有地铁在建的规划。很多投资客会选地铁规划附近房子，等地铁建成自然会带动一波。 除了地铁，还得聊下出租车&amp;网约车，不得不吐槽下到了 9 ~ 10 点，余杭这鬼地方是真的难打车，毕竟跟杭州市中心区域还是要差一截的。 其他例如户型、得房率、楼层高度、装修标准等等一般都是次要因素，他们可能不会直接影响购房，但也一定会在多个楼盘对比间占据一定的权重，我就不一一总结了。 获取楼盘资讯还记得我刚接触杭州楼市时两眼一抹黑的阶段，我向朋友问的最多的一个问题便是：你们都是从哪儿知道这么多杭州楼市的资讯的？我相信很多跟我一样的小白，一定也有同样的疑问，其实当你想要去融入这个圈子时，有很多问题都会逐渐解答开。 我最早是加入了公司内部的钉钉购房群，里面有很多久战楼市的大神，看着他们对杭州楼市谈笑风生，心生敬畏。同时我也接触到一些之前觉得生僻的词汇：倒挂、得房率、浮动利率等，一边潜水，一边 Google 这些概念，最后也能融入其中进行交流。你周围同事或者朋友组建的微信群和钉钉群是最有效的信息获取渠道之一。 再推荐一些购房常用的小工具吧，微信小程序有：小鸡选房，杭州房小团。 里面会有杭州各个地区的楼盘信息，以及楼盘信息的详细介绍，并且可以在其中联系到销售，一般销售的服务都会非常热情。 微信公众号有：杭州发布、小鸡评房、杭州房叔。我关注的也不多，因为周围能摇的盘就那么几个。等掌握了必要的筛选技能之后，自然就掌握了自我判断的能力了，所谓久病成良医，久摇成能手。我认识一些摇号摇了几十次的朋友，他们真的已经是对杭州各个楼盘的情况烂熟于心了，当然我还是祝愿大家能不成为这样的高手，早日摇到号。 当你确定要摇一个盘时，该楼盘的销售是领着你走完最后一公里的人，包括你摇号的资格满不满足，楼盘的详细信息介绍，需要准备什么材料，以及你所想要了解的任何细节信息，都可以咨询销售。你所要做的，仅仅是在整整登记时，将他标记为你的销售，这样就算对他最大的支持了。一个好的销售至关重要，销售会可以理解为你购房的导师，陪伴着你直到交房。 摇号如果要选择一个跟买房最挂钩的一个词，那一定是“摇号”。其实不仅仅是杭州，目前国内一二线城市基本都会涉及到摇号这个问题。简单理解下这个流程：假设房源数量为 500 套，参与摇号的登记人数为 5000 人，每个人完成登记之后会拿到一个登记号，从 15000 依次递增，登记截止后不日便会正式开始摇号，摇号的依据就是登记号了，摇号结果公开后，可以根据登记号查询到一个选房顺序号，选房顺序号才是最终真正的那个“号”，1500 号的锦鲤状态是“摇中”，501~5000 号为轮候，轮候状态并不是意味着一定选不到房，出于众多原因，例如剩余房源预算不够，楼层户型不满足预期，许多靠后的号会弃选，红盘的弃选率相对较低，但轮候顺延 10 号一般也都还好选到房子。我当时摇天空之城二期时便是轮候顺延了 60 号，到我的时候还有一个 600w 的一楼洋房可以选，但奈何实在是超出预算太多，最终还是错过了。 摇号流程最关键的准备好登记材料，因为整个登记时间非常短，只有三天时间，其中比较关键的材料我认为有两个。 征信报告。贷款买房需要征信报告，没有信用卡的小白，建议去开一张信用卡，周期大概在一周左右。有了信用卡就可以在网上查询到个人的征信报告了。 冻资证明。提前把资金从理财产品中取出，避免 T+1 赎回到账较慢影响冻资流程。一般冻资的金额是楼盘中最便宜那套房子的 30%。 除此之外，杭州新政中，还有一个新的概念不得不提：无房家庭。首先问一个问题：一个本科刚毕业的小伙子，来到了杭州打拼，打拼了三年之后 27 岁，在杭州准备长期发展，但还至今未婚，请问这个小伙子是否满足“无房家庭”的认证呢？答案可能违背直觉，并不符合。政策规定只有同时单身 30 岁以上无房或者已婚无房才能认定为无房家庭。 除了限购政策外，大多数新开的楼盘都增加了无房家庭的限制，这些盘也被人们称呼为“红盘”，在没有无房家庭限制时，这些红盘在历史开盘过程中，甚至出现过万人摇的盛况。而如今不满足无房家庭的认定，是没有资格摇这些红盘的，以至于我周围不少朋友为了摇号而去领了证，才成为了无房家庭。楼市新政封杀炒房客的同时，也同时堵死了一些单身年轻人买房的路。 最后在摇号资格这个话题里面要介绍的是高层次人才。人才并不是杭州独有的政策，例如我呆过的南京，也有人才的说法，只不过各个城市对待人才的政策是不同的，例如杭州的高层次人才就比较吃香，可以参与红盘的摇号，并且能够提升摇中的概率，特别是不满 30 岁的无房人才，则更是受益了。至于高层次人才认定的标准，我这里给出传送门，大家可以自行评估自己是否满足条件：http://rc.zjhz.hrss.gov.cn/articles/detail/6679.html。 贷款由于我在写这篇文章时，贷款还没有真正办理完，所以只能将我所了解的信息给大家陈述下，仅供参考。 同样是先说政策，我这篇文章偏小白向，所以大概率咱们都是第一套房的用户，首付比例为 30%，剩下的 70% 就需要我们向银行办理贷款了。如果是纯小白，这里面涉及的知识点就比较多了，但了解起来也不难，我就三个关键点介绍下：贷款类别、贷款利率、还款方式。 贷款类别主要关注两种即可：商业贷款和组合型贷款。在大城市工作的同学应该都知道公司有一个福利叫五险一金，其中的一金便是我们买房时有用的住房公积金。如果你的公司没有避税，并且交纳了比较高比例额度的公积金，那么恭喜你，在买房时可以喘一口气了，除了可以将平时的公积金提取出来还款之外，还可以用来做贷款，这就是组合贷款（商业贷款+公积金贷款）的优势。在房贷计算器中可以发现，公积金贷款的利率是要远低于商业贷款的利率的，全国统一的 3.25% 利率，真是太香了。但公积金贷款是有上限的，杭州个人公积金贷款最高只有 50w，家庭则为 100w，各个地区额度有所差异，房贷计算器底部也有一些其他城市的额度信息，可供参考。剩余贷款部分的大头，还是需要使用商业贷款，相对于公积金贷款，肯定要贵一点，毕竟银行也是要恰饭的，具体的利率则会因银行不同，而有所差异，需要自行找到房地产开发商所处的银行去了解。既然组合贷款可以使用公积金，小白可能会问了，为啥还有人会选择使用纯商贷呢？这里面的考虑主要基于政策，希望将公积金贷款用于第二套改善房使用，我就不展开介绍了。 贷款利率需要介绍的是 LPR 浮动利率这个概念。前面已经提到了商业贷款比公积金贷款要贵的事实，原因也很简单，前者是银行给你放的贷款，后者相当于是国家给你的福利。但银行给你定的利率自然不能太高，不然跟高利贷有什么区别，所以央行每隔一段时间会规定一个基准利率，银行自行参考这个基准利率进行浮动，当然大部分情况是上浮，例如 2021 年 1 月 1 日的基础利率是 4.90%，而大多数银行在办理组合贷时，给出的利率是 5.2%。当然，利率上浮还不是 LPR 浮动利率的核心，只是一个引子。在选择办理贷款是可以选择 LRP 浮动利率和固定利率，在本小白看来其实就是一场小赌局，每年都在变，能不能说得准今年就是一个最低点呢？这个选项一旦确定，以后就不能更改了。如何选择我就不介绍了，因为我也不懂，希望其他小白看到这里能够理解上浮的概念即可。 最后要聊的就是等额本息和等额本金了，其实这两点房贷计算器上的说明已经很清楚了。 等额本息还款：把按揭贷款的本金总额与利息总额相加，然后平均分摊到还款期限的每个月中。作为还款人，每个月还给银行固定金额，但每月还款额中的本金比重逐月递增、利息比重逐月递减。 等额本金还款：将本金分摊到每个月内,同时付清上一交易日至本次还款日之间的利息。这种还款方式相对等额本息而言,总的利息支出较低,但是前期支付的本金和利息较多,还款负担逐月递减。 看一下相同贷款金额的每月还款金额就可以了解二者的区别了，等额本息每个月还款金额一样，刚开始还款的金额少，总还款金额相比后者多；等额本金还款金额逐月递减，刚开始还款的金额多，总还款金额相关前者少。 我选择的是等额本息，主要原因是，70% 的贷款额度已经把杠杆加的足够高了，选择等额本金的话，刚开始还贷时每月还款压力会比较大。利用好首套房 30% 的首付去加杠杆，个人认为也是一个比较好的投资方式。另外，虽然整体还款总额变多了，但今天的 100 块，到了十年后，又能相当于多少块的购买力呢？ 贷款资质的评定要求贷款人提供收入证明，月供要小于月收入的一半，在办理贷款需要量力而为，否则不满足条件，银行是不会贷给你的。 最后对于刚接触杭州楼市，特别是刚毕业在杭州打拼了几年，还没有买到房的小伙伴，也不用着急，杭州楼市有点饥饿营销的感觉，总给人一种这波楼盘过后，再无新盘的感觉，但开发商其实一直在拿地建房，政府为了吸引人才，肯定也会制定政策优待人才的。 最后祝大家都成为摇号潮中的锦鲤。","link":"/hangzhou-buy-house/"},{"title":"使用堆内内存HeapByteBuffer的注意事项","text":"前言国庆假期一眨眼就过去了，本来在家躺平的很舒服，没怎么肝云原生编程挑战赛，传送门：https://tianchi.aliyun.com/s/8bf1fe4ae2aea736e692c31c6952042d ，偏偏对手们假期开始卷起来了，眼看就要被人反超了，吓得我赶紧继续优化了。比赛大概还有一个月才结束，Kirito 的详细方案也会在比赛结束后分享，这期间我会分享一些比赛中的一些通用优化或者细节知识点，例如本文就是这么一个例子。 趁着假期最后一天，分享一个很多人容易踩得一个坑：HeapByteBuffer 的使用问题。我们都知道 NIO 分装了 ByteBuffer 接口，使得 filechannel 的文件 IO API 变得非常的简单。ByteBuffer 主要有两个实现类 HeapByteBuffer 堆内内存 DirectByteBuffer 堆外内存 按我的个人经验，大多数情况，无论是读操作还是写操作，我都倾向于使用 DirectByteBuffer，主要是因为 HeapByteBuffer 在和 FileChannel 交互时，可能会有一些出乎大家意料的内部操作，也就是这篇文章的标题中提到的注意事项，这里先卖个关子。 先来看看这次比赛为什么要用到 HeapByteBuffer 呢？ 原因一：赛题需要设计分级存储，并且提供了 6G 堆内内存 + 2G 堆外内存，一个最直接的思路便是使用内存来存储热点数据，而内存存储数据最方便的数据结构便是 ByteBuffer 了。 原因二：由于堆内 6G 远大于堆外 2G，且 JVM 参数不能调整，所以要想利用好堆内富余的内存去做缓存，非 HeapByteBuffer 莫属了。 可能有一些读者并没有关注赛题，我这里简化一下前言，可以直接理解为：有一块 2G 的 HeapByteBuffer 用于文件 IO，我们该如何利用。 HeapByteBuffer 的复制问题废话不多说，直接来看 HeapByteBuffer 的坑在哪儿。 使用代码描述 HeapByteBuffer 的文件 IO 操作，大概率会写出如下的代码： 1234567public void readInOneThread() throws Exception { int bufferSize = 50 * 1024 * 1024; File file = new File(&quot;/essd&quot;); FileChannel fileChannel = new RandomAccessFile(file, &quot;rw&quot;).getChannel(); ByteBuffer byteBuffer = ByteBuffer.allocate(bufferSize); fileChannel.read(byteBuffer);} 上述的代码，将文件中的数据缓存到了内存中，无论是赛题还是生产场景，这个行为通常都是多线程的，例如在云原生编程挑战赛的评测下，有 40 个线程进行读写，如果按照线程维度进行缓存，每个线程分到 50M 用于内存缓存自然是没有问题。 而如果你直接使用上述代码，在评测中可能会直接得到内存溢出相关的异常。其实我在之前堆外内存泄漏的文章中也提到过这个问题，不过角度有所不同。原因很简单，直接来看源码。 FileChannel 使用的是 IOUtil 进行读写操作 sun.nio.ch.IOUtil#read123456789101112131415161718192021static int read(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { if (var1.isReadOnly()) { throw new IllegalArgumentException(&quot;Read-only buffer&quot;); } else if (var1 instanceof DirectBuffer) { return readIntoNativeBuffer(var0, var1, var2, var4); } else { ByteBuffer var5 = Util.getTemporaryDirectBuffer(var1.remaining()); int var7; try { int var6 = readIntoNativeBuffer(var0, var5, var2, var4); var5.flip(); if (var6 &gt; 0) { var1.put(var5); } var7 = var6; } finally { Util.offerFirstTemporaryDirectBuffer(var5); } return var7; }} 可以发现当使用 HeapByteBuffer 时，会走到下面这个分支 1Util.getTemporaryDirectBuffer(var1.remaining()); 这个 Util 封装了更为底层的一些 IO 逻辑 123456789101112131415161718192021222324package sun.nio.ch;public class Util { private static ThreadLocal&lt;Util.BufferCache&gt; bufferCache; public static ByteBuffer getTemporaryDirectBuffer(int var0) { if (isBufferTooLarge(var0)) { return ByteBuffer.allocateDirect(var0); } else { // FOUCS ON THIS LINE Util.BufferCache var1 = (Util.BufferCache)bufferCache.get(); ByteBuffer var2 = var1.get(var0); if (var2 != null) { return var2; } else { if (!var1.isEmpty()) { var2 = var1.removeFirst(); free(var2); } return ByteBuffer.allocateDirect(var0); } } }} isBufferTooLarge 这个方法会根据传入 Buffer 的大小决定如何分配堆外内存，如果过大，直接分配大缓冲区；如果不是太大，会使用 bufferCache 这个 ThreadLocal 变量来进行缓存，从而复用（实际上这个数值非常大，几乎不会走进直接分配堆外内存这个分支）。这么看来似乎发现了两个不得了的结论： 使用 HeapByteBuffer 读写都会经过 DirectByteBuffer，写入数据的流转方式其实是：HeapByteBuffer -&gt; DirectByteBuffer -&gt; PageCache -&gt; Disk，读取数据的流转方式正好相反。 使用 HeapByteBuffer 读写会申请一块跟线程绑定的 DirectByteBuffer。这意味着，线程越多，临时 DirectByteBuffer 就越会占用越多的空间。 根据这两个结论，我们再回到赛题中，如果直接按照上述的方式进行读写，40 个线程每个都持有一个 50M 的堆内内存，同时又因为 IOUtil 的内部行为，额外分配了 40*50M 的堆外内存， 堆外内存在不经意间就被用光了！出现堆外内存溢出的异常也就不奇怪了。 为什么 HeapByteBuffer 在 IO 时需要复制到 DirectByteBuffer这个我之前也介绍过，详情可以参考我的一篇旧文：《一文探讨堆外内存的监控与回收》。总结如下： 为了方便 GC 的实现，DirectByteBuffer 指向的 native memory 是不受 GC 管辖的 HeapByteBuffer 背后使用的是 byte 数组，其占用的内存不一定是连续的，不太方便 JNI 方法的调用 数组实现在不同 JVM 中可能会不同 解决方案其实我们本质上是为了给每个线程维护一块 HeapByteBuffer，用于缓存数据，并没有必要以 ByteBuffer 的大小为维度来进行 IO。可以借鉴 IOUtil 中复制 DirectByteBuffer 的思路来优化这一过程。代码示例如下： 123456789101112public void directBufferCopy() throws Exception { File file = new File(&quot;/essd&quot;); FileChannel fileChannel = new RandomAccessFile(file, &quot;rw&quot;).getChannel(); ByteBuffer byteBuffer = ByteBuffer.allocate(50 * 1024 * 1024); ByteBuffer directByteBuffer = ByteBuffer.allocateDirect(4 * 1024); for (int i = 0; i &lt; 12800; i++) { directByteBuffer.clear(); fileChannel.read(directByteBuffer, i * 4 * 1024); directByteBuffer.flip(); byteBuffer.put(directByteBuffer); }} 在 Java 中，从磁盘到堆内内存，一定无法省略堆外内存的复制，但我们可以自己复制，从而使得这个过程更加直观地被我们自己操控，而不是被 FileChannel 的内部逻辑左右。 这里也需要注意 单次 IO 使用的 DirectByteBuffer 不宜过大，仅仅作为一个运输载体，起到一个运输数据的作用。这样在多线程场景下，才不至于占用过多的堆外内存 单次 IO 使用的 DirectByteBuffer 不宜过小，否则会出现读写放大的问题，一般建议设置 4kb 的整数倍，具体以实际测试结果为准。 其他注意事项HeapByteBuffer 读写时的复制问题是本文的主角，但使用 HeapByteBuffer 作为缓存时，也需要注意一些其他问题。例如比赛场景中，你可能希望开辟一大块 HeapByteBuffer，6G 堆内内存，分配个 4G 用作缓存总可以吧？可不可以我说了不算，你感兴趣的话倒是可以测试一下是否可行，还需要考虑 GC 情况，需要综合考虑老年代和新生代的配比，如果你分配了过多堆内内存给 HeapByteBuffer 缓存，可能会直接导致 OutOfMemory 或者触发 GC。 同时，如果 HeapByteBuffer 占用了过多内存，留给操作系统的 PageCache 也会非常有限，这两者使用的可是同一块内存！如果你的程序利用到了 PageCache 的特性，可能会由于 PageCache 空间不够，导致 IO 速度变慢。 总结本文介绍了在文件 IO 中使用 HeapByteBuffer 的注意事项，需要考虑到 FileChannel 内部的复制问题，意识到这一过程会有堆外内存的复制开销。在实际使用场景中，个人更加推荐直接使用 DirectByteBuffer 进行 IO 操作。如果出于某些原因，一定需要使用 HeapByteBuffer 存储作为缓存，可以参考文中分批使用 DirectByteBuffer 进行 IO 并复制的方案。","link":"/heapbuffer-io/"},{"title":"一种心跳，两种设计","text":"1 前言在前一篇文章 《聊聊 TCP 长连接和心跳那些事》 中，我们已经聊过了 TCP 中的 KeepAlive，以及在应用层设计心跳的意义，但却对长连接心跳的设计方案没有做详细地介绍。事实上，设计一个好的心跳机制并不是一件容易的事，就我所熟知的几个 RPC 框架，它们的心跳机制可以说大相径庭，这篇文章我将探讨一下 ** 如何设计一个优雅的心跳机制，主要从 Dubbo 的现有方案以及一个改进方案来做分析 **。 2 预备知识因为后续我们将从源码层面来进行介绍，所以一些服务治理框架的细节还需要提前交代一下，方便大家理解。 2.1 客户端如何得知请求失败了？高性能的 RPC 框架几乎都会选择使用 Netty 来作为通信层的组件，非阻塞式通信的高效不需要我做过多的介绍。但也由于非阻塞的特性，导致其发送数据和接收数据是一个异步的过程，所以当存在服务端异常、网络问题时，客户端接是接收不到响应的，那我们如何判断一次 RPC 调用是失败的呢？ 误区一：Dubbo 调用不是默认同步的吗？ Dubbo 在通信层是异步的，呈现给使用者同步的错觉是因为内部做了阻塞等待，实现了异步转同步。 误区二： Channel.writeAndFlush 会返回一个 channelFuture，我只需要判断 channelFuture.isSuccess 就可以判断请求是否成功了。 注意，writeAndFlush 成功并不代表对端接受到了请求，返回值为 true 只能保证写入网络缓冲区成功，并不代表发送成功。 避开上述两个误区，我们再来回到本小节的标题：客户端如何得知请求失败？** 正确的逻辑应当是以客户端接收到失败响应为判断依据 **。等等，前面不还在说在失败的场景中，服务端是不会返回响应的吗？没错，既然服务端不会返回，那就只能客户端自己造了。 一个常见的设计是：客户端发起一个 RPC 请求，会设置一个超时时间 client_timeout，发起调用的同时，客户端会开启一个延迟 client_timeout 的定时器 接收到正常响应时，移除该定时器。 定时器倒计时完毕，还没有被移除，则认为请求超时，构造一个失败的响应传递给客户端。 Dubbo 中的超时判定逻辑： 123456789101112131415161718192021222324252627282930public static DefaultFuture newFuture(Channel channel, Request request, int timeout) { final DefaultFuture future = new DefaultFuture(channel, request, timeout); // timeout check timeoutCheck(future); return future;}private static void timeoutCheck(DefaultFuture future) { TimeoutCheckTask task = new TimeoutCheckTask(future); TIME_OUT_TIMER.newTimeout(task, future.getTimeout(), TimeUnit.MILLISECONDS);}private static class TimeoutCheckTask implements TimerTask { private DefaultFuture future; TimeoutCheckTask(DefaultFuture future) { this.future = future; } @Override public void run(Timeout timeout) { if (future == null || future.isDone()) { return; } // create exception response. Response timeoutResponse = new Response(future.getId()); // set timeout status. timeoutResponse.setStatus(future.isSent() ? Response.SERVER_TIMEOUT : Response.CLIENT_TIMEOUT); timeoutResponse.setErrorMessage(future.getTimeoutMessage(true)); // handle response. DefaultFuture.received(future.getChannel(), timeoutResponse); }} 主要逻辑涉及的类：DubboInvoker，HeaderExchangeChannel，DefaultFuture ，通过上述代码，我们可以得知一个细节，无论是何种调用，都会经过这个定时器的检测，** 超时即调用失败，一次 RPC 调用的失败，必须以客户端收到失败响应为准 **。 2.2 心跳检测需要容错网络通信永远要考虑到最坏的情况，一次心跳失败，不能认定为连接不通，多次心跳失败，才能采取相应的措施。 2.3 心跳检测不需要忙检测忙检测的对立面是空闲检测，我们做心跳的初衷，是为了保证连接的可用性，以保证及时采取断连，重连等措施。如果一条通道上有频繁的 RPC 调用正在进行，我们不应该为通道增加负担去发送心跳包。** 心跳扮演的角色应当是晴天收伞，雨天送伞。** 3 Dubbo 现有方案 本文的源码对应 Dubbo 2.7.x 版本，在 apache 孵化的该版本中，心跳机制得到了增强。 介绍完了一些基础的概念，我们再来看看 Dubbo 是如何设计应用层心跳的。Dubbo 的心跳是双向心跳，客户端会给服务端发送心跳，反之，服务端也会向客户端发送心跳。 3.1 连接建立时创建定时器1234567891011121314151617public class HeaderExchangeClient implements ExchangeClient { private int heartbeat; private int heartbeatTimeout; private HashedWheelTimer heartbeatTimer; public HeaderExchangeClient(Client client, boolean needHeartbeat) { this.client = client; this.channel = new HeaderExchangeChannel(client); this.heartbeat = client.getUrl().getParameter(Constants.HEARTBEAT_KEY, dubbo != null &amp;&amp; dubbo.startsWith(&quot;1.0.&quot;) ? Constants.DEFAULT_HEARTBEAT : 0); this.heartbeatTimeout = client.getUrl().getParameter(Constants.HEARTBEAT_TIMEOUT_KEY, heartbeat * 3); if (needHeartbeat) { &lt;1&gt; long tickDuration = calculateLeastDuration(heartbeat); heartbeatTimer = new HashedWheelTimer(new NamedThreadFactory(&quot;dubbo-client-heartbeat&quot;, true), tickDuration, TimeUnit.MILLISECONDS, Constants.TICKS_PER_WHEEL); &lt;2&gt; startHeartbeatTimer(); } } } &lt;1&gt; ** 默认开启心跳检测的定时器 ** &lt;2&gt; ** 创建了一个 HashedWheelTimer 开启心跳检测 **，这是 Netty 所提供的一个经典的时间轮定时器实现，至于它和 jdk 的实现有何不同，不了解的同学也可以关注下，我就不拓展了。 不仅 HeaderExchangeClient 客户端开起了定时器，HeaderExchangeServer 服务端同样开起了定时器，由于服务端的逻辑和客户端几乎一致，所以后续我并不会重复粘贴服务端的代码。 Dubbo 在早期版本版本中使用的是 schedule 方案，在 2.7.x 中替换成了 HashWheelTimer。 3.2 开启两个定时任务123456789private void startHeartbeatTimer() { long heartbeatTick = calculateLeastDuration(heartbeat); long heartbeatTimeoutTick = calculateLeastDuration(heartbeatTimeout); HeartbeatTimerTask heartBeatTimerTask = new HeartbeatTimerTask(cp, heartbeatTick, heartbeat); &lt;1&gt; ReconnectTimerTask reconnectTimerTask = new ReconnectTimerTask(cp, heartbeatTimeoutTick, heartbeatTimeout); &lt;2&gt; heartbeatTimer.newTimeout(heartBeatTimerTask, heartbeatTick, TimeUnit.MILLISECONDS); heartbeatTimer.newTimeout(reconnectTimerTask, heartbeatTimeoutTick, TimeUnit.MILLISECONDS);} Dubbo 在 startHeartbeatTimer 方法中主要开启了两个定时器： HeartbeatTimerTask，ReconnectTimerTask &lt;1&gt; HeartbeatTimerTask 主要用于定时发送心跳请求 &lt;2&gt; ReconnectTimerTask 主要用于心跳失败之后处理重连，断连的逻辑 至于方法中的其他代码，其实也是本文的重要分析内容，先容我卖个关子，后面再来看追溯。 3.3 定时任务一：发送心跳请求详细解析下心跳检测定时任务的逻辑 HeartbeatTimerTask#doTask： 12345678910111213protected void doTask(Channel channel) { Long lastRead = lastRead(channel); Long lastWrite = lastWrite(channel); if ((lastRead != null &amp;&amp; now() - lastRead &gt; heartbeat) || (lastWrite != null &amp;&amp; now() - lastWrite &gt; heartbeat)) { Request req = new Request(); req.setVersion(Version.getProtocolVersion()); req.setTwoWay(true); req.setEvent(Request.HEARTBEAT_EVENT); channel.send(req); } }} 前面已经介绍过，**Dubbo 采取的是是双向心跳设计 **，即服务端会向客户端发送心跳，客户端也会向服务端发送心跳，接收的一方更新 lastRead 字段，发送的一方更新 lastWrite 字段，超过心跳间隙的时间，便发送心跳请求给对端。这里的 lastRead/lastWrite 同样会被同一个通道上的普通调用更新，通过更新这两个字段，实现了只在连接空闲时才会真正发送空闲报文的机制，符合我们一开始科普的做法。 注意：不仅仅心跳请求会更新 lastRead 和 lastWrite，普通请求也会。这对应了我们预备知识中的空闲检测机制。 3.4 定时任务二：处理重连和断连继续研究下重连和断连定时器都实现了什么 ReconnectTimerTask#doTask。 1234567891011protected void doTask(Channel channel) { Long lastRead = lastRead(channel); Long now = now(); if (lastRead != null &amp;&amp; now - lastRead &gt; heartbeatTimeout) { if (channel instanceof Client) { ((Client) channel).reconnect(); } else { channel.close(); } }} 第二个定时器则负责根据客户端、服务端类型来对连接做不同的处理，当超过设置的心跳总时间之后，客户端选择的是重新连接，服务端则是选择直接断开连接。这样的考虑是合理的，客户端调用是强依赖可用连接的，而服务端可以等待客户端重新建立连接。 细心的朋友会发现，这个类被命名为 ReconnectTimerTask 是不太准确的，因为它处理的是重连和断连两个逻辑。 3.5 定时不精确的问题在 Dubbo 的 issue 中曾经有人反馈过定时不精确的问题，我们来看看是怎么一回事。 Dubbo 中默认的心跳周期是 60s，设想如下的时序： 第 0 秒，心跳检测发现连接活跃 第 1 秒，连接实际断开 第 60 秒，心跳检测发现连接不活跃 由于 ** 时间窗口的问题，死链不能够被及时检测出来，最坏情况为一个心跳周期 **。 为了解决上述问题，我们再倒回去看一下上面的 startHeartbeatTimer() 方法 12long heartbeatTick = calculateLeastDuration(heartbeat); long heartbeatTimeoutTick = calculateLeastDuration(heartbeatTimeout); 其中 calculateLeastDuration 根据心跳时间和超时时间分别计算出了一个 tick 时间，实际上就是将两个变量除以了 3，使得他们的值缩小，并传入了 HashedWheelTimer 的第二个参数之中 12heartbeatTimer.newTimeout(heartBeatTimerTask, heartbeatTick, TimeUnit.MILLISECONDS);heartbeatTimer.newTimeout(reconnectTimerTask, heartbeatTimeoutTick, TimeUnit.MILLISECONDS); tick 的含义便是定时任务执行的频率。这样，通过减少检测间隔时间，增大了及时发现死链的概率，原先的最坏情况是 60s，如今变成了 20s。这个频率依旧可以加快，但需要考虑资源消耗的问题。 定时不准确的问题出现在 Dubbo 的两个定时任务之中，所以都做了 tick 操作。事实上，所有的定时检测的逻辑都存在类似的问题。 3.6 Dubbo 心跳总结Dubbo 对于建立的每一个连接，同时在客户端和服务端开启了 2 个定时器，一个用于定时发送心跳，一个用于定时重连、断连，执行的频率均为各自检测周期的 1/3。定时发送心跳的任务负责在连接空闲时，向对端发送心跳包。定时重连、断连的任务负责检测 lastRead 是否在超时周期内仍未被更新，如果判定为超时，客户端处理的逻辑是重连，服务端则采取断连的措施。 先不急着判断这个方案好不好，再来看看改进方案是怎么设计的。 4 Dubbo 改进方案实际上我们可以更优雅地实现心跳机制，本小节开始，我将介绍一个新的心跳机制。 4.1 IdleStateHandler 介绍Netty 对空闲连接的检测提供了天然的支持，使用 IdleStateHandler 可以很方便的实现空闲检测逻辑。 123public IdleStateHandler( long readerIdleTime, long writerIdleTime, long allIdleTime, TimeUnit unit){} readerIdleTime：读超时时间 writerIdleTime：写超时时间 allIdleTime：所有类型的超时时间 IdleStateHandler 这个类会根据设置的超时参数，循环检测 channelRead 和 write 方法多久没有被调用。当在 pipeline 中加入 IdleSateHandler 之后，可以在此 pipeline 的任意 Handler 的 userEventTriggered 方法之中检测 IdleStateEvent 事件， 1234567@Overridepublic void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if (evt instanceof IdleStateEvent) { //do something } ctx.fireUserEventTriggered(evt);} 为什么需要介绍 IdleStateHandler 呢？其实提到它的空闲检测 + 定时的时候，大家应该能够想到了，这不天然是给心跳机制服务的吗？很多服务治理框架都选择了借助 IdleStateHandler 来实现心跳。 IdleStateHandler 内部使用了 eventLoop.schedule(task) 的方式来实现定时任务，使用 eventLoop 线程的好处是还同时保证了 ** 线程安全 **，这里是一个小细节。 4.2 客户端和服务端配置首先是将 IdleStateHandler 加入 pipeline 中。 ** 客户端：** 123456bootstrap.handler(new ChannelInitializer&lt;NioSocketChannel&gt;() { @Override protected void initChannel(NioSocketChannel ch) throws Exception { ch.pipeline().addLast(&quot;clientIdleHandler&quot;, new IdleStateHandler(60, 0, 0)); }}); ** 服务端：** 123456serverBootstrap.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() { @Override protected void initChannel(NioSocketChannel ch) throws Exception { ch.pipeline().addLast(&quot;serverIdleHandler&quot;,new IdleStateHandler(0, 0, 200)); }} 客户端配置了 read 超时为 60s，服务端配置了 write/read 超时为 200s，先在此埋下两个伏笔： 为什么客户端和服务端配置的超时时间不一致？ 为什么客户端检测的是读超时，而服务端检测的是读写超时？ 4.3 空闲超时逻辑 — 客户端对于空闲超时的处理逻辑，客户端和服务端是不同的。首先来看客户端 123456789@Overridepublic void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if (evt instanceof IdleStateEvent) { // send heartbeat sendHeartBeat(); } else { super.userEventTriggered(ctx, evt); }} 检测到空闲超时之后，采取的行为是向服务端发送心跳包，具体是如何发送，以及处理响应的呢？伪代码如下 12345678910111213141516171819public void sendHeartBeat() { Invocation invocation = new Invocation(); invocation.setInvocationType(InvocationType.HEART_BEAT); channel.writeAndFlush(invocation).addListener(new CallbackFuture() { @Override public void callback(Future future) { RPCResult result = future.get(); // 超时 或者 写失败 if (result.isError()) { channel.addFailedHeartBeatTimes(); if (channel.getFailedHeartBeatTimes() &gt;= channel.getMaxHeartBeatFailedTimes()) { channel.reconnect(); } } else { channel.clearHeartBeatFailedTimes(); } } });} 行为并不复杂，构造一个心跳包发送到服务端，接受响应结果 响应成功，清空请求失败标记 响应失败，心跳失败标记 +1，如果超过配置的失败次数，则重新连接 不仅仅是心跳，普通请求返回成功响应时也会清空标记 4.4 空闲超时逻辑 — 服务端12345678@Overridepublic void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if (evt instanceof IdleStateEvent) { channel.close(); } else { super.userEventTriggered(ctx, evt); }} 服务端处理空闲连接的方式非常简单粗暴，直接关闭连接。 4.5 改进方案心跳总结 为什么客户端和服务端配置的超时时间不一致？ 因为客户端有重试逻辑，不断发送心跳失败 n 次之后，才认为是连接断开；而服务端是直接断开，留给服务端时间得长一点。60 * 3 &lt; 200 还说明了一个问题，双方都拥有断开连接的能力，但连接的创建是由客户端主动发起的，那么客户端也更有权利去主动断开连接。 为什么客户端检测的是读超时，而服务端检测的是读写超时？ 这其实是一个心跳的共识了，仔细思考一下，定时逻辑是由客户端发起的，所以整个链路中不通的情况只有可能是：服务端接收，服务端发送，客户端接收。也就是说，只有客户端的 pong，服务端的 ping，pong 的检测是有意义的。 主动追求别人的是你，主动说分手的也是你。 利用 IdleStateHandler 实现心跳机制可以说是十分优雅的，借助 Netty 提供的空闲检测机制，利用客户端维护单向心跳，在收到 3 次心跳失败响应之后，客户端断开连接，交由异步线程重连，本质还是表现为客户端重连。服务端在连接空闲较长时间后，主动断开连接，以避免无谓的资源浪费。 5 心跳设计方案对比 Dubbo 现有方案 Dubbo 改进方案 ** 主体设计 ** 开启两个定时器 借助 IdleStateHandler，底层使用 schedule ** 心跳方向 ** 双向 单向（客户端 -&gt; 服务端） ** 心跳失败判定方式 ** 心跳成功更新标记，借助定时器定时扫描标记，如果超过心跳超时周期未更新标记，认为心跳失败。 通过判断心跳响应是否失败，超过失败次数，认为心跳失败 ** 扩展性 ** Dubbo 存在 mina，grizzy 等其他通信层实现，自定义定时器很容易适配多种扩展 多通信层各自实现心跳，不做心跳的抽象 ** 设计性 ** 编码复杂度高，代码量大，方案复杂，不易维护 编码量小，可维护性强 私下请教过 ** 美团点评的长连接负责人：俞超（闪电侠）**，美点使用的心跳方案和 Dubbo 改进方案几乎一致，可以说该方案是标准实现了。 6 Dubbo 实际改动点建议鉴于 Dubbo 存在一些其他通信层的实现，所以可以保留现有的定时发送心跳的逻辑。 ** 建议改动点一：** 双向心跳的设计是不必要的，兼容现有的逻辑，可以让客户端在连接空闲时发送单向心跳，服务端定时检测连接可用性。定时时间尽量保证：客户端超时时间 * 3 ≈ 服务端超时时间 ** 建议改动点二：** 去除处理重连和断连的定时任务，Dubbo 可以判断心跳请求是否响应失败，可以借鉴改进方案的设计，在连接级别维护一个心跳失败次数的标记，任意响应成功，清除标记；连续心跳失败 n 次，客户端发起重连。这样可以减少一个不必要的定时器，任何轮询的方式，都是不优雅的。 最后再聊聊可扩展性这个话题。其实我是建议把定时器交给更加底层的 Netty 去做，也就是完全使用 IdleStateHandler ，其他通信层组件各自实现自己的空闲检测逻辑，但是 Dubbo 中 mina，grizzy 的兼容问题囿住了我的拳脚，但试问一下，如今的 2019 年，又有多少人在使用 mina 和 grizzy？因为一些不太可能用的特性，而限制了主流用法的优化，这肯定不是什么好事。抽象，功能，可扩展性并不是越多越好，开源产品的人力资源是有限的，框架使用者的理解能力也是有限的，能解决大多数人问题的设计，才是好的设计。哎，谁让我不会 mina，grizzy，还懒得去学呢 [摊手]。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/heartbeat-design/"},{"title":"Higress 新增 MCP 服务管理，助力构建私有 MCP 市场","text":"前言今年 3 月份 MCP 协议成为了 AI 的新一轮热点，被大多数人所熟知，彼时 Higress 快速进行跟进，新增了 MCP 协议转换功能，详见：https://higress.cn/ai/mcp-quick-start ，该方案解决了以下问题： 引入 Redis，借助其 pub/sub 特性，解决了 SSE 协议会话保持的问题 提供了 OpenAPI 转换成 MCPServer 的能力，仅需提供符合 OAS 3.0 规范的 OpenAPI 文档，即可自动转换成网关托管的 MCPServer 提供了 Go Template 和 GJSON 表达式，来对请求和响应模版进行精细化处理，这使得用户只需要变更配置即可完成对 MCPServer 的调优，且变更过程流量完全无损，SSE 连接也不会断开。 该功能一经推出，迅速在开源社区引起了用户广泛的关注，同时在交流群中，也有大量用户反馈了配置失败的问题，因为该功能过于原子化且配置复杂，用户很容易遇到配置失败的问题，为进一步增加用户体验，我们决定将 Higress MCP 相关的能力以场景化的方式，集成在 Higress Console 中，即 MCP 服务管理模块。 用户可以在 Higress 2.1.5 版本中正式体验该文中提及的所有特性。 Higress MCP 服务管理介绍Higress MCP 服务管理功能概览 Higress MCP 服务管理模块提供了以下能力： OpenAPI 转换 MCP。根据用户提供的 OAS 3.0 文档，连接网关已有的 HTTP 后端服务，即可自动转换成 MCPServer。 DB 转换 MCP。用户仅需将数据库实例配置为网关的后端服务，即可自动转换成 MCPServer，目前支持 MySQL、PostgreSQL、Clickhouse、Sqlite。 MCP 直接路由。可以直接代理 SSE/Streamable 协议的后端服务。 MCP 认证授权能力。 站在一个 Higress 开源贡献者的视角，我还是先澄清下 Higress 本身的定位，其主要还是承担 AI 网关/MCP 网关的职责，作为一个基础设施，帮助企业更好地构建自身的 MCP 市场，其提供的 MCP 特性支持，可以非常友好地跟 MCP 应用商店（如 mcp.so）、MCP 客户端市场（Cline、Cursor、Cherry Studio）、平台型市场（百炼、魔搭、Dify）等场景结合，Higress 跟这些场景并不是竞争关系。 MCP 服务管理和 MCP 市场如果你正在构建企业私有化的 MCP 市场，一定会关心本文介绍的 MCP 服务管理和 MCP 市场之间的关系，以下是一些释疑。 部分企业有自建 MCP 市场的需求。Higress MCP 服务管理仅是 Higress MCP 相关原子能力的控制台表现形式，旨在提供给用户一个更友好的交互界面，也提供了 OpenAPI 被集成能力，它可以成为企业私有化 MCP 市场的一个重要组成部分。但并不足以完整支撑全部场景的需求，推荐集成 Higress Console 的 OpenAPI 或者 admin-sdk，再自行构建一个符合企业私有化标识的 MCP 市场前/后端应用，才能够打造属于企业自己的私有化 MCP 市场。 Higress 商业化版本（阿里云公共云 API Gateway、专有云飞天企业版 API Gateway）后续也会推出开箱即用的 MCP 市场，该方案将会基于 MCP 服务管理封装出更上层的应用，计划提供两种模式供商业化用户选择： 模式一：开箱即用提供可扩展、可定制的自建实例化 MCP 市场 模式二：提供 MCP 市场源码，方便企业用户二次开发。 MCP 服务管理和 mcp.higress.ai 此前 Higress 官方发布了一个 SaaS 版本的 MCP 市场：mcp.higress.ai，其完全基于 Higress MCP 服务管理构建。目前前后端代码未开源，以 SaaS 化的形式开放相关能力供用户使用，仅起到一个功能演示的作用，用户也可以参考 mcp.higress.ai 的交互，基于 Higress 自行构建自己的 MCP 市场。 下边重点介绍 Higress MCP 服务管理的三种服务类型 OpenAPI 转换 MCP、MCP 直接路由、DB 转换 MCP，以及他们分别支撑的业务场景。 OpenAPI 转换 MCP企业在开发 MCPServer 供 AI Agent 使用时，可以大致分为两类场景：存量场景和增量场景。存量场景即企业已有的 IT 资产，以电商场景为例，订单系统、商品系统、地址系统，这些系统需要具备被 AI Agent 调用的能力，都需要 MCP 化；增量场景即单独为 AI Agent 更好地运行而开发的 MCP 工具，典型的例子：高德地图所提供的 amap mcpserver。 高德团队在提供 amap mcpserver 之前，也有成套的 amap openapi，只不过之前都是给传统应用调用的。大多数企业的业务团队如果愿意投入大量的精力和决心，一定也可以编写出像 amap mcpserver 这样高质量的制品，但现实情况很有可能是企业存量的业务会存在诸多顾虑： 存量业务系统的维护人员更新换代了好几批，部分长尾应用不敢增量代码 业务系统数量多，全量改造时间排期长 业务人员学习 AI 技术栈的技术曲线较高 mcpserver 的部署增加了新的资源消耗 一旦一项新的技术涉及到存量系统的改造，再加上选择的改造方案门槛高，就很有可能导致改造无法落地，最后成为企业的一笔糊涂账。 Higress 提供的 OpenAPI 转换 MCP 功能，一定不是唯一的 MCP 接入方案，但其优势非常突出： 零代码改造，接入便利。仅需提供存量服务的 OpenAPI 文档（符合 OAS 3.0 规范），无需编写一行接入代码，即可被 Higress 纳管。 白屏化工具修改，维护便利。后期可以在 Higress Console 中，调整 OpenAPI 转换过后的 MCP 元数据（yaml 格式），微调工具和描述，使得 MCP 更好地与 Agent 协作。 无需提供 MCP 运行时，运维便利。与传统 stdio/sse 提供的方案不同，Higress 网关不需要拉起任何诸如 Docker 之类的 MCP 运行时资源，完全通过协议转换完成，占用的是网关自身的资源。 借助 Higress 这一特性，业务可以将重心专注在 MCP 工具的描述与 Agent 如何更好地协作上，而不是如何编写 MCPServer 的代码实现上？，给业务智能化进程大大提效。 下边结合 Higress Console 的界面，进行更直观地功能介绍。 在 AI 网关管理 - MCP 管理菜单中，选择创建 MCP 服务，可以创建服务类型为 OpenAPI 的 MCP 服务。 选择 MCP 服务，可以对其执行编辑工具操作，在此页面中，支持 Swagger 模式和 YAML 模式两种模式。 Swagger 模式。导入符合 OAS 3.0 规范的 OpenAPI 文档，即可通过 Higress Console 自动转换成 MCP YAML 元数据，推荐新增时使用。 YAML 模式。直接编辑 MCP YAML 元数据，推荐编辑时使用。 查看工具列表： 在基本信息以及工具列表下方，也可以直接查看到 SSE/Streamable 接入点的信息，供 MCP 客户端直接连接。 MCP 直接路由在 OpenAPI 转换 MCP 场景中，我提到了存量和增量的场景，虽然个人观点是存量的业务场景占据大多数，但也不能排除有部分场景会选择自行开发 MCPServer，以及开源 MCP 市场上也涌现了大量的 MCPServer，考虑到这种情况，Higress 也提供了 MCP 直接路由的方案，以对接 SSE/Streamable 协议的后端服务。 可能有部分读者会有疑问，都自己开发 MCPServer 了，MCP 客户端可以直接连接，为啥还需要再由 Higress 代理呢？我的观点是，此处的 Higress 充当了 MCP 网关的作用，有以下优势： 可以借由网关实现 MCPServer 的认证授权、限流、可观测 统一管理 MCPServer 的对外开放 其实在 Higress 目前以及未来的特性规划中，一直将 MCP 当成了一个 API 类型，AI 场景下可以有 API 类型有： LLM API MCP API Agent API 再结合传统 API 网关的 API 类型： Rest API HTTP API Websocket API 再往下可以衍生出 API &amp; AI 开放平台的话题，不过这些都还在探索阶段，可以关注 Higress 社区了解后续相关进展。 DB 转换 MCPHIgress 提供的 DB 转换 MCPServer 能力，用户仅需提供数据库连接必要的连接信息（用户名、密码、域名/IP、端口），即可生成实例级别的 MCPServer，无需编写代码，无需提供运行时资源。 目前该特性仍处于探索阶段，使用时请注意以下限制： 仅提供了部分数据库类型的支持：MySQL、PostgreSQL、Clickhouse、Sqlite。 仅支持固定的 Tool 列表：ListTables、DescribeTable、Query、Execute，不支持动态增加 DB 转换 MCP 是 Higress 在数据库以及中间件类通用组件 MCP 化的一个尝试，这提供了一个未来演进可能的方向，我们也希望收到用户的更多反馈。 基于该功能，未来 Higress 也可以演进出 SQL MCP BI 的能力，用于编排符合业务场景的 SQL，转换成 MCP 工具，提供给上层业务用于智能化分析，格式如下： 12345678910111213141516171819db_tool: name: xxxx kind: postgres-sql source: my-pg-source # 通过 name 关联到对应 db_source # tools 所需元数据 name: search-hotels-by-name # tools 的名 description: Search for hotels based on name. # tools 的描述 inputSchema: # 内容为自定义的 RawMessage - type: string properties: table: type: string description: 'hotel name' required: - table # 执行语句 statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%'; 也欢迎其他 Higress 开源贡献者参与此特性的贡献。 MCP 认证MCP 官方认证方案当前状态MCP 社区目前对认证方案主要关注用户级别的权限管理，在企业中完整应用这套方案，需要深入到企业员工账号体系。即要从面向 Role 的权限管理，走向面向 User 的权限管理。 在这个 PR 主导下，社区目前已经接受了基于 OAuth2 PRM （ Protected Resource Metadata ）草案的认证方案，并且在最新版本中已经发布。 https://github.com/modelcontextprotocol/modelcontextprotocol/pull/284#issuecomment-2825122408 简单来说，是将 Auth Server 的职责从 MCP Server 中抽离，当 MCP Client 不带凭证请求 MCP Server 时，MCP Server 返回 401 并提供 PRM 信息，告知 MCP Client 去 Auth Server 签发 Token，MCP Client 拿到 Token 后再请求 MCP Server。 这个方案解决了 MCP Client 和 MCP Server 通信时自动发现认证端点的问题，但整体方案在 MCP 客户端生态大规模落地估计需要较长时间，并且该方案过于复杂和理想化，个人判断在企业级落地过程会有较大的阻力。 并且一个很有意思的点：https://github.com/modelcontextprotocol/modelcontextprotocol/issues/544，阿里云安全团队在该方案设计过程中提出了潜在的安全隐患，该 issue 在上周刚刚被修复。 Higress 提供的 MCP 认证方案我们的评估是即使 MCP 社区标准化的认证方案在技术层面取得突破，在企业落地进程中依然会遇到一定阻力，鉴于此，Higress 结合自身网关认证的场景，以及常见用户诉求，提供了一套网关侧的认证方案。 Higress MCP Server 的认证有两个环节，一个是发生在 MCP Client 与 Higress 之间（downstream），一个是发生在 Higress 到 MCP Server 之间（upstream）。 higress upstream 认证方案 MCP 社区中没有明确规定 remote MCP Server 实现下，MCPServer 到后端服务的认证方式，一种可能性是后端服务类型无法枚举，Higress 提供的 MCP 转换能力，设计了一些约定 Higress 按照 OpenAPI 3.0 规范提供了以下开箱即用的认证能力 HTTP Basic Auth HTTP Bearer Token API Key(Header) API Key(Query) 所以，如果 OpenAPI 转换 MCP 场景中，OAS 3.0 文档中包含了后端服务的认证和凭证，Higress 也会使用上该凭证进行后端服务的访问。 higress downstream 认证方案 Higress 作为 MCP 网关，主要价值之一就是对 MCP Server 进行统一的认证管理，推荐和模型服务代理时的 AI 网关场景一致，使用 API Key 进行认证。 MCP 服务的 downstream 认证，即网关侧的认证方式，和路由，AI 路由的用户体验一致，如果熟悉网关的认证插件，不会对该方案感到陌生。 透明认证方案 同时，透明认证凭证传递的支持，以应对一部分 MCP 直接路由的认证需求，会在未来提供支持。 Higress 商业化 vs 开源 MCP 能力对比 Higress 开源 公有云阿里云 API Gateway 专有云飞天企业版 API Gateway OpenAPI 转换 MCP 支持 支持 支持 MCP 直接路由 支持 SSE/Streamable 支持 SSE/Streamable，计划支持 Stdio 支持 SSE/Streamable，计划支持 Stdio MCP Server 认证和鉴权 API Key API Key/JWT/OAuth2 等多种认证 API Key/JWT/OAuth2 等多种认证 MCP Server Tool 粒度鉴权 暂无计划 支持 支持 MCP Server Tool 粒度配额限流 暂无计划 计划支持（7月） 计划支持（7月） MCP Server Tool 粒度可观测能力 暂无计划 计划支持（7月） 计划支持（7月） MCP Server 安全护栏 暂无计划 计划支持（7月） 计划支持（7月） MCP Server Tool 组装机制（从任意Server中选取Tool组装成一个新的Server） 暂无计划 计划支持（7月） 计划支持（7月） MCP Marketplace 暂无计划 提供两种模式给用户选择：模式一：开箱即用提供可扩展、可定制的自建实例化 MCP 市场模式二：提供 MCP 市场源码，方便企业用户二次开发。 提供两种模式给用户选择：模式一：开箱即用提供可扩展、可定制的自建实例化 MCP 市场模式二：提供 MCP 市场源码，方便企业用户二次开发。","link":"/higress-mcp-management/"},{"title":"Higress 开源贡献全攻略：共建 AI 原生网关生态","text":"概述Higress 是一个基于 Istio 和 Envoy 的云原生 API 网关，具备先进的 AI 功能。通过 Go/Rust/JS 编写的 Wasm 插件提供可扩展的架构，并提供了基于 Node 和 Java 的 console 模块，使得用户可以可视化使用 Higress。 Higress 最初由阿里巴巴研发，旨在解决 Tengine 配置 reload 对长连接造成影响，以及 gRPC/Dubbo 服务负载均衡能力不足的问题，于 2022 年开源。如今，阿里云云原生 API 网关、MSE 云原生网关、专有云飞天企业版 API 网关等网关产品系列均采用了 Higress 的统一架构，它已成为阿里云 API 网关产品的基础。 本文主要面向开发者和开源爱好者，围绕 Higress 基本的架构，分享一些 Higress 的基本原理，欢迎一起共建 Higress。 Higress 产品介绍网关产品在不同场景，不同发展阶段可能会加上很多修饰词前缀，这本质上是网关主要是一层代理，伴随着应用架构的演进，网关的身份也会发生转变。 正如单体式应用到 SOA 架构时 ESB 总线的称谓，微服务架构阶段时的微服务网关，K8s 云原生架构下的云原生网关，再到现如今 AI 时代的 AI 网关。可以发现不仅仅是 Higress 如此，传统的 API 网关产品以及国内外的 API 网关云厂商，都非常默契地将自家用户页面的入口换上了 AI 网关的皮肤。按照用户场景，Higress 可以有以下几种定位： AI 网关AI 网关相比传统 API 网关有了一些本质的变化： 传统 API 网关 AI 网关 请求响应模型 无流式处理需求，多为 HTTP 流式处理，SSE/Streamable 协议支持 内容感知深度 根据 header/query/path 等部分进行流量转发 支持 OpenAI 协议，多模型协同，提示词改写，可能对流量需要有语义级别的理解 流控差异 Query Per Second Token Per Second 内容安全 防御 DDos、SQL 注入等攻击手段 防御提示词注入、数据和模型投毒、无限资源消耗等攻击手段 AI 网关伴随 AI 原生架构演进，会提供各类 AI 原子能力和场景化能力，助力企业应用完成智能化升级。同时，随着越来越多 AI 的概念被提出，例如 MCP、A2A，为了解决对应的场景的问题，Higress 也提供了对应的解决方案，在这些场景下我们也可能会称呼 Higress 为 MCP 网关、Agent 网关。 API 网关API First 是一种以接口设计为核心的开发范式，主张在应用开发初期即优先构建标准化、可复用的 API 接口，通过契约化通信推动系统间的高效集成与生态协同。Higress 提供 API 全生命周期管理、流量治理、安全控制、多端适配及可观测性等核心能力 ，具体包括：基于 OpenAPI 规范的契约优先设计、动态路由/限流熔断的弹性保障机制、OAuth2/JWT 身份认证体系、协议转换适配（HTTP/gRPC/Dubbo）以及监控告警等能力 。 高质量的 API 管理能力不仅解决了异构系统间的数据孤岛问题，更为企业构建AI训练数据管道、实现模型服务编排及资源配额管控提供了标准化通道 ，成为连接传统业务数据与智能应用的关键枢纽。 Ingress 流量网关K8s 的 Ingress 控制器提供了良好地扩展机制，Higress 从名字上也可以看出，其必然是支持 Ingress 的，同时还支持 Gateway API。如果是存量的 nginx-ingress 用户，也可以低成本迁移至 Higress，Higress 完全兼容 nginx-ingress 注解。 微服务网关背靠阿里巴巴开源生态，Higress 与微服务中间件生态都有良好的适配度，可以对接 Nacos、Zookeeper、Consul 和 Eureka 等注册中心，支持 Dubbo 和 Grpc 协议转换，可以作为微服务网关直接对接 Dubbo、Spring Cloud 等框架构建的微服务应用。 Higress 部署架构 以 K8s 部署 Higress 为例，介绍 Higress 的基本组件 higress-console 控制台 higress-controller 控制面 higress-gateway 数据面 配置信息主要以 CRD 的形式存储在 K8s etcd 中，不依赖额外的存储组件。 higress-controller 是一个核心组件，用于监听 K8s APIServer，完成对应等规则的转换：K8s Ingress -&gt; Istio API -&gt; xDS，再实现配置下发。higress-controller 有两个容器，作用如下： higress-core：监听 Kubernetes API，将 ingress &amp; higress crd 等内容转换为istio api pilot: 移植 istiod pilot 模块的能力。将 istio api 转换为 xDS，同时将其下发给 envoy higress-gateway 是另一个核心组件，集成了 envoy ，用于数据面的实际流量转发。 Higress 开源贡献指引了解 HigressHigress 主站：https://higress.cn/ Higress AI 网关主站：https://higress.ai/ 快速开始：https://higress.cn/docs/latest/user/quickstart/ 推荐在标准 K8s 集群中进行 Higress 的部署，这也是 Higress 生产部署推荐的方案： 12helm repo add higress.io https://higress.cn/helm-chartshelm install higress -n higress-system higress.io/higress --create-namespace --render-subchart-notes 仅需两行命令即可完成 Higress 的一键安装，如果你没有现成的 K8s 集群可以使用，那么非常推荐使用 Kind 来本地搭建一个 K8s 集群，可以参考快速开始中的步骤进行 Kind 的安装（或者其他本地 K8s 部署方案均可），与标准 K8s 集群相比，本地方案仅需额外添加 –set global.local=true 开关即可。 12helm repo add higress.io https://higress.cn/helm-chartshelm install higress -n higress-system higress.io/higress --create-namespace --render-subchart-notes --set global.local=true --set global.o11y.enabled=false 拥有一个 Higress 环境是开源贡献的第一步，你可以快速体验 Higress 功能，挑选你感兴趣的能力进行功能验证。例如： 配置 httpbin.org 服务，进行简单路由场景的验证 配置 DeepSeek/通义千问等模型服务代理，感受 Higress 支持多种 AI 提供商的代理能力 搭配 cherrystudio/deepchat/LobeChat 等 AI 客户端，对接到你自己的 Higress 配置 ai-proxy/ai-token-ratelimit 等 Higress 提供的 AI 插件，感受 Higress 对 AI 流量的治理能力 … 你可以在 https://higress.cn 的用户指南中尝试寻找你感兴趣的功能文档。 组件贡献指引Higress 各个组件和模块支持多种语言进行编写，无论你熟悉哪一主流编程语言亦或是对 CICD、文档感兴趣，都是对 Higress 开源的一种贡献。 higress-console 后端基于 Java SpringBoot 构建，前端基于 NodeJS 飞冰（ICE）构建 可参考：《如何在本地开发和调试 Higress 控制台》https://higress.cn/blog/console-dev/ 仓库地址：https://github.com/higress-group/higress-console higress-controller Higress 的控制面程序，会连接 Istio ，用于生成 Istio API 对象，通过 xDS 协议发送给 Istio。 在 higress 仓库目录下执行 make build 即可进行本地环境能运行的二进制编译 可参考 https://higress.cn/docs/latest/dev/architecture/ higress-gateway higress 数据面上游为 envoy，几乎很少有需求会直接修改 envoy 本体。 插件开发 Higress 提供丰富的插件扩展机制，插件位于 https://github.com/alibaba/higress/tree/main/plugins。 核心插件主要使用 Go 进行开发，可以参考《使用 GO 语言开发 WASM 插件》https://higress.cn/docs/latest/user/wasm-go 进行开发。 文档贡献 Higress 官网仓库为 higress-group.github.io ，其中官网文档以及博客均通过 markdown 文件的方式维护在这个项目中，如果需要新增或者修改文档内容，可以通过向该项目提交 PR 完成。 仓库地址：https://github.com/higress-group/higress-group.github.io 运维 Higress 支持 helm 部署，helm 模板位于 https://github.com/alibaba/higress/tree/main/helm https://github.com/higress-group/higress-console/tree/main/helm Higress 支持通过 AI 的方式进行运维，我们可以将 higress-ops-mcp-server 组建通过mcp server方式提供给大模型调用，另外 Higress 还支持以 Higress 命令行的方式进行运维，通过 hgctl 方式进行 higress 集群的快速部署与运维。 仓库地址：https://github.com/higress-group/higress-ops-mcp-server 仓库地址：https://github.com/higress-group/hgctl 开始贡献无论贡献者是出于什么目的: 巩固技能 结交朋友 建立个人影响力 传承开源精神 工作需要 … Higress 都非常欢迎你的到来。需要明确的一点是，并非只有提交代码才算是开源贡献，提交 Higress 使用过程中的问题 issue，为新特性补充用户文档，补充任何使用过程中缺失的文档，报告安全问题，补充测试用例，参与 Higress 社区组织的各类 SIG 钉钉兴趣小组并提出 Higress 未来发展方向的建议… 如果你已经初步了解了 Higress 或者希望带着一些目标来学习 Higress，可以尝试从 github 的 issue 列表 https://github.com/alibaba/higress/issues 中，寻找你感兴趣的 issue 进行认领。认领 issue 是主要的贡献方式。 开源最佳实践有效沟通无论你出于什么样的目的：仅仅是一次性的贡献，亦或是永久性的加入社区，都得和他人进行沟通和交往，这是你要在开源圈发展必须修炼的技能。 在你开启一个 issue 或 PR 之前，或者是在交流群问问题之前，请牢记下面所列出的几点建议，会让你的工作更加的高效。 给出上下文以便于让其他人能够快速的理解。比方说你运行程序时遇到一个错误，要解释你是如何做的，并描述如何才能再现错误现象。又比方说你是提交一个新的想法，要解释你为什么这么想，对于项目有用处吗（不仅仅是只有你！） 😇 “当我做 Y 的时候 X 不能工作” 😢 “X 出问题! 请修复它。” 在进一步行动前，做好准备工作。 不知道没关系，但是要展现你尝试过、努力过。在寻求帮助之前，请确认阅读了项目的 README、文档、问题（开放的和关闭的）、邮件列表，并搜索了网络。当你表现出很强烈的求知欲的时候，人们是非常欣赏这点的，会很乐意的帮助你。 😇 “我不确定 X 是如何实现的，我查阅了相关的帮助文档，然而毫无所获。” 😢 “我该怎么做 X ?” 保持请求内容短小而直接。正如发送一份邮件，每一次的贡献，无论是多么的简单，都是需要他人去查阅的。很多项目都是请求的人多，提供帮助的人少。相信我，保持简洁，你能得到他人帮助的机会会大大的增加。 😇 “我很乐意写 API 教程。” 😢 ” 有一天我驾驶汽车行驶在高速公路上，在某个加油站加油的时候，突发奇想，我们应该这么做，不过在我进一步解释之前，我先和大家展示一下。。。” 让所有的沟通都是在公开场合下进行。哪怕是很不起眼的小事，也不要去给维护者发私信，除非是你要分享一些敏感信息（诸如安全问题或严重的过失）。你若能够保持谈话是公开的，很多人可以你们交换的意见中学习和受益。 😇 (评论) “@维护者 你好！我们该如何处理这个 PR？” 😢 (邮件) “你好，非常抱歉给发信，但是我实在很希望你能看一下我提交的 PR。” 大胆的提问（但是要谨慎！）。每个人参与社区，开始的时候都是新手，哪怕是非常有经验的贡献者也一样，在刚进入一个新的项目的时候，也是新手。出于同样的原因, 甚至长期维护人员并不总是熟悉一个项目的每一部分。给他们同样的耐心, 你也会得到同样的回报。 😇 “感谢查看了这个错误，我按照您的建议做了，这是输出结果。” 😢 “你为什么不修复我的问题？这难道不是你的项目吗？” 尊重社区的决定。你的想法可能会和社区的优先级、愿景等有差异，他们可能对于你的想法提供了反馈和最后的决定的理由，这时你应该去积极的讨论，并寻求妥协的办法，维护者必须慎重的考虑你的想法。但是如果你实在是不能同意社区的做法，你可以坚持自己！保持自己的分支，或者另起炉灶。 😇 “你不能支持我的用例，我蛮失望，但是你的解释仅仅是对一小部分用户起作用，我理解是为什么。感谢你的耐心倾听。” 😢 “你为什么不支持我的用例？这是不可接受的！” 开源是由来自世界各地的人们共同协作实现的。面临的问题是跨语言、跨文化、不同的地理为止、不同的时区，另外，撰写文字的沟通更是难上加难，无法传达语气和情绪。请让这些会话都充满善意吧！在以下情形中请保持礼貌：推动一个想法、请求更多的上下文、进一步澄清你的立场。既然你在互联网找到了自己的所需，那么请尝试让它变得更好！ 创建 issue你应该在遇到下列情况下，去创建一个 issue： 报告你自己无法解决的错误 讨论一个高级主题或想法 期望实现某新的特性，或者其它项目的想法 在 issue 的沟通中几点实用的技巧: 如果你刚好看到一个开放的 issue，恰是你打算解决的，添加评论，告诉他人你将对此展开工作，并及时响应。这样的话，可以避免他人重复劳动。 如果说某个 issue 已经开放很久了，这可能是已经有人正在解决中，又或者是早已经解决过了，所以也请添加评论，在打算开始工作之前，最好是确认一下。 如果你创建了一个 issue，但是没多久自己解决了，也要添加评论，让其他人知道，然后关闭该 issue。记录本身就是对社区的贡献。 创建 pull request在下面的情形时，请你务必使用 PR： 修复缺陷( 例如，纠正拼写错误、损坏的链接、或者是其它较明显的错误） 开始一项别人请求的任务，或者是过去在 issue 中早就讨论过的 非常推荐大家在 issue 中挑选自己自己感兴趣，能够胜任的任务，并评论，Maintainer 或者其他有权限的角色会给你进行指派，同样的，也确保某些 issue 是否已经被其他人认领了。 一个 PR 并不代表着工作已经完成。它通常是尽早的开启一个 PR，这使得其他人可以反馈意见。在发送pull request之前，请同步 github 仓库和远程仓库，这会使 pull request 简单明了，具体操作请看如下所示步骤： 123456git remote add upstream git@github.com:alibaba/higress.gitgit fetch upstreamgit rebase upstream/maingit checkout -b your_awesome_patch... add some workgit push origin your_awesome_patch Higress AI Landscape MCP 服务管理。Higress 开源控制台将直接提供开箱即用的 MCP 服务管理入口，支持三类场景： OpenAPI 转 MCP，DB 转 MCP，SSE/Streamable MCP 直接路由能力，借助该功能，用户可以基于 Higress 快速构建出自己的 MCP Marketplace。【5 月底至 6 月上旬】 Nacos MCP Registry 集成。Nacos 3.0 已经支持集成 Higress，将会进一步优化集成方案，支持从 Nacos 3.0 导入 MCP 服务，使得 Higress x Nacos 3.0 集成更加丝滑。【6 月至 7 月】 Agent 管理。Higress 将 Higress 提供面向多场景的 AI Agent 应用管理能力，实现 Agent 应用的发现、Agent Card 管理、A2A 协议转换、上下文内存管理等面向 A2A 协议的核心能力，帮助开发者以低代码方式快速构建 AI 应用。在能力规划方面，Higress 正在推进动态 Prompt 工程、可插拔工具集、Agent 应用灰度策略及基于 A2A 协议的多 Agent 协同编排等高级特性开发。【6 月至 9 月】 AI API 管理。Higress 作为 AI 原生网关，通过 API 货币化管理、统一鉴权与流量治理、全链路可观测性等核心能力为企业提供完整的 AI API 管理解决方案。此外，Higress 规划构建 MCP 开放市场、Agent 开放市场，两大开放市场加速 AI 应用生态，助力开发者快速构建与部署智能化 Agent 应用。【6 月至 9 月】 Higress 社区如有开源贡献意向或者 Higress 使用问题，可以加群交流，Higress 社区交流 3 群，钉钉群号：107690002780。","link":"/higress-opensource-md/"},{"title":"Higress Plugin Server：简化 Wasm 插件私有化部署难题","text":"这篇文章将向大家介绍 Higress 近期在 Wasm 插件生态方面的一个进展——Higress Wasm 插件服务器（Higress Plugin Server）。这个新的组件解决了用户在私有化部署 Higress 网关时拉取插件的痛点，优化了插件的下载与管理效率。 一、Wasm 插件：Higress 的扩展能力与挑战Higress 自开源以来就一直将 Wasm 技术视为核心的网关扩展手段。Wasm 带来的工程可靠性、沙箱安全性、热更新能力以及 Higress 团队在此基础上构建的域名/路由级生效、Redis 访问能力、AI 特性支持等，都丰富了网关的扩展性，并为企业用户带来了性能提升和成本降低。通过自定义 Wasm 插件，用户可以根据自身的业务需求，在网关层完成鉴权、加解密、会话管理等逻辑，减少了额外资源的小号，降低了后端服务的处理负担。 尽管 Wasm 插件技术本身具备优势，但在实际的企业级部署和大规模应用场景中，我们依然面临一些实际的挑战，主要体现在以下几个方面： 1. OCI 机制带来的私有化部署挑战 当前，Higress Wasm 插件的下载和管理主要依赖 OCI（Open Container Initiative）仓库。 关于 OCI、oras 和 Docker OCI (Open Container Initiative)： OCI 旨在为容器镜像和运行时定义开放标准，以确保不同容器技术之间的互操作性。在云原生生态中，OCI 镜像仓库（如 Docker Hub、Harbor、registry.k8s.io 等）是分发容器镜像的标准方式。Higress 最初将 Wasm 插件作为 OCI Artifacts 发布，即将其打包成符合 OCI 规范的制品并存储在 OCI 仓库中。 Docker： Docker 是目前最流行的容器平台，它使用 OCI 镜像作为其核心分发格式。通常，我们使用 docker pull 和 docker push 命令来拉取和推送容器镜像。 oras (OCI Registry As Storage)： oras 是一个命令行工具，它允许用户在 OCI 仓库中存储和管理任意内容，而不仅仅是容器镜像。对于 Higress Wasm 插件这类非标准的容器镜像（Wasm 模块），oras 提供了一种方便的方式来与 OCI 仓库进行交互（例如拉取、推送 Wasm 插件）。 OCI 机制的挑战 虽然 OCI 机制在云原生环境中是标准且高效的方式，但对于一些企业，尤其是对网络安全性有严格要求的私有化部署场景来说，OCI 仓库的引入成了一个不小的门槛，并带来了以下问题： 技术门槛与工具使用不便： 许多用户可能习惯于应用程序的直接安装或通过简单的包管理器进行部署，而对容器镜像、OCI 标准、以及 oras 这类专门用于非镜像内容的 OCI 工具的使用并不熟悉。这增加了 Wasm 插件的上手难度和运维复杂度。 网络限制与私有化部署： 许多企业内部网络严格，对外部公共网络（如 Docker Hub、GitHub Container Registry等）的访问有严格限制。私有化部署环境下，更是会因为无法访问外部公开仓库而导致无法配置插件和更新插件。 额外的基础设施搭建： 为了在私有环境中拉取插件，用户可能需要单独部署和维护一个内部 OCI 仓库。这无疑增加了部署的复杂度和运维成本，对于仅需简单使用 Wasm 插件的用户而言，为了几个插件去搭建和管理一套完整的 OCI 生态，显得过于笨重。 插件迁移困难： 在不同私有化环境（如开发、测试、生产）之间迁移 Wasm 插件时，由于 OCI 仓库的独立性，往往需要用户手动进行插件的拉取、推送和版本管理，且需要适配不同仓库的认证和网络配置，这增加了操作的复杂性和出错的可能性。 2. 重复下载与性能开销：Always 策略的隐忧 Higress 网关拉取 Wasm 插件时，支持插件拉取策略的配置，默认为 IfNotPresent，即本地存在则不重新拉取。这在大多数情况下是合理的。但当用户希望 Wasm 插件能够及时更新（例如在开发测试环境中频繁迭代）或者希望确保始终使用最新版本的插件时，会倾向于将策略设置为 Always，导致以下问题： 网络延迟与带宽消耗： Always 策略意味着每次 Wasm 插件被引用或网关 Pod 重启时，都会尝试从 OCI 仓库重新下载插件。会引入不必要的网络延迟，并消耗带宽资源。 冗余操作： 即使插件内容没有变化，Always 策略也会触发下载。尽管 OCI 协议本身支持内容哈希校验，但客户端依然需要与仓库进行通信以确认。 3. 用户体验与操作复杂度： 上述问题共同导致了用户在配置和使用 Wasm 插件时，可能会面临不必要的复杂性。我们希望能够提供一种更简单、更符合直觉的插件分发方式，让用户能够更专注于业务逻辑的实现。 正是基于这些痛点，我们开发了 Higress Wasm 插件服务器。 二、Higress Plugin Server：基于 HTTP 的简单、高效分发我们的核心思路是：提供一个简单、高可用、基于 HTTP 协议的文件服务器来分发 Wasm 插件。选择 HTTP 的原因是它无处不在，易于部署，易于集成，并且在许多企业内部环境中，HTTP 文件服务是标配且配置简单。 这个新的组件，higress-plugin-server，它承担了以下核心功能和目标： 功能目标： 提供一个 HTTP 文件服务器，支持网关通过类似 http://higress-plugin-server.higress-system.svc/plugins/&lt;plugin-name&gt;/&lt;version&gt;/plugin.wasm 的路径下载 Wasm 插件。 高可用性： 作为核心组件，它必须能够稳定运行，保障插件分发服务的 SLA。 版本管理： 镜像内部集成了所有 Wasm 插件，并与插件版本严格绑定。目前插件版本的设计思路是将插件版本和 Higress 网关版本解耦，采用的是覆盖的思路。后续如果有插件版本管理的话，Plugin Server 可以直接支持；如果仍维持现状的话也可通过 Plugin Server 提供的 metadata 中的 md5 字段来区分插件版本。 兼容性： 与现有的 OCI 插件加载方式无缝共存，用户可以根据自身需求灵活配置插件拉取策略。 它的推出，为用户带来了以下优势： 私有化部署利器： 摆脱对复杂 OCI 仓库的依赖。用户只需直接使用我们预构建的 higress-plugin-server 镜像，即可在任何网络环境下快速、安全地分发 Wasm 插件。 简化与开箱即用： 对于大多数用户而言，无需过多额外配置，甚至无需感知 OCI 仓库的存在。只需在 Higress 默认的 Helm Chart 安装命令上添加一个参数即可自动部署和配置插件服务器，实现开箱即用的体验。 性能与高可用保障： HTTP 文件服务本身高效且易于缓存，部署在内网后，能够降低插件下载的延迟和失败率，通过 K8s 部署多副本可以实现高可用。 与现有机制无缝共存： 用户仍可通过 Higress Console 或环境变量配置 OCI 仓库地址，两者互不干扰。 三、架构设计与实现：从 OCI 到 HTTP 的实现机制为了实现上述目标，我们从零开始设计和构建了 higress-plugin-server 组件。 3.1 独立组件：higress-plugin-server 仓库我们创建了一个 GitHub 仓库 https://github.com/higress-group/plugin-server，仓库结构如下： 12345678910plugin-server/├── Dockerfile # 构建 Nginx 镜像的核心├── nginx.conf # Nginx 配置文件，用于服务静态文件├── pull_plugins.py # 插件下载脚本，从 OCI 拉取插件├── plugins.properties # 插件列表及版本配置├── deploy/ # Kubernetes 部署文件（Service &amp; Deployment）│ └── service.yaml│ └── deployment.yaml└── .github/workflows/ # GitHub Action 流水线定义，实现自动化构建与推送 └── build-plugin-server-image-and-push.yml 3.2 构建流程：内部拉取 OCI 插件并转为 HTTP 服务plugin-server 的核心在于其 Dockerfile： 123456789101112131415161718192021222324252627282930313233# 构建阶段：处理插件和元数据FROM python:3.11-alpine AS builder# 安装系统依赖，包括 ORAS 客户端RUN apk add --no-cache \\ wget \\ ca-certificates \\ &amp;&amp; update-ca-certificatesRUN set -eux; \\ ORAS_VERSION=&quot;1.2.3&quot;; \\ ARCH=$(uname -m | sed 's/x86_64/amd64/;s/aarch64/arm64/'); \\ wget -O /tmp/oras.tar.gz &quot;https://github.com/oras-project/oras/releases/download/v${ORAS_VERSION}/oras_$(echo ${ORAS_VERSION})_linux_${ARCH}.tar.gz&quot; \\ &amp;&amp; tar -zxvf /tmp/oras.tar.gz -C /usr/local/bin \\ &amp;&amp; rm -rf /tmp/oras.tar.gz oras \\ &amp;&amp; oras version# 复制并执行 Python 脚本，拉取插件WORKDIR /workspaceCOPY pull_plugins.py plugins.properties ./RUN python3 pull_plugins.py# 运行阶段：最终镜像FROM docker.io/nginx:alpine# 从构建阶段复制生成的文件COPY --from=builder /workspace/plugins /usr/share/nginx/html/plugins# 复制 Nginx 配置COPY nginx.conf /etc/nginx/nginx.conf# 暴露端口并启动 NginxEXPOSE 8080CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] 关键点解析： 插件拉取： 在 builder 阶段，我们利用 Python 脚本 pull_plugins.py 和 ORAS 客户端工具，从 Higress 官方的 Wasm 插件仓库拉取所有必要的 Wasm 插件。 文件组织： 拉取下来的插件文件，严格按照 plugins///plugin.wasm 的结构组织，例如 plugins/ai-proxy/1.0.0/plugin.wasm。这种结构清晰明了，便于 HTTP 路径访问。 元数据： 除了 plugin.wasm 文件本身，我们还生成了 metadata.txt 文件，包含插件名称、大小、MD5 值、创建/修改时间等信息。这为未来实现更智能的插件更新（例如，网关在 Always 策略下，通过比对 MD5 值来判断是否需要重新下载）提供了基础。 Nginx： 最终镜像基于轻量级的 nginx:alpine。nginx.conf 配置了 Nginx 作为静态文件服务器，开启了 autoindex on（方便调试查看目录内容）和 default_type application/octet-stream（确保 Wasm 文件能正确下载）。 精简高效： 多阶段构建确保了最终的 Nginx 镜像只包含运行所需的最小文件集，避免了不必要的依赖，保持镜像体积小巧。 3.3 插件与镜像版本管理我们为 higress-plugin-server 镜像设定了一个明确的版本标签 1.0.0，而不是跟随 higress-gateway 和 higress-console 的版本演进，主要考量是判断其不会有太多版本发布的需求。 通过 GitHub Actions，我们实现了自动化构建和推送流程。当 higress-plugin-server 仓库的 main 分支有更新时，GitHub Action 会自动触发构建，并推送到 Higress 官方的镜像仓库。未来计划监听 higress 主仓库中插件相关 PR 触发镜像构建，以实现插件的动态更新。 3.4 Kubernetes 集成为了让用户能够便捷地部署 higress-plugin-server，我们将其 Kubernetes 部署配置（Deployment 和 Service）直接集成到了 Higress 主仓库的 Helm Chart 中（higress/helm/core）。 新的 Chart 模板： 在 higress/helm/core/templates 目录下新增了 plugin-server-deployment.yaml 和 plugin-server-service.yaml。 可配置性： 通过 values.yaml，用户可以灵活配置 higress-plugin-server 的副本数、镜像版本、资源限制等参数。 一键启用： 核心改动是引入了一个全局的 Helm 参数 global.enablePluginServer。当用户将此参数设置为 true 时，Helm Chart 会自动渲染并部署 higress-plugin-server 组件。 123456789101112# higress/helm/core/values.yaml...global: enablePluginServer: false # 默认关闭，用户可设置为 true 启用...pluginServer: name: &quot;higress-plugin-server&quot; replicas: 2 # 默认2副本，推荐高可用 image: plugin-server hub: higress-registry.cn-hangzhou.cr.aliyuncs.com/higress tag: &quot;1.0.0&quot; # 明确的镜像版本 # ... 其他资源限制、端口配置等 3.5 Console 联动：用户体验的最后一步为了实现无缝的用户体验，我们还修改了 higress-console 仓库。当用户选择启用 plugin-server 时，Higress Console 会自动将 Wasm 插件的默认下载 URL 配置为 http://higress-plugin-server.higress-system.svc/plugins/${name}/${version}/plugin.wasm。 1234567891011121314# higress-console/helm/templates/deployment.yaml...spec: template: containers: env: {{- if and .Values.global.enablePluginServer (not (hasKey .Values.podEnvs &quot;HIGRESS_ADMIN_WASM_PLUGIN_CUSTOM_IMAGE_URL_PATTERN&quot;))}} - name: HIGRESS_ADMIN_WASM_PLUGIN_CUSTOM_IMAGE_URL_PATTERN value: &quot;{{ .Values.pluginServer.urlPattern }}&quot; {{- end }}# higress-console/helm/values.yaml...pluginServer: urlPattern: &quot;http://higress-plugin-server.higress-system.svc/plugins/${name}/${version}/plugin.wasm&quot; 智能判断： 值得注意的是，我们增加了判断逻辑：如果用户已经通过环境变量手动配置了 HIGRESS_ADMIN_WASM_PLUGIN_CUSTOM_IMAGE_URL_PATTERN，则优先使用用户的配置，避免自动覆盖用户意图。这保证了灵活性和兼容性。 四、Higress Plugin Server 使用指南现在，介绍如何体验和使用 Higress Plugin Server。我们力求将其做到最简化，让用户能够轻松上手。 4.1 一键部署从 Higress 的下一个版本（预计 v2.1.5）开始，只需在安装 Higress 时，额外添加一个 Helm 参数，即可同时部署 Higress 核心组件和 Wasm 插件服务器： 12helm repo add higress.io https://higress.cn/helm-chartshelm install higress -n higress-system higress.io/higress --create-namespace --set global.enablePluginServer=true --render-subchart-notes 执行上述命令后，可以通过 kubectl get pods -n higress-system 查看到名为 higress-plugin-server 的 Pod 正在运行。 4.2 Higress console 自动适配：默认从插件服务器下载一旦 higress-plugin-server 被部署并运行，Higress Console 会自动检测到并将其设为 Wasm 插件的默认下载源。 无需进行任何额外配置，即可在 Higress Console 的插件管理页面看到插件的默认下载 URL 已经自动更新为： http://higress-plugin-server.higress-system.svc/plugins/${name}/${version}/plugin.wasm 4.3 灵活配置：满足个性化需求 验证插件下载： 可以进入 Higress Gateway 的任意 Pod 内部，通过命令行验证插件服务器是否正常工作，例如： 1kubectl exec -it &lt;higress-gateway-pod&gt; -n higress-system -- curl http://higress-plugin-server.higress-system.svc/plugins/key-auth/1.0.0/metadata.txt 如果返回插件的元数据信息，则表示插件服务器工作正常。 自定义镜像与插件： 如果有定制化的 Wasm 插件，可以自行构建 higress-plugin-server 镜像。克隆 higress-plugin-server 仓库，修改 plugins.properties 文件以指定自己的插件列表和 OCI 源，然后执行 docker build 构建并推送到自己的镜像仓库。最后，在 Helm 安装时，修改 pluginServer.image 和 pluginServer.tag 参数，指定为自定义镜像。 imagePullPolicy 策略： 默认情况下，Higress Wasm 插件的拉取策略是 IfNotPresent。在当前的 higress-plugin-server 版本中，即使设置为 Always 策略，网关也会每次请求 plugin.wasm 文件。未来，我们会利用 metadata.txt 中的 MD5 等信息，优化 Always 策略下的拉取逻辑，实现智能缓存和按需更新。 4.4 存量版本使用 Plugin Server由于 Higress v2.1.5 截止文章发布时还未 release，或者有存量版本升级大版本的顾虑等其他原因，希望单独使用 Higress Plugin Server，也可以参考 https://github.com/alibaba/higress/blob/main/helm/core/templates/plugin-server-deployment.yaml 单独部署 Plugin Server 并配置 higress-console 的环境变量 1HIGRESS_ADMIN_WASM_PLUGIN_CUSTOM_IMAGE_URL_PATTERN=http://higress-plugin-server.higress-system.svc/plugins/${name}/${version}/plugin.wasm 即可使用 Higress Plugin Server。 五、展望未来坦诚来讲，该特性的复杂度并不高，但 Higress Wasm 插件服务器的推出，是 Higress 团队在持续优化用户体验、降低企业级部署门槛上的重要进展。它使得 Wasm 插件在各种复杂的私有化环境中都能够顺畅运行，也为未来更多高级的插件管理功能打下了坚实的基础。 未来，我们还将持续优化 higress-plugin-server： 更智能的 Always 策略： 利用 metadata.txt 中的哈希值，实现插件内容的客户端校验，避免不必要的重复下载。 细粒度版本管理： 目前 Wasm 插件虽然提供了版本号机制，但缺少版本号更新的实践，一直在沿用 1.0.0 版本，未来会探索更灵活的插件版本管理机制，并支持回滚、灰度发布等。 更完善的可观测性： 增强插件服务器的监控和日志能力，帮助用户快速定位问题。 我们相信，Higress Wasm 插件生态将随着这些基础设施的完善而持续发展。欢迎大家积极尝试 higress-plugin-server，并向我们反馈宝贵的意见和建议。","link":"/higress-plugin-server/"},{"title":"Kirito 全屋定制记 | 纯小白向全屋定制攻略","text":"前言继上一篇文章《Kirito 杭州买房记 | 纯小白向杭州购房攻略》过去已经有 2 年了，预计在今年 6~9 月，我的房子就要交付了，所以我也开始了全屋定制之路。现在杭州的期房大多数是精装修交付，也就是说厨房和卫生间等部分都包含在房价中，我需要操心的全屋定制部分仅包含： 柜子部分 卧室衣柜 书房书柜 餐边柜 电视柜 家具部分 沙发 茶几 餐桌 最近一两个月，我跑遍了附近各个全屋定制的商家，从一个装修小白，成长为了一个略懂的小白，本文记录了我这段时间的积累，可以当做一份入门攻略。 本文主要介绍打柜子的部分，家具部分捎带提一下。 本文偏小白向，如果描述有偏差，欢迎指正，适合阅读人群：从未经历过但即将需要全屋定制，未亲自参与过全屋定制，关注了 Kirito 并好奇怎么发了一个技术无关的文章的读者。 板材分类在完全不了解板材时，我的第一印象告诉我实木肯定是最好的，价格肯定也贵，但其实稍微了解就会知道，人造板材才是全屋定制的主流。但同为人造板材，也因为工艺不同、原材料不同、胶水不同，决定了它们拥有不同的价格。 在跑完几家全屋定制之后，我从一开始的不懂，变成了一团浆糊，因为我发现，可能是同一个类型的板材，几家全屋定制都有各自的叫法！什么实木颗粒板，爱心板，康纯板，xx 板，在我心目中，商家搞这些感觉就是混淆视听，总结下来，主流的分类：颗粒板、多层板、欧松板，这是几乎每家都有的叫法，另外还有一些：生态板、密度板、指接板，基本每家都有自己的叫法，如果真的要完全搞懂这些板材的原理，还是得专门花点功夫，可以大概参考下图： 对于如此众多的板材，第一点我想聊的便是环保等级。 环保等级我国对人造板甲醛释放量的标准规定为：2021 年 10 月，新国标 GB/T39600-2021《人造板及其制品甲醛释放量分级》实施，对甲醛释放限量值进行了分级： E1≤0.124mg/m³ E0≤0.05mg/m³ ENF≤0.025mg/m³ 你以为我要说环保很重要吗？不，我其实挺无感环保的，因为这些全屋定制厂商达不到标准，早就倒闭了，还敢公然出售吗？而你能够正常逛到的这些门店，可以印证出它们板材的甲醛释放量都在国家规定的范围类，这是及格线，而不是优秀线，而如果一个厂商只会跟你吹嘘环保等级，可能说明他家也没有其他能拿得出手的东西。 以兔宝宝、千年舟、莫干山这些知名的板材提供商为例，大多数板材都可以达到 ENF 级别，少数板材如颗粒板，可以达到 E0 级别；而传统木工打出来的柜子，硬装使用的板材通常能达到 E0 级别。 另外需要注意点的是，ENF 和 E0 等环保等级主要针对人造板材料中甲醛的释放量进行规定，而对其他有害物质的含量如：苯、甲苯、氨、氡、挥发性有机物（VOC）等则没有具体规定。板材厂商在实际生产过程中，为了达到 ENF 和 E0 等环保等级，一般会同时考虑其他有害物质的含量，以确保产品的环保性能。现实情况，在选购板材时，我们无法像专业人士那样去一项项确认，只能靠选择有一定知名度的品牌，以及入住时必要的验房流程来规避。 价格影响因素颗粒板、欧松板、多层板等基材由于使用的原材料、加工工序、胶水、封边工艺等不同，价格自然也是差别很大，以我了解的一家全屋定制的报价为例，单位：每平方米投影面积 颗粒板：1000 多层板柜门 + 生态板柜体：1300 欧松板柜门 + 生态板柜体：1400 欧松板：1600 价格说明一切，有钱就上欧松板，准没错。 另外，我也了解一些知名的全屋定制厂商使用了进口的爱格板，本质上也是颗粒板，但报价却可以达到 1800，一方面自然是因为品牌溢价，另外一方面也是因为一些国外的工艺，国内的确达不到导致的（例如进口的颗粒板在做一柜到顶设计时，不用增加拉直器）。 同样类型的板材，在不同厂商之间也是有价格差异的，我的了解主要分为以下几个层面： 木头的基材，使用速生型的木材，稳定性差，价格会便宜，同样是速生型的木材，温带和寒带生长出来的品质也有差异，一般寒带的木材价格会高。 胶水，有植物大豆蛋白胶、MDI 胶等，大多数装修过程中产生的甲醛来自于胶水，而 MDI 胶则具有甲醛含量非常低的优势，所以使用了 MDI 胶的板材通常比较贵。 化学知识小科普：MDI 胶的生产需要使用二异氰酸酯（MDI）作为主要原料，而 MDI 本身是一种反应性非常强的化学品，具有毒性和腐蚀性，要求生产环境必须严格控制，工艺和设备也需要高度的安全性和稳定性。所以具备 MDI 胶水生产资质的企业在全球范围类都非常有限。MDI 与聚醚多元醇等其他原料结合后，形成多聚体，即聚氨酯，是一种高分子化合物，具有很强的化学稳定性，虽然原料毒性强，但制品稳定高。 封边工艺，封边指的是对板材、木材等建材进行边缘封口处理，以防止表面板材受到潮湿、潮气等外界因素的侵蚀而导致老化、脱落或变形等情况。销售一般会介绍三种封边工艺：EVA 封边、PUR 封边、激光封边。价格自然也是从低到高。作为一个小白，这些专业术语容易把人搞晕，我的建议是到厂商的展厅中自己观察下封边的效果，在我的视角里：PUR 封边即可。 计价方式主要分为：按投影面积算、按展开面积算。 展开方式计价更精确，投影面积计价更方便，两者没有孰优孰劣，建议可以让商家同意折算成投影面积计价，给一个粗略的报价，这样可以在多个商家之间进行价格的对比。 板材用途因材制宜使用板材，方能实现最大的性价比，什么，你不缺钱？那可以跳过这一条了。 同一品牌的颗粒板和欧松板，自然是欧松板更贵一些，但也不是说一定要全部使用欧松板。以生态板做柜体 + 欧松板做柜门的组合为例，生态板的优点是环保、耐用、防潮、防火，但缺点是其表面不够坚硬、不够平整。因此，生态板制作的柜体可以满足存储和支撑的要求，但不适合用作柜门，因为柜门需要经常开关，必须具有足够的强度和平整度。同时，柜门的外观要求一般较高，需要使用其他材料比如实木、PVC 板等。而欧松板用作柜门，就不存在生态板的这些缺点，整体搭配使用可以省下一些钱。 在一开始了解全屋定制时，我一听销售介绍说颗粒板很便宜，就对颗粒板产生了一定的偏见，但现在了解了很多东西回头再来看时，思想已经变了，在保证环保的前提下，利用颗粒板易于造型的特性，也可以打造出较好效果的全屋定制，凡事不可一概而论。 饰面 刚刚很大的篇幅都是花费在了板材上，还有一个不可或缺的部分便是饰面，简单来说就是柜门柜体上附着的那一层，有以下几个类别： 三聚氰胺饰面，市面上使用最广泛的饰面，也称为双饰面，生态板、免漆板都是一回事。最便宜，一般默认饰面都是使用该材质。 PVC 饰面，比较常见的覆膜饰面，通常搭配密度板作为基材，用这种工艺制作而成的板叫膜压板或吸塑板。PVC膜很柔软，适合做凹凸复杂的造型，一般出现在欧式、美式还有轻奢风格里面。 PET饰面（亚克力），PET板材就是在板材表面印刷纸上再压贴一层PET膜，PET膜无色透明，有肤感和高光两种效果。 烤漆饰面，与字面意思一样，喷一喷，再烤一烤，喷漆后进烘房加温干燥。烤漆号称“工艺之王”，工艺复杂，非常耗时。所以价格昂贵，而且不耐划，一旦损坏，不好修补。 实木贴皮饰面，将实木皮包覆在基材上，再进行油漆工艺。拥有实木的纹理和质感，效果真实，价格也最高。 印象比较深的是一些比较高端的全屋定制品牌 rara、vifa 偏向于使用烤漆来打造意式风格，价格也真的是不便宜，在已有基材的基础上几乎要增加一倍的价格，不过看烤漆的效果也是真的高端，高级感满满。 另外一个印象比较深的品牌是木里木外，主要打造新中式风格，很多实木贴片的造型，价格比烤漆还要贵。 我个人更偏实用主义，追求性价比，更希望把钱花费在收纳上，并没有对饰面有过多的追求，暂定使用普通的饰面纸就行。 增值项需要留意的一点是，很多品牌商家通常以一个比较低价的套餐吸引你进店，但到了设计阶段，很多细节都是需要加钱的，这一点无可厚非，所以必须要知道有哪些增值项： 一门到顶的柜门设计 隐藏式拉手、长拉手、反弹器 五金（一般仅赠送基础的铰链） 拉直器（国内的大多数板材在设计一门到顶风格时，需要安装拉直器，避免柜门变形） 玻璃柜门 灯带、变压器、灯带开关 抽屉 非标尺寸（背板加厚，抽屉加长，柜身加深） 这些统统是要加钱的，好看的东西要额外付费，天经地义。可以跟商家沟通好，哪些是可以自己网上购买的，不一定全部从全屋定制厂商处购买。 品牌我个人看过的全屋定制品牌有以下几家，为了避免得罪这些厂家，事先声明，以下言论纯属个人观点，存在主观成分，具体是否适合你，还是现场去咨询下销售比较合适，我的观点仅供参考。排序按照我线下去门店的时间顺序： 满屋严选：服务好，销售比较热情，但感觉营销味道和包装成分太重，而我深知，售前服务其实也是算进了最后价格中的，追求性价比的话这一点就扣分了。板材主要采用的是颗粒板，跟尚品宅配有合作，沙发 + 茶几 + 餐桌需要和打柜子配套购买，不太灵活，且套餐内的商品单价不透明。如果不太想操心全屋定制所有的东西，倒是可以选择满屋严选这种模式，比较省心，我个人还不至于忙到连全屋定制都没时间参与设计的程度，不太适用于该模式。天空之城三期开始进场，一二期无案例。 我在家：可以只打柜子，也可以在他们家买家具，不是特别出名的品牌，但也有开了 13 年。我还挺有好感的一家，销售很专业，价格也比较美丽。有天空之城一二期落地案例。 兔宝宝全屋定制（加盟店）：专门打柜子的，兔宝宝的板材没得说，业内有口皆碑，我父母辈也了解的品牌，属于是板材起家，向全屋定制市场进军了。但我看的这家是一个加盟店，并不是直营店，接待我的甚至都不是销售，而是设计师，说明商业模式上比其他家欠缺很多。价格相比前一家高一些，但这点品牌溢价完全在可以接受的范围内。有个别天空之城落地案例，但不多。有意思的一点，一些善于营销的全屋定制都开始建小区群营销了，兔宝宝全屋定制却没有，这一点不算加分也不算减分吧，仅此一提。 某不知名品牌工厂店：在一个相对偏僻的地方，一度以为找错了地方，之所以找到他们家，是因为他们家卖的是千年舟的板材，所以去了解了一下，毕竟是工厂店，价格真的便宜，既有千年舟购买的板材，也有其他品牌的板材，非加盟模式。但我这毕竟是第一套在杭自住的房子，这类工厂店追求的极致性价比不太符合我的需求。 顾家：顾家家居卖的沙发感觉还是挺不错的，打柜子貌似就比较一般了，了解了一下价格，不太美丽，而且销售不太专业，很多细节问了都不清楚，也没有介绍清楚优势。最后在顾家买了一个沙发和床垫。 RARA：我线下逛过的真正意义上第一家能称得上全屋定制的厂商，姑且用高端全屋定制来形容吧。主打意式风格，柜门一般都是烤漆饰面，价格也不便宜，按投影算的话，7000 一个平方左右，算是设计的溢价了，大量采用开放的设计，其实不太适合于我这种 135 平方的商品房，可能大平层大别墅用起来效果会很好。总的来说 RARA 告诉了我，原来全屋定制还可以这样，不然我以为只是打打柜子呢，有很多设计理念，销售介绍的非常专业。RARA 各方面都挺好的，至于价格，那是我的问题。 VIFA：和 RARA 类似，也是偏高端的全屋定制，都是烤漆设计，意式风格，连价格都是一个区间的，由于没有提前预约，销售对待我们也是对待散客一样的态度，花了不到 20 分钟就看完了，没有了解到和 RARA 有差异性的地方。 木里木外：如果 RARA 是金发碧眼的意大利女郎，木里木外便是亭亭玉立的阁中闺秀了。从进入木里木外展厅需要穿鞋套时，我就意识到问题的严重性了。木里木外是中国本土的品牌，主打的是新中式的设计，用我爱人的原话评价：在来木里木外之前，从未设想过原来实木贴皮的设计风格可以做到这么高端大气，并且木里木外的模式采用的深度优化模式，即需要用户先提供一般全屋的概要设计，再由木里木外的设计师进行精加工，属实不是我需要的“全屋定制”了。 丽堂：回归到一个普通的全屋定制了，欧松板是自己的品牌直营，多层板/颗粒板是跟千年舟拿的板材，综合来看性价比也非常不错，但也没有看到特别惊艳的地方。 选品逻辑这么多全屋定制品牌，如何选择？我并非家装行业的专业人士，所以偏主观，我认为有四个基本考虑因素： 材料和工艺 设计 品牌 安装和售后 首先看最后一点安装和售后，经常听到有销售说全屋定制 = x 分产品 + x 分安装，强调安装在全屋定制中的重要性，品牌再高端，无法确定师傅的水平，可能安排给你上门的师傅，就是个学艺不精的学徒，这只能靠第 3 点品牌来保证，降低安装出错的可能性。当然也不是把这点完全交给运气，据我的了解，只有 RARA 的销售提出了一个点，解决了安装的问题，即 RARA 的组装都是模块化的，很多地不平之类的问题是可以通过调节柜子底部的支撑部件去解决的，相当于通过产品化方案解决了交付时的问题，这点给了我一定的震撼。 再看设计这一个环节，其实是一个重灾区，我搜索了一些资料，这么说吧，很多小品牌其实是无设计可言的，卖板材、卖家具、卖五金起家的厂商都投入到全屋定制的行业中，导致鱼龙混杂，给你做设计的设计师，可能刚培训出来，无从考证，毕竟设计这个东西是很难量化的，如果我告诉你一个设计师来自于意大利米兰，另一个来自于浙江杭州余杭，人们总是下意识地觉得前者具有艺术气息，人们不会去考察两者的设计图，这就是刻板印象。所以设计这一点，要么亲力亲为，全程参与，要么只能靠品牌溢价去支撑。 所以这四个基本考虑因素中，唯一真实可以考察的只有第一和第三点了。 当我在选择全屋定制品牌时，我在考虑什么？ 第一类：高端品牌 RARA、木里木外、VIFA 35% 材料与工艺 + 35% 设计 + 30% 品牌价值 第二类：大众品牌 欧派、索菲亚 30% 材料与工艺 + 10% 设计 + 60% 品牌价值 第三类：大众品牌 兔宝宝全屋定制 60% 材料与工艺 + 10% 设计 + 30% 品牌价值 第四类：各类工厂店 100% 材料与工艺 第一类品牌适用于高端人群，核心竞争力是产品和设计 第二类品牌的核心竞争力是知名度，往往都会请一些明星当自己的代言人 第三类品牌的核心竞争力是材料和工艺，比较务实 第四类品牌价格自然最低，但设计和品牌价值欠缺，不会给人任何信任感，完全看运气 明白自己需要什么，就以我个人为例，我的核心诉求是自住 我不追求高端的设计，钱包不允许，所以第一类品牌等我有钱了，下一套房一定上 第二类品牌我选择的起，但花了一定的溢价在品牌价值上，我获得的材料和工艺变差了，性价比不高 我比较青睐第三类品牌，但设计上我对这类品牌是没有信心的，所以需要亲自参与进去，也没有太复杂，B 站小红书，或者找朋友装修好的房子抄作业，都可以，又想省心又想省钱，没有这么好的事情。 第四类，如果不是我自住的投资房，可能考虑吧。 顺带一提：家具在逛全屋定制的过程中，也顺带了解了一下家具，我自己的想法是，买家具和打柜子还是要分开的，但是设计要一起设计。 分开的原因是，很少有一家厂商能提供好全屋设计方案的同时，又能做好家具的，二者不可得兼。 设计要一起设计，是因为在做全屋定制时，柜子的颜色和布局需要和沙发等家具保持一致的风格，无论是先买好沙发，告知设计师如何设计，还是按照设计师的推荐，购买推荐的款型，均可。 全屋定制无法走线上渠道，但家具一般都有线下门店和电商两个渠道，虽然很多厂商为了避免比价，特地做了两个渠道产品线的差异化，但从个人了解的情况来看，相似款型的家具，电商渠道会有比较大的优惠，如果线下有特别喜欢款型的家具，我会考虑线下购买，如果仅仅是一般，我会考虑直接网购。 总结再次强调，本文的建议都是主观视角，哪怕是对某一品牌的负面评价，也有可能是由于个别门店，个别人员导致，并非针对该品牌的评价。还是推荐线下去门店了解，绝知此事要躬行。 关于上文提及的设计部分如何亲力亲为，本文暂未详细提及，其实也就是多看看 B 站 + 小红书 + 真实案例，可能后续会单独整理一篇文章介绍。总之的我建议便是，设计部分不亲自参与，要么就多花钱为之付费，要么就等着后悔。 目前，我还没有完全确定使用哪家全屋定制方案，如果有推荐或者建议，也欢迎微信跟我交流：xiayimiaoshenghua。","link":"/house-customization/"},{"title":"JAVA 拾遗 --Instrument 机制","text":"最近在研究 skywalking，发现其作为一个 APM 框架，比起作为 trace 框架的 zipkin 多了一个监控维度：对 JVM 的监控。而 skywalking 集成进系统的方式也和传统的框架不太一样，由于其需要对 JVM 进行无侵入式的监控，所以借助了 JAVA5 提供的 Instrument 机制。关于“Instrument”这个单词，没找到准确的翻译，个人理解为“增强，装配”。 如果我们想要无侵入式的修改一个方法，大多数人想到的可能是 AOP 技术，Instrument 有异曲同工之处，它可以对方法进行增强，甚至替换整个类。 下面借助一个 demo，了解下 Instrument 是如何使用的。第一个 demo 很简单，在某一方法调用时，额外打印出其调用时的时间。 12345public class Dog { public String hello() { return &quot;wow wow~&quot;; }} 1234567public class Main { public static void main(String[] args) { System.out.println(new Dog().hello()); }} Dog 存在一个 hello 方法，希望在调用该方法时打印出是什么时刻发生的调用。 实现 AgentGreetingTransformer 12345678910public class GreetingTransformer implements ClassFileTransformer { @Override public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) throws IllegalClassFormatException { if (&quot;moe/cnkirito/agent/Dog&quot;.equals(className)) { System.out.println(&quot;Dog's method invoke at\\t&quot; + new Date()); } return null; }} 对类进行装配的第一步是编写一个 GreetingTransformer 类，其继承自：java.lang.instrument.ClassFileTransformer，打印语句便编写在其中。对于入参和返参我们先不去纠结，因为仅仅完成这么一个简单的 AOP 功能，还不需要了解它们。 GreetingAgent 除了上述的 Transformer，我们还需要有一个容器去加载它。 12345678910public class GreetingAgent { public static void premain(String options, Instrumentation ins) { if (options != null) { System.out.printf(&quot;I've been called with options: \\&quot;%s\\&quot;\\n&quot;, options); } else System.out.println(&quot;I've been called with no options.&quot;); ins.addTransformer(new GreetingTransformer()); }} GreetingAgent 便是我们后面要用的代理，可以发现它只有一个 premain 方法，很简单很形象，它和 main 方法真的很像 12public static void main(String[] args) {} 不同的是 main 函数的参数是一个 string[]，而 premain 的入参是一个 String 和一个 Instrumentation。 前者不用过多赘述，而后者 Instrumentation 便是 JAVA5 的 Instrument 机制的核心，它负责为类添加 ClassFileTransformer 的实现，从而对类进行装配。注意 premain 和它的两个参数不能随意修改，为啥？我们使用 main 函数的时候也没问为啥一定是 public static void main(String[] args) 啊，规定！规定！从 premain 的命名也可以看出，它的运行显然是在 main 函数之前的。 MANIFEST.MF 我们最终会把上面的 GreetingTransformer 和 GreetingAgent 打成一个 jar 包，然后让 Main 函数在启动时加载，但想要使用这个 jar 包还得额外做的工作。 我们得告诉 JVM 在哪儿加载我们的 premain 方法，所以需要在 classpath 下增加一个 resources\\META-INF\\MANIFEST.MF 文件 123Manifest-Version: 1.0Premain-Class: moe.cnkirito.agent.GreetingAgentCan-Redefine-Classes: true **MAVEN 插件 ** 为了打包 agent 我们需要额外添加 maven 插件，将 mf 文件和两个类一起打包 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;build&gt; &lt;finalName&gt;agent&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/manifestFile&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;outputDirectory&gt;${basedir}&lt;/outputDirectory&gt; &lt;archive&gt; &lt;index&gt;true&lt;/index&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;/manifest&gt; &lt;manifestEntries&gt; &lt;Premain-Class&gt;moe.cnkirito.agent.GreetingAgent&lt;/Premain-Class&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 完成上述的配置，使用 maven install 即可得到一个 agent.jar，到这儿一切的准备工作就完成了。 使用代理运行 Main 方法如果不使用代理运行 Main 方法，毫无疑问我们只会得到一行 wow wow~。 如果你使用的 IDEA，eclipse，只需要添加一行启动参数即可： -javaagent:jar_path=[options] 其中的 jar_path 为 agent.jar 的路径，options 是一个可选参数，其值会被 premain 方法的第一个参数接收 public static void premain(String options, Instrumentation ins). 当需要装配多个 agent.jar 时，重复书写多次即可 -javaagent:C:\\Users\\xujingfeng\\Desktop\\agent.jar=hello -javaagent:C:\\Users\\xujingfeng\\Desktop\\agent.jar=hello2 … 运行 Main.jar 的话就是这样的形式：java -javaagent:C:\\Users\\xujingfeng\\Desktop\\agent.jar=hello Main ** 运行结果 ** 123 I've been called with options:&quot;hello&quot;Dog's method invoke at Sun Feb 04 23:54:45 CST 2018wow wow~ I’ve been called with options:”hello” 代表我们的 premain 已经装载成功，并且正确接收到了启动参数。第二行语句也正常打印出了调用时间，至此便完成了 Dog 的装配。 Instrument 进阶什么？为了打印一行调用时间，我们花了这么大精力，这是要跟自己过不去吗？你可能会有这样的疑惑，但请不要质疑 Instrument 的价值。 12345678public interface ClassFileTransformer { byte[] transform( ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) throws IllegalClassFormatException;} ClassFileTransformer 可以对所有的方法进行拦截，看见返回值 byte[] 了没有 The implementation of this method may transform the supplied class file and return a new replacement class file. 这个方法的实现可能会改变提供的类文件并返回一个新的替换类文件。 这给了我们足够的操作自由度，我们甚至可以替换一个类的实现，只要你能够返回一个正确的替换类。ClassLoader 代表被转换类的类加载器，如果是 bootstrap loader 则可以省略，className 代表全类名，注意是以 / 作为分隔符。其他参数我也不是太懂，想深究的同学自行翻看下文档。byte[] 代表被转换后的类的字节，为 null 则代表不转换。 替换 Dog 的实现12345public class Dog { public String hello() { return &quot;miao miao~&quot;; }} 注意，这里我修改了 Dog 的实现，不是打印 wow wow~ 而是 miao miao ~，只是为了得到新 Dog 的字节码 Dog.class。我将新的 Dog.class 丢在了我的桌面方便加载：C:/Users/xujingfeng/Desktop 1234567891011121314151617181920212223242526272829303132333435363738394041public class DogTransformer implements ClassFileTransformer { public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) throws IllegalClassFormatException { System.out.println(&quot;className:&quot; + className); if (!className.equalsIgnoreCase(&quot;moe/cnkirito/agent/Dog&quot;)) { return null; } return getBytesFromFile(&quot;C:/Users/xujingfeng/Desktop/Dog.class&quot;);// 新的 Dog// return getBytesFromFile(&quot;app/target/classes/moe/cnkirito/agent/Dog.class&quot;); } public static byte[] getBytesFromFile(String fileName) { File file = new File(fileName); try (InputStream is = new FileInputStream(file)) { // precondition long length = file.length(); byte[] bytes = new byte[(int) length]; // Read in the bytes int offset = 0; int numRead = 0; while (offset &lt;bytes.length &amp;&amp; (numRead = is.read(bytes, offset, bytes.length - offset)) &gt;= 0) { offset += numRead; } if (offset &lt; bytes.length) { throw new IOException(&quot;Could not completely read file&quot; + file.getName()); } is.close(); return bytes; } catch (Exception e) { System.out.println(&quot;error occurs in _ClassTransformer!&quot; + e.getClass().getName()); return null; } }} return getBytesFromFile(“C:/Users/xujingfeng/Desktop/Dog.class”) 一行返回了新的 Dog 试图替换原先的 Dog。注意，这一切都放生在 Agent.jar 之中，我并没有对 Main 函数（也就是我们自己的源代码）做任何改动。 ** 控制台输出 ** 1miao miao~ 替换成功！我们并没有对 Main 程序的 Dog 做任何修改，只是加载了一个新的 Dog.class 替换了 Main 程序中的 Dog。 统计方法运行耗时这个需求有点接近我们研究 Instrument 的初衷了，统计方法的运行耗时。由于代码的篇幅问题，在本文中只给出思路，详细的实现，可以参考文末的 github 链接，本文的三个例子： 打印 hello 替换 Dog 统计方法运行耗时 代码都在其中。 思路：对每个需要统计耗时的方法替换字节码，在方法开始前插入开始时间，在方法结束时插入结束时间，计算差值，more 你可以连同 methodName 和耗时一起发送出去，给 collector 统一采集…wait，这不就是一个简易的监控吗?!~ 运行结果： 12Call to method hello_timing took 1 ms.wow wow~ JAVA6 的 agentmain值得一提的是，java6 提供了 public static void agentmain (String agentArgs, Instrumentation inst); 这个新的方法，可以在 main 函数之后装配（premain 是在 main 之前），这使得操控现有程序的自由度变得更高了，有兴趣的朋友可以去了解下 premain 和 agentmain 的特性。 本文示例代码https://github.com/lexburner/java5-Instrumentation-demo 参考资料Java 5 特性 Instrumentation 实践 Java SE 6 的新特性：虚拟机启动后的动态 instrument 芋道源码","link":"/instrument/"},{"title":"如何更快地将string转换成int&#x2F;long","text":"在很多追求性能的程序挑战赛中，经常会遇到一个操作：将 String 转换成 Integer/Long。如果你没有开发过高并发的系统，或者没有参加过任何性能挑战赛，可能会有这样的疑问：这有啥好讲究的，Integer.valueOf/Long.valueOf 又不是不能用。实际上，很多内置的转换工具类只满足了功能性的需求，在高并发场景下，可能会是热点方法，成为系统性能的瓶颈。 文章开头，我先做一下说明，本文的测试结论出自：https://kholdstare.github.io/technical/2020/05/26/faster-integer-parsing.html 。测试代码基于 C++，我会在翻译原文的同时，添加了部分自己的理解，以协助读者更好地理解其中的细节。 问题提出假设现在有一些文本信息，固定长度为 16 位，例如下文给出的时间戳，需要尽可能快地解析这些时间戳 1234timestamp158520108712356715852010871235851585201087123621 方法体如下所示： 1234std::uint64_t parse_timestamp(std::string_view s){ // ???} 问题提出后，大家不妨先思考下，如果是你，你会采取什么方案呢？带着这样的思考，我们进入下面的一个个方案。 Native 方案我们有哪些现成的转换方案呢？ 继承自 C 的 std::atoll std::stringstream C++17 提供的 charconv boost::spirit::qi 评测程序采用 Google Benchmark 进行对比评测。同时，我们以不做任何转换的方案来充当 baseline，以供对比。（baseline 方案在底层，相当于将数值放进来了寄存器中，所以命名成了 BM_mov） 下面给出的评测代码不是那么地关键，只是为了给大家展示评测是如何运行的。 1234567891011121314151617181920212223242526272829303132333435363738static void BM_mov(benchmark::State&amp; state) { for (auto _ : state) { benchmark::DoNotOptimize(1585201087123789); }}static void BM_atoll(benchmark::State&amp; state) { for (auto _ : state) { benchmark::DoNotOptimize(std::atoll(example_timestamp)); }}static void BM_sstream(benchmark::State&amp; state) { std::stringstream s(example_timestamp); for (auto _ : state) { s.seekg(0); std::uint64_t i = 0; s &gt;&gt; i; benchmark::DoNotOptimize(i); }}static void BM_charconv(benchmark::State&amp; state) { auto s = example_timestamp; for (auto _ : state) { std::uint64_t result = 0; std::from_chars(s.data(), s.data() + s.size(), result); benchmark::DoNotOptimize(result); }}static void BM_boost_spirit(benchmark::State&amp; state) { using boost::spirit::qi::parse; for (auto _ : state) { std::uint64_t result = 0; parse(s.data(), s.data() + s.size(), result); benchmark::DoNotOptimize(result); }} 可以发现 stringstream 表现的非常差。当然，这并不是一个公平的比较，但从测评结果来看，使用 stringstream 来实现数值转换相比 baseline 慢了 391 倍。相比之下， &lt;charconv&gt; 和 boost::spirit 表现的更好。 既然我们已经知道了目标字符串包含了要解析的数字，而且不需要做任何的数值校验，基于这些前提，我们可以思考下，还有更快的方案吗？ Naive 方案我们可以通过一个再简单不过的循环方案，一个个地解析字符。 12345678910inline std::uint64_t parse_naive(std::string_view s) noexcept{ std::uint64_t result = 0; for(char digit : s) { result *= 10; result += digit - '0'; } return result;} 虽然这层 for 循环看起来呆呆的，但如果这样一个呆呆的解决方案能够击败标准库实现，何乐而不为呢？前提是，标准库的实现考虑了异常场景，做了一些校验，这种 for 循环写法的一个前提是，我们的输入一定是合理的。 之前我的文章也提到过这个方案。显然， naive 的方案之后还会有更优的替代方案。 循环展开方案记得我们在文章的开头加了一个限定，限定了字符串长度固定是 16 位，所以循环是可以被省略的，循环展开之后，方案可以更快。 1234567891011121314151617181920212223inline std::uint64_t parse_unrolled(std::string_view s) noexcept{ std::uint64_t result = 0; result += (s[0] - '0') * 1000000000000000ULL; result += (s[1] - '0') * 100000000000000ULL; result += (s[2] - '0') * 10000000000000ULL; result += (s[3] - '0') * 1000000000000ULL; result += (s[4] - '0') * 100000000000ULL; result += (s[5] - '0') * 10000000000ULL; result += (s[6] - '0') * 1000000000ULL; result += (s[7] - '0') * 100000000ULL; result += (s[8] - '0') * 10000000ULL; result += (s[9] - '0') * 1000000ULL; result += (s[10] - '0') * 100000ULL; result += (s[11] - '0') * 10000ULL; result += (s[12] - '0') * 1000ULL; result += (s[13] - '0') * 100ULL; result += (s[14] - '0') * 10ULL; result += (s[15] - '0'); return result;} 关于循环展开为什么会更快，可以参考我过去关于 JMH 的文章。 byteswap 方案先思考下，如果继续围绕上述的方案进行，我们可能只有两个方向： 并发执行加法和乘法计算，但这种 CPU 操作似乎又不能通过多线程之类的手段进行加速，该如何优化是个问题 将乘法和加法运算转换成位运算，获得更快的 CPU 执行速度，但如何转换又是个问题 相信读者们都会有这样的疑问，那我们继续带着这样疑问往下看原作者的优化思路是什么。 紧接着上述的循环展开方案，将 “1234” 解析为 32 位整数对应的循环展开操作绘制为图，过程如下： 我们可以看到，乘法和加法的操作次数跟字符的数量是线性相关的。由于每一次乘法都是由不同的乘数进行，所以我们不能只乘“一次”，在乘法的最后，我们还需要将所有结果相加。乍一看，好像很难优化。 下面的优化技巧，需要一些操作系统、编译原理相关的知识作为辅助，你需要了解 byteswap 这个系统调用，了解大端序和小端序的字节序表示方法（后面我也会分享相关的文章），如果你不关心这些细节，也可以直接跳到本段的最后，直接看结论。 理解清楚下图的含义，需要理解几个概念： 字符 1 对应的 ascii 值是 31，相应的 2 对应 32，4 对应 34 在小端序机器上（例如 x86），字符串是以大端序存储的，而 Integer 是以小端序存储的 byteswap 可以实现字节序调换 上图展示了十六进制表示下的转换过程，可以在更少的操作下达到最终的解析状态。 将上图的流程使用 C++ 来实现，将 String 重新解释为 Integer，必须使用 std::memcpy（避免命名冲突），执行相减操作，然后通过编译器内置的 __builtin_bswap64 在一条指令中交换字节。到目前为止，这是最快的一个优化。 1234567891011121314151617181920template &lt;typename T&gt;inline T get_zeros_string() noexcept;template &lt;&gt;inline std::uint64_t get_zeros_string&lt;std::uint64_t&gt;() noexcept{ std::uint64_t result = 0; constexpr char zeros[] = &quot;00000000&quot;; std::memcpy(&amp;result, zeros, sizeof(result)); return result;}inline std::uint64_t parse_8_chars(const char* string) noexcept{ std::uint64_t chunk = 0; std::memcpy(&amp;chunk, string, sizeof(chunk)); chunk = __builtin_bswap64(chunk - get_zeros_string&lt;std::uint64_t&gt;()); // ...} 我们看上去得到了想要的结果，但是这个方案从时间复杂度来看，仍然是 O(n) 的，是否可以在这个方案的基础上，继续进行优化呢？ 分治方案从最初的 Native 方案，到上一节的 byteswap 方案，我们都只是优化了 CPU 操作，并没有优化复杂度，既然不满足于 O(n)，那下一个复杂度可能性是什么？ O(logn)！ 我们可以将每个相邻的数字组合成一对，然后将每对数字继续组合成一组四个，依此类推，直到我们得到整个整数。 如何同时处理邻近的数字，这是让算法跑进 O(logn) 的关键 该方案的关键之处在于：将偶数位的数字乘以 10 的幂，并且单独留下奇数位的数字。 这可以通过位掩码（bitmasking）来实现 通过 bitmasking，我们可以一次对多个数字进行操作，将它们组合成一个更大的组合 通过使用这个掩码技巧来实现前文提到的 parse_8_chars 函数。 使用 bitmasking 的另一好处在于，我们不用减去 ‘0’ ，因为位掩码的副作用，使得我们正好可以省略这一步。 12345678910111213141516171819202122inline std::uint64_t parse_8_chars(const char* string) noexcept{ std::uint64_t chunk = 0; std::memcpy(&amp;chunk, string, sizeof(chunk)); // 1-byte mask trick (works on 4 pairs of single digits) std::uint64_t lower_digits = (chunk &amp; 0x0f000f000f000f00) &gt;&gt; 8; std::uint64_t upper_digits = (chunk &amp; 0x000f000f000f000f) * 10; chunk = lower_digits + upper_digits; // 2-byte mask trick (works on 2 pairs of two digits) lower_digits = (chunk &amp; 0x00ff000000ff0000) &gt;&gt; 16; upper_digits = (chunk &amp; 0x000000ff000000ff) * 100; chunk = lower_digits + upper_digits; // 4-byte mask trick (works on pair of four digits) lower_digits = (chunk &amp; 0x0000ffff00000000) &gt;&gt; 32; upper_digits = (chunk &amp; 0x000000000000ffff) * 10000; chunk = lower_digits + upper_digits; return chunk;} trick 方案综合前面两节，解析 16 位的数字，我们将它分成两个 8 字节的块，运行刚刚编写的 parse_8_chars，并对其进行基准测试！ 123456789101112inline std::uint64_t parse_trick(std::string_view s) noexcept{ std::uint64_t upper_digits = parse_8_chars(s.data()); std::uint64_t lower_digits = parse_8_chars(s.data() + 8); return upper_digits * 100000000 + lower_digits;}static void BM_trick(benchmark::State&amp; state) { for (auto _ : state) { benchmark::DoNotOptimize(parse_trick(example_stringview)); }} 看上去优化的不错，我们将循环展开方案的基准测试优化了近 56% 的性能。能做到这一点，主要得益于我们手动进行一系列 CPU 优化的操作，虽然这些并不是特别通用的技巧。这样算不算开了个不好的头呢？我们看起来对 CPU 操作干预地太多了，或许我们应该放弃这些优化，让 CPU 自由地飞翔。 SIMD trick 方案你是不是以为上面已经是最终方案了呢？不，优化还剩最后一步。 我们已经得到了一个结论 同时组合多组数字以实现 O(logn) 复杂度 如果有 16 个字符或 128 位的字符串要解析，还可以使用 SIMD。感兴趣的读者可以参考SIMD stands for Single Instruction Multiple Data。 Intel 和 AMD CPU 都支持 SSE 和 AVX 指令，并且它们通常使用更宽的寄存器。 SIMD 简单来说就是一组 CPU 的扩展指令，可以通过调用多组寄存器实现并行的乘法运算，从而提升系统性能。我们一般提到的向量化运算就是 SIMD。 让我们先设置 16 个字节中的每一个数字： 12345678910inline std::uint64_t parse_16_chars(const char* string) noexcept{ auto chunk = _mm_lddqu_si128( reinterpret_cast&lt;const __m128i*&gt;(string) ); auto zeros = _mm_set1_epi8('0'); chunk = chunk - zeros; // ...} 现在，主角变成了 madd 该系统调用。 这些 SIMD 函数与我们使用位掩码技巧所做的操作完全一样——它们采用同一个宽寄存器，将其解释为一个由较小整数组成的向量，每个乘以一个特定的乘数，然后将相邻位的结果相加到一个更宽的整数向量中。所有操作一步完成。 12345// The 1-byte &quot;trick&quot; in one instructionconst auto mult = _mm_set_epi8( 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10);chunk = _mm_maddubs_epi16(chunk, mult); 2 字节方案其实还有另一条指令，但不幸的是我并没有找到 4 字节方案的指令，还是需要两条指令。 这是完整的 parse_16_chars 方案： 1234567891011121314151617181920212223242526inline std::uint64_t parse_16_chars(const char* string) noexcept{ auto chunk = _mm_lddqu_si128( reinterpret_cast&lt;const __m128i*&gt;(string) ); auto zeros = _mm_set1_epi8('0'); chunk = chunk - zeros; { const auto mult = _mm_set_epi8( 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10 ); chunk = _mm_maddubs_epi16(chunk, mult); } { const auto mult = _mm_set_epi16(1, 100, 1, 100, 1, 100, 1, 100); chunk = _mm_madd_epi16(chunk, mult); } { chunk = _mm_packus_epi32(chunk, chunk); const auto mult = _mm_set_epi16(0, 0, 0, 0, 1, 10000, 1, 10000); chunk = _mm_madd_epi16(chunk, mult); } return ((chunk[0] &amp; 0xffffffff) * 100000000) + (chunk[0] &gt;&gt; 32);} 0.75 nanoseconds! 是不是大吃一惊呢. 总结 有人可能会问，你为啥要用 C++ 来介绍下，不能用 Java 吗？我再补充下，本文的测试结论，均来自于老外的文章，文章出处见开头，其次，本文的后半部分的优化，都是基于一些系统调用，和 CPU 指令的优化，这些在 C++ 中实现起来方便一些，Java 只能走系统调用。 在最近过去的性能挑战赛中，由于限定了不能使用 JNI，使得选手们只能将方案止步于循环展开方案，试想一下，如果允许走系统调用，加上比赛中字符串也基本是固定的长度，完全可以采用 SIMD 的 trick 方案，String 转 Long 的速度会更快。 实际上，在之前 polarDB 的比赛中，普哥就给我介绍过 bswap 的向量化方案，这也是为啥 Java 方案就是比 C++ 方案逊色的原因之一，C++ 在执行一些 CPU 指令集以及系统调用上，比 Java 方便很多。 如何看待这一系列的优化呢？从 std::stringstream 的 86.23 到 sima trick 方案的 0.75，这个优化的过程是令人兴奋的，但我们也发现，越往后，越是用到一些底层的优化技巧，正如方案中的 trick 而言，适用性是有限的。也有一种声音是在说：花费这么大精力去优化，为啥不去写汇编呢？这又回到了“优化是万恶之源”这个话题。在业务项目中，可能你不用过多关注 String 是如何转换为 Long 和 Integer 的，可能 Integer.valueOf 和 Long.valueOf 就可以满足你的诉求，但如果你是一个需要大数据解析系统，String 转换是系统的瓶颈之一，相信本文的方案会给你一定的启发。 另外对于 SIMD 这些方案，我想再多说一句。其实一些性能挑战赛进行到最后，大家的整体方案其实都相差无几，无非是参数差异，因为比赛场景通常不会太复杂，最后前几名的差距，就是在一些非常小的细节上。正如 SIMA 提供的向量化运算等优化技巧，它就是可以帮助你比其他人快个几百毫秒，甚至 1~2s。这时候你会感叹，原来我跟大神的差距，就是在这些细节上。但反观整个过程，似乎这些优化并不能帮助程序设计竞赛发挥更大的能量，一个比赛如果只能依靠 CPU 优化来实现区分度，我觉得一定不是成功的。所以，对于主办方而言，禁用掉一些类库，其实有效的避免了内卷，于参赛者而言，算是一种减负了。希望以后的比赛也都朝着让选手花更多精力去优化方案，而不是优化通用的细节上。 再回到 String 解析成 Long/Integer 的话题上。在实际使用时，大家也不用避讳继续使用 Integer.valueOf 或者 Long.valueOf，大多数情况下，这不是系统的瓶颈。而如果你恰好在某些场景下遇到了 String 转换的瓶颈，希望本文能够帮到你。","link":"/integer-parse/"},{"title":"java 并发实践 --ConcurrentHashMap 与 CAS","text":"前言最近在做接口限流时涉及到了一个有意思问题，牵扯出了关于 concurrentHashMap 的一些用法，以及 CAS 的一些概念。限流算法很多，我主要就以最简单的计数器法来做引。先抽象化一下需求：统计每个接口访问的次数。一个接口对应一个 url，也就是一个字符串，每调用一次对其进行加一处理。可能出现的问题主要有三个： 多线程访问，需要选择合适的并发容器 分布式下多个实例统计接口流量需要共享内存 流量统计应该尽可能不损耗服务器性能 但这次的博客并不是想描述怎么去实现接口限流，而是主要想描述一下遇到的问题，所以，第二点暂时不考虑，即不使用 redis。 说到并发的字符串统计，立即让人联想到的数据结构便是 ConcurrentHashpMap&lt;String,Long&gt; urlCounter; 如果你刚刚接触并发可能会写出如代码清单 1 的代码 代码清单 112345678910111213141516171819202122232425262728293031323334353637383940414243444546public class CounterDemo1 { private final Map&lt;String, Long&gt; urlCounter = new ConcurrentHashMap&lt;&gt;(); // 接口调用次数 +1 public long increase(String url) { Long oldValue = urlCounter.get(url); Long newValue = (oldValue == null) ? 1L : oldValue + 1; urlCounter.put(url, newValue); return newValue; } // 获取调用次数 public Long getCount(String url){ return urlCounter.get(url); } public static void main(String[] args) { ExecutorService executor = Executors.newFixedThreadPool(10); final CounterDemo1 counterDemo = new CounterDemo1(); int callTime = 100000; final String url = &quot;http://localhost:8080/hello&quot;; CountDownLatch countDownLatch = new CountDownLatch(callTime); // 模拟并发情况下的接口调用统计 for(int i=0;i&lt;callTime;i++){ executor.execute(new Runnable() { @Override public void run() { counterDemo.increase(url); countDownLatch.countDown(); } }); } try { countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } executor.shutdown(); // 等待所有线程统计完成后输出调用次数 System.out.println(&quot;调用次数：&quot;+counterDemo.getCount(url)); }}console output：调用次数：96526 都说 concurrentHashMap 是个线程安全的并发容器，所以没有显示加同步，实际效果呢并不如所愿。 问题就出在 increase 方法，concurrentHashMap 能保证的是每一个操作（put，get,delete…）本身是线程安全的，但是我们的 increase 方法，对 concurrentHashMap 的操作是一个组合，先 get 再 put，所以多个线程的操作出现了覆盖。如果对整个 increase 方法加锁，那么又违背了我们使用并发容器的初衷，因为锁的开销很大。我们有没有方法改善统计方法呢？代码清单 2 罗列了 concurrentHashMap 父接口 concurrentMap 的一个非常有用但是又常常被忽略的方法。 代码清单 21234567891011121314/** * Replaces the entry for a key only if currently mapped to a given value. * This is equivalent to * &lt;pre&gt; {@code * if (map.containsKey(key) &amp;&amp; Objects.equals(map.get(key), oldValue)) { * map.put(key, newValue); * return true; * } else * return false; * }&lt;/pre&gt; * * except that the action is performed atomically. */ boolean replace(K key, V oldValue, V newValue); 这其实就是一个最典型的 CAS 操作，except that the action is performed atomically. 这句话真是帮了大忙，我们可以保证比较和设置是一个原子操作，当 A 线程尝试在 increase 时，旧值被修改的话就回导致 replace 失效，而我们只需要用一个循环，不断获取最新值，直到成功 replace 一次，即可完成统计。 改进后的 increase 方法如下 代码清单 31234567891011121314151617181920212223public long increase2(String url) { Long oldValue, newValue; while (true) { oldValue = urlCounter.get(url); if (oldValue == null) { newValue = 1l; // 初始化成功，退出循环 if (urlCounter.putIfAbsent(url, 1l) == null) break; // 如果初始化失败，说明其他线程已经初始化过了 } else { newValue = oldValue + 1; //+1 成功，退出循环 if (urlCounter.replace(url, oldValue, newValue)) break; // 如果 +1 失败，说明其他线程已经修改过了旧值 } } return newValue;}console output：调用次数：100000 再次调用后获得了正确的结果，上述方案看上去比较繁琐，因为第一次调用时需要进行一次初始化，所以多了一个判断，也用到了另一个 CAS 操作 putIfAbsent，他的源代码描述如下： 代码清单 412345678910111213141516171819202122232425/** * If the specified key is not already associated * with a value, associate it with the given value. * This is equivalent to * &lt;pre&gt; {@code * if (!map.containsKey(key)) * return map.put(key, value); * else * return map.get(key); * }&lt;/pre&gt; * * except that the action is performed atomically. * * @implNote This implementation intentionally re-abstracts the * inappropriate default provided in {@code Map}. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with the specified key, or * {@code null} if there was no mapping for the key. * (A {@code null} return can also indicate that the map * previously associated {@code null} with the key, * if the implementation supports null values.) */ V putIfAbsent(K key, V value); 简单翻译如下：“如果（调用该方法时）key-value 已经存在，则返回那个 value 值。如果调用时 map 里没有找到 key 的 mapping，返回一个 null 值”。值得注意点的一点就是 concurrentHashMap 的 value 是不能存在 null 值的。实际上呢，上述的方案也可以把 Long 替换成 AtomicLong，可以简化实现， ConcurrentHashMap&lt;String,AtomicLong&gt;。 juc 包下的各类 Atomic 类也提供了大量的 CAS 操作，可以不用加锁，也可以实现原子操作，以后看到其他类库有类似比较后设值，不存在即设值，加一并获取返回值等等一系列的组合操作合并成了一个接口的，都应该意识到很有可能是 CAS 操作。如 redis 的 IncreamtAndGet，setIfAbsent，Atomic 类的一系列 api，以及上述描述的 concurrentHashMap 中相关的 api（不同 api 的 CAS 组合接口可能名称类似，但是返回值含义不大相同，我们使用 CAS 的 api 很大程度需要获取其返回值来进行分支处理，所以一定要搞清楚每个接口的特性。如 redistemplate 提供的 setIfAbsent，当设置成功时返回的是 true，而与之名称类似的 ConcurrentHashMap 的 putIfAbsent 在设置成功后返回的是 null，要足够小心，加以区分）。凡事没有绝对，但是一个大体上正确的编程建议便是 ** 能使用编程类库并发容器（线程安全的类）完成的操作，尽量不要显示加锁同步 **。 再扯一句关于 CAS 的知识点，CAS 不能代替同步，由它引出了一个经典的 ABA 问题，即修改过一次之后，第二次修改又变为了原值，可能会在一些逻辑中出现问题。不过对于计数这个逻辑而言，只是单调的增，不会受到影响。 最后介绍一个和主题非常贴切的并发容器：Guava 包中 AtomicLongMap，使用他来做计数器非常容易。 代码清单 51234567891011private AtomicLongMap&lt;String&gt; urlCounter3 = AtomicLongMap.create();public long increase3(String url) { long newValue = urlCounter3.incrementAndGet(url); return newValue;}public Long getCount3(String url) { return urlCounter3.get(url);} 看一下他的源码就会发现，其实和代码清单 3 思路差不多，只不过功能更完善了一点。 和 CAS 很像的操作，我之前的博客中提到过数据库的乐观锁，用 version 字段来进行并发控制，其实也是一种 compare and swap 的思想。 杂谈：网上很多对 ConcurrentHashMap 的介绍，众所周知，这是一个用分段锁实现的一个线程安全的 map 容器，但是真正对他的使用场景有介绍的少之又少。面试中能知道这个容器的人也确实不少，问出去，也就回答一个分段锁就没有下文了，但我觉得吧，有时候一知半解反而会比不知道更可怕。 参考 https://my.oschina.net/mononite/blog/144329 http://www.tuicool.com/articles/zuui6z","link":"/java-ConcurrentHashMap-CAS/"},{"title":"Java 并发计数器探秘","text":"前言一提到线程安全的并发计数器，AtomicLong 必然是第一个被联想到的工具。Atomic* 一系列的原子类以及它们背后的 CAS 无锁算法，常常是高性能，高并发的代名词。本文将会阐释，在并发场景下，使用 AtomicLong 来充当并发计数器将会是一个糟糕的设计，实际上存在不少 AtomicLong 之外的计数器方案。近期我研究了一些 Jdk1.8 以及 JCTools 的优化方案，并将它们的对比与实现细节整理于此。 相关面试题： 单机场景下，有比 AtomicLong 更高效的并发计数器方案吗？ 阅读本文前本文相关的基准测试代码均可在博主的 github 中找到，测试方式全部采用 JMH，这篇文章可以帮助你 入门 JMH。 AtomicLong 的前世今生在 Java 中，Atomic* 是高效的，这得益于 sun.misc.Unsafe 提供的一系列底层 API，使得 Java 这样的高级语言能够直接和硬件层面的 CPU 指令打交道。并且在 Jdk1.7 中，这样的底层指令可以配合 CAS 操作，达到 Lock-Free。 在 Jdk1.7 中，AtomicLong 的关键代码如下： 123456789101112public final long getAndIncrement() { while (true) { long current = get(); long next = current + 1; if (compareAndSet(current, next)) return current; }}public final boolean compareAndSet(long expect, long update) { return unsafe.compareAndSwapLong(this, valueOffset, expect, update);} get() 方法 volatile 读当前 long 值 自增 自旋判断新值与当前值 自旋成功，返回；否则返回 1 我们特别留意到 Jdk1.7 中 unsafe 使用的方法是 compareAndSwapLong，它与 x86 CPU 上的 LOCK CMPXCHG 指令对应，并且在应用层使用 while(true) 完成自旋，这个细节在 Jdk1.8 中发生了变化。 在 Jdk1.8 中，AtomicLong 的关键代码如下： 123public final long getAndIncrement() { return unsafe.getAndAddLong(this, valueOffset, 1L);} Jdk1.7 的 CAS 操作已经不复存在了，转而使用了 getAndAddLong 方法，它与 x86 CPU 上的 LOCK XADD 指令对应，以原子方式返回当前值并递增（fetch and add）。 当问及 Atomic* 高效的原因，回答 CAS 是不够全面且不够严谨的，Jdk1.7 的 unsafe.compareAndSwapLong 以及 Jdk1.8 的 unsafe.getAndAddLong 才是关键，且 Jdk1.8 中不存在 CAS。 Jdk1.8 AtomicLong 相比 Jdk1.7 AtomicLong 的表现是要优秀的，这点我们将在后续的测评中见证。 AtomicLong 真的高效吗？无论在 Jdk1.7 还是 Jdk1.8 中，Atomic* 的开销都是很大的，主要体现在： 高并发下，CAS 操作可能会频繁失败，真正更新成功的线程占少数。(Jdk1.7 独有的问题) 我之前的文章中介绍过“伪共享” (false sharing) 问题，但在 CAS 中，问题则表现的更为直接，这是“真共享”，与”伪共享“存在相同的问题：缓存行失效，缓存一致性开销变大。 底层指令的开销不见得很低，无论是 LOCK XADD 还是 LOCK CMPXCHG，想深究的朋友可以参考 instruction_tables ，（这一点可能有点钻牛角尖，但不失为一个角度去分析高并发下可行的优化） Atomic* 所做的，比我们的诉求可能更大，有时候我们只需要计数器具备线程安全地递增这样的特性，但 Atomic* 的相关操作每一次都伴随着值的返回。他是个带返回值的方法，而不是 void 方法，而多做了活大概率意味着额外的开销。 抛开上述导致 AtomicLong 慢的原因，AtomicLong 仍然具备优势： 上述的第 4 点换一个角度也是 AtomicLong 的有点，相比下面要介绍的其他计数器方案，AtomicLong 能够保证每次操作都精确的返回真实的递增值。你可以借助 AtomicLong 来做并发场景下的递增序列号方案，注意，本文主要讨论的是计数器方案，而不是序列号方案。 实现简单，回到那句话：“简单的架构通常性能不高，高性能的架构通常复杂度很高”，AtomicLong 属于性能相对较高，但实现极其简单的那种方案，因为大部分的复杂性，由 JMM 和 JNI 方法屏蔽了。相比下面要介绍的其他计数器实现，AtomicLong 真的太“简易”了。 看一组 AtomicLong 在不同并发量下的性能表现。 线程数 increment get 1 22.31 ns/op 11.75 ns/op 3 78.80 ns/op 26.58 ns/op 5 132.85 ns/op 38.57 ns/op 10 242.61 ns/op 67.58 ns/op 20 488.74 ns/op 121.22 ns/op 横向对比，写的性能相比读的性能要差很多，在 20 个线程下写性能比读性能差距了 4~5 倍。 纵向对比，主要关注并发写，线程竞争激烈的情况下，单次自增耗时从 22 ns 增长为了 488 ns，有明显的性能下降。 实际场景中，我们需要统计系统的 qps、接口调用次数，都需要使用到计数的功能，写才是关键，并不是每时每刻都需要关注自增后的返回值，而 AtomicLong 恰恰在核心的写性能上有所欠缺。由此引出其他计数器方案。 认识 LongAdderDoug Lea 在 JDK1.8 中找到了一个上述问题的解决方案，他实现了一个 LongAdder 类。 123@since 1.8@author Doug Leapublic class LongAdder extends Striped64 implements Serializable {} LongAdder 的 API 如下 你应当发现，LongAdder 和 AtomicLong 明显的区别在于，increment 是一个 void 方法。直接来看看 LongAdder 的性能表现如何。(LA = LongAdder, AL = AtomicLong, 单位 ns/op) 线程数 LA.incr AL.incr LA.get AL.get 1 25.51 22.31 11.82 11.75 3 14.99 78.80 52.94 26.58 5 30.26 132.85 75.88 38.57 10 44.33 160.61 139.59 67.58 20 77.81 488.74 306.39 121.22 我们从中可以发现一些有意思的现象，网上不少很多文章没有从读写上对比二者，直接宣称 LongAdder 性能优于 AtomicLong，其实不太严谨。在单线程下，并发问题没有暴露，两者没有体现出差距；随着并发量加大，LongAdder 的 increment 操作更加优秀，而 AtomicLong 的 get 操作则更加优秀。鉴于在计数器场景下的特点—写多读少，所以写性能更高的 LongAdder 更加适合。 LongAdder 写速度快的背后网上分析 LongAdder 源码的文章并不少，我不打算详细分析源码，而是挑选了一些必要的细节以及多数文章没有提及但我认为值得分析的内容。 Cell 设计减少并发修改时的冲突 在 LongAdder 的父类 Striped64 中存在一个 volatile Cell[] cells; 数组，其长度是 2 的幂次方，每个 Cell 都填充了一个 @Contended 的 Long 字段，为了避免伪共享问题。 12345@sun.misc.Contended static final class Cell { volatile long value; Cell(long x) {value = x;} // ... ignore} LongAdder 通过一系列算法，将计数结果分散在了多个 Cell 中，Cell 会随着并发量升高时发生扩容，最坏情况下 Cell == CPU core 的数量。Cell 也是 LongAdder 高效的关键，它将计数的总值分散在了各个 Cell 中，例如 5 = 3 + 2，下一刻，某个线程完成了 3 + (2 + 1) = 6 的操作，而不是在 5 的基础上完成直接相加操作。通过 LongAdder 的 sum() 方法可以直观的感受到这一点（LongAdder 不存在 get 方法） 1234567891011public long sum() { Cell[] as = cells; Cell a; long sum = base; if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum;} 这种惰性求值的思想，在 ConcurrentHashMap 中的 size() 中也存在，毕竟他们的作者都是 Doug Lea。 并发场景下高效获取随机数 LongAdder 内部算法需要获取随机数，而 Random 类在并发场景下也是可以优化的。 12ThreadLocalRandom random = ThreadLocalRandom.current();random.nextInt(5); 使用 ThreadLocalRandom 替代 Random，同样出现在了 LongAdder 的代码中。 longAccumulate longAccumulate 方法是 LongAdder 的核心方法，内部存在大量的分支判断。首先和 Jdk1.7 的 AtomicLong 一样，它使用的是 UNSAFE.compareAndSwapLong 来完成自旋，不同之处在于，其在初次 cas 方式失败的情况下 (说明多个线程同时想更新这个值)，尝试将这个值分隔成多个 Cell，让这些竞争的线程只负责更新自己所属的 Cell，这样将竞争压力分散开。 LongAdder 的前世今生其实在 Jdk1.7 时代，LongAdder 还未诞生时，就有一些人想着自己去实现一个高性能的计数器了，比如一款 Java 性能监控框架 dropwizard/metrics 就做了这样事，在早期版本中，其优化手段并没有 Jdk1.8 的 LongAdder 丰富，而在 metrics 的最新版本中，其已经使用 Jdk1.8 的 LongAdder 替换掉了自己的轮子。在最后的测评中，我们将 metrics 版本的 LongAdder 也作为一个参考对象。 JCTools 中的 ConcurrentAutoTable并非只有 LongAdder 考虑到了并发场景下计数器的优化，大名鼎鼎的并发容器框架 JCTool 中也提供了和今天主题相关的实现，虽然其名称和 Counter 看似没有关系，但通过其 Java 文档和 API ，可以发现其设计意图考虑到了计数器的场景。 An auto-resizing table of longs, supporting low-contention CAS operations.Updates are done with CAS’s to no particular table element.The intent is to support highly scalable counters, r/w locks, and other structures where the updates are associative, loss-free (no-brainer), and otherwise happen at such a high volume that the cache contention for CAS’ing a single word is unacceptable. 在最后的测评中，我们将 JCTools 的 ConcurrentAutoTable 也作为一个参考对象。 最终测评Jdk1.7 的 AtomicLong，Jdk1.8 的 AtomicLong，Jdk 1.8 的 LongAdder，Metrics 的 LongAdder，JCTools 的 ConcurrentAutoTable，我对这五种类型的计数器使用 JMH 进行基准测试。 1234public interface Counter { void inc(); long get();} 将 5 个类都适配成 Counter 接口的实现类，采用 @State(Scope.Group)，@Group 将各组测试用例进行隔离，尽可能地排除了互相之间的干扰，由于计数器场景的特性，我安排了 20 个线程进行并发写，1 个线程与之前的写线程共存，进行并发读。Mode=avgt 代表测试的是方法的耗时，越低代表性能越高。 12345678910111213141516Benchmark (counterType) Mode Cnt Score Error UnitsCounterBenchmark.rw Atomic7 avgt 3 1049.906 ± 2146.838 ns/opCounterBenchmark.rw:get Atomic7 avgt 3 143.352 ± 125.388 ns/opCounterBenchmark.rw:inc Atomic7 avgt 3 1095.234 ± 2247.913 ns/opCounterBenchmark.rw Atomic8 avgt 3 441.837 ± 364.270 ns/opCounterBenchmark.rw:get Atomic8 avgt 3 149.817 ± 66.134 ns/opCounterBenchmark.rw:inc Atomic8 avgt 3 456.438 ± 384.646 ns/opCounterBenchmark.rw ConcurrentAutoTable avgt 3 144.490 ± 577.390 ns/opCounterBenchmark.rw:get ConcurrentAutoTable avgt 3 1243.494 ± 14313.764 ns/opCounterBenchmark.rw:inc ConcurrentAutoTable avgt 3 89.540 ± 166.375 ns/opCounterBenchmark.rw LongAdderMetrics avgt 3 105.736 ± 114.330 ns/opCounterBenchmark.rw:get LongAdderMetrics avgt 3 313.087 ± 307.381 ns/opCounterBenchmark.rw:inc LongAdderMetrics avgt 3 95.369 ± 132.379 ns/opCounterBenchmark.rw LongAdder8 avgt 3 98.338 ± 80.112 ns/opCounterBenchmark.rw:get LongAdder8 avgt 3 274.169 ± 113.247 ns/opCounterBenchmark.rw:inc LongAdder8 avgt 3 89.547 ± 78.720 ns/op 如果我们只关注 inc 即写性能，可以发现 jdk1.8 的 LongAdder 表现的最为优秀，ConcurrentAutoTable 以及两个版本的 LongAdder 在一个数量级之上；1.8 的 AtomicLong 相比 1.7 的 AtomicLong 优秀很多，可以得出这样的结论，1.7 的 CAS+LOCK CMPXCHG 方案的确不如 1.8 的 LOCK XADD 来的优秀，但如果与特地优化过的其他计数器方案来进行比较，便相形见绌了。 如果关注 get 性能，虽然这意义不大，但可以见得，AtomicLong 的 get 性能在高并发下表现依旧优秀，而 LongAdder 组合求值的特性，导致其性能必然存在一定下降，位列第二梯队，而 ConcurrentAutoTable 的并发读性能最差。 关注整体性能，CounterBenchmark.rw 是对一组场景的整合打分，可以发现，在我们模拟的高并发计数器场景下，1.8 的 LongAdder 获得整体最低的延迟 98 ns，相比性能最差的 Jdk1.7 AtomicLong 实现，高了整整 10 倍有余，并且，随着并发度提升，这个数值还会增大。 AtomicLong 可以被废弃吗？既然 LongAdder 的性能高出 AtomicLong 这么多，我们还有理由使用 AtomicLong 吗？ 本文重点讨论的角度还是比较局限的：单机场景下并发计数器的高效实现。AtomicLong 依然在很多场景下有其存在的价值，例如一个内存中的序列号生成器，AtomicLong 可以满足每次递增之后都精准的返回其递增值，而 LongAdder 并不具备这样的特性。LongAdder 为了性能而丧失了一部分功能，这体现了计算机的哲学，无处不在的 trade off。 高性能计数器总结 AtomicLong ：并发场景下读性能优秀，写性能急剧下降，不适合作为高性能的计数器方案。内存需求量少。 LongAdder ：并发场景下写性能优秀，读性能由于组合求值的原因，不如直接读值的方案，但由于计数器场景写多读少的缘故，整体性能在几个方案中最优，是高性能计数器的首选方案。由于 Cells 数组以及缓存行填充的缘故，占用内存较大。 ConcurrentAutoTable ：拥有和 LongAdder 相近的写入性能，读性能则更加不如 LongAdder。它的使用需要引入 JCTools 依赖，相比 Jdk 自带的 LongAdder 并没有优势。但额外说明一点，ConcurrentAutoTable 的使用并非局限于计数器场景，其仍然存在很大的价值。 在前面提到的性能监控框架 Metrics，以及著名的熔断框架 Hystrix 中，都存在 LongAdder 的使用场景，有兴趣的朋友快去实践一下 LongAdder 吧。 本文所有的 JMH 测试代码，均可在我的 github 中获得：https://github.com/lexburner/JMH-samples.git ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/java-concurrent-counter/"},{"title":"JAVA 拾遗 --eqauls 和 hashCode 方法","text":"缘起—lombok 引发的惨案 Lombok 是一种 Java™ 实用工具，可用于帮助开发人员消除 Java 的冗长，尤其是对于简单的 Java 对象（POJO）。它通过注解实现这一目的。 最近一个新项目中开始使用了 lombok，由于其真的是太简单易懂了，以至于我连文档都没看，直接就上手使用了，引发了一桩惨案。 ** 实体类定义 ** 123456@Datapublic class Project { private Long id; private String projectName; private List&lt;Project&gt; projects;} 我在项目中设计了一个 Project 类，其包含了一个 List projects 属性，表达了项目间的依赖关系。@Data 便是 Lombok 提供的常用注解，我的本意是使用它来自动生成 getter/setter 方法。这样的实体类定义再简单不过了。 ** 意外出现 ** 使用 Project 类表达项目间的依赖关系是我的初衷，具体的分析步骤不在此赘述，对 Project 类的操作主要包括创建，打印，保存几个简单操作。运行初期，一切看似风平浪静，但经过长时间运行后，我意外的获得了如下的异常： 123Exception in thread &quot;Tmoe.cnkirito.dependency0&quot; java.lang.StackOverflowError at moe.cnkirito.dependency.model.Project.hashCode(Project.java:20) at java.util.AbstracList.hashCode(AbstractList.java:541) 这让我感到很意外，我并没有对 Project 类进行什么复杂的操作，也没有进行什么递归操作，怎么会得到 StackOverflowError 这个错误呢？更令我百思不得其解的地方在于，怎么报错的日志中还出现了 hashCode 和 AbstractList 这两个家伙？等等…hashCode…emmmmm…我压根没有重写过它啊，怎么可能会报错呢…. 再想了想 Lombok 的 @Data 注解，我似乎发现了什么…emmmmm…抱着怀疑的态度翻阅了下 Lombok 的文档，看到了如下的介绍 @Data is a convenient shortcut annotation that bundles the features of @ToString, @EqualsAndHashCode, @Getter / @Setter and @RequiredArgsConstructor together: In other words, @Data generates all the boilerplate that is normally associated with simple POJOs (Plain Old Java Objects) and beans: getters for all fields, setters for all non-final fields, and appropriate toString, equals and hashCode implementations that involve the fields of the class, and a constructor that initializes all final fields, as well as all non-final fields with no initializer that have been marked with @NonNull, in order to ensure the field is never null. 原来 @Data 注解不仅帮我们实现了生成了 @Getter / @Setter 注解，还包含了 @ToString, @EqualsAndHashCode, 和 @RequiredArgsConstructor 注解，这其中的 @EqualsAndHashCode 注解似乎和我这次的惨案密切相关了。顺藤摸瓜，看看 @EqualsAndHashCode 的文档： Any class definition may be annotated with @EqualsAndHashCode to let lombok generate implementations of the equals(Object other) and hashCode() methods. By default, it’ll use all non-static, non-transient fields @EqualsAndHashCode 会自动生成 equals(Object other) 和 hashCode() 两个方法，默认会使用所有非静态，非瞬时状态的字段。 回到我的案例中，也就是说，Lombok 会将 Project 类中的 List projects 当做是 hashCode 计算的一部分（同理，equals,toString 也会存在同样的问题），而如果我的项目中出现循环引用，这就会导致死循环，最终就会抛出 StackOverFlowError。 为了验证我的想法，简化的项目中的代码后，来测试下 12345678public String testHashCode(){ Project project = new Project(); Project other = new Project(); other.setProjects(Arrays.asList(project)); project.setProjects(Arrays.asList(other)); System.out.println(project.hashCode()); return &quot;success&quot;;} 调用该代码后，复现了上述的异常。 123Exception in thread &quot;Tmoe.cnkirito.dependency0&quot; java.lang.StackOverflowError at moe.cnkirito.dependency.model.Project.hashCode(Project.java:20) at java.util.AbstracList.hashCode(AbstractList.java:541) 紧接着，继续测试下 toString 和 eqauls 方法 1234567891011121314## 测试循环引用实体类中下的 toString 方法java.lang.StackOverflowError: null at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:125) ~[na:1.8.0_161] at java.lang.AbstractStringBuilder.appendNull(AbstractStringBuilder.java:493) ~[na:1.8.0_161] at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:446) ~[na:1.8.0_161] at java.lang.StringBuilder.append(StringBuilder.java:136) ~[na:1.8.0_161] at com.qianmi.dependency.model.Project.toString(Project.java:18) ~[classes/:na]## 测试循环引用实体类中下的 equals 方法 java.lang.StackOverflowError: null at java.util.AbstractList.rangeCheckForAdd(AbstractList.java:604) ~[na:1.8.0_161] at java.util.AbstractList.listIterator(AbstractList.java:325) ~[na:1.8.0_161] at java.util.AbstractList.listIterator(AbstractList.java:299) ~[na:1.8.0_161] at java.util.AbstractList.equals(AbstractList.java:518) ~[na:1.8.0_161] at com.qianmi.dependency.model.Project.equals(Project.java:18) ~[classes/:na] 不出所料，都存在同样的问题。 这一案例可以稍微总结下，一是在使用新的技术框架（Lombok）之前没有看文档，对其特性不太了解，望文生义，认为 @Data 不会重写 hashCode 等方法，二是没有考虑到 hashCode，eqauls 等方法应该如何正确地覆盖。 回顾 JAVA 中最基础的方法： hashCode 和 equals这两个方法说是 JAVA 最基础的方法一点不为过，但往往越基础的东西越容易被人忽视，让我想起了 JAVA 闲聊群中一位长者经常吐槽的一点：『现在的面试、群聊动不动就是高并发，JVM，中间件，却把基础给遗忘了』。 我感觉很幸运，在当初刚学 JAVA 时，便接触了一本神书《effective java》，一本号称怎么夸都不为过的书，它的序是这么写的 我很希望 10 年前就拥有这本书。可能有人认为我不需要任何 Java 方面的书籍，但是我需要这本书。 ——Java 之父 James Gosling 其书中的第三章第 8 条，第 9 条阐述了 equals 和 hashCode 的一些重写原则，我将一些理论言简意赅的阐述在本节中，喜欢的话推荐去看原书哦。 第 8 条：覆盖 equals 时请遵守通用约定 什么时候应该覆盖 Object.equals 呢？如果类具有自己特有的“逻辑相等”概念（不同于对象等同的概念），而且超类还没有覆盖 equals 以实现期望的行为，这时我们就需要覆盖 equals 方法。这通常属于“值类（value class）”的情形。值类仅仅是一个表示值的类，例如 Integer 或者 Date。程序员在利用 equals 方法来比较值对象的引用时，希望知道它们在逻辑上是否相等，而不是想了解它们是否指向同一个对象。为了满足程序员的要求，不仅必需覆盖 equals 方法，而且这样做也使得这个类的实例可以被用作映射表（map）的键（key），或者集合（set）的元素，使映射或者集合表现出预期的行为。 在覆盖 equals 方法的时候，你必须要遵守它的通用约定。下面是约定的内容，来自 Object 的规范 [JavaSE6]： equals 方法实现了 * 等价关系（equivalence relation）*： ** 自反性（reflexive）**。对于任何非 null 的引用值 x，x.equals(x) 必须返回 true。 ** 对称性（symmetric）**。对于任何非 null 的引用值 x 和 y，当且仅当 y.equals(x) 返回 true 时，x.equals(y) 必须返回 true。 ** 传递性（transitive）**。对于任何非 null 的引用值 x、y 和 z。如果 x.equals(y) 返回 true，并且 y.equals(z) 也返回 true，那么 x.equals(z) 也必须返回 true。 ** 一致性（consistent）**。对于任何非 null 的引用值 x 和 y，只要 equals 的比较操作在对象中所用的信息没有被修改，多次调用 x.equals(x) 就会一致地返回 true，或者一致的返回 false。 对于任何非 null 的引用值 x，x.equals(null) 必须返回 false。 学过高数，离散的同学不会对上述的理论陌生，它们源自于数学理论，没了解过这些概念的同学也不必有所顾忌，因为你只需要养成习惯，在设计一个实体类时时刻惦记着上述几个关系，能符合的话大概就没有问题。结合所有这些要求，得出了以下实现高质量 equals 方法的诀窍： ** 使用 == 操作符检查“参数是否为这个对象的引用”**。如果是，则返回 true。这只不过是一种性能优化，如果比较操作有可能很昂贵，就值得这么做。 ** 使用 instanceof 操作符检查“参数是否为正确的类型”**。如果不是，则返回 false。一般说来，所谓“正确的类型”是指 equals 方法所在的那个类。有些情况下，是指该类所实现的某个接口。如果类实现的接口改进了 equals 约定，允许在实现了该接口的类之间进行比较，那么就使用接口。集合接口（collection interface）如 Set、List、Map 和 Map.Entry 具有这样的特性。 ** 把参数转换成正确的类型 **。因为转换之前进行过 instanceof 测试，所以确保会成功。 ** 对于该类中每个“关键（significant）域，检查参数中的域是否与该对象中对应的域相匹配”**。如果这些测试全部成功，则返回 true；否则返回 false。如果第 2 步中的类型是个借口，就必须通过接口方法访问参数中的域；如果该类型是个类，也许就能够直接访问参数中的域，这要取决于它们的可访问性。 对于既不是 float 也不是 double 类型的基本类型域，可以使用 == 操作符进行比较；对于对象引用域，可以递归地调用 equals 方法；对于 float 域，可以使用 Float.compare 方法；对于 double 域，则使用 Double.compare。对于 float 和 double 域进行特殊的处理是有必要的，因为存在着 Float.NaN、-0.0f 以及类似的 double 常量；详细信息请参考 Float.equals 的文档。对于数组域，则要把以上这些指导原则应用到每个元素上。如果数组域中的每个元素都很重要，就可以使用发行版本 1.5 中新增的其中一个 Arrays.equals 方法。 有些对象引用域包含 null 可能是合法的，所以，为了避免可能导致 NullPointerException 异常，则使用下面的习惯用法来比较这样的域： 1(field == null ? o.field == null : field.equals(o.field)) 如果 field 域和 o.field 通常是相同的对象引用，那么下面的做法就会更快一些： 1(field == o.field || (field != null &amp;&amp; field.equals(o.field))) ** 当你编写完成了 equals 方法之后，应该问自己三个问题：它是不是对称的、传递的、一致的？** 并且不要只是自问，还要编写单元测试来检验这些特性！如果答案是否定的，就要找出原因，再相应地修改 equals 方法的代码。当然，equals 方法也必须满足其他两个特性（自反性和非空性），但是这两种特性通常会自动满足。 其他原则还包括： ** 覆盖 equals 时总要覆盖 hashCode**。(在下一节中介绍) ** 不要企图让 equals 方法过于智能 **。如果只是简单地测试域中的值是否相等，则不难做到遵守 equals 约定。如果想过度地去寻求各种等价关系，则很容易陷入麻烦之中。把任何一种别名形式考虑到等价的范围内，往往不会是个好主意。例如，File 类不应该视图把指向同一个文件的符号链接（symbolic link）当作相等的对象来看待。所幸 File 类没有这样做。 ** 不要将 equals 声明中的 Object 对象替换为其他的类型 **。 第 9 条：覆盖 equals 时总要覆盖 hashCode 一个很常见的错误根源在于没有覆盖 hashCode 方法。* 在每个覆盖了 equals 方法的类中，也必须覆盖 hashCode 方法 *。如果不这样做的话，就会违反 Object.hashcode 的通用约定，从而导致该类无法结合所有基于散列的集合一起正常工作，这样的集合包括 HashMap、HashSet 和 Hashtable。 下面是约定的内容，摘自 Object 规范 [JavaSE6]： 在应用程序的执行期间，只要对象的 equals 方法的比较操作所用到的信息没有被修改，那么对同一个对象调用多次，hashCode 方法都必须始终如一地返回同一个整数。在同一个应用程序的多次执行过程中，每次执行所返回的整数可以不一致。 如果两个对象根据 equals(Object) 方法比较是相等的，那么调用这两个对象中任意一个对象的 hashCode 方法都必须产生同样的整数结果。 如果两个对象根据 equals(Object) 方法比较是不相等的，那么调用这两个对象中任意一个对象的 hashCode 方法，则不一定要产生不同的整数结果。但是程序员应该知道，给不相等的对象产生截然不同的整数结果，有可能提高散列表（hash table）的性能。 因没有覆盖 hashCode 而违反的关键约定是第二条：相等的对象必须具有相等的散列码 *（hash code）。根据类的 equals 方法，两个截然不同的实例在逻辑上有可能是相等的，但是，根据 Object 类的 hashCode 方法，它们仅仅是两个没有任何共同之处的对象。因此，对象的 hashCode 方法返回两个看起来是随机的整数，而不是根据第二个约定所要求的那样，返回两个相等的整数。 默默看完书中的文字，是不是觉得有点哲学的韵味呢，写好一手代码真的不容易。 实战中如何重写 hashCode 和 equals？hashCode 和 equals 很重要，在使用中，与之密切相关的一般是几个容器类：HashMap 和 HashSet，意味着当我们将一个类作为其中的元素时，尤其需要考量下 hashCode 和 equals 的写法。 话不多数，即刻介绍。对了，你指望我手敲 hashCode 和 equals 吗？不存在的，程序员应该优雅的偷懒，无论你是 eclipse 玩家还是 idea 玩家，都能找到对应的快捷键，帮你自动重写这两个方式，我们要做的就是对参数的选择做一些微调。例如使用 idea 生成下面这个类的 hashCode 和 equals 方法，设置前提：将所有字段当做关键（significant）域。 123456789public class Example { private int a; private float b; private double c; private BigDecimal d; private char e; private byte f; private String g;} 方法一：Intellij Default12345678910111213141516171819202122232425262728293031@Overridepublic boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; if (!super.equals(o)) return false; Example example = (Example) o; if (a != example.a) return false; if (Float.compare(example.b, b) != 0) return false; if (Double.compare(example.c, c) != 0) return false; if (e != example.e) return false; if (f != example.f) return false; if (!d.equals(example.d)) return false; return g.equals(example.g);}@Overridepublic int hashCode() { int result = super.hashCode(); long temp; result = 31 * result + a; result = 31 * result + (b != +0.0f ? Float.floatToIntBits(b) : 0); temp = Double.doubleToLongBits(c); result = 31 * result + (int) (temp ^ (temp &gt;&gt;&gt; 32)); result = 31 * result + d.hashCode(); result = 31 * result + (int) e; result = 31 * result + (int) f; result = 31 * result + g.hashCode(); return result;} 这可能是大家最熟悉的方法，先来分析下 equals 的写法。看样子的确是遵循了《effective java》中提及的 java1.6 规范的，值得注意的点再强调下：Float 和 Double 类型的比较应该使用各自的静态方法 Float.compare 和 Double.compare。 hashCode 方法则更加有趣一点，你可能会有如下的疑问： Double.doubleToLongBits 是干嘛用的？ 为啥是 31？ 为什么还有 ^，&gt;&gt;&gt; 这些运算符号？ 带着疑问来看看下面的解释。 一个好的散列函数通常倾向于“为不相等的对象产生不相等的散列码”。这正是上一节中 hashCode 约定中第三条的含义。理想情况下，散列函数应该把集合中不相等的实例均匀地分布到所有可能的散列值上。要想完全达到这种理想的情形是非常困难的。但相对接近这种理想情形则并不太苦难。《effective java》给出了一种简单的解决办法： 把某个非零的常数值，比如说 17，保存在一个名为 result 的 int 类型的变量中。 对于对象中每个关键域 f（指 equals 方法中涉及的每个域），完成以下步骤： a. 为该域计算 int 类型的散列码 c: i. 如果该域是 boolean 类型，则计算 (f ? 1 : 0). ii. 如果该域是 byte、char、short 或者 int 类型，则计算 (int)f。 iii. 如果该域是 long 类型，则计算 (int)(f ^ (f &gt;&gt;&gt; 32))。 iv. 如果该域是 float 类型，则计算 Float.floatToIntBits(f)。 v. 如果该域是 double 类型，则计算 Double.doubleToLongBits(f)，然后按照步骤 2.a.iii，为得到的 long 类型值计算散列值。 vi. 如果该域是一个对象引用，并且该域的 equals 方法通过递归地调用 equals 的方式来比较这个域，则同样为这个域递归地调用 hashCode。如果需要更复杂的比较，则为这个域计算一个“范式（canonical representation）”，然后针对这个范式调用 hashCode。如果这个域的值为 null，则返回 0（或者其他某个常数，但通常是 0）。 vii. 如果该域是一个数组，则要把每一个元素当做单独的域来处理。也就是说，递归地应用上述规则，对每个重要的元素计算一个散列码，然后根据步骤 2.b 中的做法把这些散列值组合起来。如果数组域中的每个元素都很重要，可以利用发行版本 1.5 中增加的其中一个 Arrays.hashCode 方法。 b. 按照下面的公式，把步骤 2.a 中计算得到的散列码 c 合并到 result 中： 1result = 31 * result + c; 返回 result。 写完了 hashCode 方法之后，问问自己“相等的实例是否都具有相等的散列码”。要编写单元测试来验证你的推断。如果相等实例有着不相等的散列码，则要找出原因，并修正错误。 在散列码的计算过程中，可以把 * 冗余域（redundant field）* 排除在外。换句话说，如果一个域的值可以根据参与计算的其他域值计算出来，则可以把这样的域排除在外。必须排除 equals 比较计算中没有用到的任何域，否则很有可能违反 hashCode 约定的第二条。 上述步骤 1 中用到了一个非零的初始值，因此步骤 2.a 中计算的散列值为 0 的那些初始域，会影响到散列值。如果步骤 1 中的初始值为 0，则整个散列值将不受这些初始域的影响，因为这些初始域会增加冲突的可能性。值 17 则是任选的。 步骤 2.b 中的乘法部分使得散列值依赖于域的顺序，如果一个类包含多个相似的域，这样的乘法运算就会产生一个更好的散列函数。例如，如果 String 散列函数省略了这个乘法部分，那么只是字母顺序不同的所有字符串都会有相同的散列码。之所以选择 31，是因为它是一个奇素数。如果乘数是偶数，并且乘法溢出的话，信息就会丢失，因为与 2 相乘等价于位移运算。使用素数的好处并不很明显，但是习惯上都使用素数来计算散列结果。31 有个很好的特性，即用位移和减法来代替乘法，可以得到更好的性能，31 * i == (i &lt;&lt; 5) - i。现代的 VM 可以自动完成这种优化。 是不是几个疑惑都解开了呢？ 方法二：Objects.hash 和 Objects.equals12345678910111213141516171819@Overridepublic boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; if (!super.equals(o)) return false; Example example = (Example) o; return a == example.a &amp;&amp; Float.compare(example.b, b) == 0 &amp;&amp; Double.compare(example.c, c) == 0 &amp;&amp; e == example.e &amp;&amp; f == example.f &amp;&amp; Objects.equals(d, example.d) &amp;&amp; Objects.equals(g, example.g);}@Overridepublic int hashCode() { return Objects.hash(super.hashCode(), a, b, c, d, e, f, g);} JAVA 是一个与时俱进的语言，有问题从自身解决，便利了开发者，如《effective java》所言，在 jdk1.6 中上述那些原则只是一纸空文。错误同真理的关系，就象睡梦同清醒的关系一样。一个人从错误中醒来，就会以新的力量走向真理。在 jdk1.7 中便造就了诸多的方法 Objects.hash 和 Objects.equals 帮助你智能的实现 hashCode 和 equals 方法。很明显，代码量上比方法一少了很多，并且有了 jdk 的原生支持，心里也更加有底了。 方法三：Lombok 的 @EqualsAndHashCode前面已经提到了 Lombok 的这个注解，在此详细介绍下这个注解的用法，方便大家写出规范的 hashCode 和 equals 方法。 此注解会生成 equals(Object other) 和 hashCode() 方法。 它默认使用非静态，非瞬态的属性 可通过参数 exclude 排除一些属性 可通过参数 of 指定仅使用哪些属性 它默认仅使用该类中定义的属性且不调用父类的方法 可通过 callSuper=true 解决上一点问题。让其生成的方法中调用父类的方法。 使用 Lombok 很便捷，整个代码也很清爽 12345678910111213@Data@EqualsAndHashCode(of = {&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;,&quot;g&quot;})// 默认就是所有参数public class Example { private int a; private float b; private double c; private BigDecimal d; private char e; private byte f; private String g;} 如果想知道编译过后的庐山真面目，也可以在 target 包中找到 Example.java 生成的 Example.class，: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public boolean equals(Object o) { if (o == this) { return true; } else if (!(o instanceof Example)) { return false; } else { Example other = (Example)o; if (!other.canEqual(this)) { return false; } else if (this.getA() != other.getA()) { return false; } else if (Float.compare(this.getB(), other.getB())!= 0) { return false; } else if (Double.compare(this.getC(), other.getC())!= 0) { return false; } else { Object this$d = this.getD(); Object other$d = other.getD(); if (this$d == null) { if (other$d != null) { return false; } } else if (!this$d.equals(other$d)) { return false; } if (this.getE() != other.getE()) { return false; } else if (this.getF() != other.getF()) { return false; } else { Object this$g = this.getG(); Object other$g = other.getG(); if (this$g == null) { if (other$g != null) { return false; } } else if (!this$g.equals(other$g)) { return false; } return true; } } }}protected boolean canEqual(Object other) { return other instanceof Example;}public int hashCode() { int PRIME = true; int result = 1; int result = result * 59 + this.getA(); result = result * 59 + Float.floatToIntBits(this.getB()); long $c = Double.doubleToLongBits(this.getC()); result = result * 59 + (int)($c &gt;&gt;&gt; 32 ^ $c); Object $d = this.getD(); result = result * 59 + ($d == null ? 43 : $d.hashCode()); result = result * 59 + this.getE(); result = result * 59 + this.getF(); Object $g = this.getG(); result = result * 59 + ($g == null ? 43 : $g.hashCode()); return result;} 大致和前两种行为一致，这里选择素数从 31 替换成了 59，没有太大差异。 总结我在开发时也曾考虑一个问题：一个数据库持久化对象到底怎么正确覆盖 hashCode 和 equals？以订单为例，是用主键 id 来判断，还是 流水编号 orderNo 来判断，可能没有准确的答案，各有各的道理，但如果将它丢进 HashSet，HashMap 中就要额外注意，hashCode 和 equals 会影响它们的行为！ 这次 Lombok 发生的惨案主要还是由于不合理的 hashCode 和 equals（也包括了 toString）方法导致的，循环引用这种问题虽然没有直接在《effective java》中介绍，但一个引用，一个集合类是不是应该作为 hashCode 和 equals 的关键域参与计算，还是值得开发者仔细推敲的。本文还介绍了一些 hashCode 和 equals 的通用原则，弱弱地推荐 Lombok 便捷开发，强烈安利《effective java》一书。","link":"/java-eqaulsandhashcode/"},{"title":"技术精进的三境界","text":"最近更新了一篇 Docker 的文章，朋友跟我反馈说效果并不是很好，我回头看了下，的确没有我自己的特色，没有太多思考，让公众号显得有些「百货」了。经过反思，今后只在个人博客更新 Docker 相关的个人学习经验（传送门），个人公众号还是主要推送和 Java 结合较为紧密的内容。 前言前不久公众号后台有人给我留言，请教如何体系地学习 Java 知识，我当时心想：这话题太大，Java 技术栈也太深，不是一篇文章能说清楚的，但要说学习技巧，却的确是有规律所寻，秉持着「授人以鱼不如授人以渔」的想法，这篇文章便分享下我个人的一些学习技巧。 王国维在《人间词话》中提到了古今之成大事业、大学问者，必经的三种境界： “昨夜西风凋碧树，独上高楼，望尽天涯路。” 此第一境也。 “ 衣带渐宽终不悔，为伊消得人憔悴。” 此第二境也。 “ 众里寻他千百度，蓦然回首，那人却在，灯火阑珊处。”此第三境也。我也按照我的理解，将精进技术分成了三个境界。 第一境: 从 overview 和 guides 获取新知识的直观感受善用搜索，推荐 google。google 第一页搜索出来权威内容的概率要大于 baidu，人很容易有先入为主的观念，前五篇文章对一个知识点的直观感受，就会形成固化映像。（** 技术精进的第一层关卡：你可能缺少一个梯子 **） 以一个可能大多数读者都觉得陌生的知识点：图形数据库 Neo4j 作为引子，来介绍。 搜索 neo4j 关键词时大概率会搜索到官网，但作为一门技术的初学者，我更习惯于阅读下中文博客，教程，以对这个陌生的技术有一个宏观的了解。虽然大家都很赞同一个观点：英文文档更加权威，但是我个人还是觉得，中文教程更加直观，实际上我读中文文档的速度是英文文档的 2-3 倍。 不需要细读每个语法，我一般习惯看下概览，再看下目录对整体的知识点有个宏观的掌握。这种入门级的网站有很多，大多出现在搜索引擎的第一页。而官方的 overview 通常也是很适合入门的手段。 英文阅读能力强的读者可以直接上手官方的 guides，overview，get started 等入门教程，它们比中文教程强在很多方面 (** 技术精进的第二层关卡：你可能需要一定的英语阅读能力 **)： 避免了语言转换带来的翻译失真。很多中文教程读起来总觉得拗口，太过于书面化，语句也不符合中国人阅读的习惯。翻译烂的原因不需要我总结，事实上我对翻译这个事有过比较深的真实体验，我曾经和 spring4all 的小伙伴们一起对 spring 文档进行过翻译，大家积极性都很高，一周之类几乎所有的 guides 文档全部翻译完成了，但质量真的惨不忍睹，参与校对时，竟然有人将 「jar」翻译成「蜜罐」，而且不在少数。 官方文档维护着最新的版本。对于一些热门的软件技术，比如我最近接触的 docker，之前研究过得 lucene，它们都一些共同点：版本差异非常大，版本更迭速度快。翻看 docker 在国内零散的博客，大多数比最新的 docker 版本落后 3 个大版本以上；而像 lucene 这种大版本演进非常迅速的技术（大版本演进意味着不兼容老版本），api 几乎是翻天覆地的变化。 权威性。 英语能力要求并不是很高，大学英语 4 级足矣，毕竟有个东西叫 ** 谷歌翻译 **。 再比如 spring 体系的知识点，有一个通用的学习路线。还是以 neo4j 为例，得知 spring 对 neo4j 有二次封装之后，便熟练地来到了 spring 的 guides 专栏 (https://spring.io/guides)，spring 对所有的知识点提供了两个维度的学习文档，其中 https://spring.io/guides 一般都是一个 15 分钟上手的 hello world，让你快速上手一门新的技术；另一个是较为完善的文档：https://spring.io/docs/reference，成体系地介绍技术细节。第一阶段主要关注前者，一般它长这样： 敲完 hello world，一般就可以对应简历上「了解 XXXX」的描述了 ( 斜眼笑。 第二境：官方文档与社区大多数时候，一个停留在 hello world 认知级别的知识点对于我们的帮助不会很大，你甚至连在群里装逼的机会都没有！恐怖如斯！ 了解完毕这门技术是用来解决什么问题的，它大概是怎么使用的，有了这些直观体验之后再来看文档，会直观不少。下面主要介绍个人阅读官方文档的一些心路历程。 首先来看看宇宙级开源项目 spring 的官方文档，它长这样： 琳琅满目的项目，所有与 spring 相关的技术都可以在这儿获取最权威的解读，怎么学习 springboot，springcloud 还需要问吗？平时经常接触 spring 的同学如果这个页面都没见过，私以为在学习认知上是有所欠缺的。 文档除了主体知识内容之外还有整体介绍，新旧版本迭代的改动，新特性，依赖分析，性能测试等，文档内容一般都是非常多的，可以撷取其中核心的几节，将主要用法和注意事项掌握，至于不常用的特性，可以在碰到时再翻阅。 曾经一个项目需求中涉及到 spring security 的改造，而相关的文档又比较少，我至少通读过 5 遍 spring security 完整的文档，从一开始的大概了解，到最后的基本掌握，很多细节点一开始无法 get 到，读多了之后很多常用词汇和语感都会提升，细节会被消化，整体阅读速度也会随着文档阅读量提高而提升（比如 out-of-box 这个高频词一开始是不知道什么意思，谷歌翻译也翻不出来，后来得知是「开箱即用」）。 开源社区也是精进一门技术的重要途径，比如 spring4all，k8s，netty，elk 社区汇聚了不少文章和问答，初中高级的使用者都在社区中扮演着各自的角色（部分中文社区甚至样式都差不多，估计是用同一套开源代码搭建的 [捂脸])，社区活跃度也是一门技术火热的衡量指标。经常被大家调侃的面向 github 编程背后的 github，以及 Stack Overflow，都是质量比较硬的社区。 第三境：源码阅读（** 技术精进的第三层关卡：对源码的恐惧阻止了一个人探索的步伐 **）如果你觉得看完前面的文字是在说废话，那么这一节可能稍微能勾起你的兴趣，权当之前是一个铺垫，照顾下一些初学者。 在交流群里面发现的一个现象，很多人对源码有一种天生的畏惧感，诸如：“我才刚毕业 / 我才工作三年，还没到看源码的阶段”，“源码不是架构师看的吗”，“源码看不懂“，”看看别人的源码分析不就行了“…这一节主要聊聊源码阅读技巧，以及一些个人对阅读源码的感悟。 首先澄清几点：阅读源码绝对和工作年限无关；阅读源码绝对和工作职位无关；大多数源码并不是很难；debug+ 源码分析绝对比看文章来的直观。 1 源码中的测试用例还是以 neo4j 为例，我们在 github 找到 spring-data-neo4j 的源码，然后 git clone 到本地，在本地 idea 中打开。 和源码相关的第一点介绍的便是源码中的测试用例，对于大多数的开源项目而言，测试覆盖率是一个质量衡量的指标。大部分 Java 相关开源项目会包含测试用例，项目的一些功能特性可能在文档中无法一一介绍，通常可以在 src/test/java 中找到对应的用法，比如 neo4j 是怎么支持事务的，怎么维护边和边的关系的，在测试用例中都可以看到官方是怎么使用的。再举个例子，之前在使用 orika 这个拷贝工具时，一开始不知道怎么实现泛型的拷贝（泛型的运行时擦除特性），谷歌搜索和 Stack Overflow 提问无果之后，终于在源码的诸多测试用例中找到了我需要的代码。 2 通过核心接口 / 包结构分析框架层次结构我猜测有人拒绝阅读源码的一个原因：源码注释量不够，压根不知道一段代码是干嘛用的。的确，我在阅读有些源码时也会出现这样的情况：这儿会什么要加锁？为什么要用 AtomicReference 这个类？为什么这个方法放在父类，而另外看似功能差不多的代码放在子类实现？框架编写者不会像培训班的老师一样跟你讲解他为什么要这么写，也不是所有的源码都能像 HashMap 的源码那样被大家泛滥地解读，是的，可能大多数情况下你的境地是「虽然不懂，还没法问！」气不气，尴不尴尬？没办法，因为这已经是第三境了，曲高和寡，但还是有些方法规避这样的情况的，那就是：主要关心核心接口，通过接口暴露的方法，猜测出作者的意图。据我不多的源码阅读经验，一个实现类的注释可能不多，但接口的注释通常会很多，毕竟一个原则是面向接口编程。 上图是我分析 spring security 源码时，根据接口间的关系整理出来的 UML 类图，对于绿色实现类的细节我可能并不是特别关注，浅蓝色代表的接口才是我们理解整个架构体系的切入点，配合 idea 这些优秀的集成开发环境，可以很方便的整理出 UML 类图。接口是全局架构，实现类是源码细节。 顺带一提：熟练使用 IDE 很重要。无论你是 eclipse 玩家还是 intellij idea 玩家，你都应该熟练掌握快捷键和一些常用操作，方便你阅读源码。比如 idea 右键可以自动生成类的继承关系图（聚合关系的体现不够智能），方便分析层次关系；显示方法 outline 快速查看一个类的方法概览，在成片的源码中非常有用；快速定位一个接口的实现类，一个类的子类等等。 再比如我在阅读 motan 这款 rpc 框架源码时遵循的顺序是其包结构的层次关系。 写源码分析文章时基本就是按照一个包一篇文章来分析，从而化繁为简。无论是模块结构，包结构，还是代码层面的接口结构，重点都是在强调：我们需要从宏观掌握一个框架，再去扣细节，否则我个人感觉学习状态就是很迷，不知道学到哪儿了。 3 带着问题阅读实现类源码具体的实现类源码真的没什么技巧可讲，一定是看一个人的写代码功底，以及代码敏感度了，非要说技巧的话，可能就是多写代码，培养代码敏感度了。此外，带着问题去读源码个人体验下来感觉不错，在阅读 spring security 源码时，我带着的问题是，怎么结合 zuul 实现动态的权限控制，一步步地 debug，看它原来的实现，之后是改源码，debug 看改变的效果。 具体的源码的阅读难度也是参差不齐的，个人学习经历中发现 motan 的源码就很容易阅读，spring 的源码因为文档比较齐全，阅读体验也很好，但 lucene 的源码和 hibernate 的源码，我也尝试阅读过，简直是天书，又比如说 netty，单单熟练使用它就已很难，何论源码。一方面跟个人阅历有关，一方面跟框架实现难度有关，很难盖棺定论得出方法论。个人建议是，明确自己需要解决什么问题去阅读源码，不必为了装逼而读源码。 贴下之前几个系列的源码解读链接： 【RPC 系列】 https://www.cnkirito.moe/categories/RPC/ 【Spring Security 系列】https://www.cnkirito.moe/categories/Spring-Security/ 【OAuth2 系列】https://www.cnkirito.moe/categories/Spring-Security-OAuth2/ 三境之外除了学习技术的三种境界，还有一些其他个人的感悟。比如类比学习法，一开始学习 spring-data-jpa 时效率比较慢，这对于我是一个比较新的技术，但当我后来再接触 spring-data-redis，spring-data-neo4j 时，虽然同样是第一次接触这些数据访问层，但有了之前 spring-data-jpa 的参考，可以说是事半功倍。关于视频，博客，书，文档可以说关系很微妙，从视频到文档，越来越不直观，但学习效率越来越高，这些没有高低贵贱之分，私以为都是很好的学习方法。怎么提升代码技巧？说真的方法论归方法论，重点还是代码行数锻炼出来的代码敏感度，这是看书，看代码，写博客，看方法论学不来的，不多说了，滚去写代码了 [抱拳]。","link":"/java-how-to-learn/"},{"title":"JAVA 拾遗 — JMH 与 8 个测试陷阱","text":"前言JMH 是 Java Microbenchmark Harness（微基准测试）框架的缩写（2013 年首次发布）。与其他众多测试框架相比，其特色优势在于它是由 Oracle 实现 JIT 的相同人员开发的。在此，我想特别提一下 Aleksey Shipilev（JMH 的作者兼布道者）和他优秀的博客文章。笔者花费了一个周末，将 Aleksey 大神的博客，特别是那些和 JMH 相关的文章通读了几遍，外加一部公开课视频 《”The Lesser of Two Evils” Story》 ，将自己的收获归纳在这篇文章中，文中不少图片都来自 Aleksey 公开课视频。 阅读本文前本文没有花费专门的篇幅在文中介绍 JMH 的语法，如果你使用过 JMH，那当然最好，但如果没听过它，也不需要担心（跟我一周前的状态一样）。我会从 Java Developer 角度来谈谈一些常见的代码测试陷阱，分析他们和操作系统底层以及 Java 底层的关联性，并借助 JMH 来帮助大家摆脱这些陷阱。 通读本文，需要一些操作系统相关以及部分 JIT 的基础知识，如果遇到陌生的知识点，可以留意章节中的维基百科链接，以及笔者推荐的博客。 笔者能力有限，未能完全理解 JMH 解决的全部问题，如有错误以及疏漏欢迎留言与我交流。 初识 JMH测试精度 上图给出了不同类型测试的耗时数量级，可以发现 JMH 可以达到 ** 微秒 ** 级别的的精度。 这样几个数量级的测试所面临的挑战也是不同的。 毫秒级别的测试并不是很困难 微秒级别的测试是具备挑战性的，但并非无法完成，JMH 就做到了 纳秒级别的测试，目前还没有办法精准测试 皮秒级别…Holy Shit 图解： Linpack : Linpack benchmark 一类基础测试，度量系统的浮点计算能力 SPEC：Standard Performance Evaluation Corporation 工业界的测试标准组织 pipelining：系统总线通信的耗时 Benchmark 分类测试在不同的维度可以分为很多类：集成测试，单元测试，API 测试，压力测试… 而 Benchmark 通常译为基准测试（性能测试）。你可以在很多开源框架的包层级中发现 Benchmark，用于阐释该框架的基准水平，从而量化其性能。 基准测试又可以细分为 ：Micro benchmark，Kernels，Synthetic benchmark，Application benchmarks.etc. 本文的主角便属于 Benchmark 的 Micro benchmark。基础测试分类详细介绍 here 为什么需要有 Benchmark If you cannot measure it, you cannot improve it. –Lord Kelvin 俗话说，没有实践就没有发言权，Benchmark 为应用提供了数据支持，是评价和比较方法好坏的基准，Benchmark 的准确性，多样性便显得尤为重要。 Benchmark 作为应用框架，产品的基准画像，存在统一的标准，避免了不同测评对象自说自话的尴尬，应用框架各自使用有利于自身场景的测评方式必然不可取，例如 Standard Performance Evaluation Corporation (SPEC) 即上文“测试精度”提到的词便是工业界的标准组织之一，JMH 的作者 Aleksey 也是其中的成员。 JMH 长这样1234@Benchmarkpublic void measure() { // this method was intentionally left blank.} 使用起来和单元测试一样的简单 它的测评结果 12Benchmark Mode Cnt Score Error UnitsJMHSample_HelloWorld.measure thrpt 5 3126699413.430 ± 179167212.838 ops/s 为什么需要 JMH 测试你可能会想，我用下面的方式来测试有什么不好？ 123long start = System.currentTimeMillis();measure();System.out.println(System.currentTimeMillis()-start); 难道 JMH 不是这么测试的吗？ 123@Benchmarkpublic void measure() {} 事实上，这是本文的核心问题，建议在阅读时时刻带着这样的疑问，为什么不使用第一种方式来测试。** 在下面的章节中，我将列举诸多的测试陷阱，他们都会为这个问题提供论据，这些陷阱会启发那些对“测试”不感冒的开发者。**。 预热在初识 JMH 小节的最后，花少量的篇幅来给 JMH 涉及的知识点开个头，介绍一个 Java 测试中比较老生常谈的话题 — 预热 (warm up)，它存在于下面所有的测试中。 «Warmup» = waiting for the transient responses to settle down 特别是在编写 Java 测试程序时，预热从来都是不可或缺的一环，它使得结果更加真实可信。 上图展示了一个样例测评程序随着迭代次数增多执行耗时变化的曲线，可以发现在 120 次迭代之后，性能才趋于最终稳定，这意味着：预热阶段需要有至少 120 次迭代，才能得到准确的基础测试报告。（JVM 初始化时的一些准备工作以及 JIT 优化是主要原因，但不是唯一原因）。需要被说明的事，JMH 的运行相对耗时，因为，预热被前置在每一个测评任务之前。 使用 JMH 解决 12 个测试陷阱陷阱 1：死码消除 measureWrong 方法想要测试 Math.log 的性能，得到的结果和空方法 baseline 一致，而 measureRight 相比 measureWrong 多了一个 return，正确的得到了测试结果。 这是由于 JIT 擅长删除“无效”的代码，这给我们的测试带来了一些意外，当你意识到 DCE 现象后，应当有意识的去消费掉这些孤立的代码，例如 return。JMH 不会自动实施对冗余代码的消除。 死码消除 这个概念很多人其实并不陌生，注释的代码，不可达的代码块，可达但不被使用的代码等等，我这里补充一些 Aleksey 提到的概念，用以阐释为何一般测试方法难以避免引用对象发生死码消除现象： Fast object combinator. Need to escape object to limit thread-local optimizations. Publishing the object ⇒ reference heap write ⇒ store barrier. 很绝望，个人水平有限，我没能 get 到这些点，只能原封不动地贴给大家看了。 JMH 提供了专门的 API — Blockhole 来避免死码消除问题。 1234@Benchmarkpublic void measureRight(Blackhole bh) { bh.consume(Math.log(PI));} 陷阱 2：常量折叠与常量传播常量折叠 (Constant folding) 是一个在编译时期简化常数的一个过程，常数在表示式中仅仅代表一个简单的数值，就像是整数 2，若是一个变数从未被修改也可作为常数，或者直接将一个变数被明确地被标注为常数，例如下面的描述： 1i = 320 * 200 * 32; 多数的现代编译器不会真的产生两个乘法的指令再将结果储存下来，取而代之的，他们会辨识出语句的结构，并在编译时期将数值计算出来（在这个例子，结果为 2,048,000）。 有些编译器，常数折叠会在初期就处理完，例如 Java 中的 final 关键字修饰的变量就会被特殊处理。而将常数折叠放在较后期的阶段的编译器，也相当常见。 1234567891011121314151617181920212223242526private double x = Math.PI;// 编译器会对 final 变量特殊处理 private final double wrongX = Math.PI;@Benchmarkpublic double baseline() { // 2.220 ± 0.352 ns/op return Math.PI;}@Benchmarkpublic double measureWrong_1() { // 2.220 ± 0.352 ns/op // 错误，结果可以被预测，会发生常量折叠 return Math.log(Math.PI);}@Benchmarkpublic double measureWrong_2() { // 2.220 ± 0.352 ns/op // 错误，结果可以被预测，会发生常量折叠 return Math.log(wrongX);}@Benchmarkpublic double measureRight() { // 22.590 ± 2.636 ns/op return Math.log(x);} 经过 JMH 可以验证这一点：只有最后的 measureRight 正确测试出了 Math.log 的性能，measureWrong_1，measureWrong_2 都受到了常量折叠的影响。 ** 常数传播 (Constant propagation)** 是一个替代表示式中已知常数的过程，也是在编译时期进行，包含前述所定义，内建函数也适用于常数，以下列描述为例： 123int x = 14;int y = 7 - x / 2;return y * (28 / x + 2); 传播可以理解变量的替换，如果进行持续传播，上式会变成： 123int x = 14;int y = 0;return 0; 陷阱 3：永远不要在测试中写循环这个陷阱对我们做日常测试时的影响也是巨大的，所以我直接将他作为了标题：永远不要在测试中写循环！ 本节设计不少知识点，循环展开(loop unrolling)，JIT &amp; OSR 对循环的优化。对于前者循环展开的定义，建议读者直接查看 wiki 的定义，而对于后者 JIT &amp; OSR 对循环的优化，推荐两篇 R 大的知乎回答： 循环长度的相同、循环体代码相同的两次 for 循环的执行时间相差了 100 倍? OSR（On-Stack Replacement）是怎样的机制？ 对于第一个回答，建议不要看问题，直接看答案；第二个回答，阐释了 OSR 都对循环做了哪些手脚。 测试一个耗时较短的方法，入门级程序员（不了解动态编译的同学）会这样写，通过循环放大，再求均值。 12345678910public class BadMicrobenchmark { public static void main(String[] args) { long startTime = System.nanoTime(); for (int i = 0; i &lt; 10_000_000; i++) { reps(); } long endTime = System.nanoTime(); System.out.println(&quot;ns/op :&quot; + (endTime - startTime)); }} 实际上，这段代码的结果是不可预测的，太多影响因子会干扰结果。原理暂时不表，通过 JMH 来看看几个测试方法，下面的 Benchmark 尝试对 reps 方法迭代不同的次数，想从中获得 reps 真实的性能。（注意，在 JMH 中使用循环也是不可取的，除非你是 Benchmark 方面的专家，否则在任何时候，你都不应该写循环） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051int x = 1;int y = 2;@Benchmarkpublic int measureRight() { return (x + y);}private int reps(int reps) { int s = 0; for (int i = 0; i &lt; reps; i++) { s += (x + y); } return s;}@Benchmark@OperationsPerInvocation(1)public int measureWrong_1() { return reps(1);}@Benchmark@OperationsPerInvocation(10)public int measureWrong_10() { return reps(10);}@Benchmark@OperationsPerInvocation(100)public int measureWrong_100() { return reps(100);}@Benchmark@OperationsPerInvocation(1000)public int measureWrong_1000() { return reps(1000);}@Benchmark@OperationsPerInvocation(10000)public int measureWrong_10000() { return reps(10000);}@Benchmark@OperationsPerInvocation(100000)public int measureWrong_100000() { return reps(100000);} 结果如下： 12345678Benchmark Mode Cnt Score Error UnitsJMHSample_11_Loops.measureRight avgt 5 2.343 ± 0.199 ns/opJMHSample_11_Loops.measureWrong_1 avgt 5 2.358 ± 0.166 ns/opJMHSample_11_Loops.measureWrong_10 avgt 5 0.326 ± 0.354 ns/opJMHSample_11_Loops.measureWrong_100 avgt 5 0.032 ± 0.011 ns/opJMHSample_11_Loops.measureWrong_1000 avgt 5 0.025 ± 0.002 ns/opJMHSample_11_Loops.measureWrong_10000 avgt 5 0.022 ± 0.005 ns/opJMHSample_11_Loops.measureWrong_100000 avgt 5 0.019 ± 0.001 ns/op 如果不看事先给出的错误和正确的提示，上述的结果，你会选择相信哪一个？实际上跑分耗时从 2.358 随着迭代次数变大，降为了 0.019。手动测试循环的代码 BadMicrobenchmark 也存在同样的问题，实际上它没有做预热，效果只会比 JMH 测试循环更加不可信。 Aleksey 在视频中给出结论：假设单词迭代的耗时是 𝑀 ns. 在 JIT，OSR，循环展开等因素的多重作用下，多次迭代的耗时理论值为 𝛼𝑀 ns, 其中 𝛼 ∈ [0; +∞)。 正确的测试循环的姿势可以看这里：here 陷阱 4：使用 Fork 隔离多个测试方法相信我，这个陷阱中涉及到的例子绝对是 JMH sample 中最诡异的，并且我还没有找到科学的解释（说实话视频中这一段我尝试听了好几遍，没听懂，原谅我的听力） 首先定义一个 Counter 接口，并实现了两份代码完全相同的实现类：Counter1，Counter2 123456789101112131415161718192021public interface Counter { int inc();}public class Counter1 implements Counter { private int x; @Override public int inc() { return x++; }}public class Counter2 implements Counter { private int x; @Override public int inc() { return x++; }} 接着让他们在 ** 同一个 VM** 中按照先手顺序进行评测： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public int measure(Counter c) { int s = 0; for (int i = 0; i &lt; 10; i++) { s += c.inc(); } return s;}/* * These are two counters. */Counter c1 = new Counter1();Counter c2 = new Counter2();/* * We first measure the Counter1 alone... * Fork(0) helps to run in the same JVM. */@Benchmark@Fork(0)public int measure_1_c1() { return measure(c1);}/* * Then Counter2... */@Benchmark@Fork(0)public int measure_2_c2() { return measure(c1);}/* * Then Counter1 again... */@Benchmark@Fork(0)public int measure_3_c1_again() { return measure(c1);}@Benchmark@Fork(1)public int measure_4_forked_c1() { return measure(c1);}@Benchmark@Fork(1)public int measure_5_forked_c2() { return measure(c2);} 这一个例子中多了一个 Fork 注解，让我来简单介绍下它。Fork 这个关键字顾名思义，是用来将运行环境复制一份的意思，在我们之前的多个测试中，实际上每次测评都是默认使用了 ** 相互隔离的，完全一致 ** 的测评环境，这得益于 JMH。每个试验运行在单独的 JVM 进程中。也可以指定 (额外的) JVM 参数，例如这里为了演示运行在同一个 JVM 中的弊端，特地做了反面的教材：Fork(0)。试想一下 c1，c2，c1 again 的耗时结果会如何？ 123456Benchmark Mode Cnt Score Error UnitsJMHSample_12_Forking.measure_1_c1 avgt 5 2.518 ± 0.622 ns/opJMHSample_12_Forking.measure_2_c2 avgt 5 14.080 ± 0.283 ns/opJMHSample_12_Forking.measure_3_c1_again avgt 5 13.462 ± 0.164 ns/opJMHSample_12_Forking.measure_4_forked_c1 avgt 5 3.861 ± 0.712 ns/opJMHSample_12_Forking.measure_5_forked_c2 avgt 5 3.574 ± 0.220 ns/op 你会不会感到惊讶，第一次运行的 c1 竟然耗时最低，在我的认知中，JIT 起码会启动预热的作用，无论如何都不可能先运行的方法比之后的方法快这么多！但这个结果也和 Aleksey 视频中介绍的相符。 JMH samples 中的这个示例主要还是想要表达同一个 JVM 中运行的测评代码会互相影响，从结果也可以发现：c1,c2,c1_again 的实现相同，跑分却不同，因为运行在同一个 JVM 中；而 forked_c1 和 forked_c2 则表现出了一致的性能。所以没有特殊原因，Fork 的值一般都需要设置为 &gt;0。 陷阱 5：方法内联熟悉 C/C++ 的朋友不会对方法内联感到陌生，方法内联就是把目标方法的代码“复制”到发起调用的方法之中，避免发生真实的方法调用（减少了操作指令周期）。在 Java 中，无法手动编写内联方法，但 JVM 会自动识别热点方法，并对它们使用方法内联优化。一段代码需要执行多少次才会触发 JIT 优化通常这个值由 -XX:CompileThreshold 参数进行设置： 1、使用 client 编译器时，默认为 1500； 2、使用 server 编译器时，默认为 10000； 但是一个方法就算被 JVM 标注成为热点方法，JVM 仍然不一定会对它做方法内联优化。其中有个比较常见的原因就是这个方法体太大了，分为两种情况。 如果方法是经常执行的，默认情况下，方法大小小于 325 字节的都会进行内联（可以通过 -XX:MaxFreqInlineSize=N 来设置这个大小） 如果方法不是经常执行的，默认情况下，方法大小小于 35 字节才会进行内联（可以通过 -XX:MaxInlineSize=N 来设置这个大小） 我们可以通过增加这个大小，以便更多的方法可以进行内联；但是除非能够显著提升性能，否则不推荐修改这个参数。因为更大的方法体会导致代码内存占用更多，更少的热点方法会被缓存，最终的效果不一定好。 如果想要知道方法被内联的情况，可以使用下面的 JVM 参数来配置 123-XX:+PrintCompilation // 在控制台打印编译过程信息-XX:+UnlockDiagnosticVMOptions // 解锁对 JVM 进行诊断的选项参数。默认是关闭的，开启后支持一些特定参数对 JVM 进行诊断-XX:+PrintInlining // 将内联方法打印出来 ** 方法内联的其他隐含条件 ** 虽然 JIT 号称可以针对代码全局的运行情况而优化，但是 JIT 对一个方法内联之后，还是可能因为方法被继承，导致需要类型检查而没有达到性能的效果 想要对热点的方法使用上内联的优化方法，最好尽量使用 final、private、static 这些修饰符修饰方法，避免方法因为继承，导致需要额外的类型检查，而出现效果不好情况。 方法内联也可能对 Benchmark 产生影响；或者说有时候我们为了优化代码，而故意触发内联，也可以通过 JMH 来和非内联方法进行性能对比: 12345678910111213public void target_blank() { // this method was intentionally left blank}@CompilerControl(CompilerControl.Mode.DONT_INLINE)public void target_dontInline() { // this method was intentionally left blank}@CompilerControl(CompilerControl.Mode.INLINE)public void target_inline() { // this method was intentionally left blank} 1234Benchmark Mode Cnt Score Error UnitsJMHSample_16_CompilerControl.blank avgt 3 0.323 ± 0.544 ns/opJMHSample_16_CompilerControl.dontinline avgt 3 2.099 ± 7.515 ns/opJMHSample_16_CompilerControl.inline avgt 3 0.308 ± 0.264 ns/op 可以发现，内联与不内联的性能差距是巨大的，有一些空间换时间的味道，在 JMH 中使用 CompilerControl.Mode 来控制内联是否开启。 陷阱 6：伪共享与缓存行又遇到了我们的老朋友：CPU Cache 和缓存行填充。这个并发性能杀手，我在之前的文章中专门介绍过，如果你没有看过，可以戳这里：JAVA 拾遗 — CPU Cache 与缓存行。在 Benchmark 中，有时也不能忽视缓存行对测评的影响。 受限于篇幅，在此不展开有关伪共享的陷阱，完整的测评可以戳这里：JMHSample_22_FalseSharing JMH 为解决伪共享问题，提供了 @State 注解，但并不能在单一对象内部对个别的字段增加，如果有必要，可以使用并发包中的 @Contended 注解来处理。 Aleksey 曾为 Java 并发包提供过优化，其中就包括 @Contended 注解。 陷阱 7：分支预测分支预测（Branch Prediction）是这篇文章中介绍的最后一个 Benchmark 中的“捣蛋鬼”。还是从一个具体的 Benchmark 中观察结果。下面的代码尝试遍历了两个长度相等的数组，一个有序，一个无序，并在迭代时加入了一个判断语句，这是分支预测的关键：if(v &gt; 0) 1234567891011121314151617181920212223242526272829303132333435363738private static final int COUNT = 1024 * 1024;private byte[] sorted;private byte[] unsorted;@Setuppublic void setup() { sorted = new byte[COUNT]; unsorted = new byte[COUNT]; Random random = new Random(1234); random.nextBytes(sorted); random.nextBytes(unsorted); Arrays.sort(sorted);}@Benchmark@OperationsPerInvocation(COUNT)public void sorted(Blackhole bh1, Blackhole bh2) { for (byte v : sorted) { if (v &gt; 0) { // 关键 bh1.consume(v); } else { bh2.consume(v); } }}@Benchmark@OperationsPerInvocation(COUNT)public void unsorted(Blackhole bh1, Blackhole bh2) { for (byte v : unsorted) { if (v &gt; 0) { // 关键 bh1.consume(v); } else { bh2.consume(v); } }} 123Benchmark Mode Cnt Score Error UnitsJMHSample_36_BranchPrediction.sorted avgt 25 2.752 ± 0.154 ns/opJMHSample_36_BranchPrediction.unsorted avgt 25 8.175 ± 0.883 ns/op 从结果看，有序数组的遍历比无序数组的遍历快了 2-3 倍。关于这点的介绍，最佳的解释来自于 Stack Overflow 一个 2w 多赞的答案：Why is it faster to process a sorted array than an unsorted array? 假设我们是在 19 世纪，而你负责为火车选择一个方向，那时连电话和手机还没有普及，当火车开来时，你不知道火车往哪个方向开。于是你的做法（算法）是：叫停火车，此时火车停下来，你去问司机，然后你确定了火车往哪个方向开，并把铁轨扳到了对应的轨道。 还有一个需要注意的地方是，火车的惯性是非常大的，所以司机必须在很远的地方就开始减速。当你把铁轨扳正确方向后，火车从启动到加速又要经过很长的时间。 那么是否有更好的方式可以减少火车的等待时间呢？ 有一个非常简单的方式，你提前把轨道扳到某一个方向。那么到底要扳到哪个方向呢，你使用的手段是——“瞎蒙”： 如果蒙对了，火车直接通过，耗时为 0。 如果蒙错了，火车停止，然后倒回去，你将铁轨扳至反方向，火车重新启动，加速，行驶。 如果你很幸运，每次都蒙对了，火车将从不停车，一直前行！如果不幸你蒙错了，那么将浪费很长的时间。 虽然不严谨，但你可以用同样的道理去揣测 CPU 的分支预测，有序数组使得这样的预测大部分情况下是正确的，所以带有判断条件时，有序数组的遍历要比无序数组要快。 这同时也启发我们：在大规模循环逻辑中要尽量避免大量判断（是不是可以抽取到循环外呢？）。 陷阱 8：多线程测试 在 4 核的系统之上运行一个测试方法，得到如上的测试结果， Ops/nsec 代表了单位时间内的运行次数，Scale 代表 2，4 线程相比 1 线程的运行次数倍率。 这个图可供我们提出两个问题： 为什么 2 线程 -&gt; 4 线程几乎没有变化？ 为什么 2 线程相比 1 线程只有 1.87 倍的变化，而不是 2 倍？ **1 电源管理 ** 第一个影响因素便是多线程测试会受到操作系统电源管理（Power Management）的影响，许多系统存在能耗和性能的优化管理。 (Ex: cpufreq, SpeedStep, Cool&amp;Quiet, TurboBoost) 当我们主动对机器进行降频之后，整体性能发生下降，但是 Scale 在线程数 1 -&gt; 2 的过程中变成了严谨的 2 倍。 这样的问题并非无法规避，补救方法便是禁用电源管理, 保证 CPU 的时钟频率 。 JMH 通过长时间运行，保证线程不出现 park(time waiting) 状态，来保证测试的精准性。 **2 操作系统调度和分时调用模型 ** 造成多线程测试陷阱的第二个问题，需要从线程调度模型出发来理解：分时调度模型和抢占式调度模型。 分时调度模型是指让所有的线程轮流获得 CPU 的使用权, 并且平均分配每个线程占用的 CPU 的时间片，这个也比较好理解；抢占式调度模型，是指优先让可运行池中优先级高的线程占用 CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用 CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。一个线程会因为以下原因而放弃 CPU。 需要注意的是，线程的调度不是跨平台的，它不仅仅取决于 Java 虚拟机，还依赖于操作系统。在某些操作系统中，只要运行中的线程没有遇到阻塞，就不会放弃 CPU；在某些操作系统中，即使线程没有遇到阻塞，也会运行一段时间后放弃 CPU，给其它线程运行的机会。 无论是那种模型，线程上下文的切换都会造成损耗。到这儿为止，还是只回答了第一个问题：为什么 2 线程相比 1 线程只有 1.87 倍的变化，而不是 2 倍？ 由于上述的两个图我都是从 Aleksey 的视频中抠出来的，并不清楚他的实际测试用例，对于 2 -&gt; 4 线程性能差距并不大只能理解为系统过载，按道理说 4 核的机器，运行 4 个线程应该不至于只比 2 个线程快这么一点。 对于线程分时调用以及线程调度带来的不稳定性，JMH 引入了 bogus iterations 的概念，它保障了在多线程测试过程中，只在线程处于忙碌状态的过程中进行测量。 bogus iterations 这个值得一提，我理解为“伪迭代”，并且也只在 JVM 的注释以及 Aleksey 的几个博客中有介绍，可以理解为 JMH 的内部原理的专用词。 总结本文花了大量的篇幅介绍了 JMH 存在的意义，以及 JMH sample 中提到的诸多陷阱，这些陷阱会非常容易地被那些不规范的测评程序所触发。我觉得作为 Java 语言的使用者，起码有必要了解这些现象的存在，毕竟 JMH 已经帮你解决了诸多问题了，你不用担心预热问题，不用自己写比较 low 的循环去评测，规避这些测试陷阱也变得相对容易。 实际上，本文设计的知识点，仅仅是 Aleksey 博客中的内容、 JMH 的 38 个 sample 的冰山一角，有兴趣的朋友可以戳这里查看所有的 JMH sample 陷阱内心 os：像我这么 diao 的陷阱，还有 30 个！ 例如 Kafka 这样优秀的开源框架，提供了专门的 module 来做 JMH 的基础测试。尝试使用 JMH 作为你的 Benchmark 工具吧。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/java-jmh/"},{"title":"JAVA 程序员分级，你属于哪一种？","text":"初级 — 初 掌握 java 基础，熟悉常用类库。理解 java web 中的 servlet，jsp，并了解常用的框架对 java web 的封装原理，能够借助框架完成增删改查功能。理解数据库在 web 开发中的地位。 初级 — 中 理解 java 中较为高级的特性，如反射，动态代理，JVM，内存模型，多线程等等。熟练使用框架，对框架中遇到的 bug，能够借助日志和搜索引擎分析出问题的原因。在团队中，能够独立完成普通后台业务功能的开发。了解数据库的高级特性，如索引，存储引擎等等。 初级 — 高 理解 java 分布式架构，微服务架构，了解其与集中式架构的区别，并能保证分布式代码质量。熟练使用各个中间件如 redis，mq，zookeeper 等等，并了解其工作原理和使用场景。能够在中级或高级程序员的带领之下，完成非核心功能的研发。能够关注开源，并且具有阅读源码的能力。 中级 具备一定的项目开发经验（3 年之上一线互联网产品研发经验），拥有线上 bug 的处理能力，JVM 调优能力，以及完成核心业务功能的开发。并且带领团队的新人，能够按能力分配任务。 高级 团队的核心人物，把控整个项目的质量，包括代码漏洞和规范问题。具有 5 年以上项目开发经验，2 年以上架构搭建的经验，能够根据业务选择不同的架构类型；根据团队组成，分配不同的任务。具有将自己的知识分享出去的能力，带领初级程序员走向中级，中级程序员走向高级的能力。","link":"/java-level/"},{"title":"Java 随机数探秘","text":"本文的前 3 节参考修改自微信公众号「咖啡拿铁」的文章，感谢李钊同学对这个话题热情的讨论。 1 前言一提到 Java 中的随机数，很多人就会想到 Random，当出现生成随机数这样需求时，大多数人都会选择使用 Random 来生成随机数。Random 类是线程安全的，但其内部使用 CAS 来保证线程安全性，在多线程并发的情况下的时候它的表现是存在优化空间的。在 JDK1.7 之后，Java 提供了更好的解决方案 ThreadLocalRandom，接下来，我们一起探讨下这几个随机数生成器的实现到底有何不同。 2 Random Random 这个类是 JDK 提供的用来生成随机数的一个类，这个类并不是真正的随机，而是伪随机，伪随机的意思是生成的随机数其实是有一定规律的，而这个规律出现的周期随着伪随机算法的优劣而不同，一般来说周期比较长，但是可以预测。通过下面的代码我们可以对 Random 进行简单的使用: Random 原理Random 中的方法比较多，这里就针对比较常见的 nextInt()和 nextInt(int bound) 方法进行分析，前者会计算出 int 范围内的随机数，后者如果我们传入 10，那么他会求出 [0,10) 之间的 int 类型的随机数，左闭右开。我们首先看一下 Random() 的构造方法: 可以发现在构造方法当中，根据当前时间的种子生成了一个 AtomicLong 类型的 seed，这也是我们后续的关键所在。 nextInt()nextInt() 的代码如下所示： 这个里面直接调用的是 next() 方法，传入的 32，代指的是 Int 类型的位数。 这里会根据 seed 当前的值，通过一定的规则 (伪随机算法) 算出下一个 seed，然后进行 CAS，如果 CAS 失败则继续循环上面的操作。最后根据我们需要的 bit 位数来进行返回。核心便是 CAS 算法。 nextInt(int bound)nextInt(int bound) 的代码如下所示： 这个流程比 nextInt() 多了几步，具体步骤如下: 首先获取 31 位的随机数，注意这里是 31 位，和上面 32 位不同，因为在 nextInt()方法中可以获取到随机数可能是负数，而 nextInt(int bound) 规定只能获取到 [0,bound) 之前的随机数，也就意味着必须是正数，预留一位符号位，所以只获取了 31 位。(不要想着使用取绝对值这样操作，会导致性能下降) 然后进行取 bound 操作。 如果 bound 是 2 的幂次方，可以直接将第一步获取的值乘以 bound 然后右移 31 位，解释一下: 如果 bound 是 4，那么乘以 4 其实就是左移 2 位，其实就是变成了 33 位，再右移 31 位的话，就又会变成 2 位，最后，2 位 int 的范围其实就是 [0,4) 了。 如果不是 2 的幂，通过模运算进行处理。 并发瓶颈在我之前的文章中就有相关的介绍，一般而言，CAS 相比加锁有一定的优势，但并不一定意味着高效。一个立刻被想到的解决方案是每次使用 Random 时都去 new 一个新的线程私有化的 Random 对象，或者使用 ThreadLocal 来维护线程私有化对象，但除此之外还存在更高效的方案，下面便来介绍本文的主角 ThreadLocalRandom。 3 ThreadLocalRandom在 JDK1.7 之后提供了新的类 ThreadLocalRandom 用来在并发场景下代替 Random。使用方法比较简单: 12ThreadLocalRandom.current().nextInt();ThreadLocalRandom.current().nextInt(10); 在 current 方法中有: 可以看见如果没有初始化会对其进行初始化，而这里我们的 seed 不再是一个全局变量，在我们的 Thread 中有三个变量: threadLocalRandomSeed：ThreadLocalRandom 使用它来控制随机数种子。 threadLocalRandomProbe：ThreadLocalRandom 使用它来控制初始化。 threadLocalRandomSecondarySeed：二级种子。 可以看见所有的变量都加了 @sun.misc.Contended 这个注解，用来处理伪共享问题。 在 nextInt() 方法当中代码如下: 我们的关键代码如下: 1UNSAFE.putLong(t = Thread.currentThread(), SEED,r=UNSAFE.getLong(t, SEED) + GAMMA); 可以看见由于我们每个线程各自都维护了种子，这个时候并不需要 CAS，直接进行 put，在这里利用线程之间隔离，减少了并发冲突；相比较 ThreadLocal&lt;Random&gt;，ThreadLocalRandom 不仅仅减少了对象维护的成本，其内部实现也更轻量级。所以 ThreadLocalRandom 性能很高。 4 性能测试除了文章中详细介绍的 Random，ThreadLocalRandom，我还将 netty4 实现的 ThreadLocalRandom，以及 ThreadLocal&lt;Random&gt; 作为参考对象，一起参与 JMH 测评。 123456789101112131415161718192021222324252627282930313233343536373839404142@BenchmarkMode({Mode.AverageTime})@OutputTimeUnit(TimeUnit.NANOSECONDS)@Warmup(iterations = 3, time = 5)@Measurement(iterations = 3, time = 5)@Threads(50)@Fork(1)@State(Scope.Benchmark)public class RandomBenchmark { Random random = new Random(); ThreadLocal&lt;Random&gt; threadLocalRandomHolder = ThreadLocal.withInitial(Random::new); @Benchmark public int random() { return random.nextInt(); } @Benchmark public int threadLocalRandom() { return ThreadLocalRandom.current().nextInt(); } @Benchmark public int threadLocalRandomHolder() { return threadLocalRandomHolder.get().nextInt(); } @Benchmark public int nettyThreadLocalRandom() { return io.netty.util.internal.ThreadLocalRandom.current().nextInt(); } public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(RandomBenchmark.class.getSimpleName()) .build(); new Runner(opt).run(); }} 测评结果如下： 12345Benchmark Mode Cnt Score Error UnitsRandomBenchmark.nettyThreadLocalRandom avgt 3 192.202 ± 295.897 ns/opRandomBenchmark.random avgt 3 3197.620 ± 380.981 ns/opRandomBenchmark.threadLocalRandom avgt 3 90.731 ± 39.098 ns/opRandomBenchmark.threadLocalRandomHolder avgt 3 229.502 ± 267.144 ns/op 从上图可以发现，JDK1.7 的 ThreadLocalRandom 取得了最好的成绩，仅仅需要 90 ns 就可以生成一次随机数，netty 实现的 ThreadLocalRandom 以及使用 ThreadLocal 维护 Random 的方式差距不是很大，位列 2、3 位，共享的 Random 变量则效果最差。 可见，在并发场景下，ThreadLocalRandom 可以明显的提升性能。 5 注意点注意，ThreadLocalRandom 切记不要调用 current 方法之后，作为共享变量使用 123456789public class WrongCase { ThreadLocalRandom threadLocalRandom = ThreadLocalRandom.current(); public int concurrentNextInt(){ return threadLocalRandom.nextInt(); } } 这是因为 ThreadLocalRandom.current() 会使用初始化它的线程来填充随机种子，这会带来导致多个线程使用相同的 seed。 1234567891011121314public class Main { public static void main(String[] args) { ThreadLocalRandom threadLocalRandom = ThreadLocalRandom.current(); for(int i=0;i&lt;10;i++) new Thread(new Runnable() { @Override public void run() { System.out.println(threadLocalRandom.nextInt()); } }).start(); }} 输出相同的随机数： 12345678910-1667209487-1667209487-1667209487-1667209487-1667209487-1667209487-1667209487-1667209487-1667209487-1667209487 请在确保不同线程获取不同的 seed，最简单的方式便是每次调用都是使用 current()： 12345public class RightCase { public int concurrentNextInt(){ return ThreadLocalRandom.current().nextInt(); }} 彩蛋 1梁飞博客中一句话常常在我脑海中萦绕：魔鬼在细节中。优秀的代码都是一个个小细节堆砌出来，今天介绍的 ThreadLocalRandom 也不例外。 在 incubator-dubbo-2.7.0 中，随机负载均衡器的一个小改动便是将 Random 替换为了 ThreadLocalRandom，用于优化并发性能。 彩蛋 2ThreadLocalRandom 的 nextInt(int bound) 方法中，当 bound 不为 2 的幂次方时，使用了一个循环来修改 r 的值，我认为这可能不必要，你觉得呢？ 123456789101112131415public int nextInt(int bound) { if (bound &lt;= 0) throw new IllegalArgumentException(BadBound); int r = mix32(nextSeed()); int m = bound - 1; if ((bound &amp; m) == 0) // power of two r &amp;= m; else { // reject over-represented candidates for (int u = r &gt;&gt;&gt; 1; u + m - (r = u % bound) &lt; 0; u = mix32(nextSeed()) &gt;&gt;&gt; 1) ; } return r;} ** 欢迎关注李钊同学的微信公众号：「咖啡拿铁」** ** 当然，也欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/java-random/"},{"title":"java 小技巧 (一)-- 远程 debug","text":"该系列介绍一些 java 开发中常用的一些小技巧，多小呢，从不会到会只需要一篇文章这么小。这一篇介绍如何使用 jdk 自带的扩展包配合 Intellij IDEA 实现远程 debug。 项目中经常会有出现这样的问题，会令程序员抓狂：关键代码段没有打印日志，本地环境正常生产环境却又问题… 这时候，远程 debug 可能会启动作用。 1 准备用于 debug 的代码准备一个 RestController 用于接收请求，最后可以通过本地断点验证是否成功开启了远程 debug 123456789101112131415@RestControllerpublic class TestController { @RequestMapping(&quot;/test&quot;) public Integer test() { int i = 0; i++; i++; i++; i++; i++; return i; }} 项目使用 springboot 和 maven 构建，依赖就省略了，使用 springboot 提供的 maven 打包插件，方便我们打包成可运行的 jar。 123456789101112131415161718&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;executable&gt;true&lt;/executable&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 2 使用 maven 插件打包成 jar 3 准备启动脚本1java -jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=64057 remote-debug-1.0-SNAPSHOT.jar 使用 java -jar 的方式启动程序，并且添加了一串特殊的参数，这是我们能够开启远程 debug 的关键，以 - 开头的参数是 jvm 的标准启动参数，关于 jvm 启动参数相关的知识可以先去其他博客了解。 -agentlib:libname[=options], 用于装载本地 lib 包。在这条指令中便是加载了 jdwp(Java Debug Wire Protocol) 这个用于远程调试 java 的扩展包。而 transport=dt_socket,server=y,suspend=n,address=64057 这些便是 jdwp 装载时的定制参数，详细的参数作用可以搜索 jdwp 进行了解。我们需要关心的只有 address=64057 这个参数选项，本地调试程序使用 64057 端口与其通信，从而远程调试。 4 配置 IDEA 与脚本中的指令完全一致 远程 jar 包运行的 host，由于我的 jar 运行在本地，所以使用的是 localhost，一般线上环境自然是修改为线上的地址 与远程 jar 包进行交互的端口号，idea 会根据指令自动帮我们输入 选择与远程 jar 包一致的本地代码 ** 请务必保证远程 jar 包的代码与本地代码一致！！！** 5 验证保存第 4 步的配置后，先执行脚本让远程的 jar 包跑起来，再在 IDEA 中运行 remote-debug 如上便代表连接运行成功了 在本地打上断点，访问 localhost:8080/test 可以在本地看到堆栈信息，大功告成。一行指令便完成了远程调试。","link":"/java-skill-1/"},{"title":"java trick--String.intern()","text":"《深入理解 java 虚拟机》第二版中对 String.intern() 方法的讲解中所举的例子非常有意思 不了解 String.intern() 的朋友要理解他其实也很容易，它返回的是一个字符串在字符串常亮池中的引用。直接看下面的 demo 1234567891011public class Main { public static void main(String[] args) { String str1 = new StringBuilder(&quot;计算机&quot;).append(&quot;软件&quot;).toString(); System.out.println(str1.intern() == str1);&lt;!-- more --&gt; String str2 = new StringBuilder(&quot;ja&quot;).append(&quot;va&quot;).toString(); System.out.println(str2.intern() == str2); }} 两者输出的结果如下： 12truefalse 我用的 jdk 版本为 Oracle JDK7u45。简单来说，就是一个很奇怪的现象，为什么 java 这个字符串在类加载之前就已经加载到常量池了？ 我在知乎找到了具体的说明，如下： 1234567891011package sun.misc;import java.io.PrintStream;public class Version { private static final String launcher_name = &quot;java&quot;; private static final String java_version = &quot;1.7.0_79&quot;; private static final String java_runtime_name = &quot;Java(TM) SE Runtime Environment&quot;; private static final String java_runtime_version = &quot;1.7.0_79-b15&quot;; ...} 而 HotSpot JVM 的实现会在类加载时先调用： 123456789public final class System{ ... private static void initializeSystemClass() { ... sun.misc.Version.init(); ... } ...} 原来是 sun.misc.Version 这个类在起作用。","link":"/java-trick-String.intern()/"},{"title":"java trick -- intergerCache","text":"看一段代码： 1234567public class Main { public static void main(String[] args) { Integer a=100,b=100,c=150,d=150; System.out.println(a==b); System.out.println(c==d); }} 这段代码会输出什么？ 不加留意的人可能会理所当然的认为两个答案会是一致的，但结果却是： 12truefalse 下面一个很好解释，因为自动拆装箱机制，比较的是两者的引用，而不是值，所以为 false，那么为什么前者是同一个引用呢？ 来看看 Integer 这个类，首先是自动拆装箱会调用 valueOf() 方法 123456public static Integer valueOf(int i) { assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } 这里并不是简单的返回 new Integer(i) 而是判断了一下 int 的数值，Integer 的存在一个缓存机制，默认用一个 IntegerCache 缓存了 [IntegerCache.low,IntegerCache.high] 的引用, 其中 IntegerCache 这个内部类真正在做缓存 1234567891011121314151617181920212223242526private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue != null) { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); } private IntegerCache(){} } 所以就出现了最开始的一个小 trick","link":"/java-trick-intergerCache/"},{"title":"java trick--system.out.println","text":"多线程在使用 system.out.println 时要留一个有意思的地方 123456789101112131415161718192021public class Main { public static void main(String[] args) { Thread thread = new MyThread(); thread.start(); System.out.println(&quot;end&quot;); }}class MyThread extends Thread { private int i = 0; @Override public void run() { while (true) { i++; System.out.println(i); } }} 主线程另起一个线程，然后在主线程最后打印一个 end，猜猜看结果是什么？end 会不会打印？主线程一直被 Mythread 占用原因就在于 system.out.println 是一个同步方法 12345678910111213/** * Prints an integer and then terminate the line. This method behaves as * though it invokes &lt;code&gt;{@link #print(int)}&lt;/code&gt; and then * &lt;code&gt;{@link #println()}&lt;/code&gt;. * * @param x The &lt;code&gt;int&lt;/code&gt; to be printed. */ public void println(int x) { synchronized (this) { print(x); newLine(); } }","link":"/java-trick-system.out.println/"},{"title":"XML 与 javabean 的转换","text":"XML 可以说是一种被时代淘汰的数据传输格式，毕竟相比较 JSON，其语法，表现形式，以及第三方类库的支持，都要略逊一筹，但最近在对接一些老接口时，主要还是以 XML 为主，而翻阅相关的文档以及博客，没看到很好的文章介绍如何使用 xml 进行数据传输，所以简单写下此文，做一下记录。内心多多少少还是会抵制对接如此老旧的接口，不过生活还是要继续。 Code First先上一段代码，展示一下如何封装，讲解放到后面 一个典型的对接方提供的 XML 如下： 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;&lt;ORDER&gt; &lt;ORDER_NO&gt;10086&lt;/ORDER_NO&gt; &lt;TOTAL_PRICE&gt;3.14&lt;/TOTAL_PRICE&gt; &lt;CREATE_TIME&gt;2017-08-26 03:39:30&lt;/CREATE_TIME&gt; &lt;ORDER_ITEMS&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt; 德芙 &lt;/GOODS_NAME&gt; &lt;NUM&gt;3&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt; 旺仔 &lt;/GOODS_NAME&gt; &lt;NUM&gt;10&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;/ORDER_ITEMS&gt;&lt;/ORDER&gt; 而我们要对应的实体类，则应当如下： 12345678910111213141516171819@XmlRootElement(name = &quot;ORDER&quot;)// &lt;1&gt;@XmlAccessorType(XmlAccessType.FIELD)// &lt;1&gt;public class Order { @XmlElement(name = &quot;ORDER_NO&quot;)// &lt;1&gt; private String orderNo; @XmlElement(name = &quot;TOTAL_PRICE&quot;) private BigDecimal totalPrice; @XmlElement(name = &quot;CREATE_TIME&quot;) @XmlJavaTypeAdapter(DateAdapter.class) // &lt;2&gt; private Date createTime; @XmlElementWrapper(name = &quot;ORDER_ITEMS&quot;) // &lt;3&gt; @XmlElement(name = &quot;ORDER_ITEM&quot;) private List&lt;OrderItem&gt; orderItems;} 12345678910@XmlAccessorType(XmlAccessType.FIELD)public class OrderItem { @XmlElement(name = &quot;GOODS_NAME&quot;) private String goodsName; @XmlElement(name = &quot;NUM&quot;) private Integer num;} 我举的这个示例基本包含一般情况下所有可能出现的需求 &lt;1&gt; 常用注解 XmlRootElement，XmlAccessorType，XmlElement &lt;2&gt; 日期转换的适配器注解 &lt;3&gt; 如何在 XML 中设置集合 在介绍这三点之前，先给出转换的工具类 转换工具类1234567891011121314151617181920212223242526272829public class XML { public static String toXmlString(Object obj) { String result; try { JAXBContext context = JAXBContext.newInstance(obj.getClass()); Marshaller marshaller = context.createMarshaller(); StringWriter writer = new StringWriter(); marshaller.marshal(obj, writer); result = writer.toString(); } catch (Exception e) { throw new RuntimeException(e); } return result; } public static &lt;T&gt; T parseObject(String input, Class&lt;T&gt; claaz) { Object result; try { JAXBContext context = JAXBContext.newInstance(claaz); Unmarshaller unmarshaller = context.createUnmarshaller(); result = unmarshaller.unmarshal(new StringReader(input)); } catch (Exception e) { throw new RuntimeException(e); } return (T) result; }} JSON 工具类中，笔者习惯于使用 fastjson，所以干脆连同工具类类名命名和方法命名都按照了它的风格，只有两个方法。 注解的介绍给实体类加上注解，再使用工具类，就可以实现实体和 XML 的相互转换了。那么前面提到的三个注意点中的相关注解分别代表了什么含义呢？ @XmlRootElement 作用域：类 代表一个 XML 对象的根节点，常使用 name 属性来可以指定生成 XML 之后的具体名称 @XmlElement 作用域：字段，方法，参数（不常用） 代表一个 XML 对象的普通界点信息，常使用 name 属性来指定生成 XML 之后的具体名称。需要注意与 @XmlAccessorType 搭配使用时，有一些注意点，见下 @XmlAccessorType 作用域：类，包（不常用） 告诉解析器，在解析 XML 时要如何获取类的字段属性，有 4 个枚举类型： 枚举类型 访问方式 XmlAccessType.FIELD 成员变量 XmlAccessType.PROPERTY public getter,setter XmlAccessType.PUBLIC_MEMBER（默认） public getter,setter+public 成员变量 XmlAccessType.NONE 必须显示指定 @XmlElement 我们上述的例子中，使用的方式是在类上配置 @XmlAccessorType(XmlAccessType.FIELD)，基于成员变量访问属性，并且，在每一个成员变量之上都显示指定了 name=xxx；而如果配置 @XmlAccessorType(XmlAccessType.PUBLIC_MEMBER) 即默认配置，则你需要将 @XmlElement 注解写在 getter 方法上, 笔者比较习惯例子中的写法。需要注意点的一点是，如果 @XmlAccessorType 与 @XmlElement 的配置不对应，很容易触发自动的转换方式，会导致某个节点出现两次的异常。 @XmlJavaTypeAdapter 作用域：字段, 方法, 类, 包, 参数（前三者常用） java 内置的 xml 日期转换类不能满足我们的需求（可以动手试试看默认日期的格式是什么），以及遇到自定义的类，需要配置转换器，就可以使用这个注解，@XmlJavaTypeAdapter 注解接收一个自定义的 Adapter，需要继承自 XmlAdapter&lt;ValueType,BoundType&gt; 抽象类，一个常用的日期转化适配器如下： 1234567891011121314151617181920212223public class DateAdapter extends XmlAdapter&lt;String, Date&gt; { static ThreadLocal&lt;DateFormat&gt; sdf ; static { sdf =new ThreadLocal&lt;DateFormat&gt;() { @Override protected DateFormat initialValue() { return new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); } }; } @Override public Date unmarshal(String v) throws Exception { return sdf.get().parse(v); } @Override public String marshal(Date v) throws Exception { return sdf.get().format(v); }} 使用 Adapter 的弊端也很明显，一个适配器只能对应一个日期的格式，在实际开发中我们往往会将日期区分成天维度的日期和秒维度的日期，不能像大多数 JSON 那样拥有灵活的注解，如果有读者有想到好的解决方案，欢迎跟我沟通。涉及到日期格式转化，时刻不要忘记 SimpleDateFormat 线程不安全这一点。 @XmlElementWrapper XML 中表示集合时，在最外层通常会有一个 Xxxs 或者 XxxList 这样的标签，可以通过 @XmlElementWrapper 实现，其中 name 就代表额外添加的包裹信息是什么, 如上文的 OrderItems。 一些其他的转换工具类我们主要任务是实现 XML 字符串和 javabean 之间转换，不是解析 XML，所以 dom4j 一类的类库不用考虑。熟悉 spring 的人会了解到一点，spring 其实已经封装了 xml 转换相关的类，即 org.springframework.oxm.jaxb.Jaxb2Marshaller 这个类，他的顶层接口是 org.springframework.oxm.Marshaller 和 org.springframework.oxm.UnMarshaller。而在 java 规范中，也存在同名的接口：javax.xml.bind.Marshaller,javax.xml.bind.UnMarshaller，这点在使用中需要注意下。笔者的建议是，这种数据格式转换操作，应当尽量引入最少的依赖。所以使用 javax 的类库下的相关方法进行封装。上述的工具类，仅仅只需要引入 javax 包，即可使用了。非常方便、","link":"/javabean-xml/"},{"title":"浅析 java 内存模型（JMM）","text":"并发编程模型的分类在并发编程中，我们需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写 - 读内存中的公共状态来隐式进行通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的 Java 程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。 Java 内存模型的抽象在 java 中，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享（本文使用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local variables），方法定义参数（java 语言规范称之为 formal method parameters）和异常处理器参数（exception handler parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java 线程之间的通信由 Java 内存模型（本文简称为 JMM）控制，JMM 决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读 / 写共享变量的副本。本地内存是 JMM 的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。Java 内存模型的抽象示意图如下： 从上图来看，线程 A 与线程 B 之间如要通信的话，必须要经历下面 2 个步骤： 首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。 下面通过示意图来说明这两个步骤： 如上图所示，本地内存 A 和 B 有主内存中共享变量 x 的副本。假设初始时，这三个内存中的 x 值都为 0。线程 A 在执行时，把更新后的 x 值（假设值为 1）临时存放在自己的本地内存 A 中。当线程 A 和线程 B 需要通信时，线程 A 首先会把自己本地内存中修改后的 x 值刷新到主内存中，此时主内存中的 x 值变为了 1。随后，线程 B 到主内存中去读取线程 A 更新后的 x 值，此时线程 B 的本地内存的 x 值也变为了 1。 从整体来看，这两个步骤实质上是线程 A 在向线程 B 发送消息，而且这个通信过程必须要经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 java 程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读 / 写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从 java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 java 编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel 称之为 memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序与内存屏障指令现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读 / 写操作的执行顺序，不一定与内存实际发生的读 / 写操作顺序一致！为了具体说明，请看下面示例： Processor A Processor B a = 1; //A1x = b; //A2 b = 2; //B1y = a; //B2 初始状态：a = b = 0 处理器允许执行后得到结果：x = y = 0 假设处理器 A 和处理器 B 按程序的顺序并行执行内存访问，最终却可能得到 x = y = 0 的结果。具体的原因如下图所示： 这里处理器 A 和处理器 B 可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到 x = y = 0 的结果。 从内存操作实际发生的顺序来看，直到处理器 A 执行 A3 来刷新自己的写缓存区，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1-&gt;A2，但内存操作实际发生的顺序却是：A2-&gt;A1。此时，处理器 A 的内存操作顺序被重排序了（处理器 B 的情况和处理器 A 一样，这里就不赘述了）。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写 - 读操做重排序。 下面是常见处理器允许的重排序类型的列表： Load-Load Load-Store Store-Store Store-Load 数据依赖 sparc-TSO N N N Y N x86 N N N Y N ia64 Y Y Y Y N PowerPC Y Y Y Y N 上表单元格中的“N”表示处理器不允许两个操作重排序，“Y”表示允许重排序。 从上表我们可以看出：常见的处理器都允许 Store-Load 重排序；常见的处理器都不允许对存在数据依赖的操作做重排序。sparc-TSO 和 x86 拥有相对较强的处理器内存模型，它们仅允许对写 - 读操作做重排序（因为它们都使用了写缓冲区）。 ※注 1：sparc-TSO 是指以 TSO(Total Store Order) 内存模型运行时，sparc 处理器的特性。 ※注 2：上表中的 x86 包括 x64 及 AMD64。 ※注 3：由于 ARM 处理器的内存模型与 PowerPC 处理器的内存模型非常类似，本文将忽略它。 ※注 4：数据依赖性后文会专门说明。 为了保证内存可见性，java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM 把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保 Load1 数据的装载，之前于 Load2 及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保 Store1 数据对其他处理器可见（刷新到内存），之前于 Store2 及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保 Load1 数据装载，之前于 Store2 及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保 Store1 数据对其他处理器变得可见（指刷新到内存），之前于 Load2 及所有后续装载指令的装载。StoreLoad Barriers 会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 StoreLoad Barriers 是一个“全能型”的屏障，它同时具有其他三个屏障的效果。现代的多处理器大都支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。 happens-before从 JDK5 开始，java 使用新的 JSR -133 内存模型（本文除非特别说明，针对的都是 JSR- 133 内存模型）。JSR-133 提出了 happens-before 的概念，通过这个概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的 happens-before 规则如下： 程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile 变量规则：对一个 volatile 域的写，happens- before 于任意后续对这个 volatile 域的读。 传递性：如果 A happens- before B，且 B happens- before C，那么 A happens- before C。 注意，两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens- before 的定义很微妙，后文会具体说明 happens-before 为什么要这么定义。 happens-before 与 JMM 的关系如下图所示： 如上图所示，一个 happens-before 规则通常对应于多个编译器重排序规则和处理器重排序规则。对于 java 程序员来说，happens-before 规则简单易懂，它避免程序员为了理解 JMM 提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。 原文地址","link":"/jmm-learn/"},{"title":"使用 JPA 实现乐观锁","text":"乐观锁的概念就不再赘述了，不了解的朋友请自行百度谷歌之，今天主要说的是在项目中如何使用乐观锁，做成一个小 demo。 持久层使用 jpa 时，默认提供了一个注解 @Version 先看看源码怎么描述这个注解的 1234@Target({METHOD, FIELD})@Retention(RUNTIME)public @interface Version {} 简单来说就是用一个 version 字段来充当乐观锁的作用。先来设计实体类 123456789101112131415161718192021/** * Created by xujingfeng on 2017/1/30. */@Entity@Table(name = &quot;t_student&quot;)public class Student { @Id @GenericGenerator(name = &quot;PKUUID&quot;, strategy = &quot;uuid2&quot;) @GeneratedValue(generator = &quot;PKUUID&quot;) @Column(length = 36) private String id; @Version private int version; private String name; //getter()... //setter()...} Dao 层 12345678910/** * Created by xujingfeng on 2017/1/30. */public interface StudentDao extends JpaRepository&lt;Student,String&gt;{ @Query(&quot;update Student set name=?1 where id=?2&quot;) @Modifying @Transactional int updateNameById(String name,String id);} Controller 层充当单元测试的作用，通过访问一个 requestMapping 来触发我们想要测试的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Created by xujingfeng on 2017/1/30. */@Controllerpublic class StudentController { @Autowired StudentDao studentDao; @RequestMapping(&quot;student.html&quot;) @ResponseBody public String student(){ Student student = new Student(); student.setName(&quot;xujingfeng&quot;); studentDao.save(student); return &quot;student&quot;; } @RequestMapping(&quot;testVersion.html&quot;) @ResponseBody public String testVersion() throws InterruptedException { Student student = studentDao.findOne(&quot;6ed16acc-61df-4a66-add9-d17c88b69755&quot;); student.setName(&quot;xuxuan&quot;); new Thread(new Runnable() { @Override public void run() { studentDao.findOne(&quot;6ed16acc-61df-4a66-add9-d17c88b69755&quot;); student.setName(&quot;xuxuanInThread&quot;); studentDao.save(student); } }).start(); Thread.sleep(1000); studentDao.save(student); return &quot;testVersion&quot;; } @RequestMapping(&quot;updateNameById.html&quot;) @ResponseBody public String updateNameById(){ studentDao.updateNameById(&quot;xuxuan2&quot;,&quot;6ed16acc-61df-4a66-add9-d17c88b69755&quot;); return &quot;updateNameById&quot;; }} 这里面三个方法，主要是我们想用来测试的三个注意点。第一个方法 student.html 我们想看看 springdata 如何对 version 字段进行增长的。就不贴图了，直接给结论，对于添加了 @Version 的注解，我们不需要手动去控制，每一次 save 操作会在原来的基础上 +1，如果初始为 null，则 springdata 自动设置其为 0。第二个方法 testVersion.html 是乐观锁的核心，当多个线程并发访问同一行记录时，添加了 @Version 乐观锁之后，程序会进行怎么样的控制呢？ 1org.hibernate.StaleObjectStateException: Row was updated or deleted by another transaction (or unsaved-value mapping was incorrect) : [com.example.jpa.Student#6ed16acc-61df-4a66-add9-d17c88b69755] 异常信息如上，主线程和新线程获取了同一行记录，并且新线程优先提交了事务，版本号一致，修改成功。等到了主线程再想 save 提交事务时，便得到一个版本号不一致的异常，那么在项目开发中就应该自己捕获这个异常根据业务内容做对应处理，是重试还是放弃 etc… 第三个方法，updateNameById.html 是想强调一下，@Query 中的 update，delete 操作是不会触发 springdata 的相关代理操作的，而是转化为原生 sql 的方式，所以在项目中使用时也要注意这点。 总结乐观锁，用在一些敏感业务数据上，而其本身的修饰：乐观，代表的含义便是相信大多数场景下 version 是一致的。但是从业务角度出发又要保证数据的严格一致性，避免脏读等问题，使用的场景需要斟酌。记得前面一片博文简单介绍了一下行级锁的概念，其实本质上和乐观锁都是想要再数据库层面加锁控制并发，那么什么时候该用乐观锁，行级锁，什么时候得在程序级别加同步锁，又要根据具体的业务场景去判断。找到能够满足自己项目需求的方案，找到性能和可靠性的平衡点，才是一个程序员的价值所在。","link":"/jpa-OptimisticLock/"},{"title":"鱼和熊掌兼得：同时使用 JPA 和 Mybatis","text":"前言JPA 和 Mybatis 的争论由来已久，还记得在 2 年前我就在 spring4all 社区就两者孰优孰劣的话题发表了观点，我当时是力挺 JPA 的，这当然跟自己对 JPA 熟悉程度有关，但也有深层次的原因，便是 JPA 的设计理念契合了领域驱动设计的思想，可以很好地指导我们设计数据库交互接口。这两年工作中，逐渐接触了一些使用 Mybatis 的项目，也对其有了一定新的认知。都说认知是一个螺旋上升的过程，随着经验的累积，人们会轻易推翻过去，到了两年后的今天，我也有了新的观点。本文不是为了告诉你 JPA 和 Mybatis 到底谁更好，而是尝试求同存异，甚至是在项目中同时使用 JPA 和 Mybatis。什么？要同时使用两个 ORM 框架，有这个必要吗？别急着吐槽我，希望看完本文后，你也可以考虑在某些场合下同时使用这两个框架。 ps. 本文讨论的 JPA 特指 spring-data-jpa。 建模12345678910111213@Entity@Table(name = &quot;t_order&quot;)public class Order { @Id private String oid; @Embedded private CustomerVo customer; @OneToMany(cascade = {CascadeType.ALL}, orphanRemoval = true, fetch = FetchType.LAZY, mappedBy = &quot;order&quot;) private List&lt;OrderItem&gt; orderItems;} JPA 最大的特点是 sqlless，如上述的实体定义，便将数据库的表和 Java 中的类型关联起来了，JPA 可以做到根据 @Entity 注解，自动创建表结构；基于这个实体实现的 Repository 接口，又使得 JPA 用户可以很方便地实现数据的 CRUD。所以，使用 JPA 的项目，人们很少会提到”数据库设计“，人们更关心的是领域建模，而不是数据建模。 1234567891011121314151617&lt;generatorConfiguration&gt; &lt;context id=&quot;my&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;&quot; userId=&quot;&quot; password=&quot;&quot;/&gt; &lt;javaModelGenerator targetPackage=&quot;&quot; targetProject=&quot;&quot; /&gt; &lt;sqlMapGenerator targetPackage=&quot;&quot; targetProject=&quot;&quot; /&gt; &lt;javaClientGenerator targetPackage=&quot;moe.cnkirito.demo.mapper&quot; /&gt; &lt;table tableName=&quot;t_order&quot; domainObjectName=&quot;Order&quot; /&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; Mybatis 用户更多使用的是逆向工程，例如 mybatis-generator 插件根据如上的 xml 配置，便可以直接将表结构转译成 mapper 文件和实体文件。 code first 和 table first 从结果来看是没有区别的，差异的是过程，所以设计良好的系统，并不会仅仅因为这个差异而高下立判，但从指导性来看，无疑设计系统时，更应该考虑的是实体和实体，实体和值对象的关联，领域边界的划分，而不是首先着眼于数据库表结构的设计。 建模角度来看，JPA 的领域建模思想更胜一筹。 数据更新聊数据库自然离不开 CRUD，先来看增删改这些数据更新操作，来看看两个框架一般的习惯是什么。 JPA 推崇的数据更新只有一种范式，分成三步： 先 findOne 映射成实体 内存内修改实体 实体整体 save 你可能会反驳我说，@Query 也存在 nativeQuery 和 JPQL 的用法，但这并不是主流用法。JPA 特别强调”整体 save“的思想，这与领域驱动设计所强调的有状态密不可分，即其认为，修改不应该是针对于某一个字段：”update table set a=b where colomonA=xx“ ，而应该反映成实体的变化，save 则代表了实体状态最终的持久化。 先 find 后 save 显然也适用于 Mybatis，而 Mybatis 的灵活性，使得其数据更新方式更加地百花齐放。路人甲可以认为 JPA 墨守成规不懂变通，认为 Mybatis 不羁放纵爱自由；路人乙也可以认为 JPA 格式规范易维护，Mybatis 不成方圆。这点不多加评判，留后人说。 从个人习惯来说，我还是偏爱先 find 后整体 save 这种习惯的，不是说这是 JPA 的专利，Mybatis 不具备；而是 JPA 的强制性，让我有了这个习惯。 数据更新角度来看，JPA 强制使用 find+save，mybatis 也可以做到这一点，胜者：无。 数据查询JPA 提供的查询方式主要分为两种 简单查询：findBy + 属性名 复杂查询：JpaSpecificationExecutor 简单查询在一些简单的业务场景下提供了非常大的便捷性，findBy + 属性名可以自动转译成 sql，试问如果可以少写代码，有谁不愿意呢？ 复杂查询则是 JPA 为了解决复杂的查询场景，提供的解决方案，硬是把数据库的一些聚合函数，连接操作，转换成了 Java 的方法，虽然做到了 sqlless，但写出来的代码又臭又长，也不见得有多么的易读易维护。这算是我最不喜欢 JPA 的一个地方了，但要解决复杂查询，又别无他法。 而 Mybatis 可以执行任意的查询 sql，灵活性是 JPA 比不了的。数据库小白搜索的最多的两个问题： 数据库分页怎么做 条件查询怎么做 Mybatis 都可以轻松的解决。 千万不要否认复杂查询：如聚合查询、Join 查询的场景。令一个 JPA 用户抓狂的最简单方式，就是给他一个复杂查询的 case。 1select a,b,c,sum(a) where a=xx and d=xx group by a,b,c; 来吧，展示。可能 JPA 的确可以完成上述 sql 的转义，但要知道不是所有开发都是 JPA 专家，没人关心你用 JPA 解决了多么复杂的查询语句，更多的人关心地是，能不能下班前把这个复杂查询搞定，早点回家。 在回到复杂数据查询需求本身的来分析下。我们假设需求是合理的，毕竟项目的复杂性难以估计，可能有 1000 个数据查询需求 JPA 都可以很方便的实现，但就是有那么 10 几个复杂查询 JPA hold 不住。这个时候你只能乖乖地去写 sql 了，如果这个时候又出现一个条件查询的场景，出现了 if else 意味着连 @Query 都用不了，完全退化成了 JdbcTemplate 的时代。 那为什么不使用 Mybatis 呢？Mybatis 使用者从来没有纠结过复杂查询，它简直就是为之而生的。 如今很多 Mybatis 的插件，也可以帮助使用者快速的生成基础方法，虽然仍然需要写 sql，但是这对于开发者来说，并不是一件难事。 不要质疑高并发下，JOIN 操作和聚合函数存在的可能性，数据查询场景下，Mybatis 完胜。 性能本质上 ORM 框架并没有性能的区分度，因为最终都是转换成 sql 交给数据库引擎去执行，ORM 层面那层性能损耗几乎可以忽略不计。 但从实际出发，Mybatis 提供给了开发者更高的 sql 自由度，所以在一些需要 sql 调优的场景下会更加灵活。 可维护性前面我们提到 JPA 相比 Mybatis 丧失了 sql 的自由度，凡事必有 trade off，从另一个层面上来看，其提供了高层次的抽象，尝试用统一的模型去解决数据层面的问题。sqlless 同时也屏蔽了数据库的实现，屏蔽了数据库高低版本的兼容性问题，这对可能存在的数据库迁移以及数据库升级提供了很大的便捷性。 同时使用两者其他细节我就不做分析了，相信还有很多点可以拿过来做对比，但我相信主要的点上文都应该有所提及了。进行以上维度的对比并不是我写这篇文章的初衷，更多地是想从实际开发角度出发，为大家使用这两个框架提供一些参考建议。 在大多数场景下，我习惯使用 JPA，例如设计领域对象时，得益于 JPA 的正向模型，我会优先考虑实体和值对象的关联性以及领域上下文的边界，而不用过多关注如何去设计表结构；在增删改和简单查询场景下，JPA 提供的 API 已经是刻在我 DNA 里面的范式了，使用起来非常的舒服。 在复杂查询场景下，例如 包含不存在领域关联的 join 查询 包含多个聚合函数的复杂查询 其他 JPA 较难实现的查询 我会选择使用 Mybatis，有点将 Mybatis 当做数据库视图生成器的意味。坚定不移的 JPA 拥趸者可能会质疑这些场景的存在的真实性，会质疑是不是设计的漏洞，但按照经验来看，哪怕是短期方案，这些场景也是客观存在的，所以听我一言，尝试拥抱一下 Mybatis 吧。 随着各类存储中间件的流行，例如 mongodb、ES，取代了数据库的一部分地位，重新思考下，本质上都是在用专业的工具解决特定场景的问题，最终目的都是为了解放生产力。数据库作为最古老，最基础的存储组件，的确承载了很多它本不应该承受的东西，那又何必让一个工具或者一个框架成为限制我们想象力的沟壑呢？ 两个框架其实都不重，在 springboot 的加持下，引入几行配置就可以实现两者共存了。 我自己在最近的项目中便同时使用了两者，遵循的便是本文前面聊到的这些规范，我也推荐给你，不妨试试。","link":"/jpa-and-mybatis/"},{"title":"JAVA 拾遗--JPA 二三事","text":"记得前几个月，spring4all 社区刚搞过一次技术话题讨论：如何对 JPA 或者 MyBatis 进行技术选型？传送门：http://www.spring4all.com/article/391 由于平时工作接触较多的是 JPA，所以对其更熟悉一些，这一篇文章记录下个人在使用 JPA 时的一些小技巧。补充说明：JPA 是一个规范，本文所提到的 JPA，特指 spring-data-jpa。 tips：阅读本文之前，建议了解值对象和实体这两个概念的区别。 使用 @Embedded 关联一对一的值对象现实世界有很多一对一的关联关系，如人和身份证，订单和购买者…而在 JPA 中表达一对一的关联，通常有三种方式。下面就以订单（Order）和购买者（CustomerVo）为例来介绍这三种方式，这里 CustomerVo 的 Vo 指的是 Value Object。 字段平铺 这可能是最简单的方式了，由于一对一关联的特殊性，完全可以在 Order 类中，使用几个字段记录 CustomerVo的属性。 12345678public class Order { /*其他字段*/ ... /* Customer相关字段 */ private int customerId; private String customerName; private String customerMobile;} 实际上大多数人就是这么做的，甚至都没有意识到这三个字段其实是属于同一个实体类。这种形式优点是很明显的：简单；缺点也是很明显的，这不符合 OO 的原则，且不利于统一检索和维护 CustomerVo 信息。 使用 @OneToOne 1234public class Order { @OneToOne private CustomerVo customerVo;} 这么做的确更“面向对象”了，但代价似乎太大了，我们需要在数据库中额外维护一张 CustomerVo 表，关联越多，代码处理起来就越麻烦，得不偿失。 使用 @Embedded 那有没有能中和上述矛盾的方案呢？引出 @Embedded 这个注解。分析下初始需求，我们发现：CustomerVo 仅仅是作为一个值对象，并不是一个实体（这里牵扯到一些领域驱动设计的知识，值对象的特点是：作为实体对象的修饰，即 CustomerVo 这个整体是 Order 实体的一个属性；不变性，CustomerVo 一旦生成后便不可被修改，除非被整体替换） @Embedded 注解便是内嵌值对象最好的表达形式。 12345@Entitypublic class Order { @Embedded private CustomerVo customerVo;} 123456@Embeddablepublic class CustomerVo { private int customerId; private String customerName; private String customerMobile;} Order 拥有 @Entity 注解，表明其是 DDD 中的实体；而 CustomerVo 拥有 @Embeddable 注解，表明其是 DDD 中的值对象。这也是为什么我一直在表达这样一种观点：JPA 是对 DDD 很好的实践的。 关于实体类的设计技巧，在曹祖鹏老师的 github 中可以看到很成熟的方案，可能会颠覆你对实体类设计的认知：https://github.com/JoeCao/qbike/。 使用 @Convert 关联一对多的值对象说到一对多，第一反应自然是使用 @OneToMany 注解。的确，我自己在项目中也主要使用这个注解来表达一对多的关联，但这里提供另一个思路，来关联一对多的值对象。 以商品和商品组图来举例。 使用 @OneToMany 还是先想想我们原来会怎么做，保存一个 List, 一种方式是这样 1234public class Goods { // 以逗号分隔 private String pictures;} 使用字符串存储，保存成 JSON 数组的形式，或者以逗号分隔都行。 如果图片还要保存顺序，缩略图，那就必须要得使用一对多的关联了。 12345@Entitypublic class Goods { @OneToMany private List&lt;GoodsPicture&gt; goodsPictures;} 123456@Entitypublic class GoodsPicture { private String path; private Integer index; private String thumbnail;} 我们应当发现这样的劣势是什么，从设计的角度来看：我们并不想单独为 GoodsPicture 单独建立一张表，正如前面使用 String pictures 来表示 List 一样，这违反了数据库设计的第一范式，但这对于使用者来说非常方便，这是关系型数据库的表达能力有限而进行的妥协 。关于这一点我曾和芋艿，曹大师都进行过讨论，并达成了一致的结论：数据库中可以保存 JSON，使用时在应用层进行转换。 使用 JSON 存储复杂对象 123456789@Entitypublic class Goods { /** * 图片 JSON * {@link GoodsPicture} */ @Column(columnDefinition = &quot;text&quot;) private String goodsPictures;} 使用 @Convert 上述的 String 使得在数据库层面少了一张表，使得 Goods 和 GoodsPictures 的关联更容易维护，但也有缺点：单纯的 String goodsPictures 对于使用者来说毫无含义，必须经过应用层的转换才可以使用。而 JPA 实际上也提供了自定义的转换器来帮我们自动完成这一转换工作，这便到了 @Convert 注解派上用场的时候了。 1 声明 Convert 类 123456@Entitypublic class Goods { @Convert(converter = PicturesWrapperConverter.class) @Column(columnDefinition = &quot;text&quot;) private PicturesWrapper picturesWrapper;} 2 设置转换类 PicturesWrapperConverter 12345678910public class PicturesWrapperConverter implements AttributeConverter&lt;PicturesWrapper, String&gt; { @Override public String convertToDatabaseColumn(PicturesWrapper picturesWrapper) { return JSON.toJSONString(picturesWrapper); } @Override public PicturesWrapper convertToEntityAttribute(String dbData) { return JSON.parseObject(dbData, PicturesWrapper.class); }} PicturesWrapperConverter 实现了 AttributeConverter&lt;X,Y&gt; 接口，它表明了如何将 PicturesWrapper 转换成 String 类型。这样的好处是显而易见的，对于数据库而言，它知道 String 类型如何保存；对于 Goods 的使用者而言，也只关心 PicturesWrapper 的格式，并不关心它如何持久化。 123public class PicturesWrapper { List&lt;GoodsPicture&gt; goodsPictures;} 对于 List 的保存，我暂时只找到了这种方式，借助一个 Wrapper 对象去存储一个 List 对象。没有找到直接持久化 List 的方式，如果可以实现这样的方式，会更好一些： 123456@Entitypublic class Goods { @Convert(converter = SomeConverter.class) @Column(columnDefinition = &quot;text&quot;) List&lt;GoodsPicture&gt; goodsPictures;} 但 converter 无法获取到 List 的泛型参数 GoodsPicture，在实践中没找到方案来解决这一问题，只能退而求其次，使用一个 Wrapper 对象。 与 OneToMany 对比，这样虽然使得维护变得灵活，但也丧失了查找的功能，我们将之保存成了 JSON 的形式，导致其不能作为查询条件被检索。 使用 orphanRemoval 来删除值对象你可能有两个疑问：1 在实际项目中，不是不允许对数据进行物理删除吗？ 2 删除对象还不简单，JPA 自己不是有 delete 方法吗？ 关于第一点，需要区分场景，一般实体不允许做物理删除，而是用标记位做逻辑删除，也有部分不需要追溯历史的实体可以做物理删除，而值对象一般而言是可以做物理删除的，因为它只是属性而已。 第二点就有意思了，delete 不就可以直接删除对象吗，为什么需要介绍 orphanRemoval 呢？ 以活动和礼包这个一对多的关系来举例。 12345678@Entitypublic class Activity { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; @OneToMany(cascade = CascadeType.ALL, orphanRemoval = true, mappedBy = &quot;activity&quot;) private List&lt;GiftPackVo&gt; giftPackVos;} 12345678910@Entitypublic class GiftPackVo { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String name; @ManyToOne @JoinColumn(name = &quot;activity_id&quot;) private Activity activity;} 这是一个再简单不过的一对多关系了，唯一可能觉得陌生的便是这个属性了 orphanRemoval = true 。 如果想要删除某个活动下的某个礼包，在没有 orphanRemoval 之前，你只能这么做： GiftPackVoRepository.delete(GiftPackVo); 但其实这违反了 DDD 中的聚合根模式，GiftPackVo 只是一个值对象，其不具备实体的生命周期，删除一个礼包其实是一个不准确的做法，应当是删除某一个活动下的某一个礼包，对礼包的维护，应当由活动来负责。也就是说：应该借由 Activity 删除 GiftPackVo。使用 orphanRemoval 便可以完成这一操作，它表达这样的含义：内存中的某个 Activity 对象属于持久化态，对 List 的移除操作，将被直接认为是删除操作。 于是删除某个“name = 狗年新春大礼包”的礼包便可以这样完成： 123Activity activity = activityRepository.findOne(1);activity.getGiftPackVos().removeIf(giftPackVo -&gt; &quot;狗年新春大礼包&quot;.equals(giftPackVo.getName()));activityRepository.save(activity); 整个代码中只出现了 activityRepository 这一个仓储接口。 使用 @Version 来实现乐观锁乐观锁一直是保证并发问题的一个有效途径，spring data jpa 对 @Version 进行了实现，我们给需要做乐观锁控制的对象加上一个 @Version 注解即可。 12345@Entitypublic class Activity { @Version private Integer version;} 我们在日常操作 Activity 对象时完全不需要理会 version 这个字段，当做它不存在即可，spring 借助这个字段来做乐观锁控制。每次创建对象时，version 默认值为 0，每次修改时，会检查对象获取时和保存时的 version 是否相差 1，转化为 sql 便是这样的语句：update activity set xx = xx,yy = yy,version= 10 where id = 1 and version = 9; 然后通过返回影响行数来判断是否更新成功。 测试乐观锁 12345678910111213141516@Servicepublic class ActivityService { @Autowired ActivityRepos activityRepos; public void test(){ Activity one = activityRepos.findOne(1); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } one.setName(&quot;xx&quot;+ new Random().nextInt()); activityRepos.save(one); }} 当 test 方法被并发调用时，可能会存在并发问题。控制台打印出了更新信息 123452018-02-14 23:44:25.373 INFO 16256 --- [nio-8080-exec-2] jdbc.sqltiming : update activity set name='xx-1863402614', version=1 where id=1 and version=0 2018-02-14 23:44:25.672 INFO 16256 --- [nio-8080-exec-4] jdbc.sqltiming : update activity set name='xx-1095770865', version=1 where id=1 and version=0 org.hibernate.StaleStateException: Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1 表面上看出现的是 StaleStateException，但实际捕获时，如果你想 catch 该异常，根本没有效果，通过 debug 信息，可以发现，真正的异常其实是 ObjectOptimisticLockingFailureException（以 Mysql 为例，实际可能和数据库方言有关，其他数据库未测试）。 123456789@RequestMapping(&quot;/test&quot;)public void test(){ try{ activityService.test(); }catch (ObjectOptimisticLockingFailureException oolfe){ System.out.println(&quot;捕获到乐观锁并发异常&quot;); oolfe.printStackTrace(); }} 在 Controller 层尝试捕获该异常，控制输出如下： 12捕获到乐观锁并发异常org.springframework.orm.ObjectOptimisticLockingFailureException: Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1; nested exception is org.hibernate.StaleStateException: Batch update returned unexpected row count from update [0]; actual row count: 0; expected: 1 成功捕获到了并发冲突，这一切都是 @Version 帮我们完成的，非常方便，不需要我们通过编码去实现乐观锁。 总结本文简单聊了几个个人感触比较深的 JPA 小技巧，JPA 真的很强大，也很复杂，可能还有不少“隐藏”的特性等待我们挖掘。它不仅仅是一个技术框架，本文的所有内容即使不被使用，也无伤大雅，但在领域驱动设计等软件设计思想的指导下，它完全可以实践的更好。","link":"/jpa-ddd/"},{"title":"八幅漫画理解使用 JSON Web Token 设计单点登录系统","text":"转载自：http://blog.leapoahead.com/2015/09/06/understanding-jwt/ 作者：John Wu 博主前言这篇转载的文章和上一篇《JSON Web Token - 在 Web 应用间安全地传递信息》文章均为转载，是我个人在研究 jwt 时浏览下来发现的两篇质量比较高的文章，所以分享给大家。个人对于 jwt 使用场景的理解，包括微信公众号留言中的提问，我都会在下一篇文章中来聊一聊。实际上使用 jwt 设计单点登录系统存在诸多的问题，很多有经验的工程师比较抵制用 jwt 做会话和所谓的单点登录系统，但不妨碍大家作为一个知识点去学习。 以下是原文 上次在 《JSON Web Token - 在 Web 应用间安全地传递信息》 中我提到了 JSON Web Token 可以用来设计单点登录系统。我尝试用八幅漫画先让大家理解如何设计正常的用户认证系统，然后再延伸到单点登录系统。 如果还没有阅读 《JSON Web Token - 在 Web 应用间安全地传递信息》，我强烈建议你花十分钟阅读它，理解 JWT 的生成过程和原理。 用户认证八步走所谓用户认证（Authentication），就是让用户登录，并且在接下来的一段时间内让用户访问网站时可以使用其账户，而不需要再次登录的机制。 小知识：可别把用户认证和用户授权（Authorization）搞混了。用户授权指的是规定并允许用户使用自己的权限，例如发布帖子、管理站点等。 首先，服务器应用（下面简称“应用”）让用户通过 Web 表单将自己的用户名和密码发送到服务器的接口。这一过程一般是一个 HTTP POST 请求。建议的方式是通过 SSL 加密的传输（https 协议），从而避免敏感信息被嗅探。 接下来，应用和数据库核对用户名和密码。 核对用户名和密码成功后，应用将用户的 id（图中的 user_id）作为 JWT Payload 的一个属性，将其与头部分别进行 Base64 编码拼接后签名，形成一个 JWT。这里的 JWT 就是一个形同 lll.zzz.xxx 的字符串。 应用将 JWT 字符串作为该请求 Cookie 的一部分返回给用户。注意，在这里必须使用 HttpOnly 属性来防止 Cookie 被 JavaScript 读取，从而避免 跨站脚本攻击（XSS 攻击）。 在 Cookie 失效或者被删除前，用户每次访问应用，应用都会接受到含有 jwt 的 Cookie。从而应用就可以将 JWT 从请求中提取出来。 应用通过一系列任务检查 JWT 的有效性。例如，检查签名是否正确；检查 Token 是否过期；检查 Token 的接收方是否是自己（可选）。 应用在确认 JWT 有效之后，JWT 进行 Base64 解码（可能在上一步中已经完成），然后在 Payload 中读取用户的 id 值，也就是 user_id 属性。这里用户的 id 为 1025。 应用从数据库取到 id 为 1025 的用户的信息，加载到内存中，进行 ORM 之类的一系列底层逻辑初始化。 应用根据用户请求进行响应。 和 Session 方式存储 id 的差异Session 方式存储用户 id 的最大弊病在于要占用大量服务器内存，对于较大型应用而言可能还要保存许多的状态。一般而言，大型应用还需要借助一些 KV 数据库和一系列缓存机制来实现 Session 的存储。 而 JWT 方式将用户状态分散到了客户端中，可以明显减轻服务端的内存压力。除了用户 id 之外，还可以存储其他的和用户相关的信息，例如该用户是否是管理员、用户所在的分桶（见 [《你所应该知道的 A/B 测试基础》一文] 等。 虽说 JWT 方式让服务器有一些计算压力（例如加密、编码和解码），但是这些压力相比磁盘 I/O 而言或许是半斤八两。具体是否采用，需要在不同场景下用数据说话。 单点登录Session 方式来存储用户 id，一开始用户的 Session 只会存储在一台服务器上。对于有多个子域名的站点，每个子域名至少会对应一台不同的服务器，例如： www.taobao.com nv.taobao.com nz.taobao.com login.taobao.com 所以如果要实现在 login.taobao.com 登录后，在其他的子域名下依然可以取到 Session，这要求我们在多台服务器上同步 Session。 使用 JWT 的方式则没有这个问题的存在，因为用户的状态已经被传送到了客户端。因此，我们只需要将含有 JWT 的 Cookie 的 domain 设置为顶级域名即可，例如 1Set-Cookie: jwt=lll.zzz.xxx; HttpOnly; max-age=980000; domain=.taobao.com 注意 domain 必须设置为一个点加顶级域名，即 .taobao.com。这样，taobao.com 和 *.taobao.com 就都可以接受到这个 Cookie，并获取 JWT 了。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/jwt-learn-2/"},{"title":"理解 JWT 的使用场景和优劣","text":"经过前面两篇文章《JSON Web Token - 在 Web 应用间安全地传递信息》《八幅漫画理解使用 JSON Web Token 设计单点登录系统》的科普，相信大家应该已经知道了 JWT 协议是什么了。至少看到 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJxaWFubWlJZCI6InFtMTAzNTNzaEQiLCJpc3MiOiJhcHBfcW0xMDM1M3NoRCIsInBsYXRmb3JtIjoiYXBwIn0.cMNwyDTFVYMLL4e7ts50GFHTvlSJLDpePtHXzu7z9j4 这样形如 A.B.C 的字符串时能敏感地认出这是使用了 jwt。发了这两篇文章后，有不少读者在文末留言，表达了对 jwt 使用方式的一些疑惑，以及到底哪些场景适合使用 jwt。我并不是 jwt 方面的专家，和不少读者一样，起初研究时我也存在相同疑惑，甚至在逐渐接触后产生了更大的疑惑，经过这段时间项目中的使用和一些自己思考，把个人的总结整理成此文。 编码，签名，加密这些基础知识简单地介绍下，千万别搞混了三个概念。在 jwt 中恰好同时涉及了这三个概念，笔者用大白话来做下通俗的讲解（非严谨定义，供个人理解） 编码 (encode) 和解码(decode)一般是编码解码是为了方便以字节的方式表示数据，便于存储和网络传输。整个 jwt 串会被置于 http 的 Header 或者 url 中，为了不出现乱码解析错误等意外，编码是有必要的。在 jwt 中以 . 分割的三个部分都经过 base64 编码 (secret 部分是否进行 base64 编码是可选的，header 和 payload 则是必须进行 base64 编码)。注意，编码的一个特点：编码和解码的整个过程是可逆的。得知编码方式后，整个 jwt 串便是明文了，随意找个网站验证下解码后的内容： 所以注意一点，**payload 是一定不能够携带敏感数据如密码等信息的 **。 签名 (signature)签名的目的主要是为了验证我是“我”。jwt 中常用的签名算法是 HS256，可能大多数人对这个签名算法不熟悉，但 md5,sha 这样的签名算法肯定是为人熟知的，签名算法共同的特点是整个过程是不可逆的。由于签名之前的主体内容 (header,payload) 会携带在 jwt 字符串中，所以需要使用带有密钥 (yuè) 的签名算法，密钥是服务器和签发者共享的。header 部分和 payload 部分如果被篡改，由于篡改者不知道密钥是什么，也无法生成新的 signature 部分，服务端也就无法通过，在 jwt 中，消息体是透明的，使用签名可以保证消息不被篡改。 前面转载的文章中，原作者将 HS256 称之为加密算法，不太严谨。 加密 (encryption)加密是将明文信息改变为难以读取的密文内容，使之不可读。只有拥有解密方法的对象，经由解密过程，才能将密文还原为正常可读的内容。加密算法通常按照加密方式的不同分为对称加密 (如 AES) 和非对称加密(如 RSA)。你可能会疑惑：“jwt 中哪儿涉及加密算法了？”，其实 jwt 的 第一部分(header) 中的 alg 参数便可以指定不同的算法来生成第三部分(signature)，大部分支持 jwt 的框架至少都内置 rsa 这种非对称加密方式。这里诞生了第一个疑问 疑问：一提到 rsa，大多数人第一想到的是非对称加密算法，而 jwt 的第三部分明确的英文定义是 signature，这不是矛盾吗？ 划重点！ **rsa 加密 ** 和 **rsa 签名 ** 是两个概念！(吓得我都换行了) 这两个用法很好理解： 既然是加密，自然是不希望别人知道我的消息，只有我自己才能解密，所以 ** 公钥负责加密，私钥负责解密 **。这是大多数的使用场景，使用 rsa 来加密。 既然是签名，自然是希望别人不能冒充我发消息，只有我才能发布签名，所以 ** 私钥负责签名，公钥负责验证 **。 所以，在客户端使用 rsa 算法生成 jwt 串时，是使用私钥来“加密”的，而公钥是公开的，谁都可以解密，内容也无法变更（篡改者无法得知私钥）。 所以，在 jwt 中并没有纯粹的加密过程，而是使加密之虚，行签名之实。 什么场景该适合使用 jwt？来聊聊几个场景，注意，以下的几个场景不是都和 jwt 贴合。 一次性验证 比如用户注册后需要发一封邮件让其激活账户，通常邮件中需要有一个链接，这个链接需要具备以下的特性：能够标识用户，该链接具有时效性（通常只允许几小时之内激活），不能被篡改以激活其他可能的账户…这种场景就和 jwt 的特性非常贴近，jwt 的 payload 中固定的参数：iss 签发者和 exp 过期时间正是为其做准备的。 restful api 的无状态认证 使用 jwt 来做 restful api 的身份认证也是值得推崇的一种使用方案。客户端和服务端共享 secret；过期时间由服务端校验，客户端定时刷新；签名信息不可被修改…spring security oauth jwt 提供了一套完整的 jwt 认证体系，以笔者的经验来看：使用 oauth2 或 jwt 来做 restful api 的认证都没有大问题，oauth2 功能更多，支持的场景更丰富，后者实现简单。 使用 jwt 做单点登录 + 会话管理 (不推荐) 在《八幅漫画理解使用 JSON Web Token 设计单点登录系统》一文中提及了使用 jwt 来完成单点登录，本文接下来的内容主要就是围绕这一点来进行讨论。如果你正在考虑使用 jwt+cookie 代替 session+cookie ，我强力不推荐你这么做。 首先明确一点：使用 jwt 来设计单点登录系统是一个不太严谨的说法。首先 cookie+jwt 的方案前提是非跨域的单点登录 (cookie 无法被自动携带至其他域名)，其次单点登录系统包含了很多技术细节，至少包含了身份认证和会话管理，这还不涉及到权限管理。如果觉得比较抽象，不妨用传统的 session+cookie 单点登录方案来做类比，通常我们可以选择 spring security（身份认证和权限管理的安全框架）和 spring session（session 共享）来构建，而选择用 jwt 设计单点登录系统需要解决很多传统方案中同样存在和本不存在的问题，以下一一详细罗列。 jwt token 泄露了怎么办？前面的文章下有不少人留言提到这个问题，我则认为这不是问题。传统的 session+cookie 方案，如果泄露了 sessionId，别人同样可以盗用你的身份。扬汤止沸不如釜底抽薪，不妨来追根溯源一下，什么场景会导致你的 jwt 泄露。 遵循如下的实践可以尽可能保护你的 jwt 不被泄露：使用 https 加密你的应用，返回 jwt 给客户端时设置 httpOnly=true 并且使用 cookie 而不是 LocalStorage 存储 jwt，这样可以防止 XSS 攻击和 CSRF 攻击（对这两种攻击感兴趣的童鞋可以看下 spring security 中对他们的介绍 CSRF,XSS） 你要是正在使用 jwt 访问一个接口，这个时候你的同事跑过来把你的 jwt 抄走了，这种泄露，恕在下无力 secret 如何设计jwt 唯一存储在服务端的只有一个 secret，个人认为这个 secret 应该设计成和用户相关的，而不是一个所有用户公用的统一值。这样可以有效的避免一些注销和修改密码时遇到的窘境。 注销和修改密码传统的 session+cookie 方案用户点击注销，服务端清空 session 即可，因为状态保存在服务端。但 jwt 的方案就比较难办了，因为 jwt 是无状态的，服务端通过计算来校验有效性。没有存储起来，所以即使客户端删除了 jwt，但是该 jwt 还是在有效期内，只不过处于一个游离状态。分析下痛点：注销变得复杂的原因在于 jwt 的无状态。我提供几个方案，视具体的业务来决定能不能接受。 仅仅清空客户端的 cookie，这样用户访问时就不会携带 jwt，服务端就认为用户需要重新登录。这是一个典型的假注销，对于用户表现出退出的行为，实际上这个时候携带对应的 jwt 依旧可以访问系统。 清空或修改服务端的用户对应的 secret，这样在用户注销后，jwt 本身不变，但是由于 secret 不存在或改变，则无法完成校验。这也是为什么将 secret 设计成和用户相关的原因。 借助第三方存储自己管理 jwt 的状态，可以以 jwt 为 key，实现去 redis 一类的缓存中间件中去校验存在性。方案设计并不难，但是引入 redis 之后，就把无状态的 jwt 硬生生变成了有状态了，违背了 jwt 的初衷。实际上这个方案和 session 都差不多了。 修改密码则略微有些不同，假设号被到了，修改密码（是用户密码，不是 jwt 的 secret）之后，盗号者在原 jwt 有效期之内依旧可以继续访问系统，所以仅仅清空 cookie 自然是不够的，这时，需要强制性的修改 secret。在我的实践中就是这样做的。 续签问题续签问题可以说是我抵制使用 jwt 来代替传统 session 的最大原因，因为 jwt 的设计中我就没有发现它将续签认为是自身的一个特性。传统的 cookie 续签方案一般都是框架自带的，session 有效期 30 分钟，30 分钟内如果有访问，session 有效期被刷新至 30 分钟。而 jwt 本身的 payload 之中也有一个 exp 过期时间参数，来代表一个 jwt 的时效性，而 jwt 想延期这个 exp 就有点身不由己了，因为 payload 是参与签名的，一旦过期时间被修改，整个 jwt 串就变了，jwt 的特性天然不支持续签！ 如果你一定要使用 jwt 做会话管理（payload 中存储会话信息），也不是没有解决方案，但个人认为都不是很令人满意 每次请求刷新 jwt jwt 修改 payload 中的 exp 后整个 jwt 串就会发生改变，那…就让它变好了，每次请求都返回一个新的 jwt 给客户端。太暴力了，不用我赘述这样做是多么的不优雅，以及带来的性能问题。 但，至少这是最简单的解决方案。 只要快要过期的时候刷新 jwt 一个上述方案的改造点是，只在最后的几分钟返回给客户端一个新的 jwt。这样做，触发刷新 jwt 基本就要看运气了，如果用户恰巧在最后几分钟访问了服务器，触发了刷新，万事大吉；如果用户连续操作了 27 分钟，只有最后的 3 分钟没有操作，导致未刷新 jwt，无疑会令用户抓狂。 完善 refreshToken 借鉴 oauth2 的设计，返回给客户端一个 refreshToken，允许客户端主动刷新 jwt。一般而言，jwt 的过期时间可以设置为数小时，而 refreshToken 的过期时间设置为数天。 我认为该方案并可行性是存在的，但是为了解决 jwt 的续签把整个流程改变了，为什么不考虑下 oauth2 的 password 模式和 client 模式呢？ 使用 redis 记录独立的过期时间 实际上我的项目中由于历史遗留问题，就是使用 jwt 来做登录和会话管理的，为了解决续签问题，我们在 redis 中单独会每个 jwt 设置了过期时间，每次访问时刷新 jwt 的过期时间，若 jwt 不存在与 redis 中则认为过期。 tips: 精确控制 redis 的过期时间不是件容易的事，可以参考我最近的一篇借助于 spring session 讲解 redis 过期时间的排坑记录。 同样改变了 jwt 的流程，不过嘛，世间安得两全法。我只能奉劝各位还未使用 jwt 做会话管理的朋友，尽量还是选用传统的 session+cookie 方案，有很多成熟的分布式 session 框架和安全框架供你开箱即用。 jwt,oauth2,session 千丝万缕的联系具体的对比不在此文介绍，就一位读者的留言回复下它的提问 这么长一个字符串，还不如我把数据存到数据库，给一个长的很难碰撞的 key 来映射，也就是专用 token。 这位兄弟认为 jwt 太长了，是不是可以考虑使用和 oauth2 一样的 uuid 来映射。这里面自然是有问题的，jwt 不仅仅是作为身份的认证（验证签名是否正确，签发者是否存在，有限期是否过期），还在其 payload 中存储着会话信息，这是 jwt 和 session 的最大区别，一个在客户端携带会话信息，一个在服务端存储会话信息。如果真的是要将 jwt 的信息置于在共享存储中，那再找不到任何使用 jwt 的意义了。 jwt 和 oauth2 都可以用于 restful 的认证，就我个人的使用经验来看，spring security oauth2 可以很好的使用多种认证模式：client 模式，password 模式，implicit 模式（authorization code 模式不算单纯的接口认证模式），也可以很方便的实现权限控制，什么样的 api 需要什么样的权限，什么样的资源需要什么样的 scope…而 jwt 我只用它来实现过身份认证，功能较为单一（可能是我没发现更多用法）。 总结在 web 应用中，使用 jwt 代替 session 存在不小的风险，你至少得解决本文中提及的那些问题，绝大多数情况下，传统的 cookie-session 机制工作得更好。jwt 适合做简单的 restful api 认证，颁发一个固定有效期的 jwt，降低 jwt 暴露的风险，不要对 jwt 做服务端的状态管理，这样才能体现出 jwt 无状态的优势。 可能对 jwt 的使用场景还有一些地方未被我察觉，后续会研究下 spring security oauth jwt 的源码，不知到时会不会有新发现。 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。","link":"/jwt-learn-3/"},{"title":"JSON Web Token - 在 Web 应用间安全地传递信息","text":"转载自：http://blog.leapoahead.com/2015/09/06/understanding-jwt/ 作者：John Wu JSON Web Token（JWT）是一个非常轻巧的 规范。这个规范允许我们使用 JWT 在用户和服务器之间传递安全可靠的信息。 让我们来假想一下一个场景。在 A 用户关注了 B 用户的时候，系统发邮件给 B 用户，并且附有一个链接“点此关注 A 用户”。链接的地址可以是这样的 1https://your.awesome-app.com/make-friend/?from_user=B&amp;target_user=A 上面的 URL 主要通过 URL 来描述这个当然这样做有一个弊端，那就是要求用户 B 用户是一定要先登录的。可不可以简化这个流程，让 B 用户不用登录就可以完成这个操作。JWT 就允许我们做到这点。 JWT 的组成一个 JWT 实际上就是一个字符串，它由三部分组成，** 头部 、 载荷 ** 与 ** 签名 **。 载荷（Payload）我们先将上面的添加好友的操作描述成一个 JSON 对象。其中添加了一些其他的信息，帮助今后收到这个 JWT 的服务器理解这个 JWT。 123456789{ &quot;iss&quot;: &quot;John Wu JWT&quot;, &quot;iat&quot;: 1441593502, &quot;exp&quot;: 1441594722, &quot;aud&quot;: &quot;www.example.com&quot;, &quot;sub&quot;: &quot;jrocket@example.com&quot;, &quot;from_user&quot;: &quot;B&quot;, &quot;target_user&quot;: &quot;A&quot;} 这里面的前五个字段都是由 JWT 的标准所定义的。 iss: 该 JWT 的签发者 sub: 该 JWT 所面向的用户 aud: 接收该 JWT 的一方 exp(expires): 什么时候过期，这里是一个 Unix 时间戳 iat(issued at): 在什么时候签发的 这些定义都可以在 标准 中找到。 将上面的 JSON 对象进行 [base64 编码] 可以得到下面的字符串。这个字符串我们将它称作 JWT 的 Payload（载荷）。 1eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 如果你使用 Node.js，可以用 Node.js 的包 base64url 来得到这个字符串。 1234567var base64url = require('base64url')var header = { &quot;from_user&quot;: &quot;B&quot;, &quot;target_user&quot;: &quot;A&quot;}console.log(base64url(JSON.stringify(header)))// 输出：eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 小知识：Base64 是一种编码，也就是说，它是可以被翻译回原来的样子来的。它并不是一种加密过程。 头部（Header）JWT 还需要一个头部，头部用于描述关于该 JWT 的最基本的信息，例如其类型以及签名所用的算法等。这也可以被表示成一个 JSON 对象。 1234{ &quot;typ&quot;: &quot;JWT&quot;, &quot;alg&quot;: &quot;HS256&quot;} 在这里，我们说明了这是一个 JWT，并且我们所用的签名算法（后面会提到）是 HS256 算法。 对它也要进行 Base64 编码，之后的字符串就成了 JWT 的 Header（头部）。 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 签名（签名）将上面的两个编码后的字符串都用句号 . 连接在一起（头部在前），就形成了 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0 这一部分的过程在 node-jws 的源码 中有体现 最后，我们将上面拼接完的字符串用 HS256 算法进行加密。在加密的时候，我们还需要提供一个密钥（secret）。如果我们用 mystar 作为密钥的话，那么就可以得到我们加密后的内容 1rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM 这一部分又叫做 ** 签名 **。 最后将这一部分签名也拼接在被签名的字符串后面，我们就得到了完整的 JWT 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0.rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM 于是，我们就可以将邮件中的 URL 改成 1https://your.awesome-app.com/make-friend/?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0.rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM 这样就可以安全地完成添加好友的操作了！ 且慢，我们一定会有一些问题： 签名的目的是什么？ Base64 是一种编码，是可逆的，那么我的信息不就被暴露了吗？ 让我逐一为你说明。 签名的目的最后一步签名的过程，实际上是对头部以及载荷内容进行签名。一般而言，加密算法对于不同的输入产生的输出总是不一样的。对于两个不同的输入，产生同样的输出的概率极其地小（有可能比我成世界首富的概率还小）。所以，我们就把“不一样的输入产生不一样的输出”当做必然事件来看待吧。 所以，如果有人对头部以及载荷的内容解码之后进行修改，再进行编码的话，那么新的头部和载荷的签名和之前的签名就将是不一样的。而且，如果不知道服务器加密的时候用的密钥的话，得出来的签名也一定会是不一样的。 服务器应用在接受到 JWT 后，会首先对头部和载荷的内容用同一算法再次签名。那么服务器应用是怎么知道我们用的是哪一种算法呢？别忘了，我们在 JWT 的头部中已经用 alg 字段指明了我们的加密算法了。 如果服务器应用对头部和载荷再次以同样方法签名之后发现，自己计算出来的签名和接受到的签名不一样，那么就说明这个 Token 的内容被别人动过的，我们应该拒绝这个 Token，返回一个 HTTP 401 Unauthorized 响应。 信息会暴露？是的。 所以，在 JWT 中，不应该在载荷里面加入任何敏感的数据。在上面的例子中，我们传输的是用户的 User ID。这个值实际上不是什么敏感内容，一般情况下被知道也是安全的。 但是像密码这样的内容就不能被放在 JWT 中了。如果将用户的密码放在了 JWT 中，那么怀有恶意的第三方通过 Base64 解码就能很快地知道你的密码了。 JWT 的适用场景我们可以看到，JWT 适合用于向 Web 应用传递一些非敏感信息。例如在上面提到的完成加好友的操作，还有诸如下订单的操作等等。 其实 JWT 还经常用于设计用户认证和授权系统，甚至实现 Web 应用的单点登录。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/jwt-learn/"},{"title":"选择 Kong 作为你的 API 网关","text":"Kong（https://github.com/Kong/kong）是一个云原生，高效，可扩展的分布式 API 网关。 自 2015 年在 github 开源后，广泛受到关注，目前已收获 1.68w+ 的 star，其核心价值在于高性能和可扩展性。 为什么需要 API 网关 在微服务架构之下，服务被拆的非常零散，降低了耦合度的同时也给服务的统一管理增加了难度。如上图左所示，在旧的服务治理体系之下，鉴权，限流，日志，监控等通用功能需要在每个服务中单独实现，这使得系统维护者没有一个全局的视图来统一管理这些功能。API 网关致力于解决的问题便是为微服务纳管这些通用的功能，在此基础上提高系统的可扩展性。如右图所示，微服务搭配上 API 网关，可以使得服务本身更专注于自己的领域，很好地对服务调用者和服务提供者做了隔离。 为什么是 KongSpringCloud 玩家肯定都听说过 Zuul 这个路由组件，包括 Zuul2 和 Springcloud Gateway 等框架，在国内的知名度都不低。没错，我称呼这些为组件 Or 框架，而 Kong 则更衬的上产品这个词。在此我们可以简单对比下 Zuul 和 Kong。 举例而言，如果选择使用 Zuul，当需要为应用添加限流功能，由于 Zuul 只提供了基本的路由功能，开发者需要自己研发 Zuul Filter，可能你觉得一个功能还并不麻烦，但如果在此基础上对 Zuul 提出更多的要求，很遗憾，Zuul 使用者需要自行承担这些复杂性。而对于 Kong 来说，限流功能就是一个插件，只需要简单的配置，即可开箱即用。 Kong 的插件机制是其高可扩展性的根源，Kong 可以很方便地为路由和服务提供各种插件，网关所需要的基本特性，Kong 都如数支持： 云原生: 与平台无关，Kong 可以从裸机运行到 Kubernetes 动态路由：Kong 的背后是 OpenResty+Lua，所以从 OpenResty 继承了动态路由的特性 熔断 健康检查 日志: 可以记录通过 Kong 的 HTTP，TCP，UDP 请求和响应。 鉴权: 权限控制，IP 黑白名单，同样是 OpenResty 的特性 SSL: Setup a Specific SSL Certificate for an underlying service or API. 监控: Kong 提供了实时监控插件 认证: 如数支持 HMAC, JWT, Basic, OAuth2.0 等常用协议 限流 REST API: 通过 Rest API 进行配置管理，从繁琐的配置文件中解放 可用性: 天然支持分布式 高性能: 背靠非阻塞通信的 nginx，性能自不用说 插件机制: 提供众多开箱即用的插件，且有易于扩展的自定义插件接口，用户可以使用 Lua 自行开发插件 上面这些特性中，反复提及了 Kong 背后的 OpenResty，实际上，使用 Kong 之后，Nginx 可以完全摒弃，Kong 的功能是 Nginx 的父集。 而 Zuul 除了基础的路由特性以及其本身和 SpringCloud 结合较为紧密之外，并无任何优势。 Kong 的架构 从技术的角度讲，Kong 可以认为是一个 OpenResty 应用程序。 OpenResty 运行在 Nginx 之上，使用 Lua 扩展了 Nginx。 Lua 是一种非常容易使用的脚本语言，可以让你在 Nginx 中编写一些逻辑操作。之前我们提到过一个概念 Kong = OpenResty + Nginx + Lua，但想要从全局视角了解 Kong 的工作原理，还是直接看源码比较直接。我们定位到本地的 Kong 文件夹，按照上图中的目录层级来识识 Kong 的庐山真面目。 Kong 文件下包含了全部源码和必要组件，分析他们，我们便得到了 Kong 的架构。0.13.x 是目前 Kong 的最新版本。 从 2 号块中可以看到 nginx.conf ，这其实便是一个标准的 Nginx 目录结构，这也揭示了 Kong 其实就是运行在 Nginx 的基础之上，而进行的二次封装。由 share 文件夹向下展开下一次分析。 share 文件夹中包含了 OpenResty 的相关内容，其实背后就是一堆 Lua 脚本，例如 lapis 包含了数据库操作，Nginx 生命周期，缓存控制等必要的 Lua 脚本，logging 包含了日志相关的 Lua 脚本，resty 包含了 dns，健康检查等相关功能的 Lua 脚本…而其中的 kong 目录值得我们重点分析，他包含了 Kong 的核心对象。 api 和 core 文件夹，封装了 Kong 对 service，route，upstream，target 等核心对象的操作代码（这四个核心对象将会在下面的小节重点介绍），而 plugins 文件夹则是 Kong 高可扩展性的根源，存放了 kong 的诸多扩展功能。 plugins 文件夹包含了上一节提到的 Kong 的诸多插件功能，如权限控制插件，跨域插件，jwt 插件，oauth2 插件… 如果需要自定义插件，则需要将代码置于此处。 从上述文件夹浏览下来，大概可以看到它和 Nginx 的相似之处，并在此基础之上借助于 Lua 对自身的功能进行了拓展，除了 nginx.conf 中的配置，和相对固定的文件层级，Kong 还需要连接一个数据库来管理路由配置，服务配置，upstream 配置等信息，是的，由于 Kong 支持动态路由的特性，所以几乎所有动态的配置都不是配置在文件中，而是借助于 Postgres 或者 Cassandra 进行管理。 Kong 对外暴露了 Restful API，最终的配置便是落地在了数据库之中。 Kong 的管理方式通过文件夹结构的分析，以及数据库中的表结构，我们已经对 Kong 的整体架构有了一个基本的认识，但肯定还存在一个疑问：我会配置 Nginx 来控制路由，但这个 Kong 应当怎么配置才能达到相同的目的呢？莫急，下面来看看 Kong 如何管理配置。 Kong 简单易用的背后，便是因为其所有的操作都是基于 HTTP Restful API 来进行的。 其中 8000/8443 分别是 Http 和 Https 的转发端口，等价于 Nginx 默认的 80 端口，而 8001 端口便是默认的管理端口，我们可以通过 HTTP Restful API 来动态管理 Kong 的配置。 一个典型的 Nginx 配置 12345678910upstream helloUpstream { server localhost:3000 weight=100;}server { listen 80; location /hello { proxy_pass http://helloUpstream; }} 如上这个简单的 Nginx 配置，便可以转换为如下的 Http 请求。 对应的 Kong 配置 123456789# 配置 upstreamcurl -X POST http://localhost:8001/upstreams --data &quot;name=helloUpstream&quot;# 配置 targetcurl -X POST http://localhost:8001/upstreams/hello/targets --data &quot;target=localhost:3000&quot; --data &quot;weight=100&quot;# 配置 servicecurl -X POST http://localhost:8001/services --data &quot;name=hello&quot; --data &quot;host=helloUpstream&quot;# 配置 routecurl -X POST http://localhost:8001/routes --data &quot;paths[]=/hello&quot; --data &quot;service.id=8695cc65-16c1-43b1-95a1-5d30d0a50409&quot;curl -X POST http://localhost:8001/routes --data &quot;hosts[]=a.com,b.com,*.abc.com&quot; --data &quot;service.id=8695cc65-16c1-43b1-95a1-5d30d0a50409&quot; 这一切都是动态的，无需手动 reload nginx.conf。 我们为 Kong 新增路由信息时涉及到了 upstream，target，service，route 等概念，他们便是 Kong 最最核心的四个对象。（你可能在其他 Kong 的文章中见到了 api 这个对象，在最新版本 0.13 中已经被弃用，api 已经由 service 和 route 替代） 从上面的配置以及他们的字面含义大概能够推测出他们的职责，upstream 是对上游服务器的抽象；target 代表了一个物理服务，是 ip + port 的抽象；service 是抽象层面的服务，他可以直接映射到一个物理服务 (host 指向 ip + port)，也可以指向一个 upstream 来做到负载均衡；route 是路由的抽象，他负责将实际的 request 映射到 service。 他们的关系如下 upstream 和 target ：1 对 n service 和 upstream ：1 对 1 或 1 对 0 （service 也可以直接指向具体的 target，相当于不做负载均衡） service 和 route：1 对 n 高可扩展性的背后—插件机制Kong 的另一大特色便是其插件机制，这也是我认为的 Kong 最优雅的一个设计。 文章开始时我们便提到一点，微服务架构中，网关应当承担所有服务共同需要的那部分功能，这一节我们便来介绍下，Kong 如何添加 jwt 插件，限流插件。 插件（Plugins）装在哪儿？对于部分插件，可能是全局的，影响范围是整个 Kong 服务；大多数插件都是装在 service 或者 route 之上。这使得插件的影响范围非常灵活，我们可能只需要对核心接口进行限流控制，只需要对部分接口进行权限控制，这时候，对特定的 service 和 route 进行定向的配置即可。 为 hello 服务添加 50 次 / 秒的限流 123curl -X POST http://localhost:8001/services/hello/plugins \\--data &quot;name=rate-limiting&quot; \\--data &quot;config.second=50&quot; 为 hello 服务添加 jwt 插件 12curl -X POST http://localhost:8001/services/login/plugins \\--data &quot;name=jwt&quot; 同理，插件也可以安装在 route 之上 123456curl -X POST http://localhost:8001/routes/{routeId}/plugins \\--data &quot;name=rate-limiting&quot; \\--data &quot;config.second=50&quot;curl -X POST http://localhost:8001/routes/{routeId}/plugins \\--data &quot;name=jwt&quot; 在官方文档中，我们可以获取全部的插件 https://konghq.com/plugins/，部分插件需要收费的企业版才可使用。 总结Kong 是目前市场上相对较为成熟的开源 API 网关产品，无论是性能，扩展性，还是功能特性，都决定了它是一款优秀的产品，对 OpenResty 和 Lua 感兴趣的同学，Kong 也是一个优秀的学习参考对象。基于 OpenResty，可以在现有 Kong 的基础上进行一些扩展，从而实现更复杂的特性，比如我司内部的 ABTest 插件和定制化的认证插件，开发成本都相对较低。Kong 系列的文章将会在以后持续连载。 阅读扩展 初识 Kong 之负载均衡 https://www.cnkirito.moe/kong-loadbalance/ Kong 集成 Jwt 插件 https://www.cnkirito.moe/kong-jwt/ 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。","link":"/kong-introduction/"},{"title":"Kong 集成 Jwt 插件","text":"上一篇文章使用 Kong 完成了负载均衡的配置，本文介绍下在此基础上如何集成 jwt 插件来保护内部服务的安全。前置知识点：Jwt 基础概念。推荐阅读： 通俗易懂地介绍 Jwt https://blog.leapoahead.com/2015/09/06/understanding-jwt/ Jwt 的官网 https://jwt.io/ 为 Kong 安装 Jwt 插件Kong 官方提供了 Jwt 插件，可以对 某个 service 或者 route 添加 Jwt 认证，我以 service 为例介绍 Jwt 插件的使用 为 hello（上篇文章创建的 service）添加 Jwt 插件 1curl -X POST http://localhost:8001/services/hello/plugins --data &quot;name=jwt&quot; 接着尝试访问这个受保护的服务 12kirito$ curl http://localhost:8000/hello/hi=&gt; {&quot;message&quot;:&quot;Unauthorized&quot;} 说明该 service 已经被 Jwt 保护起来了。 在 Kong 中创建用户1curl -X POST http://localhost:8001/consumers --data &quot;username=kirito&quot; 使用了新的端点 consumers 创建了一个名称为 kirito 的用户。 查看用户信息1curl http://127.0.0.1:8001/consumers/kirito/jwt 响应如下： 12345678910111213{ &quot;total&quot;: 1, &quot;data&quot;: [ { &quot;created_at&quot;: 1523432449000, &quot;id&quot;: &quot;cb01a6cf-7371-4f23-8193-fa69a0bb070c&quot;, &quot;algorithm&quot;: &quot;HS256&quot;, &quot;key&quot;: &quot;vcnvYSFzTIGyMxzKSgnNU0uvxixdYWB9&quot;, &quot;secret&quot;: &quot;qQ9tSqIYjilnJmKuZXvJpgNo4ZqJDrim&quot;, &quot;consumer_id&quot;: &quot;7d34e6bc-89ea-4f33-9346-9c10600e4afd&quot; } ]} 重点关注三个值 algorithm，key，secret，他们和 Jwt 算法的参数密切相关 生成 Jwt使用 jwt 官网 (jwt.io) 提供的 Debugger 功能可以很方便的生成 jwt。 HEADER 部分声明了验证方式为 JWT，加密算法为 HS256 PAYLOAD 部分原本有 5 个参数 1234567{ &quot;iss&quot;: &quot;kirito&quot;, &quot;iat&quot;: 1441593502, &quot;exp&quot;: 1441594722, &quot;aud&quot;: &quot;cnkirito.moe&quot;, &quot;sub&quot;: &quot;250577914@qq.com&quot;,} 这里面的前五个字段都是由 JWT 的标准（RFC7519）所定义的。 iss: 该 JWT 的签发者 sub: 该 JWT 所面向的用户 aud: 接收该 JWT 的一方 exp(expires): 什么时候过期，这里是一个 Unix 时间戳 iat(issued at): 在什么时候签发的 iss 这一参数在 Kong 的 Jwt 插件中对应的是 curl http://127.0.0.1:8001/consumers/kirito/jwt 获取的用户信息中的 key 值。 而其他值都可以不填写 最后还要一个没有用到的用户信息：secret。HS256 加密算法是对称加密算法，加密和解密都依赖于同一个密钥，在生成 Jwt 的消息签名时（Verify Signature）需要被使用到。 我们使用 jwt 官网 (jwt.io) 提供的 Debugger 功能快速生成我们的 Jwt 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJ2Y252WVNGelRJR3lNeHpLU2duTlUwdXZ4aXhkWVdCOSJ9.3iL4sXgZyvRx2XtIe2X73yplfmSSu1WPGcvyhwq7TVE 由三个圆点分隔的长串便是用户身份的标识了 携带 Jwt 访问受限资源12kirito$ curl http://localhost:8000/hello/hi=&gt; {&quot;message&quot;:&quot;Unauthorized&quot;} 在此之前直接访问 hello 服务是处于未验证状态 携带 Jwt 访问 12curl http://localhost:8000/hello/hi -H 'Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJ2Y252WVNGelRJR3lNeHpLU2duTlUwdXZ4aXhkWVdCOSJ9.3iL4sXgZyvRx2XtIe2X73yplfmSSu1WPGcvyhwq7TVE'=&gt; 3000 成功获取到了服务端的响应，Jwt 插件就这样正常工作了。 补充 可以指定生成的 key（对应 Jwt 中的 iss），和 secret 1curl -X POST http://localhost:8001/consumers/kirito/jwt --data &quot;secret=YmxvYiBkYXRh&quot; --data &quot;key=kirito&quot; 如果想要修改 secret 和 key，经过目前笔者的尝试后，似乎只能够先删除，后新增。 Jwt 也可以作为 QueryString 参数携带在 get 请求中 1curl http://localhost:8000/hello/hi?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJ2Y252WVNGelRJR3lNeHpLU2duTlUwdXZ4aXhkWVdCOSJ9.3iL4sXgZyvRx2XtIe2X73yplfmSSu1WPGcvyhwq7TVE 通常用户需要自己写一个服务去帮助 Consumer 生成自己的 Jwt，自然不能总是依赖于 Jwt 官方的 Debugger，当然也没必要重复造轮子（尽管这并不难），可以考虑使用开源实现，比如 Java 中推荐使用 jjwt(https://github.com/jwtk/jjwt) 123456String jwt = Jwts.builder() .setHeaderParam(&quot;typ&quot;,&quot;jwt&quot;) .setHeaderParam(&quot;alg&quot;,&quot;HS256&quot;) .setIssuer(&quot;kirito&quot;) .signWith(SignatureAlgorithm.HS256, Base64.getEncoder().encodeToString(&quot;YmxvYiBkYXRh&quot;.getBytes(Charset.forName(&quot;utf-8&quot;)))) .compact(); ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/kong-jwt/"},{"title":"初识 Kong 之负载均衡","text":"使用 Kong Community Edition（社区版 v1.3.0）来搭建一个负载均衡器，由于 Kong 是基于 Openresty 的，而 Openresty 又是 Nginx 的二次封装，所有很多配置项和 Nginx 类似。 来看一个较为典型的 Nginx 负载均衡配置 1234567891011upstream hello { server localhost:3000 weight=100; server localhost:3001 weight=50;}server { listen 80; location /hello { proxy_pass http://hello; }} nginx 监听来自本地 80 端口的请求，如果路径与 /hello 匹配，便将请求原封不动的转发到名称为 hello 的 upstream，而该 upstream 我们配置了一个负载均衡器，会路由到本地的 3000 端口和 3001 端口。 12345678910111213141516@SpringBootApplication@RestControllerpublic class KongDemoApplication { public static void main(String[] args) { System.setProperty(&quot;server.port&quot;,&quot;3000&quot;); //System.setProperty(&quot;server.port&quot;,&quot;3001&quot;); SpringApplication.run(KongDemoApplication.class, args); } @RequestMapping(&quot;/hi&quot;) public String port(){ return System.getProperty(&quot;server.port&quot;); }} 启动两个 server 分别监听本地 3000 端口和 3001 端口。 如何你的机器已经安装好了 Kong，并对 Kong 的 admin api 有了基础的认识，接下来便可以针对 Kong 进行负载均衡的配置了。 配置 upstream 和 target创建一个名称 hello 的 upstream 1curl -X POST http://localhost:8001/upstreams --data &quot;name=hello&quot; 为 hello 添加两个负载均衡节点 1curl -X POST http://localhost:8001/upstreams/hello/targets --data &quot;target=localhost:3000&quot; --data &quot;weight=100&quot; 1curl -X POST http://localhost:8001/upstreams/hello/targets --data &quot;target=localhost:3001&quot; --data &quot;weight=50&quot; 如上的配置对应了 Nginx 的配置 1234upstream hello { server localhost:3000 weight=100; server localhost:3001 weight=50;} 配置 service 和 route使用老版本 Kong 的用户可能会接触过 api 这个概念，但是在 Kong v1.3.0 中，已经被废除了，取而代之的是 service 和 route 的配置。 配置一个 service 1curl -X POST http://localhost:8001/services --data &quot;name=hello&quot; --data &quot;host=hello&quot; host 的值便对应了 upstream 的名称，配置成功后会返回生成的 service 的 id，我的返回结果：8695cc65-16c1-43b1-95a1-5d30d0a50409 为上面的 service 配置路由信息 1curl -X POST http://localhost:8001/routes --data &quot;paths[]=/hello&quot; --data &quot;service.id=8695cc65-16c1-43b1-95a1-5d30d0a50409&quot; 请求路径包含 /hello 的请求都会被转移到对应的 service 进行处理。 如上的配置便对应了 123location /hello { proxy_pass http://hello;} 测试 Kong 的负载均衡1curl http://localhost:8000/hello/hi 因为复杂均衡的原因，需要多测试几次，多次 curl 之后结果如下： 123456789101130003000300030003000300030013001300130003001 参考文档https://getkong.org/docs/0.13.x/loadbalancing/ https://getkong.org/docs/0.13.x/configuration/ ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/kong-loadbalance/"},{"title":"重新认识 Java 中的内存映射（mmap）","text":"mmap 基础概念mmap 是一种内存映射文件的方法，即将一个文件映射到进程的地址空间，实现文件磁盘地址和一段进程虚拟地址的映射。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页到对应的文件磁盘上，即完成了对文件的操作而不必再调用 read,write 等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。 操作系统提供了这么一系列 mmap 的配套函数 123void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);int munmap( void * addr, size_t len);int msync( void *addr, size_t len, int flags); Java 中的 mmapJava 中原生读写方式大概可以被分为三种：普通 IO，FileChannel（文件通道），mmap（内存映射）。区分他们也很简单，例如 FileWriter,FileReader 存在于 java.io 包中，他们属于普通 IO；FileChannel 存在于 java.nio 包中，也是 Java 最常用的文件操作类；而今天的主角 mmap，则是由 FileChannel 调用 map 方法衍生出来的一种特殊读写文件的方式，被称之为内存映射。 mmap 的使用方式： 12FileChannel fileChannel = new RandomAccessFile(new File(&quot;db.data&quot;), &quot;rw&quot;).getChannel();MappedByteBuffer mappedByteBuffer = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, filechannel.size(); MappedByteBuffer 便是 Java 中的 mmap 操作类。 12345678910111213141516171819// 写byte[] data = new byte[4];int position = 8;// 从当前 mmap 指针的位置写入 4b 的数据mappedByteBuffer.put(data);// 指定 position 写入 4b 的数据MappedByteBuffer subBuffer = mappedByteBuffer.slice();subBuffer.position(position);subBuffer.put(data);// 读byte[] data = new byte[4];int position = 8;// 从当前 mmap 指针的位置读取 4b 的数据mappedByteBuffer.get(data)；// 指定 position 读取 4b 的数据MappedByteBuffer subBuffer = mappedByteBuffer.slice();subBuffer.position(position);subBuffer.get(data); mmap 不是银弹促使我写这一篇文章的一大动力，来自于网络中很多关于 mmap 错误的认知。初识 mmap，很多文章提到 mmap 适用于处理大文件的场景，现在回过头看，其实这种观点是非常荒唐的，希望通过此文能够澄清 mmap 本来的面貌。 FileChannel 与 mmap 同时存在，大概率说明两者都有其合适的使用场景，而事实也的确如此。在看待二者时，可以将其看待成实现文件 IO 的两种工具，工具本身没有好坏，主要还是看使用场景。 mmap vs FileChannel这一节，详细介绍一下 FileChannel 和 mmap 在进行文件 IO 的一些异同点。 pageCacheFileChannel 和 mmap 的读写都经过 pageCache，或者更准确的说法是通过 vmstat 观测到的 cache 这一部分内存，而非用户空间的内存。 123procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 3 0 0 4622324 40736 351384 0 0 0 0 2503 200 50 1 50 0 0 至于说 mmap 映射的这部分内存能不能称之为 pageCache，我并没有去调研过，不过在操作系统看来，他们并没有太多的区别，这部分 cache 都是内核在控制。后面本文也统一称 mmap 出来的内存为 pageCache。 缺页中断对 Linux 文件 IO 有基础认识的读者，可能对缺页中断这个概念也不会太陌生。mmap 和 FileChannel 都以缺页中断的方式，进行文件读写。 以 mmap 读取 1G 文件为例， fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, _GB); 进行映射是一个消耗极少的操作，此时并不意味着 1G 的文件被读进了 pageCache。只有通过以下方式，才能够确保文件被读进 pageCache。 12345FileChannel fileChannel = new RandomAccessFile(file, &quot;rw&quot;).getChannel();MappedByteBuffer map = fileChannel.map(MapMode.READ_WRITE, 0, _GB);for (int i = 0; i &lt; _GB; i += _4kb) { temp += map.get(i);} 关于内存对齐的细节在这里就不拓展了，可以详见 java.nio.MappedByteBuffer#load 方法，load 方法也是通过按页访问的方式触发中断 如下是 pageCache 逐渐增长的过程，共计约增长了 1.034G，说明文件内容此刻已全部 load。 123456789101112procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 4824640 1056 207912 0 0 0 0 2374 195 50 0 50 0 0 2 1 0 4605300 2676 411892 0 0 205256 0 3481 1759 52 2 34 12 0 2 1 0 4432560 2676 584308 0 0 172032 0 2655 346 50 1 25 24 0 2 1 0 4255080 2684 761104 0 0 176400 0 2754 380 50 1 19 29 0 2 3 0 4086528 2688 929420 0 0 167940 40 2699 327 50 1 25 24 0 2 2 0 3909232 2692 1106300 0 0 176520 4 2810 377 50 1 23 26 0 2 2 0 3736432 2692 1278856 0 0 172172 0 2980 361 50 1 17 31 0 3 0 0 3722064 2840 1292776 0 0 14036 0 2757 392 50 1 29 21 0 2 0 0 3721784 2840 1292892 0 0 116 0 2621 283 50 1 50 0 0 2 0 0 3721996 2840 1292892 0 0 0 0 2478 237 50 0 50 0 0 两个细节： mmap 映射的过程可以理解为一个懒加载， 只有 get() 时才会触发缺页中断 预读大小是有操作系统算法决定的，可以默认当作 4kb，即如果希望懒加载变成实时加载，需要按照 step=4kb 进行一次遍历 而 FileChannel 缺页中断的原理也与之相同，都需要借助 PageCache 做一层跳板，完成文件的读写。 内存拷贝次数很多言论认为 mmap 相比 FileChannel 少一次复制，我个人觉得还是需要区分场景。 例如需求是从文件首地址读取一个 int，两者所经过的链路其实是一致的：SSD -&gt; pageCache -&gt; 应用内存，mmap 并不会少拷贝一次。 但如果需求是维护一个 100M 的复用 buffer，且涉及到文件 IO，mmap 直接就可以当做是 100M 的 buffer 来用，而不用在进程的内存（用户空间）中再维护一个 100M 的缓冲。 用户态与内核态 操作系统出于安全考虑，将一些底层的能力进行了封装，提供了系统调用（system call）给用户使用。这里就涉及到“用户态”和“内核态”的切换问题，私认为这里也是很多人概念理解模糊的重灾区，我在此梳理下个人的认知，如有错误也欢迎指正。 先看 FileChannel，下面两段代码，你认为谁更快？ 123456789101112131415161718192021// 方法一: 4kb 刷盘FileChannel fileChannel = new RandomAccessFile(file, &quot;rw&quot;).getChannel();ByteBuffer byteBuffer = ByteBuffer.allocateDirect(_4kb);for (int i = 0; i &lt; _4kb; i++) { byteBuffer.put((byte)0);}for (int i = 0; i &lt; _GB; i += _4kb) { byteBuffer.position(0); byteBuffer.limit(_4kb); fileChannel.write(byteBuffer);}// 方法二: 单字节刷盘FileChannel fileChannel = new RandomAccessFile(file, &quot;rw&quot;).getChannel();ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1);byteBuffer.put((byte)0);for (int i = 0; i &lt; _GB; i ++) { byteBuffer.position(0); byteBuffer.limit(1); fileChannel.write(byteBuffer);} 使用方法一：4kb 缓冲刷盘（常规操作），在我的测试机器上只需要 1.2s 就写完了 1G。而不使用任何缓冲的方法二，几乎是直接卡死，文件增长速度非常缓慢，在等待了 5 分钟还没写完后，中断了测试。 使用写入缓冲区是一个非常经典的优化技巧，用户只需要设置 4kb 整数倍的写入缓冲区，聚合小数据的写入，就可以使得数据从 pageCache 刷盘时，尽可能是 4kb 的整数倍，避免写入放大问题。但这不是这一节的重点，大家有没有想过，pageCache 其实本身也是一层缓冲，实际写入 1byte 并不是同步刷盘的，相当于写入了内存，pageCache 刷盘由操作系统自己决策。那为什么方法二这么慢呢？主要就在于 filechannel 的 read/write 底层相关联的系统调用，是需要切换内核态和用户态的，注意，这里跟内存拷贝没有任何关系，导致态切换的根本原因是 read/write 关联的系统调用本身。方法二比方法一多切换了 4096 倍，态的切换成为了瓶颈，导致耗时严重。 阶段总结一下重点，在 DRAM 中设置用户写入缓冲区这一行为有两个意义： 方便做 4kb 对齐，ssd 刷盘友好 减少用户态和内核态的切换次数，cpu 友好 但 mmap 不同，其底层提供的映射能力不涉及到切换内核态和用户态，注意，这里跟内存拷贝还是没有任何关系，导致态不发生切换的根本原因是 mmap 关联的系统调用本身。验证这一点，也非常容易，我们使用 mmap 实现方法二来看看速度如何： 12345FileChannel fileChannel = new RandomAccessFile(file, &quot;rw&quot;).getChannel();MappedByteBuffer map = fileChannel.map(MapMode.READ_WRITE, 0, _GB);for (int i = 0; i &lt; _GB; i++) { map.put((byte)0);} 在我的测试机器上，花费了 3s，它比 FileChannel + 4kb 缓冲写要慢，但远比 FileChannel 写单字节快。 这里也解释了我之前文章《文件 IO 操作的一些最佳实践》中一个疑问：”一次写入很小量数据的场景使用 mmap 会比 fileChannel 快的多“，其背后的原理就和上述例子一样，在小数据量下，瓶颈不在于 IO，而在于用户态和内核态的切换。 mmap 细节补充copy on write 模式我们注意到 public abstract MappedByteBuffer map(MapMode mode,long position, long size) 的第一个参数，MapMode 其实有三个值，在网络冲浪的时候，也几乎没有找到讲解 MapMode 的文章。MapMode 有三个枚举值 READ_WRITE、READ_ONLY、PRIVATE，大多数时候使用的可能是 READ_WRITE，而 READ_ONLY 不过是限制了 WRITE 而已，很容易理解，但这个 PRIVATE 身上似乎有一层神秘的面纱。 实际上 PRIVATE 模式正是 mmap 的 copy on write 模式，当使用 MapMode.PRIVATE 去映射文件时，你会获得以下的特性： 其他任何方式对文件的修改，会直接反映在当前 mmap 映射中。 private mmap 之后自身的 put 行为，会触发复制，形成自己的副本，任何修改不会会刷到文件中，也不再感知该文件该页的改动。 俗称：copy on write。 这有什么用呢？重点就在于任何修改都不会回刷文件。其一，你可以获得一个文件副本，如果你正好有这个需求，直接可以使用 PRIVATE 模式去进行映射，其二，令人有点小激动的场景，你获得了一块真正的 PageCache，不用担心它会被操作系统刷盘造成 overhead。假设你的机器配置如下：机器内存 9G，JVM 参数设置为 6G，堆外限制为 2G，那剩下的 1G 只能被内核态使用，如果想被用户态的程序利用起来，就可以使用 mmap 的 copy on write 模式，这不会占用你的堆内内存或者堆外内存。 回收 mmap 内存更正之前博文关于 mmap 内存回收的一个错误说法，回收 mmap 很简单 1((DirectBuffer) mmap).cleaner().clean(); mmap 的生命中简单可以分为：map（映射），get/load （缺页中断），clean（回收）。一个实用的技巧是动态分配的内存映射区域，在读取过后，可以异步回收掉。 mmap 使用场景使用 mmap 处理小数据的频繁读写如果 IO 非常频繁，数据却非常小，推荐使用 mmap，以避免 FileChannel 导致的切态问题。例如索引文件的追加写。 mmap 缓存当使用 FileChannel 进行文件读写时，往往需要一块写入缓存以达到聚合的目的，最常使用的是堆内/堆外内存，但他们都有一个问题，即当进程挂掉后，堆内/堆外内存会立刻丢失，这一部分没有落盘的数据也就丢了。而使用 mmap 作为缓存，会直接存储在 pageCache 中，不会导致数据丢失，尽管这只能规避进程被 kill 这种情况，无法规避掉电。 小文件的读写恰恰和网传的很多言论相反，mmap 由于其不切态的特性，特别适合顺序读写，但由于 sun.nio.ch.FileChannelImpl#map(MapMode mode, long position, long size) 中 size 的限制，只能传递一个 int 值，所以，单次 map 单个文件的长度不能超过 2G，如果将 2G 作为文件大 or 小的阈值，那么小于 2G 的文件使用 mmap 来读写一般来说是有优势的。在 RocketMQ 中也利用了这一点，为了能够方便的使用 mmap，将 commitLog 的大小按照 1G 来进行切分。对的，忘记说了，RocketMQ 等消息队列一直在使用 mmap。 cpu 紧俏下的读写在大多数场景下，FileChannel 和读写缓冲的组合相比 mmap 要占据优势，或者说不分伯仲，但在 cpu 紧俏下的读写，使用 mmap 进行读写往往能起到优化的效果，它的根据是 mmap 不会出现用户态和内核态的切换，导致 cpu 的不堪重负（但这样承担起动态映射与异步回收内存的开销）。 特殊软硬件因素例如持久化内存 Pmem、不同代数的 SSD、不同主频的 CPU、不同核数的 CPU、不同的文件系统、文件系统的挂载方式…等等因素都会影响 mmap 和 filechannel read/write 的快慢，因为他们对应的系统调用是不同的。只有 benchmark 过后，方知快慢。","link":"/learn-mmap/"},{"title":"Linux 环境写文件如何稳定跑满磁盘 I&#x2F;O 带宽?","text":"准备 要求 机器配置 测试磁盘 IO 性能 实验一: Buffer IO 写入 实验二: 4K 单次 Direct IO 写入 实验三: mmap 写入 实验四: 改进的 mmap 写入 结论 准备要求在 ** 限制内存 ** 的情况下，假定我们每次写入 4k 的数据，如何保证 kill -9 不丢数据的情况下，仍然稳定的跑满磁盘的 IO？因为需要保证 kill -9 不丢数据，所以 fwrite() 就不在我们的考虑范围之内了. 又因为限制内存，所以直观的想法是直接 Direct IO, 但 Direct IO 能否跑满磁盘 IO 呢? 机器配置CPU: 64 核 Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz ** 磁盘 **: Intel Optane SSD 测试磁盘 IO 性能官方称读 / 写带宽是 2400/2000 MB/s, 我们利用 fio 来进行实测: 顺序读性能: 1sudo fio --filename=test -iodepth=64 -ioengine=libaio --direct=1 --rw=read --bs=2m --size=2g --numjobs=4 --runtime=10 --group_reporting --name=test-read 结果: 1READ: bw=2566MiB/s (2691MB/s), 2566MiB/s-2566MiB/s (2691MB/s-2691MB/s), io=8192MiB (8590MB), run=3192-3192msec 顺序写性能: 1sudo fio --filename=test -iodepth=64 -ioengine=libaio -direct=1 -rw=write -bs=1m -size=2g -numjobs=4 -runtime=20 -group_reporting -name=test-write 结果: 1WRITE: bw=2181MiB/s (2287MB/s), 2181MiB/s-2181MiB/s (2287MB/s-2287MB/s), io=8192MiB (8590MB), run=3756-3756msec 实测读写带宽: 2566/2181 MB/s 实验一: Buffer IO 写入因为是限制内存，所以 Buffer IO 不在我们的考虑范围内，但是我们先来测试一下 Buffer IO 的具体性能到底如何? 我们使用最简单的方法，因为我们的 CPU 核数是 64，所以直接 64 线程单次 4K 字节 Buffer IO 写入, 即通过操作系统的 Page Cache 的策略来缓存，刷盘: ** 代码片段 **: 完整代码 12345678910111213141516171819202122static char data[4096] attribute((aligned(4096))) = {'a'}; void writer(int index) { std::string fname = &quot;data&quot; + std::to_string(index); int data_fd = ::open(fname.c_str(), O_RDWR | O_CREAT | O_APPEND, 0645); for (int32_t i = 0; i &lt; 1000000; i++) { ::write(data_fd, data, 4096); } close(data_fd); } int main() { std::vectorstd::thread threads; for(int i = 0; i &lt; 64; i++) { std::thread worker(writer, i); threads.push_back(std::move(worker)); } for (int i = 0; i &lt; 64; i++) { threads[i].join(); } return 0; } 我们通过 O_APPEND 单次 4k 追加写入，之后通过 vmstat 来保留 120s 的写入带宽: 1vmstat 1 120 &gt; buffer_io 经过最后的测试数据整理，我们发现 Buffer IO 的性能基本能稳定跑满带宽, 其中只有一次 I/O 抖动: 实验二: 4K 单次 Direct IO 写入Buffer IO 利用 Page Cache 帮助我们缓存了大量的数据，其实必然提高了写入带宽，但假如在限制内存的情况下，Buffer IO 就不是正确的解决方案了，这次我们绕过 Page Cache, 直接 Direct IO 单次 4K 写入: ** 代码片段 **: 完整代码 唯一需要修改的地方就是在 open() 中加入 O_DIRECT 标志: 1int data_fd = ::open(fname.c_str(), O_RDWR | O_CREAT | O_APPEND | O_DIRECT, 0645); 通过 vmstat 获取写入带宽数据, 整理如下: 通过数据我们发现，单次 4k 的 Direct IO 写入无法跑满磁盘的 I/O 带宽，仅仅只有 800MB/S 实验三: mmap 写入通过前面这两个实验我们发现，Buffer IO 是可以跑满磁盘 I/O 的，那我们可以尝试模拟 Buffer IO 的写入方式，使用较少的内存来达到 Buffer IO 的写入效果. 我们使用 mmap 来实现 Buffer IO 写入，通过限定的 Buffer Block 来模拟 Page Cache 的聚合效果, 实验中我们使用 memcpy 来完成数据拷贝，Buffer Block 我们设定为 4K * 4, 与 Direct IO 的不同，我们这次限定即 16KB 的单次写入: 代码片段: 完整代码 main() 函数不变，修改线程的 writer() 函数: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static char data[4096] attribute((aligned(4096))) = {'a'};static int32_t map_size = 4096 * 4;void MapRegion(int fd, uint64_t file_offset, char** base) { void* ptr = mmap(nullptr, map_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, file_offset); if (unlikely(ptr == MAP_FAILED)) { *base = nullptr; return; } base = reinterpret_cast&lt;char&gt;(ptr); } void UnMapRegion(char* base) { munmap(base, map_size); } void writer(int index) { std::string fname = &quot;data&quot; + std::to_string(index); char* base = nullptr; char* cursor = nullptr; uint64_t mmap_offset = 0, file_offset = 0; int data_fd = ::open(fname.c_str(), O_RDWR | O_CREAT, 0645); posix_fallocate(data_fd, 0, (4096UL * 1000000)); MapRegion(data_fd, 0, &amp;base); if (unlikely(base == nullptr)) { return; } cursor = base; file_offset += map_size; for (int32_t i = 0; i &lt; 1000000; i++) { if (unlikely(mmap_offset &gt;= map_size)) { UnMapRegion(base); MapRegion(data_fd, file_offset, &amp;base); if (unlikely(base == nullptr)) { return; } cursor = base; file_offset += map_size; mmap_offset = 0; } memcpy(cursor, data, 4096); cursor += 4096; mmap_offset += 4096; } UnMapRegion(base); close(data_fd); } 我们通过 vmstat 来获取写入带宽数据，我们发现 mmap 的 16K 写入可以跑满磁盘带宽，但 I/O 抖动较大，无法类似于 Buffer IO 稳定的写入. 我们通过 perf 生成火焰图分析: 通过 pref 生成分析瓶颈时发现，写入 writer() 时触发了大量的 Page Fault, 即缺页中断，而 mmap() 本身的调用也有一定的消耗 (关于 mmap() 的源码分析，我们在后面的文章会详细分析 )，我们实验三的思路是: 首先 fallocate 一个大文件，然后 mmap() 内存映射 16k 的 Block, memcpy() 写满之后，游标右移重新 mmap()，以此循环. 实验四: 改进的 mmap 写入为了避免 mmap() 的开销，我们使用临时文件在写入之前 mmap() 映射，之后循环利用这 16K 的 Block, 避免 mmap() 的巨大开销: 代码片段: 完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void MapRegion(int fd, uint64_t file_offset, char** base) { void* ptr = mmap(nullptr, map_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, file_offset); if (unlikely(ptr == MAP_FAILED)) { *base = nullptr; return; } *base = reinterpret_cast&lt;char*&gt;(ptr);}void UnMapRegion(char* base) { munmap(base, map_size);}void writer(int index) { std::string fname = &quot;data&quot; + std::to_string(index); std::string batch = &quot;batch&quot; + std::to_string(index); char* base = nullptr; char* cursor = nullptr; uint64_t mmap_offset = 0, file_offset = 0; int data_fd = ::open(fname.c_str(), O_RDWR | O_CREAT | O_DIRECT, 0645); int batch_fd = ::open(batch.c_str(), O_RDWR | O_CREAT | O_DIRECT, 0645); posix_fallocate(data_fd, 0, (4096UL * 1000000)); posix_fallocate(batch_fd, 0, map_size); MapRegion(batch_fd, 0, &amp;base); if (unlikely(base == nullptr)) { return; } cursor = base; file_offset += map_size; for (int32_t i = 0; i &lt; 1000000; i++) { if (unlikely(mmap_offset &gt;= map_size)) { pwrite64(data_fd, base, map_size, file_offset); cursor = base; file_offset += map_size; mmap_offset = 0; } memcpy(cursor, data, 4096); cursor += 4096; mmap_offset += 4096; } UnMapRegion(base); close(data_fd); close(batch_fd);} 使用 vmstat 来获取写入速度的数据, 整理如下: 这次避免了 mmap() 的开销，写入速度可以稳定保持在 2180 MB/S 左右，且没有 I/O 抖动. 内存使用也仅仅只有 18000KB, 大约 18M: 结论下面是四种方式的写入速度对比: 在限制内存，且需要 kill -9 不丢数据的情况下，我们可以使用 mmap() 来模拟 Buffer IO，但为了避免频繁 mmap() 的开销，我们需要临时文件来做我们的内存映射. 这种方法可以保证我们的写入速度稳定且 kill -9 不至于丢失数据.","link":"/linux-io-benchmark/"},{"title":"一个看板娘入住你的个人博客只需要三步","text":"最近在浏览别人博客时，一个萌物给了我意外的惊喜，原来博客还可以这么玩 于是果断审查元素，发现这个萌萌哒的看板娘背后使用的是一个叫 live2d 的技术，并且凭借 Google 迅速找到对应的开源代码：https://github.com/xiazeyu/live2d-widget.js，https://github.com/EYHN/hexo-helper-live2d 你可以在我的博客中先目睹下它的实际效果：https://www.cnkirito.moe/，点击会有音效哦~ 在浏览 live2-widget.js 的说明文档时，发现它对 hexo 的支持非常友好，恰好我的博客是通过 hexo 搭建的，所以本文会介绍一下如何为 hexo 集成一只看板娘。 安装使用 npm 在 hexo 下安装 hexo-helper-live2d，它将 live2d-widget.js 与 hexo 进行了整合，使得我们只需要通过简单的配置，即可生效 1npm install --save hexo-helper-live2d 接着下载一个 Kirito 良心推荐的看板娘：shizuku，也是我正在用的 1npm install live2d-widget-model-shizuku 如果安装成功，我们可以在 node_modules 目录中找到 live2d-widget 和 live2d-widget-model-shizuku 两个目录 配置向 hexo 的 _config.yml 添加如下的配置 123456789101112131415161718live2d: enable: true scriptFrom: local pluginRootPath: node_modules/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false debug: false model: use: live2d-widget-model-shizuku display: position: right width: 150 height: 300 mobile: show: true react: opacity: 0.7 需要留意的是 live2d.model.use 这个标签的值，是用于指定博客使用哪一个看板娘模型的。 发布接着只需要正常编译并发布，你的 hexo 博客就获得了一枚萌妹子了 12hexo ghexo d 模型推荐除了上述推荐的 shizuku 看板娘，作者还提供了其他一些不错的模型，下面罗列一部分，不知道你会 pick 哪一个呢？ miku haru koharu haruto nipsilon shizuku z16 hibiki Unitychan hijiki 尾记Live2d 的确是很有意思的一个技术，可以让静态博客有了一些生机，如果你对其感兴趣，还可以自己采集模型，自己发布模型，甚至在了解这样技术的同时，我还见识到一些科技感满满的博主把 AI 机器人的特性添加给了看板娘，让其可以与正在使用鼠标浏览的你进行互动，这些我就不过多介绍了，如果你也有一个博客，那就赶紧试试这个 idea 吧。","link":"/live2d/"},{"title":"阿里巴巴中间件技术部春招开始啦","text":"阿里巴巴中间件技术部 - aPaaS 产品团队春招意向沟通正式开始啦！三月前我们会完成和同学的意向沟通，三月初正式进入招聘流程。 简历投递：jingfeng.xjf@alibaba-inc.com 意向沟通联系微信：xiayimiaoshenghua 为什么考虑加入我们？阿里最核心的技术部门之一作为整个集团技术的“枢纽”，中间件技术产品向上支撑了淘宝、天猫、盒马、菜鸟、高德等几乎所有阿里巴巴集团的核心业务，向下整合存储、计算、网络等阿里云基础设施，是历年来阿里巴巴技术架构升级，618/双十一/双十二等大促活动的核心技术部门。业务体量大，技术挑战大，加入以后能力成长飞快。 阿里业务增长最快的部门之一云计算是阿里巴巴业务增长最快的板块之一，中间件技术部隶属于阿里云-基础产品事业部，是阿里的明星团队之一。在这里你绝对不用担心沦为大公司的“螺丝钉” - 中间件这几年处于高速增长期，许多新的产品、新的项目等你来主导，在享受加入阿里集团带来的稳定性、专业性以及平台优势的同时，又能体会到加入初创公司的激情、高效与掌控力。 阿里技术影响力最大的部门之一这里诞生了RocketMQ，Spring Cloud Alibaba、Dubbo、Arthas、Nacos等开源项目，在国内的微服务领域具备深远的影响力。我们每年都会举办中间件技术挑战赛、邀请业界各路大牛切磋技艺；这里是阿里“技术中台”的发源地，我们服务了许多阿里以外的知名互联网企业，我们为许多行业龙头企业提供了微服务相关的最佳实践和解决方案，帮助许多传统企业完成了数字化转型。在这里你能够全面提升你的个人技术影响力、沟通能力和行业知名度。 加入以后做哪些事？ 微服务框架与架构治理方向 加入Apache ASF成员领军、多位Apache PMC坐镇的阿里巴巴中间件微服务团队，参与顶级开源项目的建设；深入阿里“技术中台“的腹地，作为核心技术骨干参与双十一大促保障，并将阿里多年双十一稳定性保障与架构治理的沉淀产品化，为中国乃至全球的中大型企业赋能。 分布式链路追踪与故障定位方向 参与建设中国乃至世界最大规模的分布式链路跟踪与监控系统，处理每天数十PB级的分布式调用轨迹数据，通过机器学习技术完成故障自动根因定位、故障自愈，探索最前沿的数据可视化技术。 分布式应用托管与Serverless技术方向 探索云计算领域最前沿的Serverless技术，结合阿里云容器团队、内核团队的诸多黑科技，颠覆传统应用托管的使用体验，打造极致弹性、超低成本的新一代云计算产品。 面向群体招聘群体是哪些？ 2021年国内毕业的高校应届毕业生。 2020年海外毕业的高校应届毕业生。 岗位有哪些要求？熟悉Java、Golang或C++至少一门语言。有良好的计算机基础，基本功扎实：熟悉常用的算法、数据结构，熟悉操作系统、网络等基础知识。 加分项（任一即可）： 有参与过开源技术项目经验，或有大赛获奖经验，或有优秀论文发表者。 熟悉微服务领域相关技术或产品，如：gRPC，Dubbo，Spring Cloud，RocketMQ等。 熟悉容器相关技术或产品，如Docker, Kubernetes, Istio/Envoy等。 熟悉监控领域相关技术或产品，如：Prometheus，Zipkin，OpenTracing，Zabbix等。 熟悉数据处理相关技术或产品，如：Flink，Spark，HBase，ElasticSearch等。 熟悉机器学习技术，有相关实战经验。 简历投递：jingfeng.xjf@alibaba-inc.com 意向沟通联系微信：xiayimiaoshenghua","link":"/middleware-hire/"},{"title":"Motan 中使用异步 RPC 接口","text":"这周六参加了一个美团点评的技术沙龙，其中一位老师在介绍他们自研的 RPC 框架时提到一点：RPC 请求分为 sync，future，callback，oneway，并且需要遵循一个原则：能够异步的地方就不要使用同步。正好最近在优化一个业务场景：在一次页面展示中，需要调用 5 个 RPC 接口，导致页面响应很慢。正好启发了我。 为什么慢？大多数开源的 RPC 框架实现远程调用的方式都是同步的，假设 [接口 1，…，接口 5] 的每一次调用耗时为 200ms （其中接口 2 依赖接口 1，接口 5 依赖接口 3，接口 4），那么总耗时为 1s，这整个是一个串行的过程。 多线程加速 第一个想到的解决方案便是多线程，那么 [1=&gt;2] 编为一组，[[3,4]=&gt;5]编为一组，两组并发执行，[1=&gt;2]串行执行耗时 400ms，[3,4]并发执行耗时 200ms，[[3,4]=&gt;5]总耗时 400ms ，最终 [[1=&gt;2],[[3,4]=&gt;5]] 总耗时 400ms（理论耗时）。相比较于原来的 1s，的确快了不少，但实际编写接口花了不少功夫，创建线程池，管理资源，分析依赖关系… 总之代码不是很优雅。 RPC 中，多线程着重考虑的点是在客户端优化代码，这给客户端带来了一定的复杂性，并且编写并发代码对程序员的要求更高，且不利于调试。 异步调用如果有一种既能保证速度，又能像同步 RPC 调用那样方便，岂不美哉？于是引出了 RPC 中的异步调用。 在 RPC 异步调用之前，先回顾一下 java.util.concurrent 中的基础知识：Callable 和 Future 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class Main { public static void main(String[] args) throws Exception{ final ExecutorService executorService = Executors.newFixedThreadPool(10); long start = System.currentTimeMillis(); Future&lt;Integer&gt; resultFuture1 = executorService.submit(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { return method1()+ method2(); } }); Future&lt;Integer&gt; resultFuture2 = executorService.submit(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { Future&lt;Integer&gt; resultFuture3 = executorService.submit(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { return method3(); } }); Future&lt;Integer&gt; resultFuture4 = executorService.submit(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { return method4(); } }); return method5()+resultFuture3.get()+resultFuture4.get(); } }); int result = resultFuture1.get()+ resultFuture2.get(); System.out.println(&quot;result =&quot;+result+&quot;, total cost&quot;+(System.currentTimeMillis()-start)+&quot;ms&quot;); executorService.shutdown(); } static int method1(){ delay200ms(); return 1; } static int method2(){ delay200ms(); return 2; } static int method3(){ delay200ms(); return 3; } static int method4(){ delay200ms(); return 4; } static int method5(){ delay200ms(); return 5; } static void delay200ms(){ try{ Thread.sleep(200); }catch (Exception e){ e.printStackTrace(); } }} 最终控制台打印： result = 15, total cost 413 ms 五个接口，如果同步调用，便是串行的效果，最终耗时必定在 1s 之上，而异步调用的优势便是，submit 任务之后立刻返回，只有在调用 future.get() 方法时才会阻塞，而这期间多个异步方法便可以并发的执行。 RPC 异步调用我们的项目使用了 Motan 作为 RPC 框架，查看其 changeLog ，0.3.0 (2017-03-09) 该版本已经支持了 async 特性。可以让开发者很方便地实现 RPC 异步调用。 **1 为接口增加 @MotanAsync 注解 ** 1234@MotanAsyncpublic interface DemoApi { DemoDto randomDemo(String id);} **2 添加 Maven 插件 ** 12345678910111213141516171819202122&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sources&gt; &lt;source&gt;${project.build.directory}/generated-sources/annotations&lt;/source&gt; &lt;/sources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 安装插件后，可以借助它生成一个和 DemoApi 关联的异步接口 DemoApiAsync 。 123public interface DemoApiAsync extends DemoApi { ResponseFuture randomDemoAsync(String id);} **3 注入接口即可调用 ** 123456789101112131415161718192021@Servicepublic class DemoService { @MotanReferer DemoApi demoApi; @MotanReferer DemoApiAsync demoApiAsync;//&lt;1&gt; public DemoDto randomDemo(String id){ DemoDto demoDto = demoApi.randomDemo(id); return demoDto; } public DemoDto randomDemoAsync(String id){ ResponseFuture responseFuture = demoApiAsync.randomDemoAsync(id);//&lt;2&gt; DemoDto demoDto = (DemoDto) responseFuture.getValue(); return demoDto; }} &lt;1&gt; DemoApiAsync 如何生成的已经介绍过，它和 DemoApi 并没有功能性的区别，仅仅是同步异步调用的差距，而 DemoApiAsync 实现的的复杂性完全由 RPC 框架帮助我们完成，开发者无需编写 Callable 接口。 &lt;2&gt; ResponseFuture 是 RPC 中 Future 的抽象，其本身也是 juc 中 Future 的子类，当 responseFuture.getValue() 调用时会阻塞。 总结在异步调用中，如果发起一次异步调用后，立刻使用 future.get()，则大致和同步调用等同。其真正的优势是在 submit 和 future.get() 之间可以混杂一些非依赖性的耗时操作，而不是同步等待，从而充分利用时间片。 另外需要注意，如果异步调用涉及到数据的修改，则多个异步操作直接不能保证 happens-before 原则，这属于并发控制的范畴了，谨慎使用。查询操作则大多没有这样的限制。 在能使用并发的地方使用并发，不能使用的地方才选择同步，这需要我们思考更多细节，但可以最大限度的提升系统的性能。","link":"/motan-async/"},{"title":"天池中间件大赛百万队列存储设计总结【复赛】","text":"维持了 20 天的复赛终于告一段落了，国际惯例先说结果，复赛结果不太理想，一度从第 10 名掉到了最后的第 36 名，主要是写入的优化卡了 5 天，一直没有进展，最终排名也是定格在了排行榜的第二页。痛定思痛，这篇文章将自己复赛中学习的知识，成功的优化，未成功的优化都罗列一下。 赛题介绍题面描述很简单：使用 Java 或者 C++ 实现一个进程内的队列引擎，单机可支持 100 万队列以上。 1234public abstract class QueueStore { abstract void put(String queueName, byte[] message); abstract Collection&lt;byte[]&gt; get(String queueName, long offset, long num);} 编写如上接口的实现。 put 方法将一条消息写入一个队列，这个接口需要是线程安全的，评测程序会并发调用该接口进行 put，每个 queue 中的内容按发送顺序存储消息（可以理解为 Java 中的 List），同时每个消息会有一个索引，索引从 0 开始，不同 queue 中的内容，相互独立，互不影响，queueName 代表队列的名称，message 代表消息的内容，评测时内容会随机产生，大部分长度在 58 字节左右，会有少量消息在 1k 左右。 get 方法从一个队列中读出一批消息，读出的消息要按照发送顺序来，这个接口需要是线程安全的，也即评测程序会并发调用该接口进行 get，返回的 Collection 会被并发读，但不涉及写，因此只需要是线程读安全就可以了，queueName 代表队列的名字，offset 代表消息的在这个队列中的起始索引，num 代表读取的消息的条数，如果消息足够，则返回 num 条，否则只返回已有的消息即可，若消息不足，则返回一个空的集合。 ** 评测程序介绍 ** 发送阶段：消息大小在 58 字节左右，消息条数在 20 亿条左右，即发送总数据在 100G 左右，总队列数 100w 索引校验阶段：会对所有队列的索引进行随机校验；平均每个队列会校验 1~2 次；(随机消费) 顺序消费阶段：挑选 20% 的队列进行 ** 全部 ** 读取和校验； (顺序消费) 发送阶段最大耗时不能超过 1800s；索引校验阶段和顺序消费阶段加在一起，最大耗时也不能超过 1800s；超时会被判断为评测失败。 各个阶段线程数在 20~30 左右 测试环境为 4c8g 的 ECS，限定使用的最大 JVM 大小为 4GB(-Xmx 4g)。带一块 300G 左右大小的 SSD 磁盘。对于 Java 选手而言，可使用的内存可以理解为：堆外 4g 堆内 4g。 赛题剖析首先解析题面，接口描述是非常简单的，只有一个 put 和一个 get 方法。需要注意特别注意下评测程序，发送阶段需要对 100w 队列，每一次发送的量只有 58 字节，最后总数据量是 100g；索引校验和顺序消费阶段都是调用的 get 接口，不同之处在于前者索引校验是随机消费，后者是对 20% 的队列从 0 号索引开始进行全量的顺序消费，评测程序的特性对最终存储设计的影响是至关重要的。 复赛题目的难点之一在于单机百万队列的设计，据查阅的资料显示 Kafka 单机超过 64 个队列 / 分区，Kafka 分区数不宜过多 RocketMQ 单机支持最高 5 万个队列 至于百万队列的使用场景，只能想到 IOT 场景有这样的需求。相较于初赛，复赛的设计更加地具有不确定性，排名靠前的选手可能会选择大相径庭的设计方案。 复赛的考察点主要有以下几个方面：磁盘块读写，读写缓冲，顺序读写与随机读写，pageCache，稀疏索引，队列存储设计等。 由于复赛成绩并不是很理想，优化 put 接口的失败是导致失利的罪魁祸首，最终成绩是 126w TPS，而第一梯队的 TPS 则是到达了 200 w+ 的 TPS。鉴于此，不太想像初赛总结那样，按照优化历程罗列，而是将自己做的方案预研，以及设计思路分享给大家，对文件 IO 不甚了解的读者也可以将此文当做一篇科普向的文章来阅读。 思路详解确定文件读写方式作为忠实的 Java 粉丝，自然选择使用 Java 来作为参赛语言，虽然最终的排名是被 Cpp 大佬所垄断，但着实无奈，毕业后就把 Cpp 丢到一边去了。Java 中的文件读写接口大致可以分为三类： 标准 IO 读写，位于 java.io 包下，相关类：FileInputStream，FileOuputStream NIO 读写，位于 java.nio 包下，相关类：FileChannel，ByteBuffer Mmap 内存映射，位于 java.nio 包下，相关类：FileChannel，MappedByteBuffer 标准 IO 读写不具备调研价值，直接 pass，所以 NIO 和 Mmap 的抉择，成了第一步调研对象。 第一阶段调研了 Mmap。搜索一圈下来发现，几乎所有的文章都一致认为：Mmap 这样的内存映射技术是最快的。很多没有接触过内存映射技术的人可能还不太清楚这是一种什么样的技术，简而言之，Mmap 能够将文件直接映射到用户态的内存地址，使得对文件的操作不再是 write/read, 而转化为直接对内存地址的操作。 1234567891011121314151617public void test1() throws Exception { String dir = &quot;/Users/kirito/data/&quot;; ensureDirOK(dir); RandomAccessFile memoryMappedFile; int size = 1 * 1024 * 1024; try { memoryMappedFile = new RandomAccessFile(dir + &quot;testMmap.txt&quot;, &quot;rw&quot;); MappedByteBuffer mappedByteBuffer = memoryMappedFile.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, size); for (int i = 0; i &lt; 100000; i++) { mappedByteBuffer.position(i * 4); mappedByteBuffer.putInt(i); } memoryMappedFile.close(); } catch (Exception e) { e.printStackTrace(); }} 如上的代码呈现了一个最简单的 Mmap 使用方式，速度也是没话说，一个字：快！我怀着将信将疑的态度去找了更多的佐证，优秀的源码总是第一参考对象，观察下 RocketMQ 的设计，可以发现 NIO 和 Mmap 都出现在了源码中，但更多的读写操作似乎更加青睐 Mmap。RocketMQ 源码 org.apache.rocketmq.store.MappedFile 中两种写方法同时存在，请教 @匠心零度 后大概得出结论：RocketMQ 主要的写是通过 Mmap 来完成。 但是在实际使用 Mmap 来作为写方案时遇到了两大难题，单纯从使用角度来看，暴露出了 Mmap 的局限性： Mmap 在 Java 中一次只能映射 1.5~2G 的文件内存，但实际上我们的数据文件大于 100g，这带来了第一个问题：要么需要对文件做物理拆分，切分成多文件；要么需要对文件映射做逻辑拆分，大文件分段映射。RocketMQ 中限制了单文件大小来避免这个问题。 Mmap 之所以快，是因为借助了内存来加速，mappedByteBuffer 的 put 行为实际是对内存进行的操作，实际的刷盘行为依赖于操作系统的定时刷盘或者手动调用 mappedByteBuffer.force() 接口来刷盘，否则将会导致机器卡死（实测后的结论）。由于复赛的环境下内存十分有限，所以使用 Mmap 存在较难的控制问题。 经过这么一折腾，再加上资料的搜集，最终确定，**Mmap 在内存较为富足并且数据量小的场景下存在优势 **（大多数文章的结论认为 Mmap 适合大文件的读写，私以为是不严谨的结论）。 第二阶段调研 Nio 的 FileChannel，这也是我最终确定的读写方案。 由于每个消息只有 58 字节左右，直接通过 FileChannel 写入一定会遇到瓶颈，事实上，如果你这么做，复赛连成绩估计都跑不出来。另一个说法是 ssd 最小的写入单位是 4k，如果一次写入低于 4k，实际上耗时和 4k 一样。这里涉及到了赛题的一个重要考点：块读写。 根据阿里云的 ssd 云盘介绍，只有一次写入 16kb ~ 64kb 才能获得理想的 IOPS。文件系统块存储的特性，启发我们需要设置一个内存的写入缓冲区，单个消息写入内存缓冲区，缓冲区满，使用 FileChannel 进行刷盘。经过实践，使用 FileChannel 搭配缓冲区发挥的写入性能和内存充足情况下的 Mmap 并无区别，并且 FileChannel 对文件大小并无限制，控制也相对简单，所以最终确定使用 FileChannel 进行读写。 确定存储结构和索引结构由于赛题的背景是消息队列，评测 2 阶段的随机检测以及 3 阶段的顺序消费一次会读取多条连续的消息，并且 3 阶段的顺序消费是从队列的 0 号索引一直消费到最后一条消息，这些因素都启发我们：应当将同一个队列的消息尽可能的存到一起。前面一节提到了写缓冲区，便和这里的设计非常契合，例如我们可以一个队列设置一个写缓冲区（比赛中 Java 拥有 4g 的堆外内存，100w 队列，一个队列使用 DirectByteBuffer 分配 4k 堆外内存 ，可以保证缓冲区不会爆内存），这样同一个缓冲区的消息一起落盘，就保证了块内消息的顺序性，即做到了”同一个队列的消息尽可能的存到一起“。按块存取消息目前看来有两个优势： 按条读取消息 =&gt; 按块读取消息，发挥块读的优势，减少了 IO 次数 全量索引 =&gt; 稀疏索引。块内数据是连续的，所以只需要记录块的物理文件偏移量 + 块内消息数即可计算出某一条消息的物理位置。这样大大降低了索引的数量，稍微计算一下可以发现，完全可以使用一个 Map 数据结构，Key 为 queueName，Value 为 List 在内存维护队列块的索引。如果按照传统的设计方案：一个 queue 一个索引文件，百万文件必然会超过默认的系统文件句柄上限。索引存储在内存中既规避了文件句柄数的问题，速度也不必多数，文件 IO 和 内存 IO 不是一个量级。 由于赛题规定消息体是非定长的，大多数消息 58 字节，少量消息 1k 字节的数据特性，所以存储消息体时使用 short+byte[] 的结构即可，short 记录消息的实际长度，byte[] 记录完整的消息体。short 比 int 少了 2 个字节，2*20 亿消息，可以减少 4g 的数据量。 稠密索引是对全量的消息进行索引，适用于无序消息，索引量大，数据可以按条存取。 稀疏索引适用于按块存储的消息，块内有序，适用于有序消息，索引量小，数据按照块进行存取。 由于消息队列顺序存储，顺序消费的特性，加上 ssd 云盘最小存取单位为 4k（远大于单条消息）的限制，所以稀疏索引非常适用于这种场景。至于数据文件，可以做成参数，根据实际测试来判断到底是多文件效果好，还是单文件，此方案支持 100g 的单文件。 内存读写缓冲区在稀疏索引的设计中，我们提到了写入缓冲区的概念，根据计算可以发现，100w 队列如果一个队列分配一个写入缓冲区，最多只能分配 4k，这恰好是最小的 ssd 写入块大小（但根据之前 ssd 云盘给出的数据来看，一次写入 64k 才能打满 io）。 一次写入 4k，这导致物理文件中的块大小是 4k，在读取时一次同样读取出 4k。 123456789101112131415// 写缓冲区private ByteBuffer writeBuffer = ByteBuffer.allocateDirect(4 * 1024);// 用 short 记录消息长度private final static int SINGLE_MESSAGE_SIZE = 2;public void put(String queueName,byte[] message){ // 缓冲区满，先落盘 if (SINGLE_MESSAGE_SIZE + message.length &gt; writeBuffer.remaining()) { // 落盘 flush(); } writeBuffer.putInt(SINGLE_MESSAGE_SIZE); writeBuffer.put(message); this.blockLength++;} 不足 4k 的部分可以选择补 0，也可以跳过。评测程序保证了在 queue 级别的写入是同步的，所以对于同一个队列，我们无法担心同步问题。写入搞定之后，同样的逻辑搞定读取，由于 get 操作是并发的，2 阶段和 3 阶段会有 10~30 个线程并发消费同一个队列，所以 get 操作的读缓冲区可以设计成 ThreadLocal&lt;ByteBuffer&gt; ，每次使用时 clear 即可，保证了缓冲区每次读取时都是崭新的，同时减少了读缓冲区的创建，否则会导致频繁的 full gc。读取的伪代码暂时不贴，因为这样的 get 方案不是最终方案。 到这里整体的设计架构已经出来了，写入流程和读取流程的主要逻辑如下： 写入流程： 读取流程： 内存读缓存优化方案设计经过好几次的推翻重来，才算是确定了上述的架构，这样的架构优势在于非常简单明了，实际上我的第一版设计方案的代码量是上述方案代码量的 23 倍，但实际效果却不理想。上述架构的跑分成绩大概可以达到 7080w TPS，只能算作是第三梯队的成绩，在此基础上，进行了读取缓存的优化才达到了 126w 的 TPS。在介绍读取缓存优化之前，先容我介绍下 PageCache 的概念。 Linux 内核会将它最近访问过的文件页面缓存在内存中一段时间，这个文件缓存被称为 PageCache。如上图所示。一般的 read() 操作发生在应用程序提供的缓冲区与 PageCache 之间。而预读算法则负责填充这个 PageCache。应用程序的读缓存一般都比较小，比如文件拷贝命令 cp 的读写粒度就是 4KB；内核的预读算法则会以它认为更合适的大小进行预读 I/O，比如 16-128KB。 所以一般情况下我们认为顺序读比随机读是要快的，PageCache 便是最大的功臣。 回到题目，这简直 nice 啊，因为在磁盘中同一个队列的数据是部分连续（同一个块则连续），实际上一个 4KB 块中大概可以存储 70 多个数据，而在顺序消费阶段，一次的 offset 一般为 10，有了 PageCache 的预读机制，7 次文件 IO 可以减少为 1 次！这可是不得了的优化，但是上述的架构仅仅只有 70~80w 的 TPS，这让我产生了疑惑，经过多番查找资料，最终在 @江学磊 的提醒下，才定位到了问题。 两种可能导致比赛中无法使用 pageCache 来做缓存 由于我使用 FIleChannel 进行读写，NIO 的读写可能走的正是 Direct IO，所以根本不会经过 PageCache 层。 测评环境中内存有限，在 IO 密集的情况下 PageCache 效果微乎其微。 虽然说不确定到底是何种原因导致 PageCache 无法使用，但是我的存储方案仍然满足顺序读取的特性，完全可以自己使用堆外内存自己模拟一个“PageCache”，这样在 3 阶段顺序消费时，TPS 会有非常高的提升。 一个队列一个读缓冲区用于顺序读，又要使得 get 阶段不存在并发问题，所以我选择了复用读缓冲区，并且给 get 操作加上了队列级别的锁，这算是一个小的牺牲，因为 2 阶段不会发生冲突，3 阶段冲突概率也并不大。改造后的读取缓存方案如下： 经过缓存改造之后，使用 Direct IO 也可以实现类似于 PageCache 的优化，并且会更加的可控，不至于造成频繁的缺页中断。经过这个优化，加上一些 gc 的优化，可以达到 126w TPS。整体方案算是介绍完毕。 其他优化还有一些优化对整体流程影响不大，拎出来单独介绍。 2 阶段的随机索引检测和 3 阶段的顺序消费可以采取不同的策略，2 阶段可以直接读取所需要的数据，而不需要进行缓存（因为是随机检测，所以读缓存肯定不会命中）。 将文件数做成参数，调整参数来判断到底是多文件 TPS 高还是单文件，实际上测试后发现，差距并不是很大，单文件效果略好，由于是 ssd 云盘，又不存在磁头，所以真的不太懂原理。 gc 优化，能用数组的地方不要用 List。尽量减少小对象的出现，可以用数组管理基本数据类型，小对象对 gc 非常不友好，无论是初赛还是复赛，Java 比 Cpp 始终差距一个垃圾回收机制。必须保证全程不出现 full gc。 失败的优化与反思本次比赛算是留下了不小的遗憾，因为写入的优化一直没有做好，读取缓存做好之后我 2 阶段和 3 阶段的总耗时相加是 400+s，算是不错的成绩，但是写入耗时在 1300+s。我上述的方案采用的是多线程同步刷盘，但也尝试过如下的写入方案： 异步提交写缓冲区，单线程直接刷盘 异步提交写缓冲区，设置二级缓冲区 64k~64M，单线程使用二级缓冲区刷盘 同步将写缓冲区的数据拷贝至一个 LockFreeQueue，单线程平滑消费，以打满 IOPS 每 16 个队列共享一个写入缓冲区，这样控制写入缓冲区可以达到 64k，在刷盘时进行排序，将同一个 queue 的数据放置在一起。 但都以失败告终，没有 get 到写入优化的要领，算是本次比赛最大的遗憾了。 还有一个失误在于，评测环境使用的云盘 ssd 和我的本地 Mac 下的 ssd 存储结构差距太大，加上 mac os 和 Linux 的一些差距，导致本地成功的优化在线上完全体现不出来，还是租个阿里云环境比较靠谱。 另一方面的反思，则是对存储和 MQ 架构设计的不熟悉，对于 Kafka 和 RocketMQ 所做的一些优化也都是现学现用，不太确定用的对不对，导致走了一些弯路，而比赛中认识的一个 96 年的小伙子王亚普，相比之下对中间件知识理解的深度和广度实在令我钦佩，实在还有很多知识需要学习。 参赛感悟第一感受是累，第二感受是爽。相信很多选手和我一样是工作党，白天工作，只能腾出晚上的时间去搞比赛，对于 966 的我真是太不友好了，初赛时间延长了一次还算给缓了一口气，复赛一眨眼就过去了，想翻盘都没机会，实在是遗憾。爽在于这次比赛真的是汗快淋漓地实践了不少中间件相关的技术，初赛的 Netty，复赛的存储设计，都是难以忘怀的回忆，比赛中也认识了不少朋友，有学生党，有工作党，感谢你们不厌其烦的教导与发人深省的讨论，从不同的人身上是真的可以学到很多自己缺失的知识。 据消息说，阿里中间件大赛很有可能是最后一届，无论是因为什么原因，作为参赛者，我都感到深深的惋惜，希望还能有机会参加下一届的中间件大赛，也期待能看到更多的相同类型的赛事被各大互联网公司举办，和大佬们同台竞技，一边认识更多新朋友的感觉真棒。 虽然最终无缘决赛，但还是期待进入决赛的 11 位选手能带来一场精彩的答辩，也好解答我始终优化失败的写入方案。后续会考虑吸收下前几名 JAVA 的优化思路，整理成最终完善的方案。目前方案的 git 地址，仓库已公开：https://code.aliyun.com/250577914/queuerace2018.git ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/mq-million-queue/"},{"title":"《微服务》九大特性笔记","text":"服务组件化组件，是一个可以独立更换和升级的单元。就像 PC 中的 CPU、内存、显卡、硬盘一样，独立且可以更换升级而不影响其他单元。 在“微服务”架构中，需要我们对服务进行组件化分解。服务，是一种进程外的组件，它通过 http 等通信协议进行协作，而不是传统组件以嵌入的方式协同工作。服务都独立开发、部署，可以有效的避免一个服务的修改引起整个系统的重新部署。 打一个不恰当的比喻，如果我们的 PC 组件以服务的方式构建，我们只维护主板和一些必要外设之后，计算能力通过一组外部服务实现，我们只需要告诉 PC 我们从哪个地址来获得计算能力，通过服务定义的计算接口来实现我们使用过程中的计算需求，从而实现 CPU 组件的服务化。这样我们原本复杂的 PC 服务得到了更轻量化的实现，我们甚至只需要更换服务地址就能升级我们 PC 的计算能力。 按业务组织团队当我们开始决定如何划分“微服务”时，通常也意味着我们要开始对团队进行重新规划与组织。按以往的方式，我们往往会以技术的层面去划分多个不同的团队，比如：DBA 团队、运维团队、后端团队、前端团队、设计师团队等等。若我们继续按这种方式组织团队来实施“微服务”架构开发时，当有一个有问题需要更改，可能是一个非常简单的变动，比如：对人物描述增加一个字段，这就需要从数据存储开始考虑一直到设计和前端，虽然大家的修改都非常小，但这会引起跨团队的时间和预算审批。 在实施“微服务”架构时，需要采用不同的团队分割方法。由于每一个微服务都是针对特定业务的宽栈或是全栈实现，既要负责数据的持久化存储，又要负责用户的接口定义等各种跨专业领域的职能。因此，面对大型项目时候，对于微服务团队拆分更加建议按业务线的方式进行拆分，一方面可以有效减少服务内部修改所产生的内耗；另一方面，团队边界可以变得更为清晰。 做“产品”的态度实施“微服务”架构的团队中，每个小团队都应该以做产品的方式，对其产品的整个生命周期负责。而不是以项目的模式，以完成开发与交付并将成果交接给维护者为最终目标。 开发团队通过了解服务在具体生产环境中的情况，可以增加他们对具体业务的理解，比如：很多时候一些业务中发生的特殊或异常情况，很可能产品经理都并不知晓，但细心的开发者很容易通过生产环境发现这些特殊的潜在问题或需求。 所以，我们需要用做“产品”的态度来对待每一个“微服务”，持续关注服务的运作情况，并不断地分析帮助用户来提升业务功能。 智能端点与哑管道在单体应用中，组件间直接通过函数调用的方式进行交互协作。而在“微服务”架构中，服务由于不在一个进程中，组件间的通信模式发生了改变，若仅仅将原本在进程内的方法调用改成 RPC 方式的调用，会导致微服务之间产生繁琐的通信，使得系统表现更为糟糕，所以，我们需要更粗粒度的通信协议。 在“微服务”架构中，通常会使用这两个服务调用方式： 第一种，使用 HTTP 协议的 RESTful API 或轻量级的消息发送协议，来实现信息传递与服务调用的触发。第二种，通过在轻量级消息总线上传递消息，类似 RabbitMQ 等一些提供可靠异步交换的结构。 在极度强调性能的情况下，有些团队会使用二进制的消息发送协议，例如：protobuf。即使是这样，这些系统仍然会呈现出“智能端点和哑管道”的特点，为了在易读性与高效性之间取得平衡。当然大多数 Web 应用或企业系统并不需要作出在这两者间做出选择，能够获得易读性就已经是一个极大的胜利了。——Martin Fowler 去中心化治理当我们采用集中化的架构治理方案时，通常在技术平台上都会做同一的标准，但是每一种技术平台都有其短板，这会导致在碰到短板时，不得不花费大力气去解决，并且可能还是因为其底层原因解决的不是很好。 在实施“微服务”架构时，通过采用轻量级的契约定义接口，使得我们对于服务本身的具体技术平台不再那么敏感，这样我们整个“微服务”架构的系统中的组件就能针对其不同的业务特点选择不同的技术平台，终于不会出现杀鸡用牛刀或是杀牛用指甲钳的尴尬处境了。 不是每一个问题都是钉子，不是每一个解决方案都是锤子 去中心化管理数据我们在实施“微服务”架构时，都希望可以让每一个服务来管理其自有的数据库，这就是数据管理的去中心化。 在去中心化过程中，我们除了将原数据库中的存储内容拆分到新的同平台的其他数据库实例中之外（如：把原本存储在 MySQL 中的表拆分后，存储多几个不同的 MySQL 实例中），也可以针对一些具有特殊结构或业务特性的数据存储到一些其他技术的数据库实例中（如：把日志信息存储到 MongoDB 中、把用户登录信息存储到 Redis 中）。 虽然，数据管理的去中心化可以让数据管理更加细致化，通过采用更合适的技术来让数据存储和性能达到最优。但是，由于数据存储于不同的数据库实例中后，数据一致性也成为“微服务”架构中急需解决的问题之一。分布式事务的实现，本身难度就非常大，所以在“微服务”架构中，我们更强调在各服务之间进行“无事务”的调用，而对于数据一致性，只要求数据在最后的处理状态是一致的效果；若在过程中发现错误，通过补偿机制来进行处理，使得错误数据能够达到最终的一致性。 基础设施自动化近年来云计算服务与容器化技术的不断成熟，运维基础设施的工作变得越来越不那么难了。但是，当我们实施“微服务”架构时，数据库、应用程序的个头虽然都变小了，但是因为拆分的原因，数量成倍的增长。这使得运维人员需要关注的内容也成倍的增长，并且操作性任务也会成倍的增长，这些问题若没有得到妥善的解决，必将成为运维人员的噩梦。 所以，在“微服务”架构中，请务必从一开始就构建起“持续交付”平台来支撑整个实施过程，该平台需要两大内容，不可或缺： 自动化测试：每次部署前的强心剂，尽可能的获得对正在运行软件的信心。自动化部署：解放繁琐枯燥的重复操作以及对多环境的配置管理。 容错设计在单体应用中，一般不存在单个组件故障而其他还在运行的情况，通常是一挂全挂。而在“微服务”架构中，由于服务都运行在独立的进程中，所以是存在部分服务出现故障，而其他服务都正常运行的情况，比如：当正常运作的服务 B 调用到故障服务 A 时，因故障服务 A 没有返回，线程挂起开始等待，直到超时才能释放，而此时若触发服务 B 调用服务 A 的请求来自服务 C，而服务 C 频繁调用服务 B 时，由于其依赖服务 A，大量线程被挂起等待，最后导致服务 A 也不能正常服务，这时就会出现故障的蔓延。 所以，在“微服务”架构中，快速的检测出故障源并尽可能的自动恢复服务是必须要被设计和考虑的。通常，我们都希望在每个服务中实现监控和日志记录的组件，比如：服务状态、断路器状态、吞吐量、网络延迟等关键数据的仪表盘等。 演进式设计通过上面的几点特征，我们已经能够体会到，要实施一个完美的“微服务”架构，需要考虑的设计与成本并不小，对于没有足够经验的团队来说，甚至要比单体应用发付出更多的代价。 所以，很多情况下，架构师们都会以演进的方式进行系统的构建，在初期系统以单体系统的方式来设计和实施，一方面系统体量初期并不会很大，构建和维护成本都不高。另一方面，初期的核心业务在后期通常也不会发生巨大的改变。随着系统的发展或者业务的需要，架构师们会将一些经常变动或是有一定时间效应的内容进行“微服务”处理，并逐渐地将原来在单体系统中多变的模块逐步拆分出来，而稳定不太变化的就形成了一个核心“微服务”存在于整个架构之中。 原文由 程序猿 DD- 翟永超 创作 *转载自 《微服务》九大特性笔记","link":"/ms-1/"},{"title":"用了这么久配置中心，还不知道长轮询是什么？","text":"前言传统的静态配置方式想要修改某个配置时，必须重新启动一次应用，如果是数据库连接串的变更，那可能还容易接受一些，但如果变更的是一些运行时实时感知的配置，如某个功能项的开关，重启应用就显得有点大动干戈了。配置中心正是为了解决此类问题应运而生的，特别是在微服务架构体系中，更倾向于使用配置中心来统一管理配置。 配置中心最核心的能力就是配置的动态推送，常见的配置中心如 Nacos、Apollo 等都实现了这样的能力。在早期接触配置中心时，我就很好奇，配置中心是如何做到服务端感知配置变化实时推送给客户端的，在没有研究过配置中心的实现原理之前，我一度认为配置中心是通过长连接来做到配置推送的。事实上，目前比较流行的两款配置中心：Nacos 和 Apollo 恰恰都没有使用长连接，而是使用的长轮询。本文便是介绍一下长轮询这种听起来好像已经是上个世纪的技术，老戏新唱，看看能不能品出别样的韵味。文中会有代码示例，呈现一个简易的配置监听流程。 数据交互模式众所周知，数据交互有两种模式：Push（推模式）和 Pull（拉模式）。 推模式指的是客户端与服务端建立好网络长连接，服务方有相关数据，直接通过长连接通道推送到客户端。其优点是及时，一旦有数据变更，客户端立马能感知到；另外对客户端来说逻辑简单，不需要关心有无数据这些逻辑处理。缺点是不知道客户端的数据消费能力，可能导致数据积压在客户端，来不及处理。 拉模式指的是客户端主动向服务端发出请求，拉取相关数据。其优点是此过程由客户端发起请求，故不存在推模式中数据积压的问题。缺点是可能不够及时，对客户端来说需要考虑数据拉取相关逻辑，何时去拉，拉的频率怎么控制等等。 长轮询与轮询在开头，重点介绍一下长轮询（Long Polling）和轮询（Polling）的区别，两者都是拉模式的实现。 「轮询」是指不管服务端数据有无更新，客户端每隔定长时间请求拉取一次数据，可能有更新数据返回，也可能什么都没有。配置中心如果使用「轮询」实现动态推送，会有以下问题： 推送延迟。客户端每隔 5s 拉取一次配置，若配置变更发生在第 6s，则配置推送的延迟会达到 4s。 服务端压力。配置一般不会发生变化，频繁的轮询会给服务端造成很大的压力。 推送延迟和服务端压力无法中和。降低轮询的间隔，延迟降低，压力增加；增加轮询的间隔，压力降低，延迟增高。 「长轮询」则不存在上述的问题。客户端发起长轮询，如果服务端的数据没有发生变更，会 hold 住请求，直到服务端的数据发生变化，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次长轮询。配置中心使用「长轮询」如何解决「轮询」遇到的问题也就显而易见了： 推送延迟。服务端数据发生变更后，长轮询结束，立刻返回响应给客户端。 服务端压力。长轮询的间隔期一般很长，例如 30s、60s，并且服务端 hold 住连接不会消耗太多服务端资源。 以 Nacos 为例的长轮询流程如下： 可能有人会有疑问，为什么一次长轮询需要等待一定时间超时，超时后又发起长轮询，为什么不让服务端一直 hold 住？主要有两个层面的考虑，一是连接稳定性的考虑，长轮询在传输层本质上还是走的 TCP 协议，如果服务端假死、fullgc 等异常问题，或者是重启等常规操作，长轮询没有应用层的心跳机制，仅仅依靠 TCP 层的心跳保活很难确保可用性，所以一次长轮询设置一定的超时时间也是在确保可用性。除此之外，在配置中心场景，还有一定的业务需求需要这么设计。在配置中心的使用过程中，用户可能随时新增配置监听，而在此之前，长轮询可能已经发出，新增的配置监听无法包含在旧的长轮询中，所以在配置中心的设计中，一般会在一次长轮询结束后，将新增的配置监听给捎带上，而如果长轮询没有超时时间，只要配置一直不发生变化，响应就无法返回，新增的配置也就没法设置监听了。 配置中心长轮询设计上文的图中，介绍了长轮询的流程，本节会详解配置中心长轮询的设计细节。 客户端发起长轮询 客户端发起一个 HTTP 请求，请求信息包含配置中心的地址，以及监听的 dataId（本文出于简化说明的考虑，认为 dataId 是定位配置的唯一键）。若配置没有发生变化，客户端与服务端之间一直处于连接状态。 服务端监听数据变化 服务端会维护 dataId 和长轮询的映射关系，如果配置发生变化，服务端会找到对应的连接，为响应写入更新后的配置内容。如果超时内配置未发生变化，服务端找到对应的超时长轮询连接，写入 304 响应。 304 在 HTTP 响应码中代表“未改变”，并不代表错误。比较契合长轮询时，配置未发生变更的场景。 客户端接收长轮询响应 首先查看响应码是 200 还是 304，以判断配置是否变更，做出相应的回调。之后再次发起下一次长轮询。 服务端设置配置写入的接入点 主要用配置控制台和 client 发布配置，触发配置变更 这几点便是配置中心实现长轮询的核心步骤，也是指导下面章节代码实现的关键。但在编码之前，仍有一些其他的注意点需要实现阐明。 配置中心往往是为分布式的集群提供服务的，而每个机器上部署的应用，又会有多个 dataId 需要监听，实例级别 * 配置数是一个不小的数字，配置中心服务端维护这些 dataId 的长轮询连接显然不能用线程一一对应，否则会导致服务端线程数爆炸式增长。一个 Tomcat 也就 200 个线程，长轮询也不应该阻塞 Tomcat 的业务线程，所以需要配置中心在实现长轮询时，往往采用异步响应的方式来实现。而比较方便实现异步 HTTP 的常见手段便是 Servlet3.0 提供的 AsyncContext 机制。 Servlet3.0 并不是一个特别新的规范，它跟 Java 6 是同一时期的产物。例如 SpringBoot 内嵌的 Tomcat 很早就支持了 Servlet3.0，你无需担心 AsyncContext 机制不起作用。 SpringMVC 实现了 DeferredResult 和 Servlet3.0 提供的 AsyncContext 其实没有多大区别，我并没有深入研究过两个实现背后的源码，但从使用层面上来看，AsyncContext 更加的灵活，例如其可以自定义响应码，而 DeferredResult 在上层做了封装，可以快速的帮助开发者实现一个异步响应，但没法细粒度地控制响应。所以下文的示例中，我选择了 AsyncContext。 配置中心长轮询实现客户端实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Slf4jpublic class ConfigClient { private CloseableHttpClient httpClient; private RequestConfig requestConfig; public ConfigClient() { this.httpClient = HttpClientBuilder.create().build(); // ① httpClient 客户端超时时间要大于长轮询约定的超时时间 this.requestConfig = RequestConfig.custom().setSocketTimeout(40000).build(); } @SneakyThrows public void longPolling(String url, String dataId) { String endpoint = url + &quot;?dataId=&quot; + dataId; HttpGet request = new HttpGet(endpoint); CloseableHttpResponse response = httpClient.execute(request); switch (response.getStatusLine().getStatusCode()) { case 200: { BufferedReader rd = new BufferedReader(new InputStreamReader(response.getEntity() .getContent())); StringBuilder result = new StringBuilder(); String line; while ((line = rd.readLine()) != null) { result.append(line); } response.close(); String configInfo = result.toString(); log.info(&quot;dataId: [{}] changed, receive configInfo: {}&quot;, dataId, configInfo); longPolling(url, dataId); break; } // ② 304 响应码标记配置未变更 case 304: { log.info(&quot;longPolling dataId: [{}] once finished, configInfo is unchanged, longPolling again&quot;, dataId); longPolling(url, dataId); break; } default: { throw new RuntimeException(&quot;unExcepted HTTP status code&quot;); } } } public static void main(String[] args) { // httpClient 会打印很多 debug 日志，关闭掉 Logger logger = (Logger)LoggerFactory.getLogger(&quot;org.apache.http&quot;); logger.setLevel(Level.INFO); logger.setAdditive(false); ConfigClient configClient = new ConfigClient(); // ③ 对 dataId: user 进行配置监听 configClient.longPolling(&quot;http://127.0.0.1:8080/listener&quot;, &quot;user&quot;); }} 主要有三个注意点： RequestConfig.custom().setSocketTimeout(40000).build() 。httpClient 客户端超时时间要大于长轮询约定的超时时间。很好理解，不然还没等服务端返回，客户端会自行断开 HTTP 连接。 response.getStatusLine().getStatusCode() == 304 。前文介绍过，约定使用 304 响应码来标识配置未发生变更，客户端继续发起长轮询。 configClient.longPolling(&quot;http://127.0.0.1:8080/listener&quot;, &quot;user&quot;)。在示例中，我们处于简单考虑，仅仅启动一个客户端，对单一的 dataId：user 进行监听（注意，需要先启动 server 端）。 服务端实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869@RestController@Slf4j@SpringBootApplicationpublic class ConfigServer { @Data private static class AsyncTask { // 长轮询请求的上下文，包含请求和响应体 private AsyncContext asyncContext; // 超时标记 private boolean timeout; public AsyncTask(AsyncContext asyncContext, boolean timeout) { this.asyncContext = asyncContext; this.timeout = timeout; } } // guava 提供的多值 Map，一个 key 可以对应多个 value private Multimap&lt;String, AsyncTask&gt; dataIdContext = Multimaps.synchronizedSetMultimap(HashMultimap.create()); private ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;longPolling-timeout-checker-%d&quot;) .build(); private ScheduledExecutorService timeoutChecker = new ScheduledThreadPoolExecutor(1, threadFactory); // 配置监听接入点 @RequestMapping(&quot;/listener&quot;) public void addListener(HttpServletRequest request, HttpServletResponse response) { String dataId = request.getParameter(&quot;dataId&quot;); // 开启异步 AsyncContext asyncContext = request.startAsync(request, response); AsyncTask asyncTask = new AsyncTask(asyncContext, true); // 维护 dataId 和异步请求上下文的关联 dataIdContext.put(dataId, asyncTask); // 启动定时器，30s 后写入 304 响应 timeoutChecker.schedule(() -&gt; { if (asyncTask.isTimeout()) { dataIdContext.remove(dataId, asyncTask); response.setStatus(HttpServletResponse.SC_NOT_MODIFIED); asyncContext.complete(); } }, 30000, TimeUnit.MILLISECONDS); } // 配置发布接入点 @RequestMapping(&quot;/publishConfig&quot;) @SneakyThrows public String publishConfig(String dataId, String configInfo) { log.info(&quot;publish configInfo dataId: [{}], configInfo: {}&quot;, dataId, configInfo); Collection&lt;AsyncTask&gt; asyncTasks = dataIdContext.removeAll(dataId); for (AsyncTask asyncTask : asyncTasks) { asyncTask.setTimeout(false); HttpServletResponse response = (HttpServletResponse)asyncTask.getAsyncContext().getResponse(); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().println(configInfo); asyncTask.getAsyncContext().complete(); } return &quot;success&quot;; } public static void main(String[] args) { SpringApplication.run(ConfigServer.class, args); }} 对上述实现的一些说明： @RequestMapping(&quot;/listener&quot;) ，配置监听接入点，也是长轮询的入口。在获取 dataId 之后，使用 request.startAsync 将请求设置为异步，这样在方法结束后，不会占用 Tomcat 的线程池。 接着 dataIdContext.put(dataId, asyncTask) 会将 dataId 和异步请求上下文给关联起来，方便配置发布时，拿到对应的上下文。注意这里使用了一个 guava 提供的数据结构 Multimap&lt;String, AsyncTask&gt; dataIdContext ，它是一个多值 Map，一个 key 可以对应多个 value，你也可以理解为 Map&lt;String,List&lt;AsyncTask&gt;&gt; ，但使用 Multimap 维护起来可以更方便地处理一些并发逻辑。至于为什么会有多值，很好理解，因为配置中心的 Server 端会接受来自多个客户端对同一个 dataId 的监听。 timeoutChecker.schedule() 启动定时器，30s 后写入 304 响应。再结合之前客户端的逻辑，接收到 304 之后，会重新发起长轮询，形成一个循环。 @RequestMapping(&quot;/publishConfig&quot;) ，配置发布的入口。配置变更后，根据 dataId 一次拿出所有的长轮询，为之写入变更的响应，同时不要忘记取消定时任务。至此，完成了一个配置变更后推送的流程。 启动配置监听先启动 ConfigServer，再启动 ConfigClient。客户端打印长轮询的日志如下： 1222:18:09.185 [main] INFO moe.cnkirito.demo.ConfigClient - longPolling dataId: [user] once finished, configInfo is unchanged, longPolling again22:18:39.197 [main] INFO moe.cnkirito.demo.ConfigClient - longPolling dataId: [user] once finished, configInfo is unchanged, longPolling again 发布一条配置，curl -X GET &quot;localhost:8080/publishConfig?dataId=user&amp;configInfo=helloworld&quot; 服务端打印日志如下： 12021-01-24 22:18:50.801 INFO 73301 --- [nio-8080-exec-6] moe.cnkirito.demo.ConfigServer : publish configInfo dataId: [user], configInfo: helloworld 客户端接受配置推送： 122:18:50.806 [main] INFO moe.cnkirito.demo.ConfigClient - dataId: [user] changed, receive configInfo: helloworld 实现细节思考为什么需要定时器返回 304上述的实现中，服务端采用了一个定时器，在配置未发生变更时，定时返回 304，客户端接收到 304 之后，重新发起长轮询。在前文，已经解释过了为什么需要超时后重新发起长轮询，而不是由服务端一直 hold，直到配置变更再返回，但可能有读者还会有疑问，为什么不由客户端控制超时，服务端去除掉定时器，这样客户端超时后重新发起下一次长轮询，这样的设计不是更简单吗？无论是 Nacos 还是 Apollo 都有这样的定时器，而不是靠客户端控制超时，这样做主要有两点考虑： 和真正的客户端超时区分开。 仅仅使用异常（Exception）来表达异常流，而不应该用异常来表达正常的业务流。304 不是超时异常，而是长轮询中配置未变更的一种正常流程，不应该使用超时异常来表达。 客户端超时需要单独配置，且需要比服务端长轮询的超时要长。正如上述的 demo 中客户端超时设置的是 40s，服务端判断一次长轮询超时是 30s。这两个值在 Nacos 中默认是 30s 和 29.5s，在 Apollo 中默认是是 90s 和 60s。 长轮询包含多组 dataId在上述的 demo 中，一个 dataId 会发起一次长轮询，在实际配置中心的设计中肯定不能这样设计，一般的优化方式是，一批 dataId 组成一个组批量包含在一个长轮询任务中。在 Nacos 中，按照 3000 个 dataId 为一组包装成一个长轮询任务。 长轮询和长连接讲完实现细节，本文最核心的部分已经介绍完了。再回到最前面提到的数据交互模式上提到的推模型和拉模型，其实在写这篇文章时，我曾经问过交流群中的小伙伴们“配置中心实现动态推送的原理”，他们中绝大多数人认为是长连接的推模型。然而事实上，主流的配置中心几乎都是使用了本文介绍的长轮询方案，这又是为什么呢？ 我也翻阅了不少博客，显然他们给出的理由并不能说服我，我尝试着从自己的角度分析了一下这个既定的事实。 长轮询实现起来比较容易，完全依赖于 HTTP 便可以实现全部逻辑，而 HTTP 是最能够被大众接受的通信方式。 长轮询使用 HTTP，便于多语言客户端的编写，大多数语言都有 HTTP 的客户端。 那么长连接是不是真的就不适合用于配置中心场景呢？有人可能会认为维护一条长连接会消耗大量资源，而长轮询可以提升系统的吞吐量，而在配置中心场景，这一假设并没有实际的压测数据能够论证，benchmark everything！please~ 另外，翻阅了一下 Nacos 2.0 的 milestone，我发现了一个有意思的规划，Nacos 的注册中心（目前是短轮询 + udp 推送）和配置中心（目前是长轮询）都有计划改造为长连接模式。 再回过头来看，长轮询实现已经将配置中心这个组件支撑的足够好了，替换成长连接，一定需要找到合适的理由才行。 总结 本文介绍了长轮询、轮询、长连接这几种数据交互模型的差异性。 分析了 Nacos 和 Apollo 等主流配置中心均是通过长轮询的方式实现配置的实时推送的。实时感知建立在客户端拉的基础上，因为本质上还是通过 HTTP 进行的数据交互，之所以有“推”的感觉，是因为服务端 hold 住了客户端的响应体，并且在配置变更后主动写入了返回 response 对象再进行返回。 通过一个简单的 demo，实现了长轮询配置实时推送的过程演示，本文的 demo 示例存放在：https://github.com/lexburner/longPolling-demo","link":"/nacos-and-longpolling/"},{"title":"Nacos 集群部署模式最佳实践","text":"1 前言Nacos 支持两种部署模式：单机模式和集群模式。在实践中，我们往往习惯用单机模式快速构建一个 Nacos 开发/测试环境，而在生产中，出于高可用的考虑，一定需要使用 Nacos 集群部署模式。我的上一篇文章《一文详解 Nacos 高可用特性》提到了 Nacos 为高可用做了非常多的特性支持，而这些高可用特性大多数都依赖于集群部署模式。这篇模式文章便是给大家介绍一下，在实践中可以被采用的几种集群部署模式，无论你是希望自行搭建 Nacos，还是希望对 MSE 商业版 Nacos 有一个更加深刻的理解，我都很乐意跟你分享下面的内容。 由于篇幅限制，本文不会介绍如何将一个多节点的 Nacos 集群启动起来，主要介绍的是一个多节点的 Nacos 集群启动之后，我们的应用如何很好地连接到 Nacos 集群，即客户端视角。这中间我们会引入一些其他组件以解决一些问题，本文标题也可以叫做《Nacos 接入点最佳实践》。我将会介绍以下三种方案：直连模式、 VIP 模式和地址服务器模式，并对它们进行对比。 2 直连模式直连模式是部署上最简单，也是最容易理解的一种模式 采用直连模式后，典型的开发场景配置如下： nacos-client 配置123Properties properties = new Properties();properties.setProperty(PropertyKeyConst.SERVER_ADDR, &quot;192.168.0.1:8848,192.168.0.2:8848,192.168.0.3:8848&quot;);NamingService namingService = NacosFactory.createNamingService(properties); 注意这里的 PropertyKeyConst.SERVER_ADDR 的字面量是：serverAddr Dubbo 配置12dubbo.registry.address=192.168.0.1:8848,192.168.0.2:8848,192.168.0.3:8848dubbo.registry.protocol=nacos 如果有一天，Nacos 的 IP 变了，例如扩缩容，机器置换，集群迁移等场景，所有的应用都需要修改，这样的方式并不灵活。所以这种模式并不是生产推荐的模式。 模式分析 高可用性。集群本身的扩缩容必须要改动业务代码才能被感知到，出现节点故障需要紧急下线、紧急扩容等场景，让业务修改代码是不现实的，不符合高可用的原则。 可伸缩性。同上，可伸缩性不友好。 架构简单，用户理解成本低 没有引入额外的组件，没有新增组件的运维成本 3 VIP 模式VIP（Virtual IP） 模式可以很好的解决直连模式 IP 变化所带来的应用批量修改的问题。什么是 VIP 呢？ Real Server：处理实际请求的后端服务器节点。 Director Server：指的是负载均衡器节点，负责接收客户端请求，并转发给 RS。 VIP：Virtual IP，DS 用于和客户端通信的 IP 地址，作为客户端请求的目标 IP 地址。 DIP：Directors IP，DS 用于和内部 RS 通信的 IP 地址。 RIP：Real IP，后端服务器的 IP 地址。 CIP：Client IP，客户端的 IP 地址。 我这里介绍时并没有用【负载均衡模式】，而是用了【VIP 模式】，主要是为了跟 Nacos 官方文档保持一致。事实上，VIP 的叫法在阿里内部比较流行，所以在开源 Nacos 时也被习惯性的带了出去。 VIP 帮助 Nacos Client 屏蔽了后端 RIP，相对于 RIP 而言，VIP 很少会发生变化。以扩容场景为例，只需要让 VIP 感知到即可，Nacos Client 只需要关注 VIP，避免了扩容引起的代码改造。 只要是具备负载均衡能力的组件，均可以实现 VIP 模式，例如开源的 Nginx 以及阿里云负载均衡 SLB。 采用 VIP 模式后，代码不需要感知 RIP，典型的开发场景配置如下： nacos-client 配置123Properties properties = new Properties();properties.setProperty(PropertyKeyConst.SERVER_ADDR, &quot;{VIP}:8848&quot;);NamingService namingService = NacosFactory.createNamingService(properties); Dubbo 配置12dubbo.registry.address={VIP}:8848dubbo.registry.protocol=nacos 域名配置VIP 模式和直连模式都不具备可读性，所以在实际生产中，往往还会给 VIP 挂载一个域名。 域名背后甚至可以挂载 2 个 VIP 用作高可用，路由到想同的 rs；同时域名的存在也让 VIP 的置换变得更加灵活，当其中一台出现问题后，域名的 DNS 解析只会路由到另外一个正常的 VIP 上，为平滑置换预留了足够的余地。 tips：一个域名可以绑定多个 A 记录，一个 A 记录对应一个 IPv4 类型的 VIP，DNS 域名服务器了对多个 A 记录会有负载均衡策略和健康检查机制 VIP 模式的最终生产高可用版架构便产生了： 典型的开发场景配置只需要将 VIP 替换为域名即可 nacos-client 配置123Properties properties = new Properties();properties.setProperty(PropertyKeyConst.SERVER_ADDR, &quot;mse-abc123qwe-nacos.mse.aliyuncs.com:8848&quot;);NamingService namingService = NacosFactory.createNamingService(properties); Dubbo 配置12dubbo.registry.address=mse-abc123qwe-nacos.mse.aliyuncs.com:8848dubbo.registry.protocol=nacos 模式分析 高可用性。域名的可用性需要由 DNS 域名服务器负责，可用性保障较高；VIP 需要由高可用的负责均衡组件支持，且流量经过负载均衡转发，对 VIP 的实现有较高可用性的要求。 可伸缩性。水平扩缩容时，只需要让 VIP 感知即可，可伸缩性好。 依赖了域名解析系统和负载均衡系统，生产部署时，需要有配套设施的支持。 4 地址服务器模式地址服务器介绍说起地址服务器，可能大家对这个词会感到陌生，因为地址服务器的概念主要在阿里内部比较普及，也是阿里中间件使用的最广的一种地址寻址模式。但是在开源领域，鲜有人会提及，但对于 Nacos 部署模式而言，地址服务器模式是除了 VIP 模式之外，另外一个生产可用的推荐部署方式。 地址服务器是什么？顾名思义，是用来寻址地址的服务器，发送一个请求，返回一串地址列表。尽管在阿里内部使用的真实地址服务器比这复杂一些，但下图这个简单交互逻辑，几乎涵盖了地址服务器 90% 的内容。 实现一个简易版本的地址服务器并不困难，推荐使用 nginx 搭建一个静态文件服务器管理地址， 当然你可以使用 Java！ 1234567891011121314@Controllerpublic class AddressServerController { @RequestMapping(&quot;/nacos/serverlist&quot;) public ResponseEntity&lt;String&gt; serverlist() { return ResponseEntity.ok(). header(&quot;Content-Type&quot;, &quot;text/plain&quot;). body(&quot;192.168.0.1:8848\\r\\n&quot; + &quot;192.168.0.2:8848\\r\\n&quot; + &quot;192.168.0.3:8848\\r\\n&quot; ); }} 使用地址服务器可以完成集群地址和客户端配置的解耦，解决直连模式中无法动态感知集群节点变化的问题。客户端根据地址服务器返回的列表，随后采取直连模式连接；并且在客户端启动后，会启动一个定时器，轮询感知 AddressServer 的变化，进而及时更新地址列表。 并且地址服务器建议配置域名，增加可读性。所以最后的部署交互架构是这样的： 熟悉 RPC 的朋友看到这里应该能够很好地对 VIP 模式和地址服务器模式做一个类比。 VIP 模式是 DNS 类的服务端负载均衡技术 地址服务器是类似服务发现机制的客户端负载均衡技术 nacos-client 的源码专门适配了地址服务器模式，我们只需要配置好 addressServer 的 endpoint 即可 nacos-client 配置1234Properties properties = new Properties();properties.setProperty(PropertyKeyConst.ENDPOINT, &quot;{addressServerDomain}&quot;);properties.setProperty(PropertyKeyConst.ENDPOINT_PORT, &quot;8080&quot;);NamingService namingService = NacosFactory.createNamingService(properties); 注意，这里 PropertyKeyConst.ENDPOINT 的字面量是：endpoint ，配置的是地址服务器的地址。 Dubbo 配置12dubbo.registry.address=0.0.0.0?endpoint=127.0.0.1&amp;endpointPort=8080dubbo.registry.protocol=nacos dubbo.registry.address 的 url 可以任意填写，因为当 serverAddr 和 endpoint 同时存在时，默认是优先从地址服务器去选址的。 此时，只需要把真实的 Nacos Server IP 配置到地址服务器中即可。 Dubbo 通过 url 的 kv 属性将值透传给 Nacos 创建 Nacos-Client。Dubbo + Nacos 使用地址服务器模式时，建议 Dubbo 版本 &gt;= 2.7.4，nacos-client 版本 &gt;= 1.0.1 模式分析 高可用性。域名的可用性需要由 DNS 域名服务器负责，可用性保障较高；地址服务器的职责单一，有较高的可用性；运行时 Client 直连 Nacos Server 节点，可用性靠 nacos-sdk 保障。 可伸缩性。水平扩缩容时，只需要让地址服务器感知即可，可伸缩性好。 依赖了域名解析系统和地址服务器，生产部署时，需要有配套设施的支持。 5 部署模式对比 直连模式 VIP 模式 地址服务器模式 转发模式 直连 代理（网络多一跳） 直连 高可用 弱，代码配置不灵活，节点故障时无法批量变更 强 强 可伸缩性 弱 强 强 部署成本 无 负载均衡组件运维成本高 地址服务器运维成本低 负载均衡模式 nacos-sdk 客户端负载均衡 负载均衡组件提供负载均衡能力 nacos-sdk 客户端负载均衡 开源接受度 高 高 低，地址服务器模式在开源领域不太普遍 企业级能力 不方便 灵活 灵活 跨网络 内网环境，平坦网络 VIP 模式灵活地支持反向代理、安全组、ACL 等特性，可以很好的工作在内/外网环境中，使得应用服务器和 Nacos Server 可以部署在不同的网络环境中，借助 VIP 打通 内网环境，平坦网络 推荐使用环境 开发测试环境 生产环境，云环境 生产环境 Nacos 这款开源产品很好地支持了地址服务器这种模式，所以无论是大、中、小型公司在自建 Nacos 时，都可以选择地址服务器模式去构建生产高可用的 Nacos 集群，地址服务器组件相对而言维护简单，Nginx，Java 构建的 Web 服务器均可以轻松实现一个地址服务器。使用地址服务器后，nacos-client 与 nacos-server 之间仍然是直连访问，所以可以很好的运作在平坦网络下。 VIP 模式同样推荐在自建场景使用，但运维成本相对地址服务器还是要高一些，可以根据自己公司的运维体系评估。经过了 VIP 的转发，有利有弊。弊端比较明显，网络多了一跳，对于内网环境这样的平坦网络而言，是不必要的；优势也同样明显，大公司往往环境比较复杂，数据中心之间有网络隔离，应用和中间件可能部署在不同的网络环境中，借助于 VIP 可以很好地做网络打通，并且基于 VIP 可以很好实现安全组、ACL 等特性，更符合企业级诉求。 当然，组合使用地址服务器 + VIP 也是可以的，可以充分的融合两者的优势： 6 MSE Nacos 的实践上述场景主要介绍了三种模式的具体部署方案，以及自建 Nacos 场景如何做到高可用，最后要介绍的是阿里云环境 MSE 是如何部署的。 MSE（微服务引擎）提供了 Nacos 注册中心中心的全托管能力，除了要做上述提到的高可用、可伸缩、易用性，还要考虑以下的因素： 开源接受度。避免给用户带来太多理解成本，尽量做到对标开源，这样用户接受度才会高。 网络隔离。MSE 提供的是 BaaS 化的能力，Nacos Server 部署在云产品 VPC，与用户 VPC 是隔离的，需要解决网络隔离问题。 网络安全。MSE Nacos 是独享模式，网络上租户隔离是最基本的要求。除此之外企业级用户会对 MSE Nacos 提出安全组/ACL 控制的诉求，这些都需要考量。 综上，MSE Nacos 最终采用的是域名 + SLB 的 VIP 模式。 MSE Nacos 提供两个域名，其中公网域名可以用做本地开发测试，或者自建环境、混合云等场景的接入点，内网域名用做阿里云生产环境接入点。公网域名有带宽限制，需要在集群创建时根据场景选择合适的带宽，而内网域名则没有带宽限制。公网域名请注意添加 IP 访问白名单。 7 总结本文介绍了 Nacos 的三种部署模式，并就高可用、可伸缩、易用性等方面对各个模式进行了介绍，并对自建 Nacos 场景的部署选型进行了分析，同时介绍了 MSE Nacos 企业版的部署架构，对云环境部署 Nacos 进行了补充。 文章提及的三种模式其实也都是中间件组件常见的部署模式，不仅仅 Nacos，例如 Redis、DB 等场景，同样有参考价值。 本文提及了地址服务器这个可能在开源领域不太常见的组件，在阿里内部则用的非常普遍。 另外，Nacos 本身也提供 addressServer 模块，出于篇幅考虑没有在本文中提及，后续我会单独整理一篇文章介绍，感兴趣的同学可以自行参考 Nacos 官方文档和官方博客中的内容。","link":"/nacos-cluster-mode/"},{"title":"一文详解 Nacos 高可用特性","text":"前言服务注册发现是一个经久不衰的话题，Dubbo 早期开源时默认的注册中心 Zookeeper 最早进入人们的视线，并且在很长一段时间里，人们将注册中心和 Zookeeper 划上了等号，可能 Zookeeper 的设计者都没有想到这款产品对微服务领域造成了如此深厚的影响，直到 SpringCloud 开始流行，其自带的 Eureka 进入了人们的视野，人们这才意识到原来注册中心还可以有其他的选择。再到后来，热衷于开源的阿里把目光也聚焦在了注册中心这个领域，Nacos 横空出世。 Kirito 在做注册中心选型时的思考：曾经我没得选，现在我只想选择一个好的注册中心，它最好是开源的，这样开放透明，有自我的掌控力；不仅要开源，它还要有活跃的社区，以确保特性演进能够满足日益增长的业务需求，出现问题也能及时修复；最好…它的功能还要很强大，除了满足注册服务、推送服务外，还要有完善的微服务体系中所需的功能；最重要的，它还要稳定，最好有大厂的实际使用场景背书，证明这是一个经得起实战考验的产品；当然，云原生特性，安全特性也是很重要的… 似乎 Kirito 对注册中心的要求实在是太高了，但这些五花八门的注册中心呈现在用户眼前，总是免不了一番比较。正如上面所言，功能特性、成熟度、可用性、用户体验度、云原生特性、安全都是可以拉出来做比较的话题。今天这篇文章重点介绍的是 Nacos 在可用性上体现，希望借助于这篇文章，能够让你对 Nacos 有一个更加深刻的认识。 高可用介绍当我们在聊高可用时，我们在聊什么？ 系统可用性达到 99.99% 在分布式系统中，部分节点宕机，依旧不影响系统整体运行 服务端集群化部署多个节点 这些都可以认为是高可用，而我今天介绍的 Nacos 高可用，则是一些 Nacos 为了提升系统稳定性而采取的一系列手段。Nacos 的高可用不仅仅存在于服务端，同时它也存在于客户端，以及一些与可用性相关的功能特性中。这些点组装起来，共同构成了 Nacos 的高可用。 客户端重试先统一一下语义，在微服务架构中一般会有三个角色：Consumer、Provider 和 Registry，在今天注册中心的主题中，Registry 是 nacos-server，而 Consumer 和 Provider 都是 nacos-client。 在生产环境，我们往往需要搭建 Nacos 集群，在 Dubbo 也需要显式地配置上集群地址： 1&lt;dubbo:registry protocol=&quot;nacos&quot; address=&quot;192.168.0.1:8848,192.168.0.2:8848,192.168.0.3:8848&quot;/&gt; 当其中一台机器宕机时，为了不影响整体运行，客户端会存在重试机制 逻辑非常简单，拿到地址列表，在请求成功之前逐个尝试，直到成功为止。 该可用性保证存在于 nacos-client 端。 一致性协议 distro首先给各位读者打个强心剂，不用看到”一致性协议“这几个字就被劝退，本节不会探讨一致性协议的实现过程，而是重点介绍其余高可用相关的特性。有的文章介绍 Nacos 的一致性模型是 AP + CP，这么说很容易让人误解，其实 Nacos 并不是支持两种一致性模型，也并不是支持两种模型的切换，介绍一致性模型之前，需要先了解到 Nacos 中的两个概念：临时服务和持久化服务。 临时服务（Ephemeral）：临时服务健康检查失败后会从列表中删除，常用于服务注册发现场景。 持久化服务（Persistent）：持久化服务健康检查失败后会被标记成不健康，常用于 DNS 场景。 临时服务使用的是 Nacos 为服务注册发现场景定制化的私有协议 distro，其一致性模型是 AP；而持久化服务使用的是 raft 协议，其一致性模型是 CP。所以以后不要再说 Nacos 是 AP + CP 了，更建议加上服务节点状态或者使用场景的约束。 distro 协议与高可用有什么关系呢？上一节我们提到 nacos-server 节点宕机后，客户端会重试，但少了一个前提，即 nacos-server 少了一个节点后依旧可以正常工作。Nacos 这种有状态的应用和一般无状态的 Web 应用不同，并不是说只要存活一个节点就可以对外提供服务的，需要分 case 讨论，这与其一致性协议的设计有关。distro 协议的工作流程如下： Nacos 启动时首先从其他远程节点同步全部数据 Nacos 每个节点是平等的都可以处理写入请求，同时把新数据同步到其他节点 每个节点只负责部分数据，定时发送自己负责数据校验值到其他节点来保持数据一致性 如上图所示，每个节点服务一部分服务的写入，但每个节点都可以接收到写入请求，这时就存在两种写情况： 当该节点接收到属于该节点负责的服务时，直接写入。 当该节点接收到不属于该节点负责的服务时，将在集群内部路由，转发给对应的节点，从而完成写入。 读取操作则不需要路由，因为集群中的各个节点会同步服务状态，每个节点都会有一份最新的服务数据。 而当节点发生宕机后，原本该节点负责的一部分服务的写入任务会转移到其他节点，从而保证 Nacos 集群整体的可用性。 一个比较复杂的情况是，节点没有宕机，但是出现了网络分区，即下图所示： 这个情况会损害可用性，客户端会表现为有时候服务存在有时候服务不存在。 综上，Nacos 的 distro 一致性协议可以保证在大多数情况下，集群中的机器宕机后依旧不损害整体的可用性。该可用性保证存在于 nacos-server 端。 本地缓存文件 Failover 机制注册中心发生故障最坏的一个情况是整个 Server 端宕机，这时候 Nacos 依旧有高可用机制做兜底。 一道经典的 Dubbo 面试题：当 Dubbo 应用运行时，Nacos 注册中心宕机，会不会影响 RPC 调用。这个题目大多数应该都能回答出来，因为 Dubbo 内存里面是存了一份地址的，一方面这样的设计是为了性能，因为不可能每次 RPC 调用时都读取一次注册中心，另一面，这也起到了可用性的保障（尽管可能 Dubbo 设计者并没有考虑这个因素）。 那如果，我在此基础上再出一道 Dubbo 面试题：Nacos 注册中心宕机，Dubbo 应用发生重启，会不会影响 RPC 调用。如果了解了 Nacos 的 Failover 机制，应当得到和上一题同样的回答：不会。 Nacos 存在本地文件缓存机制，nacos-client 在接收到 nacos-server 的服务推送之后，会在内存中保存一份，随后会落盘存储一份快照。snapshot 默认的存储路径为：{USER_HOME}/nacos/naming/ 中 这份文件有两种价值，一是用来排查服务端是否正常推送了服务；二是当客户端加载服务时，如果无法从服务端拉取到数据，会默认从本地文件中加载。 前提是构建 NacosNaming 时传入了该参数：namingLoadCacheAtStart=true Dubbo 2.7.4 及以上版本支持该 Nacos 参数；开启该参数的方式：dubbo.registry.address=nacos://127.0.0.1:8848?namingLoadCacheAtStart=true 在生产环境，推荐开启该参数，以避免注册中心宕机后，导致服务不可用的稳定，在服务注册发现场景，可用性和一致性 trade off 时，我们大多数时候会优先考虑可用性。 细心的读者还注意到 {USER_HOME}/nacos/naming/{namespace} 下除了缓存文件之外还有一个 failover 文件夹，里面存放着和 snapshot 一致的文件夹。这是 Nacos 的另一个 failover 机制，snapshot 是按照某个历史时刻的服务快照恢复恢复，而 failover 中的服务可以人为修改，以应对一些极端场景。 该可用性保证存在于 nacos-client 端。 心跳同步服务心跳机制一般广泛存在于分布式通信领域，用于确认存活状态。一般心跳请求和普通请求的设计是有差异的，心跳请求一般被设计的足够精简，这样在定时探测时可以尽可能避免性能下降。而在 Nacos 中，处于可用性的考虑，一个心跳报文包含了全部的服务信息，这样相比仅仅发送探测信息降低了吞吐量，而提升了可用性，怎么理解呢？考虑以下的两种场景： nacos-server 节点全部宕机，服务数据全部丢失。nacos-server 即使恢复运作，也无法恢复出服务，而心跳包含全部内容可以在心跳期间就恢复出服务，保证可用性。 nacos-server 出现网络分区。由于心跳可以创建服务，从而在极端网络故障下，依旧保证基础的可用性。 以下是对心跳同步服务的测试，使用阿里云 MSE 提供 Nacos 集群进行测试 调用 OpenApi：curl -X &quot;DELETE mse-xxx-p.nacos-ans.mse.aliyuncs.com:8848/nacos/v1/ns/service?serviceName=providers:com.alibaba.edas.boot.EchoService:1.0.0:DUBBO&amp;groupName=DEFAULT_GROUP&quot; 依次删除各个服务 过 5s 后刷新，服务又再次被注册了上来，符合我们对心跳注册服务的预期。 集群部署模式高可用最后给大家分享的 Nacos 高可用特性来自于其部署架构。 节点数量我们知道在生产集群中肯定不能以单机模式运行 Nacos，那么第一个问题便是：我应该部署几台机器？前面我们提到 Nacos 有两个一致性协议：distro 和 raft，distro 协议不会有脑裂问题，所以理论来说，节点数大于等于 2 即可；raft 协议的投票选举机制则建议是 2n+1 个节点。综合来看，选择 3 个节点是起码的，其次处于吞吐量和更高可用性的考量，可以选择 5 个，7 个，甚至 9 个节点的集群。 多可用区部署组成集群的 Nacos 节点，应该尽可能考虑两个因素： 各个节点之间的网络时延不能很高，否则会影响数据同步 各个节点所处机房、可用区应当尽可能分散，以避免单点故障 以阿里云的 ECS 为例，选择同一个 Region 的不同可用区就是一个很好的实践 部署模式主要分为 K8s 部署和 ECS 部署两种模式。 ECS 部署的优点在于简单，购买三台机器即可搭建集群，如果你熟练 Nacos 集群部署的话，这不是难事，但无法解决运维问题，如果 Nacos 某个节点出现 OOM 或者磁盘问题，很难迅速摘除，无法实现自运维。 K8s 部署的有点在于云原生运维能力强，可以在节点宕机后实现自恢复，保障 Nacos 的平稳运行。前面提到过，Nacos 和无状态的 Web 应用不同，它是一个有状态的应用，所以在 K8s 中部署，往往要借助于 StatefulSet 和 Operator 等组件才能实现 Nacos 集群的部署和运维。 MSE Nacos 的高可用最佳实践阿里云 MSE（微服务引擎）提供了 Nacos 集群的托管能力，实现了集群部署模式的高可用。 当创建多个节点的集群时，系统会默认分配在不同可用区。同时，这对于用户来说又是透明的，用户只需要关心 Nacos 的功能即可，MSE 替用户兜底可用性。 MSE 底层使用 K8s 运维模式部署 Nacos。历史上出现过用户误用 Nacos 导致部分节点宕机的问题，但借助于 K8s 的自运维模式，宕机节点迅速被拉起，以至于用户可能都没有意识到自己发生宕机。 下面模拟一个节点宕机的场景，来看看 K8s 如何实现自恢复。 一个三节点的 Nacos 集群： 执行 kubectl delete pod mse-7654c960-1605278296312-reg-center-0-2 以模拟部分节点宕机的场景。 大概 2 分钟后，节点恢复，并且角色发生了转换，Leader 从杀死的 2 号节点转给 1 号节点 总结本文从多个角度出发，总结了一下 Nacos 是如何保障高可用的。高可用特性绝不是靠服务端多部署几个节点就可以获得的，而是要结合客户端使用方式、服务端部署模式、使用场景综合来考虑的一件事。 特别是在服务注册发现场景，Nacos 为可用性做了非常多的努力，而这些保障，Zookeeper 是不一定有的。在做注册中心选型时，可用性保障上，Nacos 绝对是优秀的。 「技术分享」某种程度上，是让作者和读者，不那么孤独的东西。欢迎关注我的微信公众号：「Kirito的技术分享」","link":"/nacos-high-available/"},{"title":"小白也能懂的 Nacos 服务模型介绍","text":"前言按照目前市场上的主流使用场景，Nacos 被分成了两块功能：服务注册发现（Naming）和配置中心（Config）。在之前的文章中我介绍了 Nacos 配置中心的实现原理，今天这篇文章所介绍的内容则是与 Nacos 服务注册发现功能相关，来聊一聊 Nacos 的服务模型。 说到服务模型，其实需要区分视角，一是用户视角，一个内核视角。即 Nacos 用户视角看到的服务模型和 Nacos 开发者设计的内核模型可能是完全不一样的，而今天的文章，是站在用户视角观察的，旨在探讨 Nacos 服务发现的最佳实践。 服务模型介绍一般我在聊注册中心时，都会以 Zookeeper 为引子，这也是很多人最熟悉的注册中心。但如果你真的写过或看过使用 Zookeeper 作为注册中心的适配代码，会发现并不是那么容易，再加上注册中心涉及到的一致性原理，这就导致很多人对注册中心的第一印象是：这个东西好难！ 但归根到底是因为 Zookeeper 根本不是专门为注册中心而设计的，其提供的 API 以及内核设计，并没有预留出「服务模型」的概念，这就使得开发者需要自行设计一个模型，去填补 Zookeeper 和服务发现之间的鸿沟。 微服务架构逐渐深入人心后，Nacos、Consul、Eureka 等注册中心组件进入大众的视线。可以发现，这些”真正“的注册中心都有各自的「服务模型」，在使用上也更加的方便。 为什么要有「服务模型」？理论上，一个基础组件可以被塑造成任意的模样，如果你愿意，一个数据库也可以被设计成注册中心，这并不是”夸张“的修辞手法，在阿里还真有人这么干过。那么代价是什么呢？一定会在业务发展到一定体量后遇到瓶颈，一定会遇到某些极端 case 导致其无法正常工作，一定会导致其扩展性低下。正如刚学习数据结构时，同学们常见的一个疑问一样：为什么栈只能先进后出。不是所有开发都是中间件专家，所以 Nacos 设计了自己的「服务模型」，这虽然限制了使用者的”想象力“，但保障了使用者在正确地使用 Nacos。 花了一定的篇幅介绍 Nacos 为什么需要设计「服务模型」，再来看看实际的 Nacos 模型是个啥，其实没那么玄乎，一张图就能表达清楚： 与 Consul、Eureka 设计有别，Nacos 服务发现使用的领域模型是命名空间-分组-服务-集群-实例这样的多层结构。服务 Service 和实例 Instance 是核心模型，命名空间 Namespace 、分组 Group、集群 Cluster 则是在不同粒度实现了服务的隔离。 为了更好的理解两个核心模型：Service 和 Instance，我们以 Dubbo 和 SpringCloud 这两个已经适配了 Nacos 注册中心的微服务框架为例，介绍下二者是如何映射对应模型的。 Dubbo。将接口三元组（接口名+分组名+版本号）映射为 Service，将实例 IP 和端口号定义为 Instance。一个典型的注册在 Nacos 中的 Dubbo 服务：providers:com.alibaba.mse.EchoService:1.0.0:DUBBO Spring Cloud。将应用名映射为 Service，将实例 IP 和端口号定义为 Instance。一个典型的注册在 Nacos 中的 Spring Cloud 服务：helloApp 下面我们将会更加详细地阐释 Nacos 提供的 API 和服务模型之间的关系。 环境准备需要部署一个 Nacos Server 用于测试，我这里选择直接在 https://mse.console.aliyun.com/ 购买一个 MSE 托管的 Nacos，读者们可以选择购买 MSE Nacos 或者自行搭建一个 Nacos Server。 MSE Nacos 提供的可视化控制台，也可以帮助我们更好的理解 Nacos 的服务模型。下文的一些截图，均来自 MSE Nacos 的商业化控制台。 快速开始先来实现一个最简单的服务注册与发现 demo。Nacos 支持从客户端注册服务实例和订阅服务，具体代码如下： 12345678910Properties properties = new Properties();properties.setProperty(PropertyKeyConst.SERVER_ADDR, &quot;mse-xxxx-p.nacos-ans.mse.aliyuncs.com:8848&quot;);String serviceName = &quot;nacos.test.service.1&quot;;String instanceIp = InetAddress.getLocalHost().getHostAddress();int instancePort = 8080;namingService.registerInstance(serviceName, instanceIp, instancePort);System.out.println(namingService.getAllInstances(serviceName)); 上述代码定义了一个 service：nacos.test.service.1；定义了一个 instance，以本机 host 为 IP 和 8080 为端口号，观察实际的注册情况： 并且控制台也打印出了服务的详情。至此一个最简单的 Nacos 服务发现 demo 就已经完成了。对一些细节稍作解释： 属性 PropertyKeyConst.SERVER_ADDR 表示的是 Nacos 服务端的地址。 创建一个 NamingService 实例，客户端将为该实例创建单独的资源空间，包括缓存、线程池以及配置等。Nacos 客户端没有对该实例做单例的限制，请小心维护这个实例，以防新建多于预期的实例。 注册服务 registerInstance 使用了最简单的重载方法，只需要传入服务名、IP、端口就可以。 上述的例子中，并没有出现 Namespace、Group、Cluster 等前文提及的服务模型，我会在下面一节详细介绍，这个例子主要是为了演示 Nacos 支持的一些缺省配置，其中 Service 和 Instance 是必不可少的，这也验证了前文提到的服务和实例是 Nacos 的一等公民。 通过截图我们可以发现缺省配置的默认值： Namespace：默认值是 public 或者空字符串，都可以代表默认命名空间。 Group：默认值是 DEFAULT_GROUP。 Cluster：默认值是 DEFAULT。 构建自定义实例为了展现出 Nacos 服务模型的全貌，还需要介绍下实例相关的 API。例如我们希望注册的实例中，有一些能够被分配更多的流量；或者能够传入一些实例的元信息存储到 Nacos 服务端，例如 IP 所属的应用或者所在的机房，这样在客户端可以根据服务下挂载的实例元信息，来自定义负载均衡模式。Nacos 也提供了另外的注册实例接口，使得用户在注册实例时可以指定实例的属性： 123456789/** * register a instance to service with specified instance properties. * * @param serviceName name of service * @param groupName group of service * @param instance instance to register * @throws NacosException nacos exception */void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException; 这个方法在注册实例时，可以传入一个 Instance 实例，它的属性如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Instance { /** * unique id of this instance. */ private String instanceId; /** * instance ip. */ private String ip; /** * instance port. */ private int port; /** * instance weight. */ private double weight = 1.0D; /** * instance health status. */ private boolean healthy = true; /** * If instance is enabled to accept request. */ private boolean enabled = true; /** * If instance is ephemeral. * * @since 1.0.0 */ private boolean ephemeral = true; /** * cluster information of instance. */ private String clusterName; /** * Service information of instance. */ private String serviceName; /** * user extended attributes. */ private Map&lt;String, String&gt; metadata = new HashMap&lt;String, String&gt;();} 有一些字段可以望文生义，有一些则需要花些功夫专门去了解 Nacos 的设计，我这里挑选几个我认为重要的属性重点介绍下： healthy 实例健康状态。标识该实例是否健康，一般心跳健康检查会自动更新该字段。 enable 是否启用。它跟 healthy 区别在于，healthy 一般是由内核健康检查更新，而 enable 更多是业务语义偏多，可以完全根据业务场景操控。例如在 Dubbo 中，一般使用该字段标识某个实例 IP 的上下线状态。 ephemeral 临时实例还是持久化实例。非常关键的一个字段，需要对 Nacos 有较为深入的了解才能够理解该字段的含义。区别在于，心跳检测失败一定时间之后，实例是自动下线还是标记为不健康。一般在注册中心场景下，会使用临时实例。这样心跳检测失败之后，可以让消费者及时收到下线通知；而在 DNS 模式下，使用持久化实例较多。在《一文详解 Nacos 高可用特性》中我也介绍过，该字段还会影响到 Nacos 的一致性协议。 metadata 元数据。一个 map 结构，可以存储实例的自定义扩展信息，例如机房信息，路由标签，应用信息，权重信息等。 这些信息在由服务提供者上报之后，由服务消费者获取，从而完成信息的传递。以下是一个完整的实例注册演示代码： 123456789101112131415161718192021222324252627Properties properties = new Properties();// 指定 Nacos Server 地址properties.setProperty(PropertyKeyConst.SERVER_ADDR, &quot;mse-xxxx-p.nacos-ans.mse.aliyuncs.com:8848&quot;);// 指定命名空间properties.setProperty(PropertyKeyConst.NAMESPACE, &quot;9125571e-bf50-4260-9be5-18a3b2e3605b&quot;);NamingService namingService = NacosFactory.createNamingService(properties);String serviceName = &quot;nacos.test.service.1&quot;;String group = &quot;DEFAULT_GROUP&quot;;String clusterName = &quot;cn-hangzhou&quot;;String instanceIp = InetAddress.getLocalHost().getHostAddress();int instancePort = 8080;Instance instance = new Instance();// 指定集群名instance.setClusterName(clusterName);instance.setIp(instanceIp);instance.setPort(instancePort);// 指定实例的元数据Map&lt;String, String&gt; metadata = new HashMap&lt;&gt;();metadata.put(&quot;app&quot;, &quot;nacos-demo&quot;);metadata.put(&quot;site&quot;, &quot;cn-hangzhou&quot;);metadata.put(&quot;protocol&quot;, &quot;1.3.3&quot;);instance.setMetadata(metadata);// 指定服务名、分组和实例namingService.registerInstance(serviceName, group, instance);System.out.println(namingService.getAllInstances(serviceName)); 构建自定义服务除了实例之外，服务也可以自定义配置，Nacos 的服务随着实例的注册而存在，并随着所有实例的注销而消亡。不过目前 Nacos 对于自定义服务的支持不是很友好，除使用 OpenApi 可以修改服务的属性外，就只能使用注册实例时传入的服务属性来进行自定义配置。所以在实际的 Dubbo 和 SpringCloud 中，自定义服务一般较少使用，而自定义实例信息则相对常用。 Nacos 的服务与 Consul、Eureka 的模型都不同，Consul 与 Eureka的服务等同于 Nacos 的实例，每个实例有一个服务名属性，服务本身并不是一个单独的模型。Nacos 的设计在我看来更为合理，其认为服务本身也是具有数据存储需求的，例如作用于服务下所有实例的配置、权限控制等。实例的属性应当继承自服务的属性，实例级别可以覆盖服务级别。以下是服务的数据结构： 1234567891011121314151617181920212223242526/** * Service name */ private String name; /** * Protect threshold */ private float protectThreshold = 0.0F; /** * Application name of this service */ private String app; /** * Service group which is meant to classify services into different sets. */ private String group; /** * Health check mode. */ private String healthCheckMode; private Map&lt;String, String&gt; metadata = new HashMap&lt;String, String&gt;(); 在实际使用过程中，可以像快速开始章节中介绍的那样，仅仅使用 ServiceName 标记一个服务。 服务隔离：Namespace&amp;Group&amp;Cluster出于篇幅考虑，这三个概念放到一起介绍。 襄王有意，神女无心。Nacos 提出了这几种隔离策略，目前看来只有 Namespace 在实际应用中使用较多，而 Group 和 Cluster 并没有被当回事。 Cluster 集群隔离在阿里巴巴内部使用的非常普遍。一个典型的场景是这个服务下的实例，需要配置多种健康检查方式，有一些实例使用 TCP 的健康检查方式，另外一些使用 HTTP 的健康检查方式。另一个场景是，服务下挂载的机器分属不同的环境，希望能够在某些情况下将某个环境的流量全部切走，这样可以通过集群隔离，来做到一次性切流。在 Nacos 2.0 中，也在有意的弱化集群的概念，毕竟开源还是要面向用户的，有些东西适合阿里，但不一定适合开源，等再往后演进，集群这个概念又有可能重新回到大家的视线中了，history will repeat itself。 Group 分组隔离的概念可以参考 Dubbo 的服务隔离策略，其也有一个分组。支持分组的扩展，用意当然是好的，实际使用上，也的确有一些公司会习惯使用分组来进行隔离。需要注意的一点是：Dubbo 注册三元组（接口名+分组+版本）时，其中 Dubbo 的分组是包含在 Nacos 的服务名中的，并不是映射成了 Nacos 的分组，一般 Nacos 注册的服务是默认注册到 DEFAULT_GROUP 分组的。 Namespace 命名空间隔离，我认为是 Nacos 一个比较好的设计。在实际场景中使用也比较普遍，一般用于多个环境的隔离，例如 daily，dev，test，uat，prod 等环境的隔离。特别是当环境非常多时，使用命名空间做逻辑隔离是一个比较节约成本的策略。但强烈建议大家仅仅在非线上环境使用 Namespace 进行隔离，例如多套测试环境可以共享一套 Nacos，而线上环境单独搭建另一套 Nacos 集群，以免线下测试流量干扰到线上环境。 服务发现：推拉模型上面介绍完了 Nacos 服务发现的 5 大领域模型，最后一节，介绍下如何获取服务模型。 Nacos 的服务发现，有主动拉取和推送两种模式，这与一般的服务发现架构相同。以下是拉模型的相关接口： 123456789101112131415161718192021222324252627/** * Get all instances of a service * * @param serviceName name of service * @return A list of instance * @throws NacosException */List&lt;Instance&gt; getAllInstances(String serviceName) throws NacosException;/** * Get qualified instances of service * * @param serviceName name of service * @param healthy a flag to indicate returning healthy or unhealthy instances * @return A qualified list of instance * @throws NacosException */List&lt;Instance&gt; selectInstances(String serviceName, boolean healthy) throws NacosException;/** * Select one healthy instance of service using predefined load balance strategy * * @param serviceName name of service * @return qualified instance * @throws NacosException */Instance selectOneHealthyInstance(String serviceName) throws NacosException; Nacos 提供了三个同步拉取服务的方法，一个是查询所有注册的实例，一个是只查询健康且上线的实例，还有一个是获取一个健康且上线的实例。一般情况下，订阅端并不关心不健康的实例或者权重设为 0 的实例，但是也不排除一些场景下，有一些运维或者管理的场景需要拿到所有的实例。细心的读者会注意到上述 Nacos 实例中有一个 weight 字段，便是作用在此处的selectOneHealthyInstance接口上，按照权重返回一个健康的实例。个人认为这个功能相对鸡肋，一般的 RPC 框架都有自身配套的负载均衡策略，很少会由注册中心 cover，事实上 Dubbo 和 Spring Cloud 都没有用到 Nacos 的这个接口。 除了主动查询实例列表，Nacos还提供订阅模式来感知服务下实例列表的变化，包括服务配置或者实例配置的变化。可以使用下面的接口来进行订阅或者取消订阅： 12345678910111213141516/** * Subscribe service to receive events of instances alteration * * @param serviceName name of service * @param listener event listener * @throws NacosException */void subscribe(String serviceName, EventListener listener) throws NacosException;/** * Unsubscribe event listener of service * * @param serviceName name of service * @param listener event listener * @throws NacosException */void unsubscribe(String serviceName, EventListener listener) throws NacosException; 在实际的服务发现中，订阅接口尤为重要。消费者启动时，一般会同步获取一次服务信息用于初始化，紧接着订阅服务，这样当服务发生上下线时，就可以感知变化了，从而实现服务发现。 总结Nacos 为了更好的实现服务发现，提供一套成熟的服务模型，其中重点需要关注的是 Namespace、Service 和 Instance，得益于这一套服务模型的抽象，以及对推拉模型的支持，Nacos 可以快速被微服务框架集成。 理解了 Nacos 的服务模型，也有利于我们了解 Nacos 背后的工作原理，从而确保我们正确地使用 Nacos。但 Nacos 提供的这些模型也不一定所有都需要用上，例如集群、分组、权重等概念，被实践证明是相对鸡肋的设计，在使用时，也需要根据自身业务特点去评估特性用量，不要盲目地为了使用技术而去用。","link":"/nacos-service-model/"},{"title":"Nacos Client 1.4.1 版本踩坑记录","text":"问题发现就在这周，我接到 MSE Nacos 用户的反馈，说线上 Nacos 不可用，服务都下线了，日志里面也是一堆报错，我下意识以为线上炸了，赶紧上线排查。本文主要记录这次问题的排查过程，以及解决方案。 首先看用户反馈的报错，日志如下： 并且用户反馈业务日志也出现了大量的服务地址找不到的报错，说明 Nacos 服务都下线了。 我立刻查看了服务端的监控，发现用户的 MSE Nacos 集群并无异常，cpu/内存等指标有下降，并没有异常行为，排除了服务端异常的可能性。 随即将视线聚焦在了客户端。老实说，这个报错我第一次见，看异常堆栈，字面意思便是域名解析出问题了。这个报错大概持续了 10 分钟，立刻让用户在业务节点上使用 ping、dig 等工具确认域名解析是否正常，测试发现均无异常。继续让用户 telnet mse-xx.com 8848，发现也能够 telnet 通。 根据这些现象，大概能得出结论：用户的机器上出现了短暂的域名解析问题，导致短时间访问不通 MSE Nacos。但用户继续反馈说，一部分重启以后的机器已经恢复了，但没有重启的机器，竟然还会出现调用报错。不然怎么说重启大法好呢，但也加深了问题的诡异性。 正当一筹莫展时，另一用户也找上来了，竟然也是一样的问题，并且由于第二个用户还同时使用了 redis，报错日志中除了出现 nacos 的域名解析问题，还报了 redis 的域名解析报错。至此，更加坚定了我之前推测，根因肯定是域名解析出现了故障，导致这两个用户收到了影响。但问题在于，为什么短暂的域名解析失败（大概 10 分钟），会导致持续性的 Nacos 问题呢？并且只有重启才能恢复。 分析两个用户的共性，最终我和同事将可疑点锁定在了 Nacos 客户端版本上，对比发现，用户都是同一个报错，并且竟然都是 nacos-client 1.4.1 版本。 ##Nacos 1.4.1 版本引入的 bug 在问题发生时，Nacos 1.x 最新的版本已经是 Nacos 1.4.2 了，将源码 checkout 到 1.4.1 版本，追踪堆栈附近的问题， 上述这段代码是 Nacos 访问服务端的一段代码，进入 595 行，一探究竟。 我们成功找到了堆栈中的直接报错，就是这段 IsIPv4 的判断触发。splitIPPortStr 这个方法的主要逻辑是从 Nacos 的连接串筛选出连接地址，主要是为了做默认端口号的判断，如果用户没有携带 8848，会默认带上 8848。 但问题恰恰便是出现在这儿： InetAddress.getByName(addr) 是一个内置的方法，描述如下： 1Given the name of a host, returns an array of its IP addresses, based on the configured name service on the system. 意思是把一个域名传给操作系统，返回一串 IP，这不就是域名解析吗！我当时就很好奇，你说你判断 IPv4 格式，为啥要这么判断呢？直接判断 IPv4 的 pattern 不行吗？而这段代码，恰恰是导致问题的凶手之一。 我们看看 1.4.2，已经修复了这个逻辑了，直接改成了正则判断。 但疑问还是存在的，域名解析短暂失败了，为啥会导致服务全都下线了，并且解析恢复后，服务依旧没有上线呢？ 继续追踪这段代码，发现 callServer 这段代码会被 com.alibaba.nacos.client.naming.beat.BeatReactor 持有，用于维持自身和 Nacos 的心跳。 而由于上述域名解析失败，抛出的异常是 IllegalArgumentException，并没有被里层方法转换成 NacosException，从而导致心跳线程没有 catch 住异常，彻底停止发送心跳了！ 这也就成功解释了，为什么短暂的域名解析失败，会导致服务全部下线了。（Nacos 是利用心跳维护和 server 端的存活状态的） 改进建议 修改 isIPv6 和 isIPv4 的判断方式，改为正则匹配。上文提及，这点已经在 1.4.2 修复了。 心跳线程要保证不被异常中断下一次心跳的提交。 第二点，也已经被修复了。 总结nacos-client 1.4.1 存在严重的 bug，客户端与 Nacos Server 如果发生短暂的域名解析问题，会导致心跳永久丢失，进而引发服务全量下线，即使网络恢复，也不会自动恢复心跳。 域名解析失败常见于网络抖动或者 K8s 环境下的 coreDNS 访问超时等场景，为避免域名解析对 Nacos 造成的重大影响，请务必自查应用代码中使用的 nacos-client 的版本。 该问题仅存在于 1.4.1 版本，低于此版本不受此问题的影响，使用 1.4.1 的用户建议升级至 1.4.2 以避免此问题。 使用 SpringCloud/Dubbo 的用户，需要确认实际框架使用的 nacos-client 版本，可以通过显式指定 nacos-client 的版本以覆盖框架默认的版本。其中 Dubbo 用户要格外小心，Dubbo 的 2.7.11 版本默认使用了 nacos-client 1.4.1，务必显式指定 nacos-client 的版本到 1.4.2，Dubbo 也将在下个 release 版本替换 Nacos 的默认版本。","link":"/nacos141-bug-md/"},{"title":"Netty实现长连接服务的难点和优化点","text":"转载自：https://www.dozer.cc/2014/12/netty-long-connection.html 原文作者：dozer 推送服务还记得一年半前，做的一个项目需要用到 Android 推送服务。和 iOS 不同，Android 生态中没有统一的推送服务。Google 虽然有 Google Cloud Messaging ，但是连国外都没统一，更别说国内了，直接被墙。 所以之前在 Android 上做推送大部分只能靠轮询。而我们之前在技术调研的时候，搜到了 jPush 的博客，上面介绍了一些他们的技术特点，他们主要做的其实就是移动网络下的长连接服务。单机 50W-100W 的连接的确是吓我一跳！后来我们也采用了他们的免费方案，因为是一个受众面很小的产品，所以他们的免费版够我们用了。一年多下来，运作稳定，非常不错！ 时隔两年，换了部门后，竟然接到了一项任务，优化公司自己的长连接服务端。 再次搜索网上技术资料后才发现，相关的很多难点都被攻破，网上也有了很多的总结文章，单机 50W-100W 的连接完全不是梦，其实人人都可以做到。但是光有连接还不够，QPS 也要一起上去。 所以，这篇文章就是汇总一下利用 Netty 实现长连接服务过程中的各种难点和可优化点。 Netty 是什么Netty: http://netty.io/ Netty is an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers &amp; clients. 官方的解释最精准了，期中最吸引人的就是高性能了。但是很多人会有这样的疑问：直接用 NIO 实现的话，一定会更快吧？就像我直接手写 JDBC 虽然代码量大了点，但是一定比 iBatis 快！ 但是，如果了解 Netty 后你才会发现，这个还真不一定！ 利用 Netty 而不用 NIO 直接写的优势有这些： 高性能高扩展的架构设计，大部分情况下你只需要关注业务而不需要关注架构 Zero-Copy 技术尽量减少内存拷贝 为 Linux 实现 Native 版 Socket 写同一份代码，兼容 java 1.7 的 NIO2 和 1.7 之前版本的 NIO Pooled Buffers 大大减轻 Buffer 和释放 Buffer 的压力 …… 特性太多，大家可以去看一下《Netty in Action》这本书了解更多。 另外，Netty 源码是一本很好的教科书！大家在使用的过程中可以多看看它的源码，非常棒！ 瓶颈是什么想要做一个长链服务的话，最终的目标是什么？而它的瓶颈又是什么？ 其实目标主要就两个： 更多的连接 更高的 QPS 所以，下面就针对这连个目标来说说他们的难点和注意点吧。 更多的连接非阻塞 IO其实无论是用 Java NIO 还是用 Netty，达到百万连接都没有任何难度。因为它们都是非阻塞的 IO，不需要为每个连接创建一个线程了。 欲知详情，可以搜索一下BIO,NIO,AIO的相关知识点。 Java NIO 实现百万连接123456789101112131415161718ServerSocketChannel ssc = ServerSocketChannel.open();Selector sel = Selector.open();ssc.configureBlocking(false);ssc.socket().bind(new InetSocketAddress(8080));SelectionKey key = ssc.register(sel, SelectionKey.OP_ACCEPT);while(true) { sel.select(); Iterator it = sel.selectedKeys().iterator(); while(it.hasNext()) { SelectionKey skey = (SelectionKey)it.next(); it.remove(); if(skey.isAcceptable()) { ch = ssc.accept(); } }} 这段代码只会接受连过来的连接，不做任何操作，仅仅用来测试待机连接数极限。 大家可以看到这段代码是 NIO 的基本写法，没什么特别的。 Netty 实现百万连接12345678910111213NioEventLoopGroup bossGroup = new NioEventLoopGroup();NioEventLoopGroup workerGroup= new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(bossGroup, workerGroup);bootstrap.channel( NioServerSocketChannel.class);bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ChannelPipeline pipeline = ch.pipeline(); //todo: add handler }});bootstrap.bind(8080).sync(); 这段其实也是非常简单的 Netty 初始化代码。同样，为了实现百万连接根本没有什么特殊的地方。 瓶颈到底在哪上面两种不同的实现都非常简单，没有任何难度，那有人肯定会问了：实现百万连接的瓶颈到底是什么？ 其实只要 java 中用的是非阻塞 IO（NIO 和 AIO 都算），那么它们都可以用单线程来实现大量的 Socket 连接。 不会像 BIO 那样为每个连接创建一个线程，因为代码层面不会成为瓶颈。 其实真正的瓶颈是在 Linux 内核配置上，默认的配置会限制全局最大打开文件数(Max Open Files)还会限制进程数。 所以需要对 Linux 内核配置进行一定的修改才可以。 这个东西现在看似很简单，按照网上的配置改一下就行了，但是大家一定不知道第一个研究这个人有多难。 这里直接贴几篇文章，介绍了相关配置的修改方式： 构建C1000K的服务器 100万并发连接服务器笔记之1M并发连接目标达成 淘宝技术分享 HTTP长连接200万尝试及调优 如何验证让服务器支持百万连接一点也不难，我们当时很快就搞定了一个测试服务端，但是最大的问题是，我怎么去验证这个服务器可以支撑百万连接呢？ 我们用 Netty 写了一个测试客户端，它同样用了非阻塞 IO ，所以不用开大量的线程。 但是一台机器上的端口数是有限制的，用root权限的话，最多也就 6W 多个连接了。 所以我们这里用 Netty 写一个客户端，用尽单机所有的连接吧。 1234567891011121314151617NioEventLoopGroup workerGroup = new NioEventLoopGroup();Bootstrap b = new Bootstrap();b.group(workerGroup);b.channel( NioSocketChannel.class);b.handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ChannelPipeline pipeline = ch.pipeline(); //todo:add handler } });for (int k = 0; k &lt; 60000; k++) { //请自行修改成服务端的IP b.connect(127.0.0.1, 8080);} 代码同样很简单，只要连上就行了，不需要做任何其他的操作。 这样只要找到一台电脑启动这个程序即可。这里需要注意一点，客户端最好和服务端一样，修改一下 Linux 内核参数配置。 怎么去找那么多机器按照上面的做法，单机最多可以有 6W 的连接，百万连接起码需要17台机器！ 如何才能突破这个限制呢？其实这个限制来自于网卡。 我们后来通过使用虚拟机，并且把虚拟机的虚拟网卡配置成了桥接模式解决了问题。 根据物理机内存大小，单个物理机起码可以跑4-5个虚拟机，所以最终百万连接只要4台物理机就够了。 讨巧的做法除了用虚拟机充分压榨机器资源外，还有一个非常讨巧的做法，这个做法也是我在验证过程中偶然发现的。 根据 TCP/IP 协议，任何一方发送FIN后就会启动正常的断开流程。而如果遇到网络瞬断的情况，连接并不会自动断开。 那我们是不是可以这样做？ 启动服务端，千万别设置 Socket 的keep-alive属性，默认是不设置的 用虚拟机连接服务器 强制关闭虚拟机 修改虚拟机网卡的 MAC 地址，重新启动并连接服务器 服务端接受新的连接，并保持之前的连接不断 我们要验证的是服务端的极限，所以只要一直让服务端认为有那么多连接就行了，不是吗？ 经过我们的试验后，这种方法和用真实的机器连接服务端的表现是一样的，因为服务端只是认为对方网络不好罢了，不会将你断开。 另外，禁用keep-alive是因为如果不禁用，Socket 连接会自动探测连接是否可用，如果不可用会强制断开。 更高的 QPS由于 NIO 和 Netty 都是非阻塞 IO，所以无论有多少连接，都只需要少量的线程即可。而且 QPS 不会因为连接数的增长而降低（在内存足够的前提下）。 而且 Netty 本身设计得足够好了，Netty 不是高 QPS 的瓶颈。那高 QPS 的瓶颈是什么？ 是数据结构的设计！ 如何优化数据结构首先要熟悉各种数据结构的特点是必需的，但是在复杂的项目中，不是用了一个集合就可以搞定的，有时候往往是各种集合的组合使用。 既要做到高性能，还要做到一致性，还不能有死锁，这里难度真的不小… 我在这里总结的经验是，不要过早优化。优先考虑一致性，保证数据的准确，然后再去想办法优化性能。 因为一致性比性能重要得多，而且很多性能问题在量小和量大的时候，瓶颈完全会在不同的地方。 所以，我觉得最佳的做法是，编写过程中以一致性为主，性能为辅；代码完成后再去找那个 TOP1，然后去解决它！ 解决 CPU 瓶颈在做这个优化前，先在测试环境中去狠狠地压你的服务器，量小量大，天壤之别。 有了压力测试后，就需要用工具来发现性能瓶颈了！ 我喜欢用的是 VisualVM，打开工具后看抽样器(Sample)，根据自用时间(Self Time (CPU))倒序，排名第一的就是你需要去优化的点了！ 备注：Sample 和 Profiler 有什么区别？前者是抽样，数据不是最准但是不影响性能；后者是统计准确，但是非常影响性能。 如果你的程序非常耗 CPU，那么尽量用 Sample，否则开启 Profiler 后降低性能，反而会影响准确性。 还记得我们项目第一次发现的瓶颈竟然是ConcurrentLinkedQueue这个类中的size()方法。 量小的时候没有影响，但是Queue很大的时候，它每次都是从头统计总数的，而这个size()方法我们又是非常频繁地调用的，所以对性能产生了影响。 size()的实现如下： 123456789public int size() { int count = 0; for (Node&lt;E&gt; p = first(); p != null; p = succ(p)) if (p.item != null) // Collection.size() spec says to max out if (++count == Integer.MAX_VALUE) break; return count;} 后来我们通过额外使用一个AtomicInteger来计数，解决了问题。但是分离后岂不是做不到高一致性呢？ 没关系，我们的这部分代码关心最终一致性，所以只要保证最终一致就可以了。 总之，具体案例要具体分析，不同的业务要用不同的实现。 解决 GC 瓶颈GC 瓶颈也是 CPU 瓶颈的一部分，因为不合理的 GC 会大大影响 CPU 性能。 这里还是在用 VisualVM，但是你需要装一个插件：VisualGC 有了这个插件后，你就可以直观的看到 GC 活动情况了。 按照我们的理解，在压测的时候，有大量的 New GC 是很正常的，因为有大量的对象在创建和销毁。 但是一开始有很多 Old GC 就有点说不过去了！ 后来发现，在我们压测环境中，因为 Netty 的 QPS 和连接数关联不大，所以我们只连接了少量的连接。内存分配得也不是很多。 而 JVM 中，默认的新生代和老生代的比例是1:2，所以大量的老生代被浪费了，新生代不够用。 通过调整 -XX:NewRatio 后，Old GC 有了显著的降低。 但是，生产环境又不一样了，生产环境不会有那么大的 QPS，但是连接会很多，连接相关的对象存活时间非常长，所以生产环境更应该分配更多的老生代。 总之，GC 优化和 CPU 优化一样，也需要不断调整，不断优化，不是一蹴而就的。 其他优化如果你已经完成了自己的程序，那么一定要看看《Netty in Action》作者的这个网站：Netty Best Practices a.k.a Faster == Better。 相信你会受益匪浅，经过里面提到的一些小小的优化后，我们的整体 QPS 提升了很多。 最后一点就是，java 1.7 比 java 1.6 性能高很多！因为 Netty 的编写风格是事件机制的，看似是 AIO。 可 java 1.6 是没有 AIO 的，java 1.7 是支持 AIO 的，所以如果用 java 1.7 的话，性能也会有显著提升。 最后成果经过几周的不断压测和不断优化了，我们在一台16核、120G内存(JVM只分配8G)的机器上，用 java 1.6 达到了60万的连接和20万的QPS。 其实这还不是极限，JVM 只分配了8G内存，内存配置再大一点连接数还可以上去； QPS 看似很高，System Load Average 很低，也就是说明瓶颈不在 CPU 也不在内存，那么应该是在 IO 了！ 上面的 Linux 配置是为了达到百万连接而配置的，并没有针对我们自己的业务场景去做优化。 因为目前性能完全够用，线上单机 QPS 最多才 1W，所以我们先把精力放在了其他地方。 相信后面我们还会去继续优化这块的性能，期待 QPS 能有更大的突破！","link":"/netty-persistent-connection-difficulty-and-improvement/"},{"title":"研究网卡地址注册时的一点思考","text":"我曾经写过一篇和本文标题类似的文章《研究优雅停机时的一点思考》，上文和本文都有一个共同点：网卡地址注册和优雅停机都是一个很小的知识点，但是背后牵扯到的知识点却是庞大的体系，我在写这类文章前基本也和大多数读者一样，处于“知道有这么个东西，但不了解细节”的阶段，但一旦深挖，会感受到其中的奇妙，并有机会接触到很多平时不太关注的知识点。 另外，我还想介绍一个叫做”元阅读“的技巧，可能这个词是我自己造的，也有人称之为”超视角阅读“。其内涵指的是，普通读者从我的文章中学到的是某个知识点，而元阅读者从我的文章中可能会额外关注，我是如何掌握某个知识点的，在一个知识点的学习过程中我关注了哪些相关的知识点，又是如何将它们联系在一起，最终形成一个体系的。这篇文章就是一个典型的例子，我会对一些点进行发散，大家可以尝试着跟我一起来思考”网卡地址注册“这个问题。 1 如何选择合适的网卡地址可能相当一部分人还不知道我这篇文章到底要讲什么，我说个场景，大家应该就明晰了。在分布式服务调用过程中，以 Dubbo 为例，服务提供者往往需要将自身的 IP 地址上报给注册中心，供消费者去发现。在大多数情况下 Dubbo 都可以正常工作，但如果你留意过 Dubbo 的 github issue，其实有不少人反馈：Dubbo Provider 注册了错误的 IP。如果你能立刻联想到：多网卡、内外网地址共存、VPN、虚拟网卡等关键词，那我建议你一定要继续将本文看下去，因为我也想到了这些，它们都是本文所要探讨的东西！那么“如何选择合适的网卡地址”呢，Dubbo 现有的逻辑到底算不算完备？我们不急着回答它，而是带着这些问题一起进行研究，相信到文末，其中答案，各位看官自有评说。 2 Dubbo 是怎么做的Dubbo 获取网卡地址的逻辑在各个版本中也是千回百转，走过弯路，也做过优化，我们用最新的 2.7.2-SNAPSHOT 版本来介绍，在看以下源码时，大家可以怀着质疑的心态去阅读，在 dubbo github 的 master 分支可以获取源码。获取 localhost 的逻辑位于 org.apache.dubbo.common.utils.NetUtils#getLocalAddress0() 之中 123456789101112131415161718192021222324252627private static InetAddress getLocalAddress0() { InetAddress localAddress = null; // 首先尝试获取 /etc/hosts 中 hostname 对应的 IP localAddress = InetAddress.getLocalHost(); Optional&lt;InetAddress&gt; addressOp = toValidAddress(localAddress); if (addressOp.isPresent()) { return addressOp.get(); } // 没有找到适合注册的 IP，则开始轮询网卡 Enumeration&lt;NetworkInterface&gt; interfaces = NetworkInterface.getNetworkInterfaces(); if (null == interfaces) { return localAddress; } while (interfaces.hasMoreElements()) { NetworkInterface network = interfaces.nextElement(); Enumeration&lt;InetAddress&gt; addresses = network.getInetAddresses(); while (addresses.hasMoreElements()) { // 返回第一个匹配的适合注册的 IP Optional&lt;InetAddress&gt; addressOp = toValidAddress(addresses.nextElement()); if (addressOp.isPresent()) { return addressOp.get(); } } } return localAddress;} Dubbo 这段选取本地地址的逻辑大致分成了两步 先去 /etc/hosts 文件中找 hostname 对应的 IP 地址，找到则返回；找不到则转 2 轮询网卡，寻找合适的 IP 地址，找到则返回；找不到返回 null，再 getLocalAddress0 外侧还有一段逻辑，如果返回 null，则注册 127.0.0.1 这个本地回环地址 首先强调下，这段逻辑并没有太大的问题，先别急着挑刺，让我们来分析下其中的一些细节，并进行验证。 2.1 尝试获取 hostname 映射 IPDubbo 首先选取的是 hostname 对应的 IP，在源码中对应的 InetAddress.getLocalHost(); 在 *nix 系统实际部署 Dubbo 应用时，可以首先使用 hostname 命令获取主机名 12xujingfengdeMacBook-Pro:~ xujingfeng$ hostnamexujingfengdeMacBook-Pro.local 紧接着在 /etc/hosts 配置 IP 映射，为了验证 Dubbo 的机制，我们随意为 hostname 配置一个 IP 地址 12127.0.0.1 localhost1.2.3.4 xujingfengdeMacBook-Pro.local 接着调用 NetUtils.getLocalAddress0() 进行验证，控制台打印如下： 1xujingfengdeMacBook-Pro.local/1.2.3.4 2.2 判定有效的 IP 地址在 toValidAddress 逻辑中，Dubbo 存在以下逻辑判定一个 IP 地址是否有效 123456789101112private static Optional&lt;InetAddress&gt; toValidAddress(InetAddress address) { if (address instanceof Inet6Address) { Inet6Address v6Address = (Inet6Address) address; if (isValidV6Address(v6Address)) { return Optional.ofNullable(normalizeV6Address(v6Address)); } } if (isValidV4Address(address)) { return Optional.of(address); } return Optional.empty();} 依次校验其符合 Ipv6 或者 Ipv4 的 IP 规范，对于 Ipv6 的地址，见如下代码： 123456789101112static boolean isValidV6Address(Inet6Address address) { boolean preferIpv6 = Boolean.getBoolean(&quot;java.net.preferIPv6Addresses&quot;); if (!preferIpv6) { return false; } try { return address.isReachable(100); } catch (IOException e) { // ignore } return false;} 首先获取 java.net.preferIPv6Addresses 参数，其默认值为 false，鉴于大多数应用并没有使用 Ipv6 地址作为理想的注册 IP，这问题不大，紧接着通过 isReachable 判断网卡的连通性。例如一些网卡可能是 VPN/ 虚拟网卡的地址，如果没有配置路由表，往往无法连通，可以将之过滤。 对于 Ipv4 的地址，见如下代码： 1234567891011static boolean isValidV4Address(InetAddress address) { if (address == null || address.isLoopbackAddress()) { return false; } String name = address.getHostAddress(); boolean result = (name != null &amp;&amp; IP_PATTERN.matcher(name).matches() &amp;&amp; !Constants.ANYHOST_VALUE.equals(name) &amp;&amp; !Constants.LOCALHOST_VALUE.equals(name)); return result;} 对比 Ipv6 的判断，这里我们已经发现前后不对称的情况了 Ipv4 相比 Ipv6 的逻辑多了 Ipv4 格式的正则校验、本地回环地址校验、ANYHOST 校验 Ipv4 相比 Ipv6 的逻辑少了网卡连通性的校验 大家都知道，Ipv4 将 127.0.0.1 定为本地回环地址， Ipv6 也存在回环地址：0:0:0:0:0:0:0:1 或者表示为 ::1。改进建议也很明显，我们放到文末统一总结。 2.3 轮询网卡如果上述地址获取为 null 则进入轮询网卡的逻辑（例如 hosts 未指定 hostname 的映射或者 hostname 配置成了 127.0.0.1 之类的地址便会导致获取到空的网卡地址），轮询网卡对应的源码是 NetworkInterface.getNetworkInterfaces() ，这里面涉及的知识点就比较多了，支撑起了我写这篇文章的素材，Dubbo 的逻辑并不复杂，进行简单的校验，返回第一个可用的 IP 即可。 性子急的读者可能忍不住了，多网卡！合适的网卡可能不止一个，Dubbo 怎么应对呢？按道理说，我们也替 Dubbo 说句公道话，客官要不你自己指定下？我们首先得对多网卡的场景达成一致看法，才能继续把这篇文章完成下去：我们只能 ** 尽可能 ** 过滤那些“** 不对 **”的网卡。Dubbo 看样子对所有网卡是一视同仁了，那么是不是可以尝试优化一下其中的逻辑呢？ 许多开源的服务治理框架在 stackoverflow 或者其 issue 中，注册错 IP 相关的问题都十分高频，大多数都是轮询网卡出了问题。既然事情发展到这儿，势必需要了解一些网络、网卡的知识，我们才能过滤掉那些明显不适合 RPC 服务注册的 IP 地址了。 3 Ifconfig 介绍我并没有想要让大家对后续的内容望而却步，特地选择了这个大家最熟悉的 Linux 命令！对于那些吐槽：“天呐，都 2019 年了，你怎么还在用 net-tools/ifconfig，iproute2/ip 了解一下”的言论，请大家视而不见。无论你使用的是 mac，还是 linux，都可以使用它去 CRUD 你的网卡配置。 3.1 常用指令** 启动关闭指定网卡：** 12ifconfig eth0 upifconfig eth0 down ifconfig eth0 up 为启动网卡 eth0，ifconfig eth0 down 为关闭网卡 eth0。ssh 登陆 linux 服务器操作的用户要小心执行这个操作了，千万不要蠢哭自己。不然你下一步就需要去 google：“禁用 eth0 网卡后如何远程连接 Linux 服务器” 了。 ** 为网卡配置和删除 IPv6 地址：** 12ifconfig eth0 add 33ffe:3240:800:1005::2/64 #为网卡 eth0 配置 IPv6 地址ifconfig eth0 del 33ffe:3240:800:1005::2/64 #为网卡 eth0 删除 IPv6 地址 ** 用 ifconfig 修改 MAC 地址：** 1ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE ** 配置 IP 地址：** 123[root@localhost ~]# ifconfig eth0 192.168.2.10[root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0[root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0 broadcast 192.168.2.255 ** 启用和关闭 arp 协议：** 12ifconfig eth0 arp #开启网卡 eth0 的 arp 协议ifconfig eth0 -arp #关闭网卡 eth0 的 arp 协议 ** 设置最大传输单元：** 1ifconfig eth0 mtu 1500 #设置能通过的最大数据包大小为 1500 bytes 3.2 查看网卡信息在一台 ubuntu 上执行 ifconfig -a 12345678910111213141516171819202122232425262728293031ubuntu@VM-30-130-ubuntu:~$ ifconfig -aeth0 Link encap:Ethernet HWaddr 52:54:00:a9:5f:ae inet addr:10.154.30.130 Bcast:10.154.63.255 Mask:255.255.192.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:149673 errors:0 dropped:0 overruns:0 frame:0 TX packets:152271 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:15205083 (15.2 MB) TX bytes:21386362 (21.3 MB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) docker0 Link encap:Ethernet HWaddr 02:42:58:45:c1:15 inet addr:172.17.0.1 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)tun0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 UP POINTOPOINT NOARP MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:100 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 为了防止黑客对我的 Linux 发起攻击，我还是偷偷对 IP 做了一点“改造”，请不要为难一个趁着打折 + 组团购买廉价云服务器的小伙子。对于部门网卡的详细解读: eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址，可以看到目前这个网卡的物理地址 (MAC 地址）是 02:42:38:52:70:54 inet addr 用来表示网卡的 IP 地址，此网卡的 IP 地址是 10.154.30.130，广播地址， Bcast: 172.18.255.255，掩码地址 Mask:255.255.0.0 lo 是表示主机的回环地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 HTTPD 服务器的指定到回坏地址，在浏览器输入 127.0.0.1 就能看到你所架 WEB 网站了。但只是你能看得到，局域网的其它主机或用户无从知晓。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件 mac 地址） 第二行：网卡的 IP 地址、子网、掩码 第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500 字节（ipconfig 不加 -a 则无法看到 DOWN 的网卡） 第四、五行：接收、发送数据包情况统计 第七行：接收、发送数据字节数统计信息。 紧接着的两个网卡 docker0，tun0 是怎么出来的呢？我在我的 ubuntu 上装了 docker 和 openvpn。这两个东西应该是日常干扰我们做服务注册时的罪魁祸首了，当然，也有可能存在 eth1 这样的第二块网卡。ifconfig -a 看到的东西就对应了 JDK 的 api ：NetworkInterface.getNetworkInterfaces() 。我们简单做个总结，大致有三个干扰因素 以 docker 网桥为首的虚拟网卡地址，毕竟这东西这么火，怎么也得单独列出来吧？ 以 TUN/TAP 为代表的虚拟网卡地址，多为 VPN 场景 以 eth1 为代表的多网卡场景，有钱就可以装多网卡了！ 我们后续的篇幅将针对这些场景做分别的介绍，力求让大家没吃过猪肉，起码看下猪怎么跑的。 4 干扰因素一：Docker 网桥熟悉 docker 的朋友应该知道 docker 会默认创建一个 docker0 的网桥，供容器实例连接。如果嫌默认的网桥不够直观，我们可以使用 bridge 模式自定义创建一个新的网桥： 12345678910ubuntu@VM-30-130-ubuntu:~$ docker network create kirito-bridgea38696dbbe58aa916894c674052c4aa6ab32266dcf6d8111fb794b8a344aa0d9ubuntu@VM-30-130-ubuntu:~$ ifconfig -abr-a38696dbbe58 Link encap:Ethernet HWaddr 02:42:6e:aa:fd:0c inet addr:172.19.0.1 Bcast:172.19.255.255 Mask:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 使用 docker network 指令创建网桥之后，自动创建了对应的网卡，我只给出了 ifconfig -a 的增量返回部分，可以看出多了一个 br-a38696dbbe58 的网卡。 我有意区分了“网桥”和“网卡”，可以使用 bridge-utils/brctl 来查看网桥信息： 1234ubuntu@VM-30-130-ubuntu:~$ sudo brctl showbridge name bridge id STP enabled interfacesbr-a38696dbbe58 8000.02426eaafd0c nodocker0 8000.02425845c215 no 网桥是一个虚拟设备，这个设备只有 brctl show 能看到，网桥创建之后，会自动创建一个同名的网卡，并将这个网卡加入网桥。 5 干扰因素二：TUN/TAP 虚拟网络设备平时我们所说的虚拟网卡、虚拟机，大致都跟 TUN/TAP 有关。我的读者大多数是 Java 从业者，相信我下面的内容并没有太超纲，不要被陌生的名词唬住。对于被唬住的读者，也可以直接跳过 5.1~5.3，直接看 5.4 的实战。 5.1 真实网卡工作原理 上图中的 eth0 表示我们主机已有的真实的网卡接口 (interface)。 网卡接口 eth0 所代表的真实网卡通过网线 (wire) 和外部网络相连，该物理网卡收到的数据包会经由接口 eth0 传递给内核的网络协议栈(Network Stack)。然后协议栈对这些数据包进行进一步的处理。 对于一些错误的数据包, 协议栈可以选择丢弃；对于不属于本机的数据包，协议栈可以选择转发；而对于确实是传递给本机的数据包, 而且该数据包确实被上层的应用所需要，协议栈会通过 Socket API 告知上层正在等待的应用程序。 5.2 TUN 工作原理 我们知道，普通的网卡是通过网线来收发数据包的话，而 TUN 设备比较特殊，它通过一个文件收发数据包。 如上图所示，tunX 和上面的 eth0 在逻辑上面是等价的， tunX 也代表了一个网络接口, 虽然这个接口是系统通过软件所模拟出来的. 网卡接口 tunX 所代表的虚拟网卡通过文件 /dev/tunX 与我们的应用程序 (App) 相连，应用程序每次使用 write 之类的系统调用将数据写入该文件，这些数据会以网络层数据包的形式，通过该虚拟网卡，经由网络接口 tunX 传递给网络协议栈，同时该应用程序也可以通过 read 之类的系统调用，经由文件 /dev/tunX 读取到协议栈向 tunX 传递的 ** 所有 ** 数据包。 此外，协议栈可以像操纵普通网卡一样来操纵 tunX 所代表的虚拟网卡。比如说，给 tunX 设定 IP 地址，设置路由，总之，在协议栈看来，tunX 所代表的网卡和其他普通的网卡区别不大，当然，硬要说区别，那还是有的, 那就是 tunX 设备不存在 MAC 地址，这个很好理解，tunX 只模拟到了网络层，要 MAC ** 地址没有任何意义。当然，如果是 tapX 的话，在协议栈的眼中，tapX** 和真实网卡没有任何区别。 是不是有些懵了？我是谁，为什么我要在这篇文章里面学习 TUN！因为我们常用的 VPN 基本就是基于 TUN/TAP 搭建的，如果我们使用 TUN 设备搭建一个基于 UDP 的 VPN ，那么整个处理过程可能是这幅样子： 5.3 TAP 工作原理TAP 设备与 TUN 设备工作方式完全相同，区别在于： TUN 设备是一个三层设备，它只模拟到了 IP 层，即网络层 我们可以通过 /dev/tunX 文件收发 IP 层数据包，它无法与物理网卡做 bridge，但是可以通过三层交换（如 ip_forward）与物理网卡连通。可以使用 ifconfig 之类的命令给该设备设定 IP 地址。 TAP 设备是一个二层设备，它比 TUN 更加深入，通过 /dev/tapX 文件可以收发 MAC 层数据包，即数据链路层，拥有 MAC 层功能，可以与物理网卡做 bridge，支持 MAC 层广播。同样的，我们也可以通过 ifconfig 之类的命令给该设备设定 IP 地址，你如果愿意，我们可以给它设定 MAC 地址。 关于文章中出现的二层，三层，我这里说明一下，第一层是物理层，第二层是数据链路层，第三层是网络层，第四层是传输层。 5.4 openvpn 实战openvpn 是 Linux 上一款开源的 vpn 工具，我们通过它来复现出影响我们做网卡选择的场景。 安装 openvpn 1sudo apt-get install openvpn 安装一个 TUN 设备： 123ubuntu@VM-30-130-ubuntu:~$ sudo openvpn --mktun --dev tun0Mon Apr 29 22:23:31 2019 TUN/TAP device tun0 openedMon Apr 29 22:23:31 2019 Persist state set to: ON 安装一个 TAP 设备： 123ubuntu@VM-30-130-ubuntu:~$ sudo openvpn --mktun --dev tap0Mon Apr 29 22:24:36 2019 TUN/TAP device tap0 openedMon Apr 29 22:24:36 2019 Persist state set to: ON 执行 ifconfig -a 查看网卡，只给出增量的部分： 1234567891011121314tap0 Link encap:Ethernet HWaddr 7a:a2:a8:f1:6b:df BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:100 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)tun0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 inet addr:10.154.30.131 P-t-P:10.154.30.131 Mask:255.255.255.255 UP POINTOPOINT NOARP MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:100 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 这样就解释了文章一开始为什么会有 tun0 这样的网卡了。这里读者可能会有疑惑，使用 ifconfig 不是也可以创建 tap 和 tun 网卡吗？当然啦，openvpn 是一个 vpn 工具，只能创建名为 tunX/tapX 的网卡，其遵守着一定的规范，ifconfig 可以随意创建，但没人认那些随意创建的网卡。 6 干扰因素三：多网卡 这个没有太多好说的，有多张真实的网卡，从普哥那儿搞到如上的 IP 信息。 7 MAC 下的差异虽然 ifconfig 等指令是 *nux 通用的，但是其展示信息，网卡相关的属性和命名都有较大的差异。例如这是我 MAC 下执行 ifconfig -a 的返回： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859xujingfengdeMacBook-Pro:dubbo-in-action xujingfeng$ ifconfig -alo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384 options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt; inet 127.0.0.1 netmask 0xff000000 inet6 ::1 prefixlen 128 inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 nd6 options=201&lt;PERFORMNUD,DAD&gt;gif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280stf0: flags=0&lt;&gt; mtu 1280XHC0: flags=0&lt;&gt; mtu 0XHC20: flags=0&lt;&gt; mtu 0en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether 88:e9:fe:88:a0:76 inet6 fe80::1cab:f689:60d1:bacb%en0 prefixlen 64 secured scopeid 0x6 inet 30.130.11.242 netmask 0xffffff80 broadcast 30.130.11.255 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect status: activep2p0: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 2304 ether 0a:e9:fe:88:a0:76 media: autoselect status: inactiveawdl0: flags=8943&lt;UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1484 ether 66:d2:8c:8c:dd:85 inet6 fe80::64d2:8cff:fe8c:dd85%awdl0 prefixlen 64 scopeid 0x8 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect status: activeen1: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether aa:00:d0:13:0e:01 media: autoselect &lt;full-duplex&gt; status: inactiveen2: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500 options=60&lt;TSO4,TSO6&gt; ether aa:00:d0:13:0e:00 media: autoselect &lt;full-duplex&gt; status: inactivebridge0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt; ether aa:00:d0:13:0e:01 Configuration: id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0 maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200 root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0 ipfilter disabled flags 0x2 member: en1 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 9 priority 0 path cost 0 member: en2 flags=3&lt;LEARNING,DISCOVER&gt; ifmaxaddr 0 port 10 priority 0 path cost 0 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: &lt;unknown type&gt; status: inactiveutun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000 inet6 fe80::3fe0:3e8b:384:9968%utun0 prefixlen 64 scopeid 0xc nd6 options=201&lt;PERFORMNUD,DAD&gt;utun1: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1380 inet6 fe80::7894:3abc:5abd:457d%utun1 prefixlen 64 scopeid 0xd nd6 options=201&lt;PERFORMNUD,DAD&gt; 内容很多，我挑几点差异简述下： 内容展示形式不一样，没有 Linux 下的接收、发送数据字节数等统计信息 真实网卡的命名不一样：eth0 -&gt; en0 虚拟网卡的命名格式不一样：tun/tap -&gt; utun 对于这些常见网卡命名的解读，我摘抄一部分来自 stackoverflow 的回答： In arbitrary order of my familarity / widespread relevance: lo0 is loopback. en0 at one point “ethernet”, now is WiFi (and I have no idea what extra en1 or en2 are used for). fw0 is the FireWire network interface. stf0 is an IPv6 to IPv4 tunnel interface to support the transition from IPv4 to the IPv6 standard. gif0 is a more generic tunneling interface [46]-to-[46]. awdl0 is Apple Wireless Direct Link p2p0 is related to AWDL features. Either as an old version, or virtual interface with different semantics than awdl. the “Network” panel in System Preferences to see what network devices “exist” or “can exist” with current configuration. many VPNs will add additional devices, often “utun#” or “utap#” following TUN/TAP (L3/L2)virtual networking devices. use netstat -nr to see how traffic is currently routed via network devices according to destination. interface naming conventions started in BSD were retained in OS X / macOS, and now there also additions. 8 Dubbo 改进建议我们进行了以上探索，算是对网卡有一点了解了。回过头来看看 Dubbo 获取网卡的逻辑，是否可以做出改进呢？ Dubbo Action 1: 保持 Ipv4 和 Ipv6 的一致性校验。为 Ipv4 增加连通性校验；为 Ipv6 增加 LoopBack 和 ANYHOST 等校验。 Dubbo Action 2: 1234NetworkInterface network = interfaces.nextElement();if (network.isLoopback() || network.isVirtual()|| !network.isUp()) { continue;} JDK 提供了以上的 API，我们可以利用起来，过滤一部分一定不正确的网卡。 Dubbo Action 3: 我们本文花了较多的篇幅介绍了 docker 和 TUN/TAP 两种场景导致的虚拟网卡的问题，算是较为常见的一个影响因素，虽然他们的命名具有固定性，如 docker0、tunX、tapX，但我觉得通过网卡名称的判断方式去过滤注册 IP 有一些 hack，所以不建议 dubbo contributor 提出相应的 pr 去增加这些 hack 判断，尽管可能会对判断有所帮助。 对于真实多网卡、内外网 IP 共存的场景，不能仅仅是框架侧在做努力，用户也需要做一些事，就像爱情一样，我可以主动一点，但你也得反馈，才能发展出故事。 Dubbo User Action 1: 可以配置 /etc/hosts 文件，将 hostname 对应的 IP 显示配置进去。 Dubbo User Action 2: 可以使用启动参数去显示指定注册的 IP： 1-DDUBBO_IP_TO_REGISTRY=1.2.3.4 也可以指定 Dubbo 服务绑定在哪块网卡上： 1-DDUBBO_IP_TO_BIND=1.2.3.4 9 参考文章TUN/TAP 设备浅析 what-are-en0-en1-p2p-and-so-on-that-are-displayed-after-executing-ifconfig ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/network-interfaces/"},{"title":"一文探讨堆外内存的监控与回收","text":"引子记得那是一个风和日丽的周末，太阳红彤彤，花儿五颜六色，96 年的普哥微信找到我，描述了一个诡异的线上问题：线上程序使用了 NIO FileChannel 的 堆内内存（HeapByteBuffer）作为缓冲区，读写文件，逻辑可以说相当简单，但根据监控，却发现堆外内存（DirectByteBuffer）飙升，导致了 OutOfMemeory 的异常。 由这个线上问题，引出了这篇文章的主题，主要包括：FileChannel 源码分析，堆外内存监控，堆外内存回收。 问题分析 &amp; 源码分析根据异常日志的定位，发现的确使用的是 HeapByteBuffer 来进行读写，但却导致堆外内存飙升，随即翻了 FileChannel 的源码，来一探究竟。 FileChannel 使用的是 IOUtil 进行读写操作（本文只分析读的逻辑，写和读的代码逻辑一致，不做重复分析） 12345678910111213141516171819202122//sun.nio.ch.IOUtil#readstatic int read(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { if (var1.isReadOnly()) { throw new IllegalArgumentException(&quot;Read-only buffer&quot;); } else if (var1 instanceof DirectBuffer) { return readIntoNativeBuffer(var0, var1, var2, var4); } else { ByteBuffer var5 = Util.getTemporaryDirectBuffer(var1.remaining()); int var7; try { int var6 = readIntoNativeBuffer(var0, var5, var2, var4); var5.flip(); if (var6 &gt; 0) { var1.put(var5); } var7 = var6; } finally { Util.offerFirstTemporaryDirectBuffer(var5); } return var7; }} 可以发现当使用 HeapByteBuffer 时，会走到下面这行看似有点疑问的代码分支： 1Util.getTemporaryDirectBuffer(var1.remaining()); 这个 Util 封装了更为底层的一些 IO 逻辑 123456789101112131415161718192021222324package sun.nio.ch;public class Util { private static ThreadLocal&lt;Util.BufferCache&gt; bufferCache; public static ByteBuffer getTemporaryDirectBuffer(int var0) { if (isBufferTooLarge(var0)) { return ByteBuffer.allocateDirect(var0); } else { // FOUCS ON THIS LINE Util.BufferCache var1 = (Util.BufferCache)bufferCache.get(); ByteBuffer var2 = var1.get(var0); if (var2 != null) { return var2; } else { if (!var1.isEmpty()) { var2 = var1.removeFirst(); free(var2); } return ByteBuffer.allocateDirect(var0); } } }} isBufferTooLarge 这个方法会根据传入 Buffer 的大小决定如何分配堆外内存，如果过大，直接分配大缓冲区；如果不是太大，会使用 bufferCache 这个 ThreadLocal 变量来进行缓存，从而复用（实际上这个数值非常大，几乎不会走进直接分配堆外内存这个分支）。这么看来似乎发现了两个不得了的结论： 使用 HeapByteBuffer 读写都会经过 DirectByteBuffer，写入数据的流转方式其实是：HeapByteBuffer -&gt; DirectByteBuffer -&gt; PageCache -&gt; Disk，读取数据的流转方式正好相反。 使用 HeapByteBuffer 读写会申请一块跟线程绑定的 DirectByteBuffer。这意味着，线程越多，临时 DirectByteBuffer 就越会占用越多的空间。 看到这儿，线上的问题似乎有了一点眉目：很有可能是多线程使用 HeapByteBuffer 写入文件，而额外分配的这块 DirectByteBuffer 导致了内存溢出。在验证这个猜测之前，我们最好能直观地监控到堆外内存的使用量，这才能增加我们定位问题的信心。 实现堆外内存的监控JDK 提供了一个非常好用的监控工具 —— Java VisualVM。我们只需要为它安装一个插件，即可很方便地实现堆外内存的监控。 进入本地 JDK 的可执行目录（在我本地是：/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/bin），找到 jvisualvm 命令，双击打开一个可视化的界面 左侧树状目录可以选择需要监控的 Java 进程，右侧是监控的维度信息，除了 CPU、线程、堆、类等信息，还可以通过上方的【工具 (T)】 安装插件，增加 MBeans、Buffer Pools 等维度的监控。 Buffer Pools 插件可以监控堆外内存（包含 DirectByteBuffer 和 MappedByteBuffer），如下图所示： 左侧对应 DirectByteBuffer，右侧对应 MappedByteBuffer。 复现问题为了复现线上的问题，我们使用一个程序，不断开启线程使用堆内内存作为缓冲区进行文件的读取操作，并监控该进程的堆外内存使用情况。 123456789101112131415161718192021public class ReadByHeapByteBufferTest { public static void main(String[] args) throws IOException, InterruptedException { File data = new File(&quot;/tmp/data.txt&quot;); FileChannel fileChannel = new RandomAccessFile(data, &quot;rw&quot;).getChannel(); ByteBuffer buffer = ByteBuffer.allocate(4 * 1024 * 1024); for (int i = 0; i &lt; 1000; i++) { Thread.sleep(1000); new Thread(new Runnable() { @Override public void run() { try { fileChannel.read(buffer); buffer.clear(); } catch (IOException e) { e.printStackTrace(); } } }).start(); } }} 运行一段时间后，我们观察下堆外内存的使用情况 如上图左所示，堆外内存的确开始疯涨了，的确符合我们的预期，堆外缓存和线程绑定，当线程非常多时，即使只使用了 4M 的堆内内存，也可能会造成极大的堆外内存膨胀，在中间发生了一次断崖，推测是线程执行完毕 or GC，导致了内存的释放。 知晓了这一点，相信大家今后使用堆内内存时可能就会更加注意了，我总结了两个注意点： 使用 HeapByteBuffer 还需要经过一次 DirectByteBuffer 的拷贝，在追求极致性能的场景下是可以通过直接复用堆外内存来避免的。 多线程下使用 HeapByteBuffer 进行文件读写，要注意 ThreadLocal&lt;Util.BufferCache&gt; bufferCache 导致的堆外内存膨胀的问题。 问题深究那大家有没有想过，为什么 JDK 要如此设计？为什么不直接使用堆内内存写入 PageCache 进而落盘呢？为什么一定要经过 DirectByteBuffer 的拷贝呢？ 在知乎的相关问题中，R 大和 曾泽堂 两位同学进行了解答，是我比较认同的解释： 作者：RednaxelaFX 链接：https://www.zhihu.com/question/57374068/answer/152691891 来源：知乎 这里其实是在迁就 OpenJDK 里的 HotSpot VM 的一点实现细节。 HotSpot VM 里的 GC 除了 CMS 之外都是要移动对象的，是所谓“compacting GC”。 如果要把一个 Java 里的 byte[] 对象的引用传给 native 代码，让 native 代码直接访问数组的内容的话，就必须要保证 native 代码在访问的时候这个 byte[] 对象不能被移动，也就是要被“pin”（钉）住。 可惜 HotSpot VM 出于一些取舍而决定不实现单个对象层面的 object pinning，要 pin 的话就得暂时禁用 GC——也就等于把整个 Java 堆都给 pin 住。 所以 Oracle/Sun JDK / OpenJDK 的这个地方就用了点绕弯的做法。它假设把 HeapByteBuffer 背后的 byte[] 里的内容拷贝一次是一个时间开销可以接受的操作，同时假设真正的 I/O 可能是一个很慢的操作。 于是它就先把 HeapByteBuffer 背后的 byte[] 的内容拷贝到一个 DirectByteBuffer 背后的 native memory 去，这个拷贝会涉及 sun.misc.Unsafe.copyMemory() 的调用，背后是类似 memcpy() 的实现。这个操作本质上是会在整个拷贝过程中暂时不允许发生 GC 的。 然后数据被拷贝到 native memory 之后就好办了，就去做真正的 I/O，把 DirectByteBuffer 背后的 native memory 地址传给真正做 I/O 的函数。这边就不需要再去访问 Java 对象去读写要做 I/O 的数据了。 总结一下就是： 为了方便 GC 的实现，DirectByteBuffer 指向的 native memory 是不受 GC 管辖的 HeapByteBuffer 背后使用的是 byte 数组，其占用的内存不一定是连续的，不太方便 JNI 方法的调用 数组实现在不同 JVM 中可能会不同 堆外内存的回收继续深究下一个话题，也是我的微信交流群中曾经有人提出过的一个疑问，到底该如何回收 DirectByteBuffer？既然可以监控堆外内存，那验证堆外内存的回收就变得很容易实现了。 CASE 1：分配 1G 的 DirectByteBuffer，等待用户输入后，复制为 null，之后阻塞持续观察堆外内存变化 12345678public class WriteByDirectByteBufferTest { public static void main(String[] args) throws IOException, InterruptedException { ByteBuffer buffer = ByteBuffer.allocateDirect(1024 * 1024 * 1024); System.in.read(); buffer = null; new CountDownLatch(1).await(); }} 结论：变量虽然置为了 null，但内存依旧持续占用。 CASE 2：分配 1G DirectByteBuffer，等待用户输入后，复制为 null，手动触发 GC，之后阻塞持续观察堆外内存变化 123456789public class WriteByDirectByteBufferTest { public static void main(String[] args) throws IOException, InterruptedException { ByteBuffer buffer = ByteBuffer.allocateDirect(1024 * 1024 * 1024); System.in.read(); buffer = null; System.gc(); new CountDownLatch(1).await(); }} 结论：GC 时会触发堆外空闲内存的回收。 CASE 3：分配 1G DirectByteBuffer，等待用户输入后，手动回收堆外内存，之后阻塞持续观察堆外内存变化 12345678public class WriteByDirectByteBufferTest { public static void main(String[] args) throws IOException, InterruptedException { ByteBuffer buffer = ByteBuffer.allocateDirect(1024 * 1024 * 1024); System.in.read(); ((DirectBuffer) buffer).cleaner().clean(); new CountDownLatch(1).await(); }} 结论：手动回收可以立刻释放堆外内存，不需要等待到 GC 的发生。 对于 MappedByteBuffer 这个有点神秘的类，它的回收机制大概和 DirectByteBuffer 类似，体现在右边的 Mapped 之中，我们就不重复 CASE1 和 CASE2 的测试了，直接给出结论，在 GC 发生或者操作系统主动清理时 MappedByteBuffer 会被回收。但也不是不进行测试，我们会对 MappedByteBuffer 进行更有意思的研究。 CASE 4：手动回收 MappedByteBuffer。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class MmapUtil { public static void clean(MappedByteBuffer mappedByteBuffer) { ByteBuffer buffer = mappedByteBuffer; if (buffer == null || !buffer.isDirect() || buffer.capacity()== 0) return; invoke(invoke(viewed(buffer), &quot;cleaner&quot;), &quot;clean&quot;); } private static Object invoke(final Object target, final String methodName, final Class&lt;?&gt;... args) { return AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() { public Object run() { try { Method method = method(target, methodName, args); method.setAccessible(true); return method.invoke(target); } catch (Exception e) { throw new IllegalStateException(e); } } }); } private static Method method(Object target, String methodName, Class&lt;?&gt;[] args) throws NoSuchMethodException { try { return target.getClass().getMethod(methodName, args); } catch (NoSuchMethodException e) { return target.getClass().getDeclaredMethod(methodName, args); } } private static ByteBuffer viewed(ByteBuffer buffer) { String methodName = &quot;viewedBuffer&quot;; Method[] methods = buffer.getClass().getMethods(); for (int i = 0; i &lt; methods.length; i++) { if (methods[i].getName().equals(&quot;attachment&quot;)) { methodName = &quot;attachment&quot;; break; } } ByteBuffer viewedBuffer = (ByteBuffer) invoke(buffer, methodName); if (viewedBuffer == null) return buffer; else return viewed(viewedBuffer); }} 这个类曾经在我的《文件 IO 的一些最佳实践》中有所介绍，在这里我们将验证它的作用。编写测试类： 1234567891011public class WriteByMappedByteBufferTest { public static void main(String[] args) throws IOException, InterruptedException { File data = new File(&quot;/tmp/data.txt&quot;); data.createNewFile(); FileChannel fileChannel = new RandomAccessFile(data, &quot;rw&quot;).getChannel(); MappedByteBuffer map = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1024L * 1024 * 1024); System.in.read(); MmapUtil.clean(map); new CountDownLatch(1).await(); }} 结论：通过一顿复杂的反射操作，成功地手动回收了 Mmap 的内存映射。 CASE 5：测试 Mmap 的内存占用 123456789101112public class WriteByMappedByteBufferTest { public static void main(String[] args) throws IOException, InterruptedException { File data = new File(&quot;/tmp/data.txt&quot;); data.createNewFile(); FileChannel fileChannel = new RandomAccessFile(data, &quot;rw&quot;).getChannel(); for (int i = 0; i &lt; 1000; i++) { fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1024L * 1024 * 1024); } System.out.println(&quot;map finish&quot;); new CountDownLatch(1).await(); }} 我尝试映射了 1000G 的内存，我的电脑显然没有 1000G 这么大内存，那么监控是如何反馈的呢？ 几乎在瞬间，控制台打印出了 map finish 的日志，也意味着 1000G 的内存映射几乎是不耗费时间的，为什么要做这个测试？就是为了解释内存映射并不等于内存占用，很多文章认为内存映射这种方式可以大幅度提升文件的读写速度，并宣称“写 MappedByteBuffer 就等于写内存”，实际是非常错误的认知。通过控制面板可以查看到该 Java 进程（pid 39040）实际占用的内存，仅仅不到 100M。(关于 Mmap 的使用场景和方式可以参考我之前的文章) 结论：MappedByteBuffer 映射出一片文件内容之后，不会全部加载到内存中，而是会进行一部分的预读（体现在占用的那 100M 上），MappedByteBuffer 不是文件读写的银弹，它仍然依赖于 PageCache 异步刷盘的机制。** 通过 Java VisualVM 可以监控到 mmap 总映射的大小，但并不是实际占用的内存量 **。 总结本文借助一个线上问题，分析了使用堆内内存仍然会导致堆外内存分析的现象以及背后 JDK 如此设计的原因，并借助安装了插件之后的 Java VisualVM 工具进行了堆外内存的监控，进而讨论了如何正确的回收堆外内存，以及纠正了一个关于 MappedByteBuffer 的错误认知。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/nio-buffer-recycle/"},{"title":"打开 orika 的正确方式","text":"缘起架构分层开发分布式的项目时，DO 持久化对象和 DTO 传输对象的转换是不可避免的。集中式项目中，DO-DAO-SERVICE-WEB 的分层再寻常不过，但分布式架构（或微服务架构）需要拆分模块时，不得不思考一个问题：WEB 层能不能出现 DAO 或者 DO 对象？我给出的答案是否定的。 这张图曾出现在我过去的文章中，其强调了一个分层的要素：服务层 (应用层) 和表现层应当解耦，后者不应当触碰到任何持久化对象，其所有的数据来源，均应当由前者提供。 DTO 的位置就系统的某一个模块，可以大致分成领域层 model，接口定义层 api，接口实现层 / 服务层 service，表现层 web。 service 依赖 model + api web 依赖 api 在我们系统构建初期，DTO 对象被想当然的丢到了 model 层，这导致 web 对 model 产生了依赖；而在后期，为了满足前面的架构分层，最终将 DTO 对象移动到了 api 层（没有单独做一层） 没有 DTO 时的痛点激发出 DTO 这样一个新的分层其实还有两个原因。 其一，便是我们再也不能忍受在 RPC 调用时 JPA/hibernate 懒加载这一特性带来的坑点。如果试图在消费端获取服务端传来的一个懒加载持久化对象，那么很抱歉，下意识就会发现这行不通，懒加载技术本质是使用字节码技术完成对象的代理，然而代理对象无法天然地远程传输，这与你的协议（RPC or HTTP）无关。 其二，远程调用需要额外注意网络传输的开销，如果生产者方从数据库加载出了一个一对多的依赖，而消费者只需要一这个实体的某个属性，多的实体会使得性能产生下降，并没有很好的方式对其进行控制（忽略手动 set）。可能有更多痛点，由此可见，共享持久层，缺少 DTO 层时，我们的系统灵活性和性能都受到了制约。 从 DTO 到 Orika各类博客不乏对 DTO 的讨论，对领域驱动的理解，但却鲜有文章介绍，如何完成 DO 对象到 DTO 对象的转换。我们期待有一款高性能的，易用的工具来帮助我们完成实体类的转换。便引出了今天的主角：Orika。 Orika 是什么？ Orika 是一个简单、快速的 JavaBean 拷贝框架，它能够递归地将数据从一个 JavaBean 复制到另一个 JavaBean，这在多层应用开发中是非常有用的。 Orika 的竞品相信大多数朋友接触过 apache 的 BeanUtils，直到认识了 spring 的 BeanUtils，前者被后者完爆，后来又出现了 Dozer，Orika 等重量级的 Bean 拷贝工具，在性能和特性上都有了很大的提升。 先给结论，众多 Bean 拷贝工具中，今天介绍的 Orika 具有想当大的优势。口说无凭，可参考下面文章中的各个工具的对比：http://tech.dianwoda.com/2017/11/04/gao-xing-neng-te-xing-feng-fu-de-beanying-she-gong-ju-orika/?utm_source=tuicool&amp;utm_medium=referral 简单整理后，如下所示： BeanUtils apache 的 BeanUtils 和 spring 的 BeanUtils 中拷贝方法的原理都是先用 jdk 中 java.beans.Introspector 类的 getBeanInfo() 方法获取对象的属性信息及属性 get/set 方法，接着使用反射（Method 的 invoke(Object obj, Object... args)）方法进行赋值。apache 支持名称相同但类型不同的属性的转换，spring 支持忽略某些属性不进行映射，他们都设置了缓存保存已解析过的 BeanInfo 信息。 BeanCopier cglib 的 BeanCopier 采用了不同的方法：它不是利用反射对属性进行赋值，而是直接使用 ASM 的 MethodVisitor 直接编写各属性的 get/set 方法（具体过程可见 BeanCopier 类的 generateClass(ClassVisitor v) 方法）生成 class 文件，然后进行执行。由于是直接生成字节码执行，所以 BeanCopier 的性能较采用反射的 BeanUtils 有较大提高，这一点可在后面的测试中看出。 Dozer 使用以上类库虽然可以不用手动编写 get/set 方法，但是他们都不能对不同名称的对象属性进行映射。在定制化的属性映射方面做得比较好的有 Dozer，Dozer 支持简单属性映射、复杂类型映射、双向映射、隐式映射以及递归映射。可使用 xml 或者注解进行映射的配置，支持自动类型转换，使用方便。但 Dozer 底层是使用 reflect 包下 Field 类的 set(Object obj, Object value) 方法进行属性赋值，执行速度上不是那么理想。 Orika 那么有没有特性丰富，速度又快的 Bean 映射工具呢，这就是下面要介绍的 Orika，Orika 是近期在 github 活跃的项目，底层采用了 javassist 类库生成 Bean 映射的字节码，之后直接加载执行生成的字节码文件，因此在速度上比使用反射进行赋值会快很多，下面详细介绍 Orika 的使用方法。 Orika 入门引入依赖12345&lt;dependency&gt; &lt;groupId&gt;ma.glasnost.orika&lt;/groupId&gt; &lt;artifactId&gt;orika-core&lt;/artifactId&gt; &lt;version&gt;${orika.version}&lt;/version&gt;&lt;/dependency&gt; 基础概念 MapperFactory 1MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); MapperFactory 用于注册字段映射，配置转换器，自定义映射器等，而我们关注的主要是字段映射这个特性，在下面的小节中会介绍。 MapperFacade 123MapperFacade mapper = mapperFactory.getMapperFacade();PersonSource source = new PersonSource();PersonDest destination = mapper.map(source, PersonDest.class); MapperFacade 和 spring，apache 中的 BeanUtils 具有相同的地位，负责对象间的映射，也是实际使用中，我们使用的最多的类。 至于转换器，自定义映射器等等概念，属于 Orika 的高级特性，也是 Orika 为什么被称作一个重量级框架的原因，引入 Orika 的初衷是为了高性能，易用的拷贝对象，引入它们会给系统带来一定的侵入性，所以本文暂不介绍，详细的介绍，可参考官方文档：http://orika-mapper.github.io/orika-docs/intro.html 映射字段名完全相同的对象如果 DO 对象和 DTO 对象的命名遵守一定的规范，那无疑会减少我们很大的工作量。那么，规范是怎么样的呢？ 1234567891011121314151617181920class Person { private String name; private int age; private Date birthDate; List&lt;Address&gt; addresses; // &lt;1&gt; // getters/setters omitted}class PersonDto { private String name; private int age; private Date birthDate; List&lt;AddressDto&gt; addresses; // &lt;1&gt; // getters/setters omitted}class Address { private String name;}class AddressDto { private String name;} 基本字段类型自不用说，关键是打上 &lt;1&gt; 标签的地方，按照通常的习惯，List&lt;AddressDto&gt; 变量名会被命名为 addressDtos，但我更加推荐与 DO 对象统一命名，命名为 addresses。这样 Orika 在映射时便可以自动映射两者。 1234MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build();Person person = new Person();// 一顿赋值PersonDto personDto = mapperFactory.getMapperFacade().map(person, PersonDto.class); 这样便完成了两个对象之间的拷贝，你可能会思考：需要我们指定两个类的映射关系吗？集合可以自动映射吗？这一切 Orika 都帮助我们完成了，在默认行为下，只要类的字段名相同，Orika 便会尽自己最大的努力帮助我们映射。 映射字段名不一致的对象我对于 DTO 的理解是：DTO 应当尽可能与 DO 的字段保持一致，不增不减不改，但可能出于一些特殊原因，需要映射两个名称不同的字段，Orika 当然也支持这样常见的需求。只需要在 MapperFactory 中事先注册便可。 1234567891011public class Person { private String id; private Name name; private List&lt;Name&gt; knownAliases; private Date birthDate;}public class Name { private String first; private String last; } 1234567public class PersonDto { private String personId; private String firstName; private String lastName; private Date birthDate; private String[][] aliases;} 完成上述两个结构不甚相似的对象时，则需要我们额外做一些工作，剩下的便和之前一致了： 123456789MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build();factory.classMap(Person.class, PersonDto.class) // &lt;2&gt; .field(&quot;id&quot;,&quot;personId&quot;) .field(&quot;name.first&quot;, &quot;firstName&quot;) .field(&quot;name.last&quot;, &quot;lastName&quot;) .field(&quot;knownAliases{first}&quot;, &quot;aliases{[0]}&quot;) .field(&quot;knownAliases{last}&quot;, &quot;aliases{[1]}&quot;) .byDefault() //&lt;1&gt; .register(); 这些 .{}[] 这些略微有点复杂的表达式不需要被掌握，只是想表明：如果你有这样需求，Orika 也能支持。上述连续点的行为被称为 fluent-style ，这再不少框架中有体现。 &lt;1&gt; 注意 byDefault() 这个方法，在指定了 classMap 行为之后，相同字段互相映射这样的默认行为需要调用一次这个方法，才能被继承。 &lt;2&gt; classMap()方法返回了一个 ClassMapBuilder 对象，如上所示，我们见识到了它的 field(),byDefault(),register() 方法，这个建造者指定了对象映射的众多行为，还包括几个其他有用的方法： 12345classMapBuilder.field(&quot;a&quot;,&quot;b&quot;);//Person 和 PersonDto 的双向映射classMapBuilder.fieldAToB(&quot;a&quot;,&quot;b&quot;);// 单向映射 classMapBuilder.fieldBToA(&quot;a&quot;,&quot;b&quot;);// 单向映射classMapBuilder.exclude(&quot;a&quot;);// 移除指定的字段映射，即使字段名相同也不会拷贝classMapBuilder.field(&quot;a&quot;,&quot;b&quot;).mapNulls(false).mapNullsInReverse(false);// 是否拷贝空属性，默认是 true 更多的 API 可以参见源码 集合映射在类中我们之前已经见识过了 List 与 List 的映射。如果根对象就是一个集合，List 映射为 List 也是很常见的需求，这也很方便： 123MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build();List&lt;Person&gt; persons = new ArrayList&lt;&gt;();List&lt;PersonDto&gt; personDtos = mapperFactory.getMapperFacade().mapAsList(persons, PersonDto.class); 递归映射123456789101112class A { private B b;}class B { private C c;}class C { private D d;}class D { private String name;} Orika 默认支持递归映射。 泛型映射对泛型的支持是 Orika 的另一强大功能，这点在文档中只是被提及，网上并没有找到任何一个例子，所以在此我想稍微着重的介绍一下。既然文档没有相关的介绍，那如何了解 Orika 是怎样支持泛型映射的呢？只能翻出 Orika 的源码，在其丰富的测试用例中，可以窥见其对各种泛型特性的支持：https://github.com/orika-mapper/orika/tree/master/tests/src/main/java/ma/glasnost/orika/test/generics 123456public class Response&lt;T&gt; { private T data;}public class ResponseDto&lt;T&gt; { private T data;} 当出现泛型时，按照前面的思路去拷贝，看看结果会如何，泛型示例 1 12345678@Testpublic void genericTest1(){ MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;String&gt; response = new Response&lt;&gt;(); response.setData(&quot;test generic&quot;); ResponseDto&lt;String&gt; responseDto = mapperFactory.getMapperFacade().map(response, ResponseDto.class);// * Assert.assertFalse(&quot;test generic&quot;.equals(responseDto.getData()));} 会发现 responseDto 并不会 Copy 成功吗，特别是在 * 处，你会发现无所适从，没办法把 ResponseDto 传递进去 ，同样的，还有下面的泛型示例 2 12345678910@Testpublic void genericTest2(){ MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;Person&gt; response = new Response&lt;&gt;(); Person person = new Person(); person.setName(&quot;test generic&quot;); response.setData(person); Response&lt;PersonDto&gt; responseDto = mapperFactory.getMapperFacade().map(response, Response.class); Assert.assertFalse(responseDto.getData() instanceof PersonDto);} Response 中的 String 和 PersonDto 在运行时 (Runtime) 泛型擦除这一特性难住了不少人，那么，Orika 如何解决泛型映射呢？ 我们可以发现 MapperFacade 的具有一系列的重载方法，对各种类型的泛型拷贝进行支持 可以看到几乎每个方法都传入了一个 Type，用于获取拷贝类的真实类型，而不是传入.class 字节码，下面介绍正确的打开姿势： 12345678910@Testpublic void genericTest1() { MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;String&gt; response = new Response&lt;&gt;(); response.setData(&quot;test generic&quot;); Type&lt;Response&lt;String&gt;&gt; fromType = new TypeBuilder&lt;Response&lt;String&gt;&gt;(){}.build(); Type&lt;ResponseDto&lt;String&gt;&gt; toType = new TypeBuilder&lt;ResponseDto&lt;String&gt;&gt;(){}.build(); ResponseDto&lt;String&gt; responseDto = mapperFactory.getMapperFacade().map(response, fromType, toType); Assert.assertTrue(&quot;test generic&quot;.equals(responseDto.getData()));} 123456789101112@Testpublic void genericTest2() { MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;Person&gt; response = new Response&lt;&gt;(); Person person = new Person(); person.setName(&quot;test generic&quot;); response.setData(person); Type&lt;Response&lt;Person&gt;&gt; fromType = new TypeBuilder&lt;Response&lt;Person&gt;&gt;(){}.build(); Type&lt;Response&lt;PersonDto&gt;&gt; toType = new TypeBuilder&lt;Response&lt;PersonDto&gt;&gt;(){}.build(); Response&lt;PersonDto&gt; responseDto = mapperFactory.getMapperFacade().map(response, fromType, toType); Assert.assertEquals(&quot;test generic&quot; , responseDto.getData().getName());} 浅拷贝 or 深拷贝虽然不值得一提，但职业敏感度还是催使我们想要测试一下，Orika 是深拷贝还是浅拷贝，毕竟浅拷贝有时候会出现一些意想不到的坑点 123456789@Testpublic void deepCloneTest() throws Exception { MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Person person = new Person(); Address address = new Address(); person.setAddress(address); PersonDto personDto = mapperFactory.getMapperFacade().map(person, PersonDto.class); Assert.assertFalse(personDto.getAddress().hashCode()== person.getAddress().hashCode());} 结论：在使用 Orika 时可以放心，其实现的是深拷贝，不用担心原始类和克隆类指向同一个对象的问题。 更多的特性？你如果关心 Orika 是否能完成你某项特殊的需求，在这里可能会对你有所帮助：http://orika-mapper.github.io/orika-docs/faq.html 怎么样，你是不是还在使用 BeanUtils 呢？尝试一下 Orika 吧！","link":"/orika/"},{"title":"PolarDB 数据库性能大赛 Java 选手分享","text":"1 前言 国际惯例，先报成绩，熬了无数个夜晚，最后依旧被绝杀出了第一页，最终排名第 21 名。前十名的成绩分布为 413.69~416.94，我最终的耗时是 422.43。成绩虽然不是特别亮眼，但与众多参赛选手使用 C++ 作为参赛语言不同，我使用的是 Java，一方面是我 C++ 的能力早已荒废，另一方面是我想验证一下使用 Java 编写存储引擎是否与 C++ 差距巨大 (当然，主要还是前者 QAQ)。所以在本文中，我除了介绍整体的架构之外，还会着重笔墨来探讨 Java 编写存储类型应用的一些最佳实践，文末会给出 github 的开源地址。 2 赛题概览比赛总体分成了初赛和复赛两个阶段，整体要求实现一个简化、高效的 kv 存储引擎 初赛要求支持 Write、Read 接口。 12public abstract void write(byte[] key, byte[] value);public abstract byte[] read(byte[] key); 复赛在初赛题目基础上，还需要额外实现一个 Range 接口。 1public abstract void range(byte[] lower, byte[] upper, AbstractVisitor visitor); 程序评测逻辑 分为 2 个阶段：1）Recover 正确性评测：此阶段评测程序会并发写入特定数据（key 8B、value 4KB）同时进行任意次 kill -9 来模拟进程意外退出（参赛引擎需要保证进程意外退出时数据持久化不丢失），接着重新打开 DB，调用 Read、Range 接口来进行正确性校验 2）性能评测 随机写入：64 个线程并发随机写入，每个线程使用 Write 各写 100 万次随机数据（key 8B、value 4KB） 随机读取：64 个线程并发随机读取，每个线程各使用 Read 读取 100 万次随机数据 顺序读取：64 个线程并发顺序读取，每个线程各使用 Range 有序（增序）遍历全量数据 2 次注： 2.2 阶段会对所有读取的 kv 校验是否匹配，如不通过则终止，评测失败； 2.3 阶段除了对迭代出来每条的 kv 校 验是否匹配外，还会额外校验是否严格字典序递增，如不通过则终止，评测失败。 语言限定：C++ &amp; JAVA，一起排名 3 赛题剖析关于文件 IO 操作的一些基本常识，我已经在专题文章中进行了介绍，如果你没有浏览那篇文章，建议先行浏览一下：文件 IO 操作的一些最佳实践。再回归赛题，先对赛题中的几个关键词来进行解读。 3.1 key 8B, value 4kbkey 为固定的 8 字节，因此可使用 long 来表示。 value 为 4kb，这节省了我们很大的工作量，因为 4kb 的整数倍落盘是非常磁盘 IO 友好的。 value 为 4kb 的另一个好处是我们再内存做索引时，可以使用 int 而不是 long，来记录数据的逻辑偏移量：LogicOffset = PhysicalOffset / 4096，可以将 offset 的内存占用量减少一半。 3.2 kill -9 数据不丢失首先赛题明确表示会进行 kill -9 并验证数据的一致性，这加大了我们在内存中做 write buffer 的难度。但它并没有要求断电不丢失，这间接地阐释了一点：我们可以使用 pageCache 来做写入缓存，在具体代码中我使用了 PageCache 来充当数据和索引的写入缓冲（两者策略不同）。同时这点也限制了参赛选手，不能使用 AIO 这样的异步落盘方式。 3.3 分阶段测评赛题分为了随机写，随机读，顺序读三个阶段，每个阶段都会重新 open，且不会发生随机写到一半校验随机读这样的行为，所以我们在随机写阶段不需要在内存维护索引，而是直接落盘。随机读和顺序读阶段，磁盘均存在数据，open 阶段需要恢复索引，可以使用多线程并发恢复。 ** 同时，赛题还有存在一些隐性的测评细节没有披露给大家，但通过测试，我们可以得知这些信息。** 3.4 清空 PageCache 的耗时虽然我们可以使用 PageCache，但评测程序在每个阶段之后都使用脚本清空了 PageCache，并且将这部分时间也算进了最终的成绩之中，所以有人感到奇怪：三个阶段的耗时相加比输出出来的成绩要差，其实那几秒便是清空 PageCache 的耗时。 123456#清理 pagecache (页缓存)sysctl -w vm.drop_caches=1#清理 dentries（目录缓存）和 inodessysctl -w vm.drop_caches=2#清理 pagecache、dentries 和 inodessysctl -w vm.drop_caches=3 这一点启发我们，不能毫无节制的使用 PageCache，也正是因为这一点，一定程度上使得 Direct IO 这一操作成了本次竞赛的银弹。 3.5 key 的分布这一个隐性条件可谓是本次比赛的关键，因为它涉及到 Range 部分的架构设计。本次比赛的 key 共计 6400w，但是他们的分布都是 ** 均匀 ** 的，在 《文件 IO 操作的一些最佳实践》 一文中我们已经提到了数据分区的好处，可以大大减少顺序读写的锁冲突，而 key 的分布均匀这一特性，启发我们在做数据分区时，可以按照 key 的搞 n 位来做 hash，从而确保 key 两个分区之间整体有序 (分区内部无序)。实际我尝试了将数据分成 1024、2048 个分区，效果最佳。 3.6 Range 的缓存设计赛题要求 64 个线程 Range 两次全量的数据，限时 1h，这也启发了我们，如果不对数据进行缓存，想要在 1h 内完成比赛是不可能的，所以，我们的架构设计应该尽量以 Range 为核心，兼顾随机写和随机读。Range 部分也是最容易拉开差距的一个环节。 4 架构详解首先需要明确的是，随机写指的是 key 的写入是随机的，但我们可以根据 key hash，将随机写转换为对应分区文件的顺序写。 123456789/** * using high ten bit of the given key to determine which file it hits. */public class HighTenPartitioner implements Partitionable { @Override public int getPartition(byte[] key) { return ((key[0] &amp; 0xff)&lt;&lt; 2) | ((key[1] &amp; 0xff)&gt;&gt; 6); }} 明确了高位分区的前提再来看整体的架构就变得明朗了 ** 全局视角 ** ** 分区视角 ** ** 内存视角 ** 内存中仅仅维护有序的 key[1024][625000] 数组和 offset[1024][625000] 数组。 上述两张图对整体的架构进行了一个很好的诠释，利用数据分布均匀的特性，可以将全局数据 hash 成 1024 个分区，在每个分区中存放两类文件：索引文件和数据文件。在随机写入阶段，根据 key 获得该数据对应分区位置，并按照时序，顺序追加到文件末尾，将全局随机写转换为局部顺序写。利用索引和数据一一对应的特性，我们也不需要将 data 的逻辑偏移量落盘，在 recover 阶段可以按照恢复 key 的次序，反推出 value 的逻辑偏移量。 在 range 阶段，由于我们事先按照 key 的高 10 为做了分区，所以我们可以认定一个事实，patition(N) 中的任何一个数据一定大于 partition(N-1) 中的任何一个数据，于是我们可以采用大块读，将一个 partition 整体读进内存，供 64 个 visit 线程消费。到这儿便奠定了整体的基调：读盘线程负责按分区读盘进入内存，64 个 visit 线程负责消费内存，按照 key 的次序随机访问内存，进行 Visitor 的回调。 5 随机写流程介绍完了整体架构，我们分阶段来看一下各个阶段的一些细节优化点，有一些优化在各个环节都会出现，未避免重复，第二次出现的同一优化点我就不赘述了，仅一句带过。 使用 pageCache 实现写入缓冲区主要看数据落盘，后讨论索引落盘。磁盘 IO 类型的比赛，第一步便是测量磁盘的 IOPS 以及多少个线程一次读写多大的缓存能够打满 IO，在固定 64 线程写入的前提下，16kb，64kb 均可以达到最理想 IOPS，所以理所当然的想到，可以为每一个分区分配一个写入缓存，凑齐 4 个 value 落盘。但是此次比赛，要做到 kill -9 不丢失数据，不能简单地在内存中分配一个 ByteBuffer.allocate(4096 * 4);， 而是可以考虑使用 mmap 内存映射出一片写入缓冲，凑齐 4 个刷盘，这样在 kill -9 之后，PageCache 不会丢失。实测 16kb 落盘比 4kb 落盘要快 6s 左右。 索引文件的落盘则没有太大的争议，由于 key 的数据量为固定的 8B，所以 mmap 可以发挥出它写小数据的优势，将 pageCache 利用起来，实测 mmap 相比 filechannel 写索引要快 3s 左右，相信如果把 polardb 这块盘换做其他普通的 ssd，这个数值还要增加。 写入时不维护内存索引，不写入数据偏移一开始审题不清，在随机写之后误以为会立刻随机读，实际上每个阶段都是独立的，所以不需要在写入时维护内存索引；其次，之前的架构图中也已经提及，不需要写入连带 key+offset 一起写入文件，recover 阶段可以按照恢复索引的顺序，反推出 data 的逻辑偏移，因为我们的 key 和 data 在同一个分区内的位置是一一对应的。 6 恢复流程recover 阶段的逻辑实际上包含在程序的 open 接口之中，我们需要再数据库引擎启动时，将索引从数据文件恢复到内存之中，在这之中也存在一些细节优化点。 由于 1024 个分区的存在，我们可以使用 64 个线程 (经验值) 并发地恢复索引，使用快速排序对 key[1024][625000] 数组和 offset[1024][625000] 进行 sort，之后再 compact，对 key 进行去重。需要注意的一点是，不要使用结构体，将 key 和 offset 封装在一起，这会使得排序和之后的二分效率非常低，这之中涉及到 CPU 缓存行的知识点，不了解的读者可以翻阅我之前的博客: 《CPU Cache 与缓存行》 12345// wrongpublic class KeyOffset { long key; int offset;} 整个 recover 阶段耗时为 1s，跟 cpp 选手交流后发现恢复流程比之慢了 600ms，这中间让我觉得比较诡异，加载索引和排序不应该这么慢才对，最终也没有优化成功。 7 随机读流程随机读流程没有太大的优化点，优化空间实在有限，实现思路便是先根据 key 定位到分区，之后在有序的 key 数据中二分查找到 key/offset，拿到 data 的逻辑偏移和分区编号，便可以愉快的随机读了，随机读阶段没有太大的优化点，但仍然比 cpp 选手慢了 2-3s，可能是语言无法越过的差距。 8 顺序读流程Range 环节是整个比赛的大头，也是拉开差距的分水岭。前面我们已经大概提到了 Range 的整体思路是一个生产者消费者模型，n 个生成者负责从磁盘读数据进入内存（n 作为变量，通过 benchmark 来确定多少合适，最终实测 n 为 4 时效果最佳），64 个消费者负责调用 visit 回调，来验证数据，visit 过程就是随机读内存的过程。在 Range 阶段，剩余的内存还有大概 1G 左右，所以我分配了 4 个堆外缓冲，一个 256M，从而可以缓存 4 个分区的数据，并且，我为每一个分区分配了一个读盘线程，负责 load 数据进入缓存，供 64 个消费者消费。 具体的顺序读架构可以参见下图： 大体来看，便是 4 个 fetch 线程负责读盘，fetch thread n 负责 partitionNo % 4 == n 编号的分区，完成后通知 visit 消费。这中间充斥着比较多的互斥等待逻辑，并未在图中体现出来，大体如下： fetch thread 1~4 加载磁盘数据进入缓存是并发的 visit group 1~64 访问同一个 buffer 是并发的 visit group 1~64 访问不同 partition 对应的 buffer 是按照次序来进行的 (打到全局有序) 加载 partitonN 会阻塞 visit bufferN，visit bufferN 会阻塞加载 partitionN+4(相当于复用 4 块缓存) 大块的加载读进缓存，最大程度复用，是 ReadSeq 部分的关键。顺序读两轮的成绩在 196~198s 左右，相比 C++ 又慢了 4s 左右。 9 魔鬼在细节中这儿是个分水岭，介绍完了整体架构和四个阶段的细节实现，下面就是介绍下具体的优化点了。 10 Java 实现 Direct IO由于这次比赛将 drop cache 的时间算进了测评程序之中，所以在不必要的地方应当尽量避免 pageCache，也就是说除了写索引之外，其他阶段不应该出现 pageCache。这对于 Java 选手来说可能是不小的障碍，因为 Java 原生没有提供 Direct IO，需要自己封装一套 JNA 接口，封装这套接口借鉴了开源框架 jaydio 的思路，感谢 @尘央的协助，大家可以在文末的代码中看到实现细节。这一点可以说是拦住了一大票 Java 选手。 Direct IO 需要注意的两个细节： 分配的内存需要对齐，对应 jna 方法：posix_memalign 写入的数据需要对齐通常是 pageSize 的整数倍，实际使用了 pread 的 O_DIRECT 11 直接内存优于堆内内存这一点在《文件 IO 操作的一些最佳实践》中有所提及，堆外内存的两大好处是减少了一份内存拷贝，并且对 gc 友好，在 Direct IO 的实现中，应该配备一套堆外内存的接口，才能发挥出最大的功效。尤其在 Range 阶段，一个缓存区的大小便对应一个 partition 数据分区的大小：256M，大块的内存，更加适合用 DirectByteBuffer 装载。 12 JVM 调优1-server -Xms2560m -Xmx2560m -XX:MaxDirectMemorySize=1024m -XX:NewRatio=4 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:-UseBiasedLocking 众所周知 newRatio 控制的是 young 区和 old 区大小的比例，官方推荐参数为 -XX:NewRatio=1，很多不注意的 Java 选手可能没有意识去修改它，会在无形中被 gc 拖累。经过和 @阿杜的讨论，最终得出的结论： young 区过大，对象在年轻代待得太久，多次拷贝 old 区过小，会频繁触发 old 区的 cms gc 在比赛中这显得尤为重要，-XX:NewRatio=4 放大老年代可以有效的减少 cms gc 的次数，将 126 次 cms gc，下降到最终的 5 次。 13 池化对象无论是 apache 的 ObjectPool 还是 Netty 中的 Recycler，还是 RingBuffer 中预先分配的对象，都在传达一种思想，对于那些反复需要 new 出来的东西，都可以池化，分配内存再回收，这也是一笔不小的开销。在此次比赛的场景下，没必要大费周章地动用对象池，直接一个 ThreadLocal 即可搞定，事实上我对 key/value 的写入和读取都进行了 ThreadLocal 的缓存，做到了永远不再循环中分配对象。 14 减少线程切换无论是网络 IO 还是磁盘 IO，io worker 线程的时间片都显得尤为的可贵，在我的架构中，range 阶段主要分为了两类线程：64 个 visit 线程并发随机读内存，4 个 io 线程并发读磁盘。木桶效应，我们很容易定位到瓶颈在于 4 个 io 线程，在 wait/notify 的模型中，为了尽可能的减少 io 线程的时间片流失，可以考虑使用 while(true) 进行轮询，而 visit 线程则可以 sleep(1us) 避免 cpu 空转带来的整体性能下降，由于评测机拥有 64 core，所以这样的分配算是较为合理的，为此我实现了一个简单粗暴的信号量。 12345678910111213141516171819202122232425262728public class LoopQuerySemaphore { private volatile boolean permit; public LoopQuerySemaphore(boolean permit) { this.permit = permit; } // for 64 visit thread public void acquire() throws InterruptedException { while (!permit) { Thread.sleep(0,1); } permit = false; } // for 4 fetch thread public void acquireNoSleep() throws InterruptedException { while (!permit) { } permit = false; } public void release() { permit = true; }} 正确的在 IO 中 acquireNoSleep，在 Visit 中 acquire，可以让成绩相比使用普通的阻塞 Semaphore 提升 6s 左右。 15 绑核线上机器的抖动在所难免，避免 IO 线程的切换也并不仅仅能够用依靠 while(true) 的轮询，一个 CPU 级别的优化便是腾出 4 个核心专门给 IO 线程使用，完全地避免 IO 线程的时间片争用。在 Java 中这也不难实现，依赖万能的 github，我们可以轻松地实现 Affinity。github 传送门：https://github.com/OpenHFT/Java-Thread-Affinity 使用方式： 123try (final AffinityLock al2 = AffinityLock.acquireLock()) { // do fetch ...} 这个方式可以让你的代码快 1~2 s，并且保持测评的稳定性。 0 聊聊 FileChannel，MMAP，Direct IO，聊聊比赛我在最终版本的代码中，几乎完全抛弃了 FileChannel，事实上，在不 Drop Cache 的场景下，它已经可以发挥出它利用 PageCache 的一些优势，并且优秀的 Java 存储引擎都主要使用了 FileChannel 来进行读写，在少量的场景下，使用了 MMAP 作为辅助，毕竟，MMAP 在写小数据量文件时存在其价值。 另外需要注意的一点，在跟 @96 年的亚普长谈的一个夜晚，发现 FileChannel 中出人意料的一个实现，在分配对内内存时，它仍然会拷贝一份堆外内存，这对于实际使用 FileChannel 的场景需要额外注意，这部分意料之外分配的内存很容易导致线上的问题（实际上已经遇到了，和 glibc 的 malloc 相关，当 buffer 大于 128k 时，会使用 mmap 分配一块内存作为缓存） 说回 FileChannel，MMAP，最容易想到的是 RocketMQ 之中对两者灵活的运用，不知道在其他 Java 实现的存储引擎之中，是不是可以考虑使用 Direct IO 来提升存储引擎的性能呢？我们可以设想一下，利用有限并且少量的 PageCache 来保证一致性，在主流程中使用 Direct IO 配合顺序读写是不是一种可以配套使用的方案，不仅仅 PolarDB，算作是参加本次比赛给予我的一个启发。 虽然无缘决赛，但使用 Java 取得这样的成绩还算不是特别难过，在 6400w 数据随机写，随机读，顺序读的场景下，Java 可以做到仅仅相差 C++ 不到 10s 的 overhead，我倒是觉得完全是可以接受的，哈哈。还有一些小的优化点就不在此赘述了，欢迎留言与我交流优化点和比赛感悟。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/polardb-race/"},{"title":"警惕不规范的变量命名","text":"就在最近，项目组开始强调开发规范了，今天分享一个变量名命名不规范的小案例，强调一下规范的重要性。例子虽小，但却比较有启发意义。 Boolean 变量名命名规范16 年底，阿里公开了《Java 开发规范手册》，其中有一条便是“布尔类型不能以 is 为前缀”。规范中没有举出例子，但是给出了原因：会导致部分序列化框架的无法解析。 看看错误的示范，会导致什么问题，以 Spring 中的 jdbcTemplate 来进行实验。 定义实体类1234567891011121314151617181920@Entitypublic class Bar { @Id @GeneratedValue private Integer id; private Boolean isSuccess;// 注意这是错误的命名 private boolean isSend;// 注意这是错误的命名 public Boolean getSuccess() { return isSuccess; } public void setSuccess(Boolean success) { isSuccess = success; } public boolean isSend() { return isSend; } public void setSend(boolean send) { isSend = send; }} 其中，isSuccess 使用的是包装类型 Boolean，而 isSend 使用的是原生类型 boolean，而 getter，setter 方法是使用 Intellij IDEA 自动生成的，布尔类型生成 getter，setter 方法时略微特殊，比如原生类型的 getter 方式是以 is 开头的，他们略微有点区别，注意区分。生成 getter，setter 方法之后，其实已经有点奇怪了，不急，继续下面的实验。 在数据库中，isSuccess 被映射了 is_success，isSend 被映射成了 is_send，这符合我们的预期。并且为了后续的实验，我们事先准备一条记录，用于后续的查询，在 mysql 的方言中，布尔类型被默认自动映射成 byte，1 代表 ture，0 代表 false。 id is_success is_send 1 1 1 使用 JdbcTemplate 查询12345public void test(String id) { RowMapper&lt;Bar&gt; barRowMapper = new BeanPropertyRowMapper&lt;Bar&gt;(Bar.class); Bar bar = jdbcTemplate.queryForObject(&quot;select * from bar where id = ?&quot;, new Object[]{id}, barRowMapper); System.out.println(bar);} JdbcTemplate 提供了 BeanPropertyRowMapper 完成数据库到实体类的映射，事先我重写了 Bar 类的 toString 方法，调用 test(1) 看看是否能成功映射。结果如下： 1Bar{id=1, isSuccess=null, isSend=false} 数据库中是实际存在这样的字段，并且值都是 true，而使用 JdbcTemplate，却查询不到这样的问题，这边是不遵循规范导致的问题。 相信这个例子可以让大家更加加深映像，特别是在维护老旧代码时，如果发现有 is 开头的 boolean 值，需要额外地注意。 包装类型与原生类型在回顾一下上述的 demo，原生类型和包装类型都没有封装成功，isSuccess 得到了一个 null 值，isSend 得到了一个 false 值。后者足够引起我们的警惕，如果说前者会引起一个 NullPointerExcepiton 导致程序异常，还可以引起开发者的注意，而后者很有可能一直作为一个隐藏的 bug，不被人所察觉，因为 boolean 的默认值为 false。 在类变量中，也普遍提倡使用包装类型，而原生类型的不足之处是很明显的。以 Integer num; 字段为例，num=null 代表的含义是 num 字段未被保存，未定义；而 num=0 代表的含义是明确的，数量为 0。原生类型的表达能力有限。所以提倡在局部作用域的计算中使用原生类型，而在类变量中使用包装类型。 JavaBean 规范如今的微服务的时代，都是在聊架构，聊容器编排，竟然还有人聊 JavaBean，但既然说到了规范，顺带提下。 先来做个选择题，以下选项中符合 JavaBean 命名规范的有哪些？： 1234A : ebookB : eBookC : EbookD : EBook . . . . 正确答案是：A,D 怎么样，符合你的预想吗？JavaBean 规范并不是像很多人想的那样，首字母小写，之后的每一个单词首字母大写这样的驼峰命名法。正确的命名规范应该是：要么前两个字母都是小写，要么前两个字母都是大写。因为英文单词中有 URL，USA 这样固定形式的大写词汇，所以才有了这样的规范。特别警惕 B 那种形式，一些诸如 sNo，eBook,eMail,cId 这样的命名，都是不规范的。 由此引申出了 getter，setter 命名的规范，除了第一节中 Boolean 类型的特例之外，网上还有不上文章，强调了这样的概念：eBook 对应的 getter，setter 应当为 geteBook(),seteBook()，即当类变量的首字母是小写，而第二个字母是大写时，生成的 getter，setter 应当是（get/set）+ 类变量名。但上面已经介绍过了，eBook 这样的变量命名本身就是不规范的，在不规范的变量命名下强调规范的 getter，setter 命名，出发点就错了。有兴趣的朋友可以在 eclipse，intellij idea 中试试，这几种规范，不规范的变量命名，各自对应的 getter，setter 方法是如何的。另外需要知晓一点，IDE 提供的自动生成 getter，setter 的机制，以及 lombok 这类框架的机制，都是由默认的设置，在与其他反射框架配合使用时，只有双方都遵循规范，才能够配合使用，而不能笃信框架。这一点上，有部分国产的框架做的并不是很好。 最后说一个和 JavaBean 相关的取值规范，在 jsp 的 c 标签，freemarker 一类的模板语法，以及一些 el 表达式中，${student.name} 并不是取的 student 的 name 字段，而是调用了 student 的 getName 方法，这也应当被注意，student.name 如何找到对应的 getter 方法，需要解决上一段中提到的同样的问题，建议不确定的地方多测试，尽量采取稳妥的写法。 可能有人会觉得这样的介绍类似于“茴”字有几种写法，但笔者认为恰恰是这些小的规范，容易被人忽视，才更加需要被注意。","link":"/project-rules-2/"},{"title":"sinosoft 代码规范","text":"介绍本文档主要针对我们项目内部正在使用的框架，以及代码审查发现的一些共性问题提出一些开发规范。 JavaBean 规范1 驼峰命名法【强制】 2 布尔类型规范【强制】【说明】所有的布尔类型不允许以 is 开头，否则会导致部分序列化，hibernate 框架出现解析异常。【反例】原来项目的 BaseDomain 中标记逻辑删除的字段, 在部分场景下会出现问题 123456789101112@Column(name = &quot;is_delete&quot;)private Boolean isDelete = false;public Boolean getIsDelete() { return isDelete; }public void setIsDelete(Boolean isDelete) { if(deleteFlag) this.deleteDate = new Date(); this.isDelete = isDelete;} tips: 使用 intellij idea 的快捷键（for eclipse）alt+shift+r，或者菜单栏 Refactor-&gt;Rename，可以重构字段名称【正例】 12@Column(name = &quot;is_delete&quot;)private Boolean deleteFlag = false; 3 装箱类型优于原生类型【推荐】在业务代码中，更加推荐使用装箱类型 Integer Double Boolean…【说明】在未设值的情况下，基础类型具有默认值，而装箱类型为 null以 Boolean 类型为例，如果使用 boolean，那么在未复制时，无法得知其到底是被赋值成了 false，还是未赋值 领域模型规范首先理解各个常用的领域模型的含义： 领域模型 全称 中文含义 DO Domain Object 领域对象 DTO Data Transfer Object 数据传输对象 VO View Object 视图对象 对于 View Object，PO 等等其他一些的对象不在此做要求，只说明一下常用的几个DO 就是我们最常用的数据库持久对象，是 OOP 对于现实中的抽象，一般使用 orm 框架映射到数据库DTO 这一层，目前我们的项目还没有投入使用，即将考虑投入使用，理论上来说，两个微服务模块是严禁共享数据库的所以 A 模块要查询 B 模块的数据，需要使用 B 模块 app 层暴露出来的 api 来查询，其中 B 模块返回的实体，不能是直接从数据库中查询出来的 DO，而应该是 DO 转换而成的 DTO。以及其他服务服务用语传输的变量，都叫做 DTOVO 就是常存在于视图层模板渲染使用的实体类 【推荐】领域模型命名规范【说明】由于 DO 这一层大家已经养成了习惯，不做要求了。DTO 有些特殊，他常常与业务的传输对象相关，而不限于以 Dto 结尾，如 xxxQuery 也可以是 DTO 对象。VO 对象推荐以 Vo 结尾 包结构规范1 包命名【强制】 格式如下：公司名. 模块名. 层次名包名应当尽量使用能够概括模块总体含义, 单词义, 单数, 不包含特殊字符的单词【正例】: sinosoftgz.message.admin【反例】: sinosoftgz.mailsms.admin sinosoftgz.mail.sms.admin 2 包结构【推荐】当项目模块的职责较为复杂，且考虑到以后拓展的情况下，单个模块依旧包含着很多小的业务模块时，应当优先按照业务区分包名 【反例】: 123456789101112131415161718192021sinosoftgz.message.admin config 模块公用 Config.java service 模块公用 Service.java Mail 私有 Service.java MailTemplateService.java MailMessageService.java Sms 私有 Service.java SmsTemplateService.java SmsMessageService.java web 模块公用 Controller.java IndexController.java Mail 私有 Controller.java MailTemplateController.java MailMessageController.java Sms 私有 Controller.java SmsTemplateController.java SmsMessageController.java MailSmsAdminApp.java 【正例】: 12345678910111213141516171819202122232425262728293031sinosoftgz.message.admin config 模块公用 Config.java service 模块公用 Service.java web 模块公用 Controller.java IndexController.java mail config MailConfig.java service Mail 私有 Service.java MailTemplateService.java MailMessageService.java web Mail 私有 Controller.java MailTemplateController.java MailMessageController.java sms config Smsconfig.java service Sms 私有 Service.java SmsTemplateService.java SmsMessageService.java web Sms 私有 Controller.java SmsTemplateController.java SmsMessageController.java MessageAdminApp.java service 和 controller 以及其他业务模块相关的包相隔太远，或者干脆全部丢到一个包内，单纯用前缀区分，会形成臃肿，充血的包结构。如果是项目结构较为单一，可以仅仅使用前缀区分；如果是项目中业务模块有明显的区分条件，应当单独作为一个包，用包名代表业务模块的含义。 容易忽视的细节1 运算溢出【强制】 【反例】Integer a = Integer b * Integer c; 【正例】Long a = Integer b * Integer c;(强转) 整数相乘可能会溢出，需要使用 Long 接收 2 Double 类型的精度问题【强制】 Double 不能用于商业计算，使用 BigDecimal 代替 3 BigDecimal 规范【强制】 【反例】 12BigDecimal totalMoney = new BigDecimal(&quot;100.42&quot;);BigDecimal averageMoney = totalMoney.divide(new BigDecimal(&quot;22&quot;)); 【正例】 12BigDecimal totalMoney = new BigDecimal(&quot;100.42&quot;);BigDecimal averageMoney = totalMoney.divide(new BigDecimal(&quot;22&quot;),3); 业务实体类中的与金额相关的变量统一使用 BigDecimal, 四则运算采用 BigDecimal 的相关 api 进行。做 ** 除法 ** 时需要额外注意保留精度的问题，否则可能会报异常，并且不易被测试出 4 equals 规范【强制】 【反例】 123456Integer a = 2333;Integer b = 2333;System.out.println(a == b);//fasleInteger a = 2;Integer b = 2;System.out.println(a == b);//true 【正例】 1a.equals(b) 要注意正确的比较方法，谨慎使用 ==，它比较的是引用 数据库规范1 必要的地方必须添加索引，如唯一索引，作为条件查询的列【强制】 不添加索引，会造成全表扫描，浪费性能。 2 生产环境，uat 环境，不允许使用 jpa.hibernate.ddl-auto: create 自动建表，每次 ddl 的修改需要保留脚本，统一管理【强制】3 业务数据不能使用 deleteBy… 而要使用逻辑删除 setDeleteFlag(true), 查询时，findByxxxAndDeleteFlag(xxx,false)【强制】 4 如有可替代方案，则禁止使用存储过程和触发器【强制】 5 字段的长度和类型需要按照实际含义定制【推荐】 【反例】 12345@Entityclass Person{ private String name; private Integer age;} 【正例】 1234567@Entityclass Person{ @Column(columnDefinition = &quot;varchar(50)&quot;) private String name; @Column(columnDefinition = &quot;int(3)&quot;) private Integer age;} 明确字段的长度和类型可以迫使开发者去思考字段所处的业务场景，在性能上，字段长度也可以加强索引的性能。 6 使用外键不要使用数据库层面的约束【强制】 不便于数据迁移，统一在应用层控制关联。 ORM 规范【强制】条件查询超过三个参数的，使用 criteriaQuery，predicates 而不能使用 springdata 的 findBy 【反例】 12345678910111213public Page&lt;GatewayApiDefine&gt; findAll(GatewayApiDefine gatewayApiDefine,Pageable pageable){ if(Lang.isEmpty(gatewayApiDefine.getRole())){ gatewayApiDefine.setRole(&quot;&quot;); } if(Lang.isEmpty(gatewayApiDefine.getApiName())){ gatewayApiDefine.setApiName(&quot;&quot;); } if(Lang.isEmpty(gatewayApiDefine.getEnabled())){ return gatewayApiDefineDao.findByRoleLikeAndApiNameLikeOrderByLastUpdatedDesc(&quot;%&quot;+gatewayApiDefine.getRole()+&quot;%&quot;,&quot;%&quot;+gatewayApiDefine.getApiName()+&quot;%&quot;,pageable); }else{ return gatewayApiDefineDao.findByRoleLikeAndApiNameLikeAndEnabledOrderByLastUpdatedDesc(&quot;%&quot;+gatewayApiDefine.getRole()+&quot;%&quot;,&quot;%&quot;+gatewayApiDefine.getApiName()+&quot;%&quot;,gatewayApiDefine.getEnabled(),pageable); } } 在 Dao 层定义了大量的 findBy 方法，在 Service 写了过多的 if else 判断，导致业务逻辑不清晰 【正例】 123456789101112131415161718192021222324252627public Page&lt;MailTemplateConfig&gt; findAll(MailTemplateConfig mailTemplateConfig, Pageable pageable) { Specification querySpecification = (Specification&lt;MailTemplateConfig&gt;) (root, criteriaQuery, criteriaBuilder) -&gt; { List&lt;Predicate&gt; predicates = new ArrayList&lt;&gt;(); predicates.add(criteriaBuilder.isFalse(root.get(&quot;deleteFlag&quot;))); // 级联查询 mailTemplate if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate())) { // 短信模板名称 if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate().getTemplateName())) { predicates.add(criteriaBuilder.like(root.join(&quot;mailTemplate&quot;).get(&quot;templateName&quot;), String.format(&quot;%%%s%%&quot;, mailTemplateConfig.getMailTemplate().getTemplateName()))); } // 短信模板类型 if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate().getTemplateType())) { predicates.add(criteriaBuilder.equal(root.join(&quot;mailTemplate&quot;).get(&quot;templateType&quot;), mailTemplateConfig.getMailTemplate().getTemplateType())); } } // 产品分类 if (!Lang.isEmpty(mailTemplateConfig.getProductType())) { predicates.add(criteriaBuilder.equal(root.get(&quot;productType&quot;), mailTemplateConfig.getProductType())); } // 客户类型 if (!Lang.isEmpty(mailTemplateConfig.getConsumerType())) { predicates.add(criteriaBuilder.equal(root.get(&quot;consumerType&quot;), mailTemplateConfig.getConsumerType())); } return criteriaBuilder.and(predicates.toArray(new Predicate[predicates.size()])); }; return mailTemplateConfigRepos.findAll(querySpecification, pageable); } 条件查询是 admin 模块不可避免的一个业务功能，使用 criteriaQuery 可以轻松的添加条件，使得代码容易维护，他也可以进行分页，排序，连表操作，充分发挥 jpa 面向对象的特性，使得业务开发变得快捷。 数据结构1 集合中迭代过程中增删数据使用迭代器完成 【反例】 12345678List&lt;String&gt; a = new ArrayList&lt;String&gt;();a.add(&quot;1&quot;); a.add(&quot;2&quot;); for (String temp : a) { if(&quot;1&quot;.equals(temp)){ a.remove(temp); } } 【正例】 1234567Iterator&lt;String&gt; it = a.iterator(); while(it.hasNext()){ String temp = it.next(); if((&quot;1&quot;.equals(temp)){ it.remove(); } } 2 hashCode 和 equals 重写规范【强制】 作为 Map 键值，Set 值的实体类，务必重写 hashCode 与 equals 方法，可参考《effective java》。重写时务必做到以下几点 ** 自反性 **: x.equals(x) 一定是 true ** 对 null**: x.equals(null) 一定是 false ** 对称性 **: x.equals(y) 和 y.equals(x) 结果一致 ** 传递性 **: a 和 b equals , b 和 c equals，那么 a 和 c 也一定 equals。 ** 一致性 **: 在某个运行时期间，2 个对象的状态的改变不会不影响 equals 的决策结果，那么，在这个运行时期间，无论调用多少次 equals，都返回相同的结果。做到无状态。 禁止使用魔法数字【模型层与业务层】【强制】一些固定业务含义的代码可以使用枚举类型，或者 final static 常量表示，在设值时，不能直接使用不具备业务含义的数值。 【反例】 1234567891011// 实体类定义/** * 发送设置标志 (1：立即发送 2：预设时间发送) */@Column(columnDefinition = &quot;varchar(1) comment' 发送设置标志 '&quot;)protected String sendFlag;// 业务代码赋值使用MailMessage mailMessage = new MailMessage();mailMessage.setSendSuccessFlag(&quot;1&quot;);mailMessage.setValidStatus(&quot;0&quot;);mailMessage.setCustom(true); 【正例】：使用 final static 常量: 1234567891011121314151617181920212223242526272829303132333435// 实体类定义 /** * 发送设置标志 * * @see sendFlag */ public final static String SEND_FLAG_NOW = &quot;1&quot;; // 立即发送 public final static String SEND_FLAG_DELAY = &quot;2&quot;; // 预设时间发送 /** * 发送成功标志 * * @see sendSuccessFlag */ public final static Map&lt;String, String&gt; SEND_SUCCESS_FLAG_MAP = new LinkedHashMap&lt;&gt;(); public final static String SEND_WAIT = &quot;0&quot;; public final static String SEND_SUCCESS = &quot;1&quot;; public final static String SEND_FAIL = &quot;2&quot;; static { SEND_SUCCESS_FLAG_MAP.put(SEND_WAIT, &quot;未发送&quot;); SEND_SUCCESS_FLAG_MAP.put(SEND_SUCCESS, &quot;发送成功&quot;); SEND_SUCCESS_FLAG_MAP.put(SEND_FAIL, &quot;发送失败&quot;); } /** * 发送设置标志 (1：立即发送 2：预设时间发送) */ @Column(columnDefinition = &quot;varchar(1) comment' 发送设置标志 '&quot;) protected String sendFlag;// 业务代码赋值使用MailMessage mailMessage = new MailMessage();mailMessage.setSendSuccessFlag(MailMessage.SEND_WAIT);mailMessage.setValidStatus(MailMessage.VALID_WAIT);mailMessage.setCustom(true); 【说明】魔法数字不能使代码一眼能够看明白到底赋的是什么值，并且，实体类发生变化后，可能会导致赋值错误，与预期赋值不符合且错误不容易被发现。 【正例】：也可以使用枚举类型避免魔法数字 1234567891011121314151617181920212223242526protected String productType;protected String productName;@Enumerated(EnumType.STRING)protected ConsumerTypeEnum consumerType;@Enumerated(EnumType.STRING)protected PolicyTypeEnum policyType;@Enumerated(EnumType.STRING)protected ReceiverEnum receiver;public enum ConsumerTypeEnum { PERSONAL, ORGANIZATION; public String getLabel() { switch (this) { case PERSONAL: return &quot;个人&quot;; case ORGANIZATION: return &quot;团体&quot;; default: return &quot;&quot;; } }} 【视图层】【推荐】例如，页面迭代 select 的 option，不应该在 view 层判断，而应该在后台传入 map 在前台迭代【正例】： 12345678model.put(&quot;typeMap&quot;,typeMap);模板类型：&lt;select type=&quot;text&quot; name=&quot;templateType&quot;&gt; &lt;option value=&quot;&quot;&gt; 全部 &lt;/option&gt; &lt;#list typeMap?keys as key&gt; &lt;option &lt;#if ((mailTemplate.templateType!&quot;&quot;)==key)&gt;selected=&quot;selected&quot;&lt;/#if&gt;value=&quot;${key}&quot;&gt;${typeMap[key]}&lt;/option&gt; &lt;/#list&gt;&lt;/select&gt; 【反例】： 12345678模板类型：&lt;select type=&quot;text&quot; name=&quot;templateType&quot;&gt; &lt;option value=&quot;&quot;&gt; 全部 &lt;/option&gt; &lt;option &lt;#if ${xxx.templateType!}==&quot;1&quot; selected=&quot;selected&quot;&lt;/#if&gt; value=&quot;1&quot;&gt; 承保通知 &lt;/option&gt; ... &lt;option &lt;#if ${xxx.templateType!}==&quot;5&quot; selected=&quot;selected&quot;&lt;/#if&gt; value=&quot;5&quot;&gt; 核保通知 &lt;/option&gt;&lt;/select&gt; 否则修改后台代码后，前端页面也要修改，设计原则应当是修改一处，其他全部变化。且 1，2…,5 的含义可能会变化，不能从页面得知 value 和 option 的含义是否对应。 并发处理项目中会出现很多并发问题，要做到根据业务选择合适的并发解决方案，避免线程安全问题 1 simpleDateFormat 有并发问题，不能作为 static 类变量【强制】【反例】：这是我在某个项目模块中，发现的一段代码 1234567891011Class XxxController{ public final static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); @RequestMapping(&quot;/xxxx&quot;) public String xxxx(String dateStr){ XxxEntity xxxEntity = new XxxEntity(); xxxEntity.setDate(simpleDateFormat.parse(dateStr)); xxxDao.save(xxxEntity); return &quot;xxx&quot;; }} 【说明】SimpleDateFormat 是线程不安全的类，不能作为静态类变量给多线程并发访问。如果不了解多线程，可以将其作为实例变量，每次使用时都 new 一个出来使用。不过更推荐使用 ThreadLocal 来维护，减少 new 的开销。【正例】一个使用 ThreadLocal 维护 SimpleDateFormat 的线程安全的日期转换类： 1234567891011121314151617public class ConcurrentDateUtil { private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() { @Override protected DateFormat initialValue() { return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); } }; public static Date parse(String dateStr) throws ParseException { return threadLocal.get().parse(dateStr); } public static String format(Date date) { return threadLocal.get().format(date); }} 2 名称唯一性校验出现的线程安全问题【推荐】各个项目的 admin 模块在需求中经常会出现要求名称不能重复，即唯一性问题。通常在前台做 ajax 校验，后台使用 select count(1) from table_name where name=? 的方式查询数据库。这么做无可厚非，但是在极端的情况下，会出现并发问题。两个线程同时插入一条相同的 name，如果没有做并发控制，会导致出现脏数据。如果仅仅是后台系统，那么没有必要加锁去避免，只需要对数据库加上唯一索引，并且再 web 层或者 service 层捕获数据异常即可。【正例】： 12345678910111213141516171819202122232425262728293031323334353637// 实体类添加唯一索引@Entity@Table(name = &quot;mns_mail_template&quot;, uniqueConstraints = {@UniqueConstraint(columnNames = {&quot;templateName&quot;})})public class MailTemplate extends AbstractTemplate { /** * 模板名称 */ @Column(columnDefinition = &quot;varchar(160) comment' 模板名称 '&quot;) private String templateName;}// 业务代码捕获异常@RequestMapping(value = {&quot;/saveOrUpdate&quot;}, method = RequestMethod.POST) @ResponseBody public AjaxResponseVo saveOrUpdate(MailTemplate mailTemplate) { AjaxResponseVo ajaxResponseVo = new AjaxResponseVo(AjaxResponseVo.STATUS_CODE_SUCCESS, &quot;操作成功&quot;, &quot;邮件模板定义&quot;, AjaxResponseVo.CALLBACK_TYPE_CLOSE_CURRENT); try { // 管理端新增时初始化一些数据 if (Lang.isEmpty(mailTemplate.getId())) { mailTemplate.setValidStatus(MailTemplate.VALID_WAIT); } mailTemplateService.save(mailTemplate); } catch (DataIntegrityViolationException ce) { ajaxResponseVo.setStatusCode(AjaxResponseVo.STATUS_CODE_ERROR); ajaxResponseVo.setMessage(&quot;模板名称已经存在&quot;); ajaxResponseVo.setCallbackType(null); logger.error(ce); } catch (Exception e) { ajaxResponseVo.setStatusCode(AjaxResponseVo.STATUS_CODE_ERROR); ajaxResponseVo.setMessage(&quot;操作失败!&quot;); ajaxResponseVo.setCallbackType(null); logger.error(e); } return ajaxResponseVo; } 【说明】关于其他一些并发问题, 如分布式锁，CAS，不仅仅是一篇文档能够讲解清楚的，需要对开发有很深的理解。 3 余额扣减，库存扣减，积分发放等敏感并发操作【强制】 这一块通常交给有经验的开发来完成，但所有人都需要注意。原则是事务保障，幂等保障等等设计原则。 【反例】 123456//Transaction startUser user = UserDao.findById(&quot;1&quot;);user.setBalance(user.getBalance()+100.00);...// 其他耗时操作UserDao.save(user);//Transaction commit 【正例】 12345678//Transaction startlock...User user = UserDao.findById(&quot;1&quot;);user.setBalance(user.getBalance()+100.00);...// 其他耗时操作UserDao.save(user);release lock...//Transaction commit 并发场景必须加锁，根据业务场景决定到底加什么锁，sychronized，ReentrantLock，version 乐观锁，for update 悲观锁（不推荐），redis，zookeeper 实现的分布式锁等等。 moton 使用注意事项1 包的扫描【注意】 每个模块都要扫描自身的项目结构 12345678910mail-sms-admin:application.ymlmotan: client-group: sinosoftrpc client-access-log: false server-group: sinosoftrpc server-access-log: false export-port: ${random.int[9001,9999]} zookeeper-host: 127.0.0.1:2181 annotaiong-package: sinosoftgz.message.admin app 模块由于将 api-impl 脱离出了自身的模块，通常还需要扫描 api-impl 的模块 配置 pom.xml 依赖 1234&lt;dependency&gt; &lt;groupId&gt;sinosoftgz&lt;/groupId&gt; &lt;artifactId&gt;mail-sms-api-impl&lt;/artifactId&gt;&lt;/dependency&gt; 配置 spring ioc 扫描 AutoImportConfig.java 123@ComponentScans({ @ComponentScan(basePackages = {&quot;sinosoftgz.message.app&quot;, &quot;sinosoftgz.message.api&quot;})}) 配置 motan 扫描 mail-sms-app:application.yml 12345678motan: annotaiong-package: sinosoftgz.message.app,sinosoftgz.message.api client-group: sinosoftrpc client-access-log: true server-group: sinosoftrpc server-access-log: true export-port: ${random.int[9001,9999]} zookeeper-host: localhost:2181 2 motan 跨模块传输实体类时懒加载失效【注意】遇到的时候注意一下，由于 jpa，hibernate 懒加载的问题，因为其内部使用动态代理去实现的懒加载，导致懒加载对象无法被正确的跨模块传输，此时需要进行深拷贝。【正例】： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 深拷贝 OrderMain 对象，主要用于防止 Hibernate 序列化懒加载 Session 关闭问题 * &lt;p/&gt; * // * @param order * * @return */ public OrderMain cpyOrder(OrderMain from, OrderMain to) { OrderMain orderMainNew = to == null ? new OrderMain() : to; Copys copys = Copys.create(); List&lt;OrderItem&gt; orderItemList = new ArrayList&lt;&gt;(); List&lt;SubOrder&gt; subOrders = new ArrayList&lt;&gt;(); List&lt;OrderGift&gt; orderGifts = new ArrayList&lt;&gt;(); List&lt;OrderMainAttr&gt; orderMainAttrs = new ArrayList&lt;&gt;(); OrderItem orderItemTmp; SubOrder subOrderTmp; OrderGift orderGiftTmp; OrderMainAttr orderMainAttrTmp; copys.from(from).excludes(&quot;orderItems&quot;, &quot;subOrders&quot;, &quot;orderGifts&quot;, &quot;orderAttrs&quot;).to(orderMainNew).clear(); if (!Lang.isEmpty(from.getOrderItems())) { for (OrderItem i : from.getOrderItems()) { orderItemTmp = new OrderItem(); copys.from(i).excludes(&quot;order&quot;).to(orderItemTmp).clear(); orderItemTmp.setOrder(orderMainNew); orderItemList.add(orderItemTmp); } orderMainNew.setOrderItems(orderItemList); } SubOrderItem subOrderItem; List&lt;SubOrderItem&gt; subOrderItemList = new ArrayList&lt;&gt;(); if (from.getSubOrders() != null) { for (SubOrder s : from.getSubOrders()) { subOrderTmp = new SubOrder(); copys.from(s).excludes(&quot;order&quot;, &quot;subOrderItems&quot;).to(subOrderTmp).clear(); subOrderTmp.setOrder(from); for (SubOrderItem soi : s.getSubOrderItems()) { subOrderItem = new SubOrderItem(); copys.from(soi).excludes(&quot;order&quot;, &quot;subOrder&quot;, &quot;orderItem&quot;).to(subOrderItem).clear(); subOrderItem.setOrder(orderMainNew); subOrderItem.setSubOrder(subOrderTmp); subOrderItemList.add(subOrderItem); if (!Lang.isEmpty(soi.getOrderItem())) { for (OrderItem i : orderMainNew.getOrderItems()) { if (i.getId().equals(soi.getOrderItem().getId())) { subOrderItem.setOrderItem(soi.getOrderItem()); } else { subOrderItem.setOrderItem(soi.getOrderItem()); } } } } subOrderTmp.setSubOrderItems(subOrderItemList); subOrders.add(subOrderTmp); } orderMainNew.setSubOrders(subOrders); } if (from.getOrderGifts() != null) { for (OrderGift og : from.getOrderGifts()) { orderGiftTmp = new OrderGift(); copys.from(og).excludes(&quot;order&quot;).to(orderGiftTmp).clear(); orderGiftTmp.setOrder(orderMainNew); orderGifts.add(orderGiftTmp); } orderMainNew.setOrderGifts(orderGifts); } if (from.getOrderAttrs() != null) { for (OrderMainAttr attr : from.getOrderAttrs()) { orderMainAttrTmp = new OrderMainAttr(); copys.from(attr).excludes(&quot;order&quot;).to(orderMainAttrTmp).clear(); orderMainAttrTmp.setOrder(orderMainNew); orderMainAttrs.add(orderMainAttrTmp); } orderMainNew.setOrderAttrs(orderMainAttrs); } return orderMainNew; } 公用常量规范1 模块常量【强制】模块自身公用的常量放置于模块的 Constants 类中，以 final static 的方式声明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Constants { public static final String birthdayPattern = &quot;yyyy-MM-dd&quot;; // 生日格式 public static final String inputTimePattern = &quot;yyyy-MM-dd HH:mm:ss&quot;; // 录入时间格式 public static class PolicyType { public static final String personal = &quot;0&quot;; // 个单 public static final String group = &quot;1&quot;; // 团单 } public static class InsuredNature { public static final String naturePerson = &quot;1&quot;; // 自然人 public static final String artificialPerson = &quot;0&quot;; // 法人 } public static class InsuredIdentity { public static final String myself = &quot;0&quot;; // 本人 } public static class JfeeFlag { public static final String noFeeFlag = &quot;0&quot;; // 非见费标志 public static final String feeFlag = &quot;1&quot;; // 见费标志 } public static class ItemKindFlag { public static final String mainRiskFlag = &quot;1&quot;; // 主险标志 public static final String additionalRiskFlag = &quot;2&quot;; // 附加险标志 public static final String otherRiskFlag = &quot;3&quot;; // 其它标志 } public static class CalculateAmountFlag { public static final String calculateFlag = &quot;Y&quot;; // 计算保额标志 public static final String noCalculateFlag = &quot;N&quot;; // 不计算保额标志 } public static class LimitGrade { public static final String policyLevel = &quot;1&quot;; // 限额 / 免赔保单级别 public static final String clauseLevel = &quot;2&quot;; // 限额 / 免赔条款级别 } /** * 批改类型 * * 命名规则：对象（可选）+ 行为 */ public static class EndorType { public static final String collectivePolicyInsuredModify = &quot;22&quot;; // 团单变更被保险人 public static final String collectivePolicyInsuredAdd = &quot;Z1&quot;; // 团单批增被保险人 public static final String collectivePolicyInsuredRemove = &quot;J1&quot;; // 团单批减被保险人 public static final String surrender = &quot;04&quot;; // 全单退保 public static final String withdraw = &quot;05&quot;; // 注销 public static final String insurancePeriodModify = &quot;06&quot;; // 平移保险期限 public static final String applicantModify = &quot;H01&quot;; // 更改投保人 public static final String customerModify = &quot;50&quot;; // 变更客户信息 public static final String insuredModify = &quot;29&quot;; // 变更被保人职业 public static final String individualPolicyBeneficiaryModify = &quot;03&quot;; // 变更受益人信息 public static final String engageModify = &quot;15&quot;; // 变更特别约定 public static final String individualPolicyInsuredModify = &quot;77&quot;;// 个单变更被保人 }} Constants 类在一个限界上下文只能有一个，一个限界上下文包含了一整个业务模块（如 policy-admin,policy-admin,policy-api,policy-model）构成一个限界上下文 在 Constants 类中使用静态内部类尽量细化到常量的归属，不要散放 2 项目常量【强制】项目公用的常量放置于 util 模块的 GlobalContants 类中，以内部类和 final static 的方式声明 1234567891011121314151617181920public abstract class GlobalContants { /** * 返回的状态 */ public class ResponseStatus{ public static final String SUCCESS = &quot;success&quot;;// 成功 public static final String ERROR = &quot;error&quot;;// 错误 } /** * 响应状态 */ public class ResponseString{ public static final String STATUS = &quot;status&quot;;// 状态 public static final String ERROR_CODE = &quot;error&quot;;// 错误代码 public static final String MESSAGE = &quot;message&quot;;// 消息 public static final String DATA = &quot;data&quot;;// 数据 } ...} 日志规范1 打印日志时不允许拼接字符串【强制】 【反例】log.debug (“Load No.” + i + “object,” + object); 【正例】log.debug(“Load No.{} object, {}” , i , object); 字符串的计算是在编译期，日志级别如果是 INFO，就等于在浪费机器的性能，无谓的字符串拼接。 2 预防空指针【强制】 【反例】log.debug(“Load student(id={}), name: {}” , id , student.getName() ); 【正例】log.debug(“Load student(id={}), student: {}” , id , student); 不要在日志中调用对象的方法获取值，除非确保该对象肯定不为 null，否则很有可能会因为日志的问题而导致应用产生空指针异常。实现需要打印日志的实体类的 toString 方法或者使用 JSON.toString 3 输出异常信息 【反例】log.error(e.getMessage,e); log.error(“邮件发送失败，接收人姓名：{} ，e : {}”, username, e); 【正例】log.error(“邮件发送失败，接收人姓名：{}”, username, e); e 包含了全部的异常堆栈信息，是 e.getMessage 的父集，出现异常一定要保证输出堆栈信息。并且要保证 exception 作为 log 的重载方法的最后一个参数。 4 Logger 声明规范 【正例】Logger logger = LoggerFactory.getLogger(Student.class); 保证某个类的字节码作为日志跟踪标识，方便定位日志的出处。 2018-02-27 补充规范日志规范1 与外部对接接口的返回报文需要使用 Info 级别打印，以便于跟踪接口信息 【正例】log.info(“供应商接口返回报文:{}”,JSON.toString(venderDto)); 2 内部接口的关键参数需要使用 Info 级别打印，如下单时的订单号，下单人信息，订单金额等关键信息。 3 一般方法为了方便排查问题，建议打上必要的日志 编码细节1 session，request，response 等 http 生命周期的对象不应该传入 service 层 原因：不便于单元测试；不便于 service 重用 2 注意判空 123456789String memberName = (String) request.getSession().getAttribute(GlobalContants.SESSION_MEMBER_NAME);if(Lang.isE)userService.getByName(memberName); List&lt;UserDto&gt; users = userApi.findByStatus(String status);if()for(UserDto user:users){ } 如果确定不为空，可以不判断；对于不确定的情况一定要做空判断 3 motan 的重试次数 所有的操作分为 CRUD，查询 – 一般可以设置 2 次重试，增删改不可以重试，除非保证幂等。 全局配置设置重试次数应当为 0 次。 123ProtocolConfigBean.setRetries(0);//protocol 级别@MotanService(retries = 2)// 注意! 服务端配置是无效的@MotanReferer(retries = 2)// 有效 referer 级别 motan 中的配置覆盖优先级：method &gt; referer &gt; basic referer &gt; protocol 可以修改单个 service 的重试次数 4 XxxProperties 类代替 @Value @Value 容器加载顺序的导致空值的 bug，使用 @ConfigurationProperties 实现 Properties 类更加面向对象 5 RedisTemplate 和 StringRedisTemplate 的使用细节 RedisTemplate.put(“hello”,”world”); StringRedisTemplate.get(“hello”).equals(“world”) == false 6 及时清理不再使用的代码，可以在系统回归之后的节点或者合并到主干的节点删除注释掉的代码 软件设计原则与微服务设计原则1 接口设计应当符合聚合根模式 orderMain 主订单包含 List 订单项，包含 List 子订单 等等项 设计 Api 时，只能存在一个 orderMainApi ，而不能存在 orderItemApi 和 subOrderApi。 其他模块如何获取订单项 orderItem 的数据？只能通过访问 orderMain ，从中获取 orderItem。 不同服务之间进行远程调用，只能访问对方的聚合根对象。 2 面向对象，函数式，设计模式等编程范式 面向对象：继承，封装，多态 函数式：lamba，streamAPI 设计模式：单例模式，工厂模式，适配器模式，模板方法模式 多范式编程与最小表达力原则 3 DTO 的意义 dto 应该存在于 api 层，不应该存在于 model 层，model 只应该对本模块的 service 可见，web 不可见，其他模块不可见。使用 DTO 解耦模块之间的依赖。 4 Api 层的注释要全 5 ApiImpl 层的意义 仅仅作为转换，不添加任何业务逻辑。ApiImpl 层不应该出现 DO 对象。 6 Stub 的意义 Facede 对于外部接口的调用，使用 Stub 作为外部接口的包装，在本模块的 service 类中需要调用外部 API 时，则应当调用 Stub。Stub 代表着远程接口在本地的代理。 7 DevOps 八荣八耻 以可配置为荣，以硬编码为耻 以互备为荣，以单点为耻 以随时重启为荣，以不能迁移为耻 以整体交付为荣，以部分交付为耻 以无状态为荣，以有状态为耻 以标准化为荣，以特殊化为耻 以自动化工具为荣，以手动和人肉为耻 以无人值守为荣，以人工介入为耻 8 领域驱动设计与微服务设计 ** 实体（Entity）和值对象（Value Object）的区分 ** 实体具有生命周期，需要继承 BaseDomain；值对象没有生命周期，只起到修饰作用。 举例：Protocol 协议下包含 List 协议商品, ProtocolProduct 协议商品包含 List 商品轮播图。 此时 Protocol 是聚合根也是实体，List 介于实体和值对象之间，需要视需求而定，而 ProtocolProductPicture 则必然是值对象属性。 对于实体的删除使用逻辑删除，对于值对象的删除使用物理删除。 ** 数据库操作使用充血模型而不是贫血模型 ** 代码见 ProtocolService，查询使用 Specification 模式，曾经强调过，在公会礼包和协议采购已经在实践。具体表现：Repository 层应该为空实现。update = find + 持久化对象的内存操作 + save ** 微服务设计 ** 确定领域的限界上下文，微服务的边界。微服务架构是一件好事，逼着大家关注设计软件的合理性，如果原来在单体式架构中领域分析、面向对象设计做不好，换成微服务会把这个问题成倍的放大。微服务架构首先要关注的不是 RPC/ServiceDiscovery/Circuit Breaker 这些概念，也不是 Eureka/Docker/SpringCloud/Zipkin 这些技术框架，而是服务的边界、职责划分，划分错误就会陷入大量的服务间的相互调用和分布式事务中，这种情况微服务带来的不是便利而是麻烦。 线程池注意事项1 如果在每个方法中实例化线程池，那么要在方法结束时 shutdown 线程池，否则会导致内存溢出，导致服务器崩溃。 @Service public class SomeService { ​ ​ public void concurrentExecute() { ​ ExecutorService executorService = Executors.newFixedThreadPool(10); ​ executorService.execute(new Runnable() { ​ @Override ​ public void run() { ​ System.out.println(“executed…”); ​ } ​ }); ​ executorService.shutdown();// 否则 executorService 永远不会被回收 ​ } } 2 线程池嵌套使用可能会导致死锁 @Service 123456789101112131415161718192021public class SomeService { public void concurrentExecute() { ExecutorService executorService = Executors.newFixedThreadPool(10); executorService.execute(new Runnable() { @Override public void run() { // 复用了一个线程池，会导致子任务卡死其他的主任务 executorService.execute(new Runnable() { @Override public voud run() { //doSomething... } }) } }); executorService.shutdown(); }} 3【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 我下面整理了一些 线程池 相关的知识点 ExecutorsExecutors 是一个线程池框架，其最终还是通过 new ThreadPoolExecutor 的方式创建的线程池。Executors 提供了几个工厂方法。但这几种都不应该在生产中直接使用 newSingleThreadExecutor创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 1new ThreadPoolExecutor(1, 1,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue&lt;Runnable&gt;()); newFixedThreadPool创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 1new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); newCachedThreadPool创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60 秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说 JVM）能够创建的最大线程大小。 1new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS,new SynchronousQueue&lt;Runnable&gt;()); ThreadPoolExecutor再看看如何使用 ThreadPoolExecutor 创建线程池，我们需要理解各个构造方法的参数： 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize - 线程池核心池的大小。maximumPoolSize - 线程池的最大线程数。keepAliveTime - 当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间。unit - keepAliveTime 的时间单位。workQueue - 用来储存等待执行任务的队列。threadFactory - 线程工厂。handler - 拒绝策略。 关注点 1 线程池大小线程池有两个线程数的设置，一个为核心池线程数，一个为最大线程数。在创建了线程池后，默认情况下，线程池中并没有任何线程，等到有任务来才创建线程去执行任务，除非调用了 prestartAllCoreThreads()或者 prestartCoreThread() 方法当创建的线程数等于 corePoolSize 时，会加入设置的阻塞队列。当队列满时，会创建线程执行任务直到线程池中的数量等于 maximumPoolSize。 关注点 2 适当的阻塞队列java.lang.IllegalStateException: Queue full方法 抛出异常 返回特殊值 一直阻塞 超时退出插入方法 add(e) offer(e) put(e) offer(e,time,unit)移除方法 remove()poll() take()poll(time,unit)检查方法 element()peek() 不可用 不可用 ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。DelayQueue： 一个使用优先级队列实现的无界阻塞队列。SynchronousQueue： 一个不存储元素的阻塞队列。LinkedTransferQueue： 一个由链表结构组成的无界阻塞队列。LinkedBlockingDeque： 一个由链表结构组成的双向阻塞队列。 关注点 3 明确拒绝策略ThreadPoolExecutor.AbortPolicy: 丢弃任务并抛出 RejectedExecutionException 异常。 (默认)ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 说明：Executors 各个方法的弊端：1）newFixedThreadPool 和 newSingleThreadExecutor:主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至 OOM。2）newCachedThreadPool 和 newScheduledThreadPool:主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。 我推荐的创建线程池的方式： 1 new ThreadPoolExecutor(全参构造) 自己控制 corePoolSize - 线程池核心池的大小。 maximumPoolSize - 线程池的最大线程数。 keepAliveTime - 当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间。 unit - keepAliveTime 的时间单位。 workQueue - 用来储存等待执行任务的队列。 threadFactory - 线程工厂。 handler - 拒绝策略。 2 使用 Spring 提供的线程池（强烈推荐） 12345678910@Beanpublic ThreadPoolTaskExecutor someBizThreadPool(){ ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor(); threadPoolTaskExecutor.setCorePoolSize(10); threadPoolTaskExecutor.setMaxPoolSize(100); threadPoolTaskExecutor.setQueueCapacity(200); threadPoolTaskExecutor.setKeepAliveSeconds(60); threadPoolTaskExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.AbortPolicy()); return threadPoolTaskExecutor;} 运行规则如下： 如果此时线程池中的数量小于 corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处理被添加的任务。 如果此时线程池中的数量等于 corePoolSize，但是缓冲队列 workQueue 未满，那么任务被放入缓冲队列。 如果此时线程池中的数量大于 corePoolSize，缓冲队列 workQueue 满，并且线程池中的数量小于 maxPoolSize，建新的线程来处理被添加的任务。 如果此时线程池中的数量大于 corePoolSize，缓冲队列 workQueue 满，并且线程池中的数量等于 maxPoolSize，那么通过 handler 所指定的策略来处理此任务。也就是：处理任务的优先级为：核心线程 corePoolSize、任务队列 workQueue、最大线程 maximumPoolSize，如果三者都满了，使用 handler 处理被拒绝的任务（抛出异常）。 当线程池中的线程数量大于 corePoolSize 时，如果某线程空闲时间超过 keepAliveTime，线程将被终止。这样，线程池可以动态的调整池中的线程数。","link":"/project-rules/"},{"title":"第三届数据库大赛 ADB 性能挑战赛赛题总结","text":"前言之前在分享《海量无序数据寻找第 K 大的数》这篇文章时，就已经提到过我参加了阿里云举办的《第三届数据库大赛创新上云性能挑战赛–高性能分析型查询引擎赛道》，传送门：https://tianchi.aliyun.com/competition/entrance/531895/introduction。 截止到 8 月 20 日，终于结束了漫长的赛程。作为阿里云员工的我，按照赛题规定，只能参加初赛，不能参加复赛，出于不影响比赛的目的，终于等到了比赛完全结束，才动笔写下了这篇参赛总结。 照例先说成绩，这里贴一下排行榜，总共有 1446 只队伍，可以看到不少学生和其他公司的员工都参赛了。 我的成绩是第 14 名（普哥忙于 KPI，没有能带飞我，diss 一下嘿嘿），内部排名也是进入了前五，虽然被剥夺了参加复赛的资格，但是也给了内部的奖励作为补偿，奖品是啥呢？ 怎么评价这个内部赛奖品呢？食之无味，弃之可惜。我还特地拍了照留作纪念，后边要不直接公众号抽奖抽掉吧？不知道有没有人要。 看看人家隔壁的云原生挑战赛 (内部赛)奖品，虽然奖励不如外部赛，但整体吸引力还是有的。 好了，讲完比赛结果，吐槽完内部奖励，简单先点评下这次比赛吧。首先是主办方的出题水平，还是非常高的。 全程没有改过赛题描述 全程没有清过榜单 没有暗改过数据 赛题做到了”题面描述简单，实现具有区分度“，让不同水平的参赛者都得到了发挥 评测友好，失败不计算提交次数 这些点要大大点一个赞，建议其他比赛的主办方都来参考下，向这次出题的水准看齐。 不好的点，我也吐槽下： 内部赛奖励太敷衍了，倒不如送我点公仔来的实在 复赛延期了两周，让很多选手的肝有些吃不消 这篇文章我觉得得先明确一个定位，复赛和初赛技术架构相差是很大的，而我没有参加复赛，并没有发言权，所以就不去详细介绍最终的复赛方案了，我已经跟复赛前排的选手预约了转载，后边还会再分享一下复赛前排选手的方案。我的这篇文章还是以初赛为主，一方面聊的话题也轻松些，另一方面初赛的架构简单一些，可以供那些希望参加性能挑战赛而又苦于没有学习资料的同学、初赛没有找到优化点的同学一些参考。 赛题介绍选手需要设计实现 quantile 分析函数，导入指定的数据，并回答若干次 quantile 查询。 quantile(column, p) 函数定义 column: 查询列。 p: 百分比，范围 [0, 1]。 函数返回：将列的所有值排序后，返回第 N * p 个值，评测保证 N * p 是整数。 样例： column = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # column 为整型，有 10个 元素 quantile(column, 0.5) = 5quantile(column, 0.8) = 8quantile(column, 0.25) = 3quantile(column, 0) = 0quantile(column, 1) = 10 题面可以说是非常简单的，其实就是实现一个查询第 N 大的数函数。它的输入数据一共有两列： 第一行是列名，第二行开始是列的数据。 初赛测试数据：只有一张表 lineitem，只有两列 L_ORDERKEY (bigint), L_PARTKEY (bigint)，数据量3亿行，单线程查询 10 次。 如果你还不理解这个题面是什么意思，建议去看下官方提供的评测 demo：https://code.aliyun.com/analytic_db/2021-tianchi-contest-1，本地就能运行，debug 跑一遍，既能读懂题意，也能得到一个 baseline（尽管它不能提交，会爆内存）。 另外需要格外注意的一点，千万不要漏看赛题描述 本题结合英特尔® 傲腾™ 持久内存技术(PMem)，探索新介质和新软件系统上极致的持久化和性能 我们的方案一定需要围绕着存储介质的特性去设计，你问我持久内存 PMem 是啥？老实说，我参赛之前对这个新的技术也是一知半解，但为了成绩，一定需要啃下这个技术，下面我就先介绍下 PMem。 持久内存 PMem 介绍 提到内存（DRAM）、固态磁盘（SSD）、机械硬盘（HDD）这些概念，相信大多数人不会感到陌生，都能够道出这几个介质的访问速度差异。而持久内存 PMem 这个概念，相对而言不太为人所知，的确，它是近几年才兴起的一个概念。 持久内存 (PMem) 是驻留在内存总线上的固态高性能按字节寻址的内存设备。PMem 位于内存总线上，支持像 DRAM 一样访问数据，这意味着它具备与 DRAM 相当的速度和延迟，而且兼具 NAND 闪存的非易失性。NVDIMM（非易失性双列直插式内存模块）和 Intel 3D XPoint DIMM（也称为 Optane DC 持久内存模块）是持久内存技术的两个示例。 当我们在讨论这些存储介质时，我们有哪些关注点呢？本节开头的金字塔图片的三条边直观地诠释了众多存储介质在造价、访问延时、容量三方面的对比。我下面从访问延时、造价和持久化特性三个方面做一下对比。 访问延时内存 DRAM 的访问延时在 80100ns，固态硬盘 SSD 的访问延时在 10100us，而持久内存介于两者之间，比内存慢 10 倍，比固态硬盘快 10~100 倍。 造价目前为止，将 PMem 技术正式商用的公司，貌似只有 Intel，也就是本次比赛的赞助商。Intel optane DC persistent memory 是 Intel 推出的基于 3D Xpoint 技术的持久内存产品，其代号为 Apace Pass (AEP)。所以大家今后如果看到其他人提到 AEP，基本心理就有数了，说的就是 PMem 这个存储介质。在某电商平台看看这东西怎么卖的 好家伙，128 * 4 条，卖 15000，折合下来，一根 PMem 就要 3000~4000。同时我们也注意到，傲腾系列的 PMem 产品最高规格也就只有 512M，基本佐证了金字塔中的 Capacity 这一维度，属于内存嫌大，磁盘嫌小的一个数值。 持久化这东西咱们的电脑可以装吗？当然可以，直接插在内存条上就成。我们都知道内存是易失性的存储，磁盘是持久化的存储，而介于两者之间的持久内存，持久化特性是什么样的呢？这一点，不能望文生义地认为 PMem 就是持久化的，而是要看其工作模式：Memory Mode 和 AppDirect Mode 本文就不过多展开介绍了这两种模式了。简单来说，PMem 工作在 Memory Mode 时，是易失性的，这时候，你需要使用专门的一套系统指令去进行存取；PMem 工作在 AppDirect Mode 时，可以直接把 PMem 当成一块磁盘来用，PMem 背后适配了一整套文件系统的指令，使得常规的文件操作可以完全兼容的跑在 PMem 之上。 我花了这么大的篇幅介绍 PMem，仍然只介绍了 PMem 特性非常小的一部分，最多让大家知道 PMem 是个啥，至于怎么利用好这块盘，我后边会花专门的一篇文章去介绍。 回到赛题，尽管 intel 提供了一套 PMem 专用的 API：https://github.com/pmem/pmemkv-java，但由于比赛限定了不能引入三方类库，所以等于直接告诉了参赛选手，PMem 这块盘是工作在 AppDirect Mode 之下的，大家可以完全把它当成一块磁盘去存取。这个时候，选手们就需要围绕 PMem 的特性，去设计存储引擎的架构，可能你在固态硬盘、常规文件操作中的一些认知会被颠覆，这很正常，毕竟 PMem 的出现，就是为了颠覆传统存储架构而生的。在不能直接操作 PMem 的情况下选手们需要设计 PMem 友好的架构。 赛题剖析此次的数据输入方式和之前的比赛有很大的不同，选手们需要自行去解析文件，获得输入数据，同时进行处理，如何高效的处理文件是很大的一块优化点。 初赛一共 3 亿行数据，一共 2 列，内存一共 4 G，稍微计算下就会发现，全部存储在内存中是存不下的（不考虑压缩），所以需要用到 PMem 充当存储引擎。 查询的需求是查找到第 N 大的数，所以我们的架构一定是需要做到整体有序，允许局部无序。 赛题数据的说明尤为重要：测试数据随机，均匀分布。看过我之前文章的读者，应当敏锐地注意到了均匀分布这个关键词，这意味着我们又可以使用数据的头 n 位来分区了。 这里先给出初赛的最终架构，明确下如何串联各个流程。 把大象放进冰箱总共需要三步，这道题目仅仅多了一步。 第一步：将输入文件从逻辑上分成 12 等分，这样 12 个线程可以并发读取输入文件。可以借助预处理程序，找到等分的边界。 第二步：每个线程都需要读取各自的文件分片，将读取到的文件流在内存中解析成 long，并且需要根据逗号、换行符来区分出两列数据。 第三步：读取到 long 之后，需要根据头 n 位进行分区，例如选择头 8 位，可以获得 2^(8-1) 即 128 个分区，因为比赛中的数据都是正数，所以减了一个符号位。这样分区之后，可以保证分区之间有序，分区内部无序。 第四步：获取第 N 大的数字时，可以直接根据分区内的数据量，直接定位到最终在哪个分区，这样就可以确保只加载一部分数据到内存中。t1_pn ~ t12_pn 在逻辑上组成了 partitionN，将 partitionN 的数据加载进内存之后，这道题就变成：查询无序数组中第 N 大数的问题了。 实现细节以下的实现细节，我会给出实现难度，以供大家参考，打分标准：编码实现难度，容不容易想到等综合评分。 多线程按分区读文件（难度：2 颗星）输入文件按行来分隔数据，第一时间联想到的就是 JDK 提供的 java.io.BufferedReader#readLine() 方法，但稍微懂点文件 IO 基础的读者都应该意识到，文件 IO 要想快，一定得顺序按块读，所以 readLine 这种方法，想都不用想，必定是低效的。那有人问了，博主，你给解释解释，什么叫按块读？最简单的做法是按照固定 4kb 的 buffer 来读取文件，在内存中，自行判断逗号、换行符来区分两列数据，不断移动这个 4kb 的块，这就是按块读了。 12345if (readBufferArray[i] == '\\n') {} if(readBufferArray[i] == ',') {} 光是按块读还不够，为了充分发挥 IO 特性，还可以用上多线程，按照上图中的第一步，我的方案将文件分成了 12 份，这样 12 个线程可以齐头并进地进行读取和解析。 经过测试，多线程比单线程要快 70 多秒，所以没有使用多线程的选手，名次肯定不会高，这是一个通用优化点。 Long 转换（难度：1 颗星）将文件中的字节读取到内存中，一定会经过 byte[] 到 long 的转换过程，千万不要小看这个环节，这可是众多选手分数的分水岭。先来看下 demo 中给出的方案 12String[] columns = reader.readLine().split(&quot;,&quot;);Long.parseLong(row[i]); 这里面存在两个问题 先转 String，再转成 Long，多了一次无效转换 Long.parseLong 方法比较低效，有很多无用判断 我贴一下我方案的伪代码： 1234long val = 0;for (int i = 0; i &lt; size; i++) { val = val * 10 + (readBuffer[i] - '0');} 直接从字节数组中解析出 Long，解析出 Long 主要还是为了落盘的时候进行数据对齐，主流方案应该都会解析。 按头 n 位分桶落盘（难度：1 颗星）在读取到一个 Long 之后，我们可以按照数据的头 n 位，将其写入对应的分区文件中。这其实也是一个通用的优化点，我在《华为云 TaurusDB 性能挑战赛赛题总结》也介绍过，分区之后，可以保证分区之间有序，即 partition1 中的任意数据一定小于 partition2 中的任意数据。分区之间无序，主要是为了可以实现分区文件的顺序写。至于 n 具体是多少，取决于我们想分多个区。分区太多，会导致整体写入速度下降；分区太少，读取阶段加载的数据会过多，甚至可能导致内存放不下。 每个写线程维护自己的分区文件（难度：3 颗星）在赛题剖析里面，我给出了我最终方案的流程图，里面有一个细节，每个读取线程从 1/12 个文件分片中读取解析到的 Long 数值，写入了自己线程编号对应的文件中，进行落盘。并没有采用写入同一个分区文件这样的设计。对比下两种做法的利弊： 写线程写入同一个分区文件。好处是读取阶段不需要聚合多份分区文件，坏处是多个线程写入同一个分区需要加锁，会导致竞争。 写线程写入自己的分区文件。好处是不需要加锁写，坏处是读取阶段需要聚合读取。 也好理解，两个方案的优劣正好相反，稍微分析一下，由于初赛的查询只有 10 次，所以聚合的开销不会太大，再加上，我们本来就希望读取能做到并发，聚合没有那么可怕。反而是写入时加锁导致的冲突，会严重浪费 CPU。 该优化点，帮助我的方案从 80s 缩短到 50s。 分支预测优化（难度：4 颗星）这次比赛因为有了 PMem，导致瓶颈根本不出在 IO 上，以往比赛中，大家都是想尽一切方法，把 CPU 让给 IO，而这次比赛，PMem 直接起飞了，导致大家需要考虑，怎么优化 CPU。而 JVM 虚拟机的一系列机制中，就有很多注意事项，是跟 CPU 优化相关的。如果你对 CPU 优化一无所知，我强烈建议你先去阅读下我之前的文章《JAVA 拾遗 — JMH 与 8 个测试陷阱》和《JAVA 拾遗 — CPU Cache 与缓存行》。在解析 Long 时，我们需要从 4kb 的读缓冲区中解析出 Long 数值，由于文件中的数值是以不定长的字节数组形式出现的，我们只能通过判断 逗号、换行符 来解析出数值，所以难免会写出这样的代码： 1234567891011121314int blockReadPosition = 0;for (int i = 0; i &lt; size; i++) { if (readBufferArray[i] == '\\n') { partRaceEngine.add(threadNo, val); val = 0; blockReadPosition = i + 1; } else if(readBufferArray[i] == ',') { orderRaceEngine.add(threadNo, val); val = 0; blockReadPosition = i + 1; } else { val = val * 10 + (readBufferArray[i] - '0'); }} 思考下，这段代码会有什么逻辑问题吗？当然没有，相信很多选手也会这么判断。但不妨分析下，输入文件大概有 10G 左右，所有的字节都会经过 if 判断一次，而实际上，大多数的字符并不是 \\n 和 , 。这会导致 CPU 被浪费在分支预测上。 我的优化思路也很简单，直接用循环，干掉 if/else 判断 1234567891011121314151617181920for (int i = 0; i &lt; size; i++) { byte temp = readBufferArray[i]; do { val = val * 10 + (temp - '0'); temp = readBufferArray[++i]; } while (temp != ','); orderRaceEngine.add(threadNo, val); val = 0; // skip ， i++; temp = readBufferArray[i]; do { val = val * 10 + (temp - '0'); temp = readBufferArray[++i]; } while (temp != '\\n'); partRaceEngine.add(threadNo, val); val = 0; // skip \\n}readPosition += size; 一般来说，再没有办法去掉 if/else 的前提下，我们可以遵循的一个最佳实践是，将容易命中的条件放到最前面。 该优化帮助我从 48s 优化到了 24s。 另外，也可以利用数据特性，因为大多数数据是 19 位的数字，可以直接判断第 20 位是不是 , 或者 \\n 从而减少分支预测的次数。 quickSelect（难度：4 颗星）在查询阶段，查询一个分区内第 N 大的数，最简单的思路是排序之后直接返回，a[N]，受到评测 demo 的影响，很多选手可能忽略了可以使用 quickSelect 算法。 关于 TopN 问题，我其实已经专门写过一篇文章了，对这个优化点和算法感兴趣的朋友可以阅读我之前的文章《海量无序数据寻找第 K 大的数》。 该优化帮我从 24s 优化到 17s。 查询阶段多线程读分区（难度：2 颗星）前文提到了为了避免写入阶段的冲突，每个线程维护了自己的分区文件，在查询时，则需要聚合多个线程的数据。这个时候不要忘记也可以多线程读取，因为初赛的评测程序是单线程测评的，IO 无法打满，需要我们控制多线程，充分利用 IO。 该优化帮我从 17s 优化到了 15s。 循环展开（难度：4 颗星）尽管得知我们可以知道字节数组的长度，从而用循环来解析出 Long，但根据 JMH 的优化项来看，手动展开循环，可以让程序更加地快，例如像下面这样。 这样的优化大概仅仅能提升 1s2s，甚至不到，但越是到前排，12s 的优化就越会显得弥足珍贵。 总结还有很多之前我提到过的一些通用优化技巧，例如顺序读写、读写缓冲、对象复用等等，就不在这里继续赘述了，尽管 PMem 和固态硬盘这两种介质有一定的差异，但这些优化技巧依旧是通用的。 因为这次比赛，IO 的速度实在是太快了，导致优化的方向变成如何优化 CPU，合理分配 CPU，让 CPU 更多地分配给瓶颈操作，这是其他比赛中没有过的经历。 还有一些点是通过调参来实现的，例如文件分片数，读写缓冲区的大小，读写的线程数等等，也会导致成绩相差非常大，这就需要不断地肝，不断地 benchmark 了。 不光是成功的优化点值得分享，也拿一个失败的优化分享一下，例如，将一半的数据存储在内存中，最终发现，申请内存的时间，倒不如拿去进行文件 IO，最终放弃了，可以见得在合理的架构设计下，PMem 的表现的确彪悍，不属于内存存取。 这次 ADB 的性能挑战赛，虽然只参加了初赛，但收获的技能点还是不少的。印象最深的便是 PMem 这块盘的表现和我理解中的 SSD 还是有一定差距的，导致之前的一些经验不能直接在这场比赛中运用。我也大概了解了很多复赛前排选手使用到了很多的奇技淫巧，每一个看似奇葩的优化点背后，可能都蕴含着该选手对操作系统、文件系统、编程语言等方面超出常人的认知，值得喝彩。 感到遗憾的地方还是有的，这次比赛只能让 PMem 工作在 APP Direct 模式下，没有能够真正做到颠覆性。如果有一场比赛，能够支持 Memory Mode，那我应该能收获到对持久内存更加深刻的认知。 我一直反复安利我的读者尽可能地参加各类性能挑战赛，特别是在校生、实习生或者刚进入职场的新人，这种比赛是实践的最好机会，看书不是。 好了，最后，我将我的代码开源在了 github：https://github.com/lexburner/2021-tianchi-adb-race。如果你对实现细节感兴趣，欢迎与我交流。 推荐阅读： 《文件 IO 操作的一些最佳实践》 《华为云 TaurusDB 性能挑战赛赛题总结》 《PolarDB 数据库性能大赛 Java 选手分享》 《天池中间件大赛 dubboMesh 优化总结（qps 从 1000 到 6850）》 《天池中间件大赛百万队列存储设计总结【复赛】》","link":"/race-adb-md/"},{"title":"记一次 Redis 连接问题排查","text":"问题发现客户端：业务应用使用 lettuce 客户端 服务端：Redis server 部署架构采用 1 主 + 1 从 + 3 哨兵 Redis 和业务应用部署在同一个 K8s 集群中，Redis Server 暴露了一个 redis-service，指向到 master 节点，业务应用通过 redis-service 连接 Redis。 某个时刻起，开始发现业务报错，稍加定位，发现是 Redis 访问出了问题，搜索业务应用日志，发现关键信息： 1org.springframework.data.redis.RedisSystemException: Error in execution; nested exception is io.lettuce.core.RedisCommandExecutionException: READONLY You can't write against a read only replica. 这是一个 Redis 访问的报错，看起来跟 Redis 的读写配置有关。 问题定位首先排查下业务应用和 Redis 的连接情况 12# netstat -ano | grep 6379tcp 0 0 172.24.7.34:44602 10.96.113.219:6379 ESTABLISHED off (0.00/0/0) 其中 172.24.7.34 是业务 pod 的 ip，10.96.113.219 是 redis 的 K8s service ip，连接是 ESTABLISHED 状态，说明连接没有断。 继续排查 Redis 的 pod 是否正常： 12345redis-shareredis-0 2/2 Running 0redis-shareredis-1 2/2 Running 0redis-shareredis-sentinel-5f7458cd89-7dwpz 2/2 Running 0redis-shareredis-sentinel-5f7458cd89-rrfz7 2/2 Running 0redis-shareredis-sentinel-5f7458cd89-xzpmb 2/2 Running 0 无论是读写节点还是哨兵节点，都没有重启过。 既然报了只读节点的异常，索性看下 redis 节点的读写角色情况。 123456789101112root@redis-shareredis-0:/data# redis-cli -h 172.24.1.95 -a xxxx role1) &quot;slave&quot;2) &quot;172.24.1.96&quot;3) (integer) 63794) &quot;connected&quot;5) (integer) 6942040980root@redis-shareredis-0:/data# redis-cli -h 172.24.1.96 -a xxxx role1) &quot;master&quot;2) (integer) 69421730723) 1) 1) &quot;172.24.1.95&quot; 2) &quot;6379&quot; 3) &quot;6942173072&quot; 可以看到此时 redis-shareredis-0（172.24.1.95）是 slave 节点，redis-shareredis-1（172.24.1.96）是 master 节点。 排查到这里，猜测是业务 pod 实际通过 K8s service 连到了 slave 节点。进入 slave 确认这一信息，发现果然如此，并且 master 节点并没有检查到有该业务 pod 的连接 12root@redis-shareredis-0:/data# netstat -ano | grep 172.24.7.34:44602tcp 0 0 172.24.1.95:6379 172.24.7.34:44602 ESTABLISHED keepalive (24.09/0/0) 怀疑是某个时刻开始，master 和 slave 角色发生了互换，而主从切换过程中由于 pod 没有重启，长连接会一直保留着，此时即使 Redis service 的 endpoint 被修正，也不会影响到已有的连接。 为了验证上述猜想，着手排查 Redis server 节点和 sentinel 节点。 查看 Redis 哨兵日志： 12341:X 03 Feb 2023 06:21:41.357 * +slave slave 172.24.1.96:6379 172.24.1.96 6379 @ mymaster 172.24.1.95 63791:X 14 Feb 2023 06:53:27.683 # +reset-master master mymaster 172.24.1.96 63791:X 14 Feb 2023 06:53:28.692 * +slave slave 172.24.1.95:6379 172.24.1.95 6379 @ mymaster 172.24.1.96 63791:X 14 Feb 2023 06:53:33.271 # +reset-master master mymaster 172.24.1.96 6379 可以看到在 2023/2/14 14:53 (时区+8)时发生了主从切换。 尝试排查主从切换的原因，进到 redis-0 查看日志： 123456789101112131415161:M 14 Feb 2023 14:53:27.343 # Connection with replica 172.24.1.96:6379 lost.1:S 14 Feb 2023 14:53:27.616 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.1:S 14 Feb 2023 14:53:27.616 * REPLICAOF 172.24.1.96:6379 enabled (user request from 'id=1238496 addr=172.24.1.91:49388 fd=7 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=45 qbuf-free=32723 obl=0 oll=0 omem=0 events=r cmd=slaveof')1:S 14 Feb 2023 14:53:27.646 * REPLICAOF would result into synchronization with the master we are already connected with. No operation performed.1:S 14 Feb 2023 14:53:27.670 * REPLICAOF would result into synchronization with the master we are already connected with. No operation performed.1:S 14 Feb 2023 14:53:28.076 * Connecting to MASTER 172.24.1.96:63791:S 14 Feb 2023 14:53:28.076 * MASTER &lt;-&gt; REPLICA sync started1:S 14 Feb 2023 14:53:28.076 * Non blocking connect for SYNC fired the event.1:S 14 Feb 2023 14:53:28.076 * Master replied to PING, replication can continue...1:S 14 Feb 2023 14:53:28.077 * Trying a partial resynchronization (request 816c44412b9008e6969b2fef6401a6cef85fff87:6901666283).1:S 14 Feb 2023 14:53:28.081 * Full resync from master: 86aa2f4759f73114594586e2e7d2cfbdd1ed2b69:69016649781:S 14 Feb 2023 14:53:28.081 * Discarding previously cached master state.1:S 14 Feb 2023 14:53:28.140 * MASTER &lt;-&gt; REPLICA sync: receiving 1117094 bytes from master1:S 14 Feb 2023 14:53:28.144 * MASTER &lt;-&gt; REPLICA sync: Flushing old data1:S 14 Feb 2023 14:53:28.157 * MASTER &lt;-&gt; REPLICA sync: Loading DB in memory1:S 14 Feb 2023 14:53:28.234 * MASTER &lt;-&gt; REPLICA sync: Finished with success 从日志分析是主从同步时出现了网络分区，导致哨兵进行重新选主，但为什么出现网络分区，就无从得知了，K8s 中两个 pod 之间的通信都能出现 Connection lost 的确挺诡异的。 到这里，问题的根源基本定位清楚了。 问题复盘无论 Redis 的主从切换是故意的还是不小心，都应当被当做是一个常态，程序需要兼容这类场景。反映出两个问题： 问题一，Redis 使用了哨兵机制，程序应当首选通过哨兵连接 Redis 问题二，Lettuce 客户端没有自动断开错误的连接 那么改进思路自然是有两种，一是改用哨兵连接 Redis，二是替换掉 Lettuce。对于本文遇到的问题，方案一可能可以，但不能确保没有其他极端情况导致其他连接问题，所以我实际采用的是方案二，使用 Jedis 替换掉 Lettuce。 项目一开始采用 Lettuce，主要是因为 spring-boot-data-redis 默认采用了 Lettuce 的实现，尽管我一开始已经留意到搜索引擎中诸多关于 Lettuce 的问题，但实际测试发现，高版本 Lettuce 基本均已修复了这些问题，忽略了特殊场景下其可能存在的风险。简单对比下 Jedis 和 Lettuce: Lettuce： Lettuce 客户端没有连接保活探测，错误连接存在连接池中会造成请求超时报错。 Lettuce 客户端未实现 testOnBorrow 等连接池检测方法，无法在使用连接之前进行连接校验。 Jedis： Jedis 客户端实现了 testOnBorrow、testWhileIdle、testOnReturn 等连接池校验配置。 开启 testOnBorrow 在每次借用连接前都会进行连接校验，可靠性最高，但是会影响性能（每次 Redis 请求前会进行探测）。 testWhileIdle 可以在连接空闲时进行连接检测，合理配置阈值可以及时剔除连接池中的异常连接，防止使用异常连接造成业务报错。 在空闲连接检测之前，连接出现问题，可能会造成使用该连接的业务报错，此处可以通过参数控制检测间隔（timeBetweenEvictionRunsMillis）。 因此，Jedis 客户端在面对连接异常，网络抖动等场景下的异常处理和检测能力明显强于 Lettuce，可靠性更强。 参数 配置介绍 配置建议 maxTotal 最大连接，单位：个 根据Web容器的Http线程数来进行配置，估算单个Http请求中可能会并行进行的Redis调用次数，例如：Tomcat中的Connector内的maxConnections配置为150，每个Http请求可能会并行执行2个Redis请求，在此之上进行部分预留，则建议配置至少为：150 x 2 + 100= 400限制条件：单个Redis实例的最大连接数。maxTotal和客户端节点数（CCE容器或业务VM数量）数值的乘积要小于单个Redis实例的最大连接数。例如：Redis主备实例配置maxClients为10000，单个客户端maxTotal配置为500，则最大客户端节点数量为20个。 maxIdle 最大空闲连接，单位：个 建议配置为maxTotal一致。 minIdle 最小空闲连接，单位：个 一般来说建议配置为maxTotal的X分之一，例如此处常规配置建议为：100。对于性能敏感的场景，防止经常连接数量抖动造成影响，也可以配置为与maxIdle一致，例如：400。 maxWaitMillis 最大获取连接等待时间，单位：毫秒 获取连接时最大的连接池等待时间，根据单次业务最长容忍的失败时间减去执行命令的超时时间得到建议值。例如：Http最大容忍超时时间为15s，Redis请求的timeout设置为10s，则此处可以配置为5s。 timeout 命令执行超时时间，单位：毫秒 单次执行Redis命令最大可容忍的超时时间，根据业务程序的逻辑进行选择，一般来说处于对网络容错等考虑至少建议配置为210ms以上。特殊的探测逻辑或者环境异常检测等，可以适当调整达到秒级。 minEvictableIdleTimeMillis 空闲连接逐出时间，大于该值的空闲连接一直未被使用则会被释放，单位：毫秒 如果希望系统不会经常对连接进行断链重建，此处可以配置一个较大值（xx分钟），或者此处配置为-1并且搭配空闲连接检测进行定期检测。 timeBetweenEvictionRunsMillis 空闲连接探测时间间隔，单位：毫秒 根据系统的空闲连接数量进行估算，例如系统的空闲连接探测时间配置为30s，则代表每隔30s会对连接进行探测，如果30s内发生异常的连接，经过探测后会进行连接排除。根据连接数的多少进行配置，如果连接数太大，配置时间太短，会造成请求资源浪费。对于几百级别的连接，常规来说建议配置为30s，可以根据系统需要进行动态调整。 testOnBorrow 向资源池借用连接时是否做连接有效性检测（ping），检测到的无效连接将会被移除。 对于业务连接极端敏感的，并且性能可以接受的情况下，可以配置为True，一般来说建议配置为False，启用连接空闲检测。 testWhileIdle 是否在空闲资源监测时通过ping命令监测连接有效性，无效连接将被销毁。 True testOnReturn 向资源池归还连接时是否做连接有效性检测（ping），检测到无效连接将会被移除。 False maxAttempts 在JedisCluster模式下，您可以配置maxAttempts参数来定义失败时的重试次数。 建议配置3-5之间，默认配置为5。根据业务接口最大超时时间和单次请求的timeout综合配置，最大配置不建议超过10，否则会造成单次请求处理时间过长，接口请求阻塞。 再次回到本次案例，如果使用了 Jedis，并且配置了合理的连接池策略，可能仍然会存在问题，因为 Jedis 底层检测连接是否可用，使用的是 ping 命令，当连接到只读节点，ping 命令仍然可以工作，所以实际上连接检查机制并不能解决本案例的问题。 但 Jedis 提供了一个 minEvictableIdleTimeMillis 参数，该参数表示一个连接至少停留在 idle 状态的最短时间，然后才能被 idle object evitor 扫描并驱逐，该参数会受到 minIdle 的影响，驱逐到 minIdle 的数量。也就意味着：默认配置 minEvictableIdleTimeMillis=60s，minIdle=0 下，连接在空闲时间达到 60s 时，将会被释放。由于实际的业务场景 Redis 读写空闲达到 60s 的场景是很常见的，所以该方案勉强可以达到在主从切换之后，在较短时间内恢复。但如果 minIdle &gt; 0，这些连接依旧会有问题。而 Lettuce 默认配置下，连接会一直存在。 出于一些不可描述的原因，我无法将应用连接 Redis 的模式切换成哨兵模式，所以最终采取了切换到 Jedis 客户端，并且配置 minIdle=0、minEvictableIdleTimeMillis=60s 的方案。 问题总结当使用域名/K8s Service 连接 Redis 集群时，需要考虑主从切换时可能存在的问题。Redis 通常使用长连接通信，主从切换时如果连接不断开，会导致无法进行写入操作。可以在客户端、服务端两个层面规避这一问题，以下是一些行之有效的方案： 客户端连接哨兵集群，哨兵会感知到主从切换，并推送给客户端这一变化 客户端配置 minIdle=0，及时断开空闲的连接，可以一定程度规避连接已经不可用但健康检测又检查不出来的场景。（即本文的场景） 服务端主从切换时断开所有已有的连接，依靠客户端的健康检测以及重连等机制，确保连接到正确的节点。 Redis 客户端推荐使用 Jedis 客户端，其在面对连接异常，网络抖动等场景下的异常处理和检测能力明显强于 Lettuce。","link":"/redis-connect/"},{"title":"解析 Spring 中的 ResponseBody 和 RequestBody","text":"spring，restful，前后端分离这些关键词都是大家耳熟能详的关键词了，一般 spring 常常需要与前端、第三方使用 JSON，XML 等形式进行交互，你也一定不会对 @RequestBody 和 @ResponseBody 这两个注解感到陌生。 @ResponseBody 的使用由于 @ResponseBody 和 @RequestBody 的内部实现是同样的原理（封装请求和封装响应），所以本文以 @ResponseBody 为主要入手点，理解清楚任何一者，都可以同时掌握另一者。 如果想要从 spring 获得一个 json 形式返回值，操作起来是非常容易的。首先定义一个实体类: 1234public class Book { private Integer id; private String bookName;} 接着定义一个后端端点： 123456789@RestControllerpublic class BookController { @GetMapping(value = &quot;/book/{bookId}&quot;) public Book getBook(@PathVariable(&quot;bookId&quot;) Integer bookId) { return new Book(bookId, &quot;book&quot; + bookId); }} 在 RestController 中，相当于给所有的 xxxMapping 端点都添加了 @ResponseBody 注解，不返回视图，只返回数据。使用 http 工具访问这个后端端点 localhost:8080/book/2，便可以得到如下的响应： 1234{ &quot;id&quot;: 2, &quot;bookName&quot;: &quot;book2&quot;} 这是一个最简单的返回 JSON 对象的使用示例了，相信这样的代码很多人在项目中都写过。 添加 XML 解析如果我们需要将 Book 对象以 XML 的形式返回，该如何操作呢？这也很简单，给 Book 对象添加 @XmlRootElement 注解，让 spring 内部能够解析 XML 对象。 12345@XmlRootElementpublic class Book { private Integer id; private String bookName;} 在我们未对 web 层的 BookController 做任何改动之前，尝试访问 localhost:8080/book/2 时，会发现得到的结果仍然是前面的 JSON 对象。这也能够理解，因为 Book 对象如今既可以被解析为 XML，也可以被解析为 JSON，我们隐隐察觉这背后有一定的解析顺序关系，但不着急，先看看如何让 RestController 返回 XML 解析结果。 方法 1 http 客户端指定接收的返回结果类型 http 协议中，可以给请求头添加 Accept 属性，笔者常用的 http 客户端是 idea 自带的 Test RESTful Web Service 以及 chrome 的插件 Postman。简单的调试，前者基本可以满足我们大多数的需求，而这里为了给大家更直观的体验，笔者使用了 Postman。以 code 形式展示： 123GET /book/2 HTTP/1.1Host: localhost:8080Accept: application/xml 响应内容如下： 12345&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;&lt;book&gt; &lt;bookName&gt;book2&lt;/bookName&gt; &lt;id&gt;2&lt;/id&gt;&lt;/book&gt; 方法 2 在 RestController 后端端点中指定返回类型 修改后的 RestController 如下所示 123456789@RestControllerpublic class BookController { @GetMapping(value = &quot;/book/{bookId}&quot;, produces = {&quot;application/xml&quot;}) public Book getBook(@PathVariable(&quot;bookId&quot;) Integer bookId) { return new Book(bookId, &quot;book&quot; + bookId); }} 此时即使将请求中的 Accept: application/xml 去除，依旧可以返回上述的 XML 结果。 通常情况下，我们的服务端返回的形式一般是固定的，即限定了是 JSON，XML 中的一种，不建议依赖于客户端添加 Accept 的信息，而是在服务端限定 produces 类型。 详解 Accpect 与 producesAccpect 包含在 http 协议的请求头中，其本身代表着客户端发起请求时，期望返回的响应结果的媒体类型。如果服务端可能返回多个媒体类型，则可以通过 Accpect 指定具体的类型。 produces 是 Spring 为我们提供的注解参数，代表着服务端能够支持返回的媒体类型，我们注意到 produces 后跟随的是一个数组类型，也就意味着服务端支持多种媒体类型的响应。 在上一节中，我们未显示指定 produces 值时，其实就隐式的表明，支持 XML 形式，JSON 形式的媒体类型响应。从实验结果，我们也可以看出，当请求未指定 Accpect，响应未指定 produces 时，具体采用何种形式返回是有 Spring 控制的。在接口交互时，最良好的对接方式，当然是客户端指定 Accpect，服务端指定 produces，这样可以避免模棱两可的请求响应，避免出现意想不到的对接结果。 详解 ContentType 与 consumes恰恰和 Accpect&amp;produces 相反，这两个参数是与用于限制请求的。理解了前两者的含义，这两个参数可以举一反三理解清楚。 ContentType 包含在 http 协议的请求头中，其本身代表着客户端发起请求时，告知服务端自己的请求媒体类型是什么。 consumes 是 Spring 为我们提供的注解参数，代表着服务端能够支持处理的请求媒体类型，同样是一个数组，意味着服务端支持多种媒体类型的请求。一般而言，consumes 与 produces 对请求响应媒体类型起到的限制作用，我们给他一个专有名词：窄化。 http 请求响应媒体类型一览上面描述的 4 个属性：Accpect 与 produces，ContentType 与 consumes 究竟有哪些类型与之对应呢？我只将常用的一些列举了出来： 媒体类型 含义 text/html HTML 格式 text/plain 纯文本格式 text/xml, application/xml XML 数据格式 application/json JSON 数据格式 image/gif gif 图片格式 image/png png 图片格式 application/octet-stream 二进制流数据 application/ x-www-form-urlencoded form 表单数据 multipart/form-data 含文件的 form 表单 其中有几个类型值得一说，web 开发中我们常用的提交表单操作，其默认的媒体类型就是 application/ x-www-form-urlencoded，而当表单中包含文件时，大家估计都踩过坑，需要将 enctype=multipart/form-data 设置在 form 参数中。text/html 也就是常见的网页了，json 与 xml 常用于数据交互，其他不再赘述。 而在 JAVA 中，提供了 MediaType 这样的抽象，来与 http 的媒体类型进行对应。‘/’之前的名词，如 text，application 被称为类型（type），‘/’之后被称为子类型 (subType)。 详解 HttpMessageConverter我们想要搞懂 Spring 到底如何完成众多实体类等复杂类型的数据转换以及与媒体类型的对应，就必须要搞懂 HttpMessageConverter 这个顶级接口： 1234567891011public interface HttpMessageConverter&lt;T&gt; { boolean canRead(Class&lt;?&gt; var1, MediaType var2); boolean canWrite(Class&lt;?&gt; var1, MediaType var2); List&lt;MediaType&gt; getSupportedMediaTypes(); T read(Class&lt;? extends T&gt; var1, HttpInputMessage var2) throws IOException, HttpMessageNotReadableException; void write(T var1, MediaType var2, HttpOutputMessage var3) throws IOException, HttpMessageNotWritableException;} 大致能看出 Spring 的处理思路。下面的流程图可以更好方便我们的理解： 对于添加了 @RequestBody 和 @ResponseBody 注解的后端端点，都会经历由 HttpMessageConverter 进行的数据转换的过程。而在 Spring 启动之初，就已经有一些默认的转换器被注册了。通过在 RequestResponseBodyMethodProcessor 中打断点，我们可以获取到一个 converters 列表： 源码方面不做过多的解读，有兴趣的朋友可以研究一下 RequestResponseBodyMethodProcessor 中的 handleReturnValue 方法，包含了转换的核心实现。 自定义 HttpMessageConverter前面已经提及了消息转换器是通过判断媒体类型来调用响应的转换类的，不禁引发了我们的思考，如果我们遇到了不常用的 MediaType，或者自定义的 MediaType，又想要使用 Spring 的 @RequestBody，@ResponseBody 注解，该如何添加代码呢？下面我们通过自定义一个 HttpMessageConverter 来了解 Spring 内部的转换过程。 先定义我们的需求，自定一个 MediaType：application/toString，当返回一个带有 @ResponseBody 注解的实体类时，将该实体类的 ToString 作为响应内容。 1 首先重写 Book 的 ToString 方法，方便后期效果展示 1234567@Overridepublic String toString() { return &quot;~~~Book{&quot; + &quot;id=&quot; + id + &quot;, bookName='&quot;+ bookName +'\\'' + &quot;}~~~&quot;;} 2 编写自定义的消息转换器 123456789101112131415161718192021222324public class ToStringHttpMessageConverter extends AbstractHttpMessageConverter&lt;Object&gt; { public ToStringHttpMessageConverter() { super(new MediaType(&quot;application&quot;, &quot;toString&quot;, Charset.forName(&quot;UTF-8&quot;)));// &lt;1&gt; } @Override protected boolean supports(Class&lt;?&gt; clazz) { return true; } // 从请求体封装数据 对应 RequestBody 用 String 接收 @Override protected Object readInternal(Class&lt;?&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException { return StreamUtils.copyToString(inputMessage.getBody(), Charset.forName(&quot;UTF-8&quot;)); } // 从响应体封装数据 对应 ResponseBody @Override protected void writeInternal(Object o, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException { String result = o.toString();//&lt;2&gt; outputMessage.getBody().write(result.getBytes()); }} &lt;1&gt; 此处指定了支持的媒体类型 &lt;2&gt; 调用类的 ToString 方法，将结果写入到输出流中 3 配置自定义的消息转换器 12345678@Configurationpublic class WebMvcConfig extends WebMvcConfigurerAdapter{ @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { converters.add(new ToStringHttpMessageConverter()); }} 4 配置后端端点，指定生产类型 12345678@RestControllerpublic class BookController { @GetMapping(value = &quot;/book/{bookId}&quot;,produces = {&quot;application/toString&quot;,&quot;application/json&quot;,&quot;application/xml&quot;}) public Book getBook(@PathVariable(&quot;bookId&quot;) Integer bookId) { return new Book(bookId, &quot;book&quot; + bookId); }} 此处只是为了演示，添加了三个生产类型，我们的后端端点可以支持输出三种类型，而具体输出哪一者，则依赖客户端的 Accept 指定。 5 客户端请求 123GET /book/2 HTTP/1.1Host: localhost:8080Accept: application/toString 响应结果如下： 1​~~~Book{id=2, bookName='book2'}~~~ 此时，你可以任意指定 Accept 的类型，即可获得不同形式的 Book 返回结果，可以是 application/toString，application/json，application/xml，都会对应各自的 HttpMessageConverter。","link":"/requestBody-and-responseBody/"},{"title":"上一个电商项目的反思","text":"从去年实习到今年转正，陆陆续续接触了大概四个项目。有电商类，互联网保险类，也经历过管理系统。幸运的是，这些项目都是从零开始，避免了让我去维护不堪入目的老旧系统。而这么多项目中令我印象最深刻的，就要属上一个电商项目了。这也是我接触到的真正意义的第一个微服务项目，到今天回首去看曾经的这个项目，有很多突破性地尝试，同时不可避免地也踩入了一些坑点，毕竟摸着石头过河。今天想聊聊我对上一个电商项目的反思。 项目简介准确的说是一个第三方的电商项目，商品来源是由主流电商的 http 接口提供（目前接入了京东，苏宁），打造我们自己的商城体系。使用的技术包括 springboot，jpa，rpc 框架使用的是 motan，数据库使用的是 oracle，基本都还算是主流的技术。 盲目地拆分微服务使用了 springboot 就是微服务了吗？使用 rpc 通信就是微服务了吗？刚接触到所谓的微服务架构时，无疑是让人兴奋的，但也没有太多的经验，以至于每提出一个新的需求，几乎就会新建一个服务。没有从宏观去思考如何拆分服务，那时还没有项目组成员尝试去使用领域驱动设计的思想去划分服务的边界，会议室中讨论最多的话题也是：我们的数据库该如何设计，而不是我们的领域该如何划分。项目初期，使用单体式的思想开发着分布式项目，新技术的引入还只是使人有点稍微的不顺手，但是项目越做越大后，越来越大的不适感逐渐侵蚀着我们的开发速度。 说道微服务的拆分，有很多个维度，这里主要谈两个维度： 系统维度：业务功能不同的需求，交给不同的系统完成，如订单，商品，地址，用户等系统需要拆分。 模块维度：基础架构层（如公用 util），领域层，接口层，服务层，表现层的拆分。 在项目的初期，我们错误地认为微服务的拆分仅仅是系统维度的拆分，如商品系统和订单系统，而在模块维度上，缺少拆分的意识，如订单模块的表现层和服务层，我们虽然做了隔离（两个独立的 tomcat）。但在后来，业务添加了一个新的需求：商城增加积分支持，让用户可以使用积分购买商品。我们突然发现，所谓的服务层和表现层严重的耦合，仅仅是在物理上进行了隔离，逻辑层面并没有拆分，这导致新的积分服务模块从原先的订单服务层拷贝了大量的代码。吸取了这个教训后，我们新的项目中采取了如下的分层方式： 其中比较关键的一点便是表现层与应用层的完全分离，交互完全使用 DTO 对象。不少同事产生了困惑，抱怨在表现层不能访问数据库，这让他们获取数据变得十分“麻烦”，应用层和表现层还多了一次数据拷贝的工作，用于将 DO 持久化对象转换成 DTO 对象。但这样的好处从长远来看，是不言而喻的。总结为以下几点： 1 应用层高度重用，没有表现形式的阻碍，PC 端，移动端，外部服务都可以同时接入，需要组装什么样的数据，请自行组装。 2 应用层和领域层可以交由经验较为丰富的程序员负责，避免了一些低性能的数据操作，错误的并发控制等等。 3 解决远程调用数据懒加载的问题。从前的设计中，表现层拿到了领域层的对象，而领域层会使用懒加载技术，当表现层想要获取懒加载属性时，或得到一个 no session 的异常。在没有这个分层之前，如何方便地解决这个问题一度困扰了我们很长的一段时间。 数据库的滥用项目使用了 oracle，我们所有的数据都存在于同一个 oracle 实例中，各个系统模块并没有做到物理层面的数据库隔离。这并不符合设计，一方面这给那些想要跨模块执行 join 操作的人留了后门，如执行订单模块和用户模块的级联查询；另一方面，还困扰了一部分对微服务架构不甚了解的程序员，在他们的想法中，同一个数据库实例反而方便了他们的数据操作。 严格意义上，不仅仅是不同系统之间的数据库不能互相访问。同一个系统维度的不同模块也应当限制，正如前面一节的分层架构中，表现层（web 层）是不应该出现 DAO 的，pom 文件中也不应该出现任何 JPA，Hibernate，Mybatis 一类的依赖，它所有的数据来源，必须是应用层。 另外一方面，由于历史遗留问题，需要对接一个老系统，他们的表和这个电商的 oracle 实例是同一个，而我竟然在他们的表上发现了触发器这种操作… 在新的项目中，我们已经禁止使用数据库层面的触发器和物理约束。 在新的项目中，我们采用了阿里云的 RDS(mysql) 作为 oracle 的替代品，核心业务数据则放到了分布式数据库 DRDS 中，严格做到了数据库层面的拆分。 并发的控制电商系统不同于 OA 系统，CMS 系统，余额，订单等等操作都是敏感操作，实实在在跟钱打交道的东西容不得半点马虎，然而即使是一些有经验的程序员，也写出了这样的扣减余额操作： 12345678public void reduce(String accountId,BigDecimal cost){ Account account = accountService.findOne(accountId); BigDecimal balance = account.getBalance(); if(balance &gt; cost) balance = balance - cost;// 用四则运算代替 BigDecimal 的 api，方便表达 account.setBalance(balance); accountService.save(account);} 很多人没有控制并发的意识，即使意识到了，也不知道如何根据业务场景采取合适的手段控制并发，是使用 JPA 中的乐观锁，还是使用数据库的行级自旋锁完成简单并发控制，还是 for update 悲观锁（这不建议被使用），还是基于 redis 或 zookeeper 一类的分布式锁？ 这种错误甚至都不容许等到 code revivew 时才被发现，而应该是尽力地杜绝。 代码规范小到 java 的变量的驼峰命名法，数据库中用‘_’分割单词，到业务代码该如何规范的书写，再到并发规范，性能调优。准确的说，没有人管理这些事，这样的工作落到了每个有悟性的开发者身上。模块公用的常量，系统公用的常量应当区分放置，禁止使用魔鬼数字，bool 变量名不能以 is 开头等等细小的但是重要的规范，大量的条件查询 findByxxx 污染了 DAO 层，完全可以被 predicates，criteria 替代，RESTFUL 规范指导设计 web 接口等等… 在新的项目中，一条条规范被逐渐添加到了项目单独的模块 READ.me 中。作为公司的一个 junior developer，在建议其他成员使用规范开发项目时，得到的回应通常是：我的功能都已经实现了，干嘛要改；不符合规范又怎么样，要改你改时。有时候也是挺无力的，算是个人的一点牢骚吧。 软件设计的一点不足还是拿订单系统和商品系统来说事，虽然两个系统在物理上被拆分开了，但如果需要展示订单列表，订单详情，如今系统的设计会发起多次的远程调用，用于查询订单的归属商品，这是违背领域驱动设计的。订单中的商品就应当是归属于订单模块，正确的设计应该是使用冗余，代替频繁的跨网络节点远程调用。 另外一点便是高可用，由于机器内存的限制，所有的系统都只部署了单个实例，这其实并不是微服务的最佳实践。从系统应用，到 zookeeper，redis，mq 等中间件，都应当保证高可用，避免单点问题。没有真正实现做到横向扩展（知识理论上实现了），实在是有点遗憾。 系统没有熔断，降级处理，在新的项目中，由于我们引入了 Spring Cloud，很多地方都可以 out of box 式使用框架提供的 fallback 处理，而这上一个电商项目由于框架的限制以及接口设计之初就没有预想到要做这样的操作，使得可靠性再减了几分。 自动化运维的缺失单体式应用的美好时代，只需要发布同一份 war 包。而微服务项目中，一切都变得不同，在我们这个不算特别庞大的电商系统中，需要被运行的服务模块也到达了 30-40 个。由于这个电商系统是部署在甲方自己的服务器中，一方面是业务部门的业务审批流程，一方面是如此众多的 jar 包运行，没有自动发布，没有持续集成。令我比较难忘的是初期发布版本，始终有一两个服务莫名奇妙的挂掉，对着终端中的服务列表，一个个排查，这种痛苦的经历。至今，这个系统仍然依靠运维人员，手动管理版本。 上一个项目有一些不可控的项目因素，而新的项目中，系统服务全部在阿里云上部署，也引入了 Jenkins，一切都在逐渐变好，其他的 devops 工具仍然需要完善，以及 docker 一类的容器技术还未在计划日程之内，这些都是我们今年努力的目标。 总结原本积累了很多自己的想法，可惜落笔之后能够捕捉到一些点，便只汇聚成了上述这些，而这上一个电商项目在逐渐的迭代开发之后也变得越来越好了（我去了新的项目组后，其他同事负责了后续的开发）。这个经历，于我是非常珍贵的，它比那些大牛直接告诉我微服务设计的要素要更加有意义。知道了不足之处，经历了自己解决问题的过程，才会了解到好的方案的优势，了解到开源方案到底是为了解决什么样的问题而设计的。","link":"/rethink-1/"},{"title":"深入理解 RPC 之集群篇","text":"上一篇文章分析了服务的注册与发现，这一篇文章着重分析下 RPC 框架都会用到的集群的相关知识。 集群 (Cluster) 本身并不具备太多知识点，在分布式系统中，集群一般涵盖了负载均衡（LoadBalance），高可用（HA），路由（Route）等等概念，每个 RPC 框架对集群支持的程度不同，本文着重分析前两者 – 负载均衡和高可用。 集群概述在此之前的《深入理解 RPC》系列文章，对 RPC 的分析着重还是放在服务之间的点对点调用，而分布式服务中每个服务必然不止一个实例，不同服务的实例和相同服务的多个实例构成了一个错综复杂的分布式环境，在服务治理框架中正是借助了 Cluster 这一层来应对这一难题。还是以博主较为熟悉的 motan 这个框架来介绍 Cluster 的作用。 先来看看 Cluster 的顶层接口： 1234567891011@Spi(scope = Scope.PROTOTYPE)public interface Cluster&lt;T&gt; extends Caller&lt;T&gt; { @Override void init(); void setUrl(URL url); void setLoadBalance(LoadBalance&lt;T&gt; loadBalance);//&lt;1&gt; void setHaStrategy(HaStrategy&lt;T&gt; haStrategy);//&lt;2&gt; void onRefresh(List&lt;Referer&lt;T&gt;&gt; referers); List&lt;Referer&lt;T&gt;&gt; getReferers(); LoadBalance&lt;T&gt; getLoadBalance();} 在概述中，我们只关心 Cluster 接口中的两个方法，它揭示了 Cluster 在服务治理中的地位 &lt;1&gt; 指定负载均衡算法 &lt;2&gt; 指定高可用策略（容错机制） 我们需要对所谓的负载均衡策略和高可用策略有一定的理解，才能够搞清楚集群是如何运作的。 负载均衡说到负载均衡，大多数人可能立刻联想到了 nginx。负载均衡可以分为服务端负载均衡和客户端负载均衡，而服务端负载均衡又按照实现方式的不同可以划分为软件负载均衡和硬件负载均衡，nginx 便是典型的软件负载均衡。而我们今天所要介绍的 RPC 中的负载均衡则主要是客户端负载均衡。如何区分也很简单，用笔者自己的话来描述下 在 RPC 调用中，客户端持有所有的服务端节点引用，自行通过负载均衡算法选择一个节点进行访问，这便是客户端负载均衡。 客户端如何获取到所有的服务端节点引用呢？一般是通过配置的方式，或者是从上一篇文章介绍的服务注册与发现组件中获取。 负载均衡接口分析motan 中的负载均衡抽象： 1234567@Spi(scope = Scope.PROTOTYPE)public interface LoadBalance&lt;T&gt; { void onRefresh(List&lt;Referer&lt;T&gt;&gt; referers); Referer&lt;T&gt; select(Request request);//&lt;1&gt; void selectToHolder(Request request, List&lt;Referer&lt;T&gt;&gt; refersHolder); void setWeightString(String weightString);} ribbon 中的负载均衡抽象： 12345public interface IRule{ public Server choose(Object key);//&lt;1&gt; public void setLoadBalancer(ILoadBalancer lb); public ILoadBalancer getLoadBalancer();} &lt;1&gt; 对比下两个 RPC 框架对负载均衡的抽象可以发现，其实负载均衡策略干的事很简单，就是根据请求返回一个服务节点。在 motan 中对服务端的点对点调用抽象成了 Referer，而在 ribbon 中则是 Server。 几种负载均衡算法负载均衡算法有几种经典实现，已经是老生常谈了，总结后主要有如下几个： 轮询（Round Robin） 加权轮询（Weight Round Robin） 随机（Random） 加权随机（Weight Random） 源地址哈希（Hash） 一致性哈希（ConsistentHash） 最小连接数（Least Connections） 低并发优先（Active Weight） 每个框架支持的实现都不太一样，如 **ribbon 支持的负载均衡策略 **： 策略名 策略描述 实现说明 BestAvailableRule 选择一个最小并发请求的 server 逐个考察 Server，如果 Server 被 tripped 了，则忽略，在选择其中 ActiveRequestsCount 最小的 server AvailabilityFilteringRule 过滤掉那些因为一直连接失败的被标记为 circuit tripped 的后端 server，并过滤掉那些高并发的的后端 server（active connections 超过配置的阈值） 使用一个 AvailabilityPredicate 来包含过滤 server 的逻辑，其实就就是检查 status 里记录的各个 server 的运行状态 WeightedResponseTimeRule 根据响应时间分配一个 weight，响应时间越长，weight 越小，被选中的可能性越低。 一个后台线程定期的从 status 里面读取评价响应时间，为每个 server 计算一个 weight。Weight 的计算也比较简单 responsetime 减去每个 server 自己平均的 responsetime 是 server 的权重。当刚开始运行，没有形成 status 时，使用 RoundRobinRule 策略选择 server。 RetryRule 对选定的负载均衡策略机上重试机制。 在一个配置时间段内当选择 server 不成功，则一直尝试使用 subRule 的方式选择一个可用的 server RoundRobinRule roundRobin 方式轮询选择 server 轮询 index，选择 index 对应位置的 server RandomRule 随机选择一个 server 在 index 上随机，选择 index 对应位置的 server ZoneAvoidanceRule 复合判断 server 所在区域的性能和 server 的可用性选择 server 使用 ZoneAvoidancePredicate 和 AvailabilityPredicate 来判断是否选择某个 server，前一个判断判定一个 zone 的运行性能是否可用，剔除不可用的 zone（的所有 server），AvailabilityPredicate 用于过滤掉连接数过多的 Server。 **motan 支持的负载均衡策略 **： 策略名 策略描述 Random 随机选择一个 server RoundRobin roundRobin 方式轮询选择 server ConsistentHash 一致性 Hash，保证同一源地址的请求落到同一个服务端，能够应对服务端机器的动态上下线 (实际上并没有严格做到一致性 hash，motan 的实现只能满足粘滞 hash，只保证 server 节点变更周期内相同对请求落在相同的 server 上，比较适合用在二级缓存场景) LocalFirst 当 server 列表中包含本地暴露的可用服务时，优先使用此服务。否则使用低并发优先 ActiveWeight 负载均衡策略 ActiveWeight 并发量越小的 server，优先级越高 ConfigurableWeight 加权随机 算法很多，有些负载均衡算法的实现复杂度也很高，请教了一些朋友，发现用的最多还是 RoundRobin，Random 这两种。可能和他们实现起来很简单有关，很多运用到 RPC 框架的项目也都是保持了默认配置。 而这两种经典复杂均衡算法实现起来是很简单的，在此给出网上的简易实现，方便大家更直观的了解。 ** 服务列表 ** 1234567891011121314151617181920212223public class IpMap{ // 待路由的 Ip 列表，Key 代表 Ip，Value 代表该 Ip 的权重 public static HashMap&lt;String, Integer&gt; serverWeightMap = new HashMap&lt;String, Integer&gt;(); static { serverWeightMap.put(&quot;192.168.1.100&quot;, 1); serverWeightMap.put(&quot;192.168.1.101&quot;, 1); // 权重为 4 serverWeightMap.put(&quot;192.168.1.102&quot;, 4); serverWeightMap.put(&quot;192.168.1.103&quot;, 1); serverWeightMap.put(&quot;192.168.1.104&quot;, 1); // 权重为 3 serverWeightMap.put(&quot;192.168.1.105&quot;, 3); serverWeightMap.put(&quot;192.168.1.106&quot;, 1); // 权重为 2 serverWeightMap.put(&quot;192.168.1.107&quot;, 2); serverWeightMap.put(&quot;192.168.1.108&quot;, 1); serverWeightMap.put(&quot;192.168.1.109&quot;, 1); serverWeightMap.put(&quot;192.168.1.110&quot;, 1); }} ** 轮询（Round Robin）** 12345678910111213141516171819202122232425262728public class RoundRobin{ private static Integer pos = 0; public static String getServer() { // 重建一个 Map，避免服务器的上下线导致的并发问题 Map&lt;String, Integer&gt; serverMap = new HashMap&lt;String, Integer&gt;(); serverMap.putAll(IpMap.serverWeightMap); // 取得 Ip 地址 List Set&lt;String&gt; keySet = serverMap.keySet(); ArrayList&lt;String&gt; keyList = new ArrayList&lt;String&gt;(); keyList.addAll(keySet); String server = null; synchronized (pos) { if (pos &gt; keySet.size()) pos = 0; server = keyList.get(pos); pos ++; } return server; }} ** 随机（Random）** 1234567891011121314151617181920public class Random{ public static String getServer() { // 重建一个 Map，避免服务器的上下线导致的并发问题 Map&lt;String, Integer&gt; serverMap = new HashMap&lt;String, Integer&gt;(); serverMap.putAll(IpMap.serverWeightMap); // 取得 Ip 地址 List Set&lt;String&gt; keySet = serverMap.keySet(); ArrayList&lt;String&gt; keyList = new ArrayList&lt;String&gt;(); keyList.addAll(keySet); java.util.Random random = new java.util.Random(); int randomPos = random.nextInt(keyList.size()); return keyList.get(randomPos); }} 高可用策略高可用（HA）策略一般也被称作容错机制，分布式系统中出错是常态，但服务却不能停止响应，6 个 9 一直是各个公司的努力方向。当一次请求失败之后，是重试呢？还是继续请求其他机器？抑或是记录下这次失败？下面是集群中的几种常用高可用策略： 失效转移（failover） 当出现失败，重试其他服务器，通常用于读操作等幂等行为，重试会带来更长延迟。该高可用策略会受到负载均衡算法的限制，比如失效转移强调需要重试其他机器，但一致性 Hash 这类负载均衡算法便会与其存在冲突（个人认为一致性 Hash 在 RPC 的客户端负载均衡中意义不是很大） 快速失败（failfast） 只发起一次调用，失败立即报错，通常用于非幂等性的写操作。 如果在 motan，dubbo 等配置中设置了重试次数 &gt;0，又配置了该高可用策略，则重试效果也不会生效，由此可见集群中的各个配置可能是会相互影响的。 失效安全（failsafe） 出现异常时忽略，但记录这一次失败，存入日志中。 失效自动恢复（failback） 后台记录失败请求，定时重发。通常用于消息通知操作。 并行调用（forking） 只要一个成功即返回，通常用于实时性要求较高的读操作。需要牺牲一定的服务资源。 广播（broadcast） 广播调用，所有提供逐个调用，任意一台报错则报错。通常用于更新提供方本地状态，速度慢，任意一台报错则报错。 高可用接口分析以 motan 的 HaStrategy 为例来介绍高可用在集群中的实现细节 12345@Spi(scope = Scope.PROTOTYPE)public interface HaStrategy&lt;T&gt; { void setUrl(URL url); Response call(Request request, LoadBalance&lt;T&gt; loadBalance);//&lt;1&gt;} &lt;1&gt; 如我之前所述，高可用策略依赖于请求和一个特定的负载均衡算法，返回一个响应。 ** 快速失败（failfast）** 123456789@SpiMeta(name = &quot;failfast&quot;)public class FailfastHaStrategy&lt;T&gt; extends AbstractHaStrategy&lt;T&gt; { @Override public Response call(Request request, LoadBalance&lt;T&gt; loadBalance) { Referer&lt;T&gt; refer = loadBalance.select(request); return refer.call(request); }} motan 实现了两个高可用策略，其一便是 failfast，非常简单，只进行一次负载均衡节点的选取，接着发起点对点的调用。 ** 失效转移（failover）** 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@SpiMeta(name = &quot;failover&quot;)public class FailoverHaStrategy&lt;T&gt; extends AbstractHaStrategy&lt;T&gt; { protected ThreadLocal&lt;List&lt;Referer&lt;T&gt;&gt;&gt; referersHolder = new ThreadLocal&lt;List&lt;Referer&lt;T&gt;&gt;&gt;() { @Override protected java.util.List&lt;com.weibo.api.motan.rpc.Referer&lt;T&gt;&gt; initialValue() { return new ArrayList&lt;Referer&lt;T&gt;&gt;(); } }; @Override public Response call(Request request, LoadBalance&lt;T&gt; loadBalance) { List&lt;Referer&lt;T&gt;&gt; referers = selectReferers(request, loadBalance); if (referers.isEmpty()) { throw new MotanServiceException(String.format(&quot;FailoverHaStrategy No referers for request:%s, loadbalance:%s&quot;, request, loadBalance)); } URL refUrl = referers.get(0).getUrl(); // 先使用 method 的配置 int tryCount = refUrl.getMethodParameter(request.getMethodName(), request.getParamtersDesc(), URLParamType.retries.getName(), URLParamType.retries.getIntValue()); // 如果有问题，则设置为不重试 if (tryCount &lt; 0) { tryCount = 0; } // 只有 failover 策略才会有重试 for (int i = 0; i &lt;= tryCount; i++) { Referer&lt;T&gt; refer = referers.get(i % referers.size()); try { request.setRetries(i); return refer.call(request); } catch (RuntimeException e) { // 对于业务异常，直接抛出 if (ExceptionUtil.isBizException(e)) { throw e; } else if (i &gt;= tryCount) { throw e; } LoggerUtil.warn(String.format(&quot;FailoverHaStrategy Call false for request:%s error=%s&quot;, request, e.getMessage())); } } throw new MotanFrameworkException(&quot;FailoverHaStrategy.call should not come here!&quot;); } protected List&lt;Referer&lt;T&gt;&gt; selectReferers(Request request, LoadBalance&lt;T&gt; loadBalance) { List&lt;Referer&lt;T&gt;&gt; referers = referersHolder.get(); referers.clear(); loadBalance.selectToHolder(request, referers); return referers; }} 其二的高可用策略是 failover，实现相对复杂一些，容忍在重试次数内的失败调用。这也是 motan 提供的默认策略。 其他集群相关的知识点在 Dubbo 中也有 cluster 这一分层，除了 loadbalance 和 ha 这两层之外还包含了路由（Router）用来做读写分离，应用隔离；合并结果（Merger）用来做响应结果的分组聚合。 在 SpringCloud-Netflix 中整合了 Zuul 来做服务端的负载均衡 参考资料 几种简单的负载均衡算法及其 Java 代码实现 搜索业务和技术介绍及容错机制","link":"/rpc-cluster/"},{"title":"深入理解 RPC 之动态代理篇","text":"提到 JAVA 中的动态代理，大多数人都不会对 JDK 动态代理感到陌生，Proxy，InvocationHandler 等类都是 J2SE 中的基础概念。动态代理发生在服务调用方 / 客户端，RPC 框架需要解决的一个问题是：像调用本地接口一样调用远程的接口。于是如何组装数据报文，经过网络传输发送至服务提供方，屏蔽远程接口调用的细节，便是动态代理需要做的工作了。RPC 框架中的代理层往往是单独的一层，以方便替换代理方式（如 motan 代理层位于 com.weibo.api.motan.proxy ，dubbo 代理层位于 com.alibaba.dubbo.common.bytecode ）。 实现动态代理的方案有下列几种： jdk 动态代理 cglib 动态代理 javassist 动态代理 ASM 字节码 javassist 字节码 其中 cglib 底层实现依赖于 ASM，javassist 自成一派。由于 ASM 和 javassist 需要程序员直接操作字节码，导致使用门槛相对较高，但实际上他们的应用是非常广泛的，如 Hibernate 底层使用了 javassist（默认）和 cglib，Spring 使用了 cglib 和 jdk 动态代理。 RPC 框架无论选择何种代理技术，所需要完成的任务其实是固定的，不外乎‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’… 技术选型的影响因素框架中使用何种动态代理技术，影响因素也不少。 性能从早期 dubbo 的作者梁飞的博客 http://javatar.iteye.com/blog/814426 中可以得知 dubbo 选择使用 javassist 作为动态代理方案主要考虑的因素是 ** 性能 **。 从其博客的测试结果来看 javassist &gt; cglib &gt; jdk 。但实际上他的测试过程稍微有点瑕疵：在 cglib 和 jdk 代理对象调用时，走的是反射调用，而在 javassist 生成的代理对象调用时，走的是直接调用（可以先阅读下梁飞大大的博客）。这意味着 cglib 和 jdk 慢的原因并不是由动态代理产生的，而是由反射调用产生的（顺带一提，很多人认为 jdk 动态代理的原理是反射，其实它的底层也是使用的字节码技术）。而最终我的测试结果，结论如下： javassist ≈ cglib &gt; jdk 。javassist 和 cglib 的效率基本持平 ，而他们两者的执行效率基本可以达到 jdk 动态代理的 2 倍（这取决于测试的机器以及 jdk 的版本，jdk1.8 相较于 jdk1.6 动态代理技术有了质的提升，所以并不是传闻中的那样：cglib 比 jdk 快 10 倍）。文末会给出我的测试代码。 依赖 motan 默认的实现是 jdk 动态代理，代理方案支持 SPI 扩展，可以自行扩展其他实现方式。 使用 jdk 做为默认，主要是减少 core 包依赖，性能不是唯一考虑因素。另外使用字节码方式 javaassist 性能比较优秀，动态代理模式下 jdk 性能也不会差多少。 – rayzhang0603(motan 贡献者) motan 选择使用 jdk 动态代理，原因主要有两个：减少 motan-core 的依赖，方便。至于扩展性，dubbo 并没有预留出动态代理的扩展接口，而是写死了 bytecode ，这点上 motan 做的较好。 易用性从 dubbo 和 motan 的源码中便可以直观的看出两者的差距了，dubbo 为了使用 javassist 技术花费不少的精力，而 motan 使用 jdk 动态代理只用了一个类。dubbo 的设计者为了追求极致的性能而做出的工作是值得肯定的，motan 也预留了扩展机制，两者各有千秋。 动态代理入门指南为了方便对比几种动态代理技术，先准备一个统一接口。 123public interface BookApi { void sell();} JDK 动态代理12345678910111213141516171819202122232425private static BookApi createJdkDynamicProxy(final BookApi delegate) { BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(), new Class[]{BookApi.class}, new JdkHandler(delegate)); return jdkProxy;}private static class JdkHandler implements InvocationHandler { final Object delegate; JdkHandler(Object delegate) { this.delegate = delegate; } @Override public Object invoke(Object object, Method method, Object[] objects) throws Throwable { // 添加代理逻辑 &lt;1&gt; if(method.getName().equals(&quot;sell&quot;)){ System.out.print(&quot;&quot;); } return null;// return method.invoke(delegate, objects); } &lt;1&gt; 在真正的 RPC 调用中 ，需要填充‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’ 等逻辑。 Cglib 动态代理123456789101112131415161718192021222324252627private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception { Enhancer enhancer = new Enhancer(); enhancer.setCallback(new CglibInterceptor(delegate)); enhancer.setInterfaces(new Class[]{BookApi.class}); BookApi cglibProxy = (BookApi) enhancer.create(); return cglibProxy; } private static class CglibInterceptor implements MethodInterceptor { final Object delegate; CglibInterceptor(Object delegate) { this.delegate = delegate; } @Override public Object intercept(Object object, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { // 添加代理逻辑 if(method.getName().equals(&quot;sell&quot;)) { System.out.print(&quot;&quot;); } return null;// return methodProxy.invoke(delegate, objects); } } 和 JDK 动态代理的操作步骤没有太大的区别，只不过是替换了 cglib 的 API 而已。 需要引入 cglib 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.2.5&lt;/version&gt;&lt;/dependency&gt; Javassist 字节码到了 javassist，稍微有点不同了。因为它是通过直接操作字节码来生成代理对象。 1234567891011private static BookApi createJavassistBytecodeDynamicProxy() throws Exception { ClassPool mPool = new ClassPool(true); CtClass mCtc = mPool.makeClass(BookApi.class.getName() + &quot;JavaassistProxy&quot;); mCtc.addInterface(mPool.get(BookApi.class.getName())); mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc)); mCtc.addMethod(CtNewMethod.make( &quot;public void sell(){ System.out.print(\\&quot;\\&quot;) ; }&quot;, mCtc)); Class&lt;?&gt; pc = mCtc.toClass(); BookApi bytecodeProxy = (BookApi) pc.newInstance(); return bytecodeProxy;} 需要引入 javassist 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;version&gt;3.21.0-GA&lt;/version&gt;&lt;/dependency&gt; 动态代理测试测试环境：window i5 8g jdk1.8 cglib3.2.5 javassist3.21.0-GA 动态代理其实分成了两步：代理对象的创建，代理对象的调用。坊间流传的动态代理性能对比主要指的是后者；前者一般不被大家考虑，如果远程 Refer 的对象是单例的，其只会被创建一次，而如果是原型模式，多例对象的创建其实也是性能损耗的一个考虑因素（只不过远没有调用占比大）。 Create JDK Proxy: 21 ms Create CGLIB Proxy: 342 ms Create Javassist Bytecode Proxy: 419 ms 可能出乎大家的意料，JDK 创建动态代理的速度比后两者要快 10 倍左右。 下面是调用速度的测试： case 1: JDK Proxy invoke cost 1912 ms CGLIB Proxy invoke cost 1015 ms JavassistBytecode Proxy invoke cost 1280 ms case 2: JDK Proxy invoke cost 1747 ms CGLIB Proxy invoke cost 1234 ms JavassistBytecode Proxy invoke cost 1175 ms case 3: JDK Proxy invoke cost 2616 ms CGLIB Proxy invoke cost 1373 ms JavassistBytecode Proxy invoke cost 1335 ms Jdk 的执行速度一定会慢于 Cglib 和 Javassist，但最慢也就 2 倍，并没有达到数量级的差距；Cglib 和 Javassist 不相上下，差距不大（测试中偶尔发现 Cglib 实行速度会比平时慢 10 倍，不清楚是什么原因） 所以出于易用性和性能，私以为使用 Cglib 是一个很好的选择（性能和 Javassist 持平，易用性和 Jdk 持平）。 反射调用既然提到了动态代理和 cglib ，顺带提一下反射调用如何加速的问题。RPC 框架中在 Provider 服务端需要根据客户端传递来的 className + method + param 来找到容器中的实际方法执行反射调用。除了反射调用外，还可以使用 Cglib 来加速。 JDK 反射调用12Method method = serviceClass.getMethod(methodName, new Class[]{});method.invoke(delegate, new Object[]{}); Cglib 调用123FastClass serviceFastClass = FastClass.create(serviceClass);FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{});serviceFastMethod.invoke(delegate, new Object[]{}); 但实测效果发现 Cglib 并不一定比 JDK 反射执行速度快，还会跟具体的方法实现有关 (大雾)。 测试代码略长… 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147public class Main { public static void main(String[] args) throws Exception { BookApi delegate = new BookApiImpl(); long time = System.currentTimeMillis(); BookApi jdkProxy = createJdkDynamicProxy(delegate); time = System.currentTimeMillis() - time; System.out.println(&quot;Create JDK Proxy:&quot; + time + &quot;ms&quot;); time = System.currentTimeMillis(); BookApi cglibProxy = createCglibDynamicProxy(delegate); time = System.currentTimeMillis() - time; System.out.println(&quot;Create CGLIB Proxy:&quot; + time + &quot;ms&quot;); time = System.currentTimeMillis(); BookApi javassistBytecodeProxy = createJavassistBytecodeDynamicProxy(); time = System.currentTimeMillis() - time; System.out.println(&quot;Create JavassistBytecode Proxy:&quot; + time + &quot;ms&quot;); for (int i = 0; i &lt; 10; i++) { jdkProxy.sell();//warm } long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) { jdkProxy.sell(); } System.out.println(&quot;JDK Proxy invoke cost&quot; + (System.currentTimeMillis() - start)+ &quot;ms&quot;); for (int i = 0; i &lt; 10; i++) { cglibProxy.sell();//warm } start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) { cglibProxy.sell(); } System.out.println(&quot;CGLIB Proxy invoke cost&quot; + (System.currentTimeMillis() - start)+ &quot;ms&quot;); for (int i = 0; i &lt; 10; i++) { javassistBytecodeProxy.sell();//warm } start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) { javassistBytecodeProxy.sell(); } System.out.println(&quot;JavassistBytecode Proxy invoke cost&quot; + (System.currentTimeMillis() - start)+ &quot;ms&quot;); Class&lt;?&gt; serviceClass = delegate.getClass(); String methodName = &quot;sell&quot;; for (int i = 0; i &lt; 10; i++) { cglibProxy.sell();//warm } // 执行反射调用 for (int i = 0; i &lt; 10; i++) {//warm Method method = serviceClass.getMethod(methodName, new Class[]{}); method.invoke(delegate, new Object[]{}); } start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) { Method method = serviceClass.getMethod(methodName, new Class[]{}); method.invoke(delegate, new Object[]{}); } System.out.println(&quot;反射 invoke cost&quot; + (System.currentTimeMillis() - start)+ &quot;ms&quot;); // 使用 CGLib 执行反射调用 for (int i = 0; i &lt; 10; i++) {//warm FastClass serviceFastClass = FastClass.create(serviceClass); FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{}); serviceFastMethod.invoke(delegate, new Object[]{}); } start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) { FastClass serviceFastClass = FastClass.create(serviceClass); FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]{}); serviceFastMethod.invoke(delegate, new Object[]{}); } System.out.println(&quot;CGLIB invoke cost&quot; + (System.currentTimeMillis() - start)+ &quot;ms&quot;); } private static BookApi createJdkDynamicProxy(final BookApi delegate) { BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(), new Class[]{BookApi.class}, new JdkHandler(delegate)); return jdkProxy; } private static class JdkHandler implements InvocationHandler { final Object delegate; JdkHandler(Object delegate) { this.delegate = delegate; } @Override public Object invoke(Object object, Method method, Object[] objects) throws Throwable { // 添加代理逻辑 if(method.getName().equals(&quot;sell&quot;)){ System.out.print(&quot;&quot;); } return null;// return method.invoke(delegate, objects); } } private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception { Enhancer enhancer = new Enhancer(); enhancer.setCallback(new CglibInterceptor(delegate)); enhancer.setInterfaces(new Class[]{BookApi.class}); BookApi cglibProxy = (BookApi) enhancer.create(); return cglibProxy; } private static class CglibInterceptor implements MethodInterceptor { final Object delegate; CglibInterceptor(Object delegate) { this.delegate = delegate; } @Override public Object intercept(Object object, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { // 添加代理逻辑 if(method.getName().equals(&quot;sell&quot;)) { System.out.print(&quot;&quot;); } return null;// return methodProxy.invoke(delegate, objects); } } private static BookApi createJavassistBytecodeDynamicProxy() throws Exception { ClassPool mPool = new ClassPool(true); CtClass mCtc = mPool.makeClass(BookApi.class.getName() + &quot;JavaassistProxy&quot;); mCtc.addInterface(mPool.get(BookApi.class.getName())); mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc)); mCtc.addMethod(CtNewMethod.make( &quot;public void sell(){ System.out.print(\\&quot;\\&quot;) ; }&quot;, mCtc)); Class&lt;?&gt; pc = mCtc.toClass(); BookApi bytecodeProxy = (BookApi) pc.newInstance(); return bytecodeProxy; }}","link":"/rpc-dynamic-proxy/"},{"title":"设计 RPC 接口时，你有考虑过这些吗？","text":"RPC 框架的讨论一直是各个技术交流群中的热点话题，阿里的 dubbo，新浪微博的 motan，谷歌的 grpc，以及不久前蚂蚁金服开源的 sofa，都是比较出名的 RPC 框架。RPC 框架，或者一部分人习惯称之为服务治理框架，更多的讨论是存在于其技术架构，比如 RPC 的实现原理，RPC 各个分层的意义，具体 RPC 框架的源码分析…但却并没有太多话题和“如何设计 RPC 接口”这样的业务架构相关。 可能很多小公司程序员还是比较关心这个问题的，这篇文章主要分享下一些个人眼中 RPC 接口设计的最佳实践。 初识 RPC 接口设计由于 RPC 中的术语每个程序员的理解可能不同，所以文章开始，先统一下 RPC 术语，方便后续阐述。 大家都知道共享接口是 RPC 最典型的一个特点，每个服务对外暴露自己的接口，该模块一般称之为 api；外部模块想要实现对该模块的远程调用，则需要依赖其 api；每个服务都需要有一个应用来负责实现自己的 api，一般体现为一个独立的进程，该模块一般称之为 app。 api 和 app 是构建微服务项目的最简单组成部分，如果使用 maven 的多 module 组织代码，则体现为如下的形式。 serviceA 服务 serviceA/pom.xml 定义父 pom 文件 123456789&lt;modules&gt; &lt;module&gt;serviceA-api&lt;/module&gt; &lt;module&gt;serviceA-app&lt;/module&gt;&lt;/modules&gt;&lt;packaging&gt;pom&lt;/packaging&gt;&lt;groupId&gt;moe.cnkirito&lt;/groupId&gt;&lt;artifactId&gt;serviceA&lt;/artifactId&gt;&lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; serviceA/serviceA-api/pom.xml 定义对外暴露的接口，最终会被打成 jar 包供外部服务依赖 12345678&lt;parent&gt; &lt;artifactId&gt;serviceA&lt;/artifactId&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;packaging&gt;jar&lt;/packaging&gt;&lt;artifactId&gt;serviceA-api&lt;/artifactId&gt; serviceA/serviceA-app/pom.xml 定义了服务的实现，一般是 springboot 应用，所以下面的配置文件中，我配置了 springboot 应用打包的插件，最终会被打成 jar 包，作为独立的进程运行。 1234567891011121314151617&lt;parent&gt; &lt;artifactId&gt;serviceA&lt;/artifactId&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;packaging&gt;jar&lt;/packaging&gt;&lt;artifactId&gt;serviceA-app&lt;/artifactId&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 麻雀虽小，五脏俱全，这样一个微服务模块就实现了。 旧 RPC 接口的痛点统一好术语，这一节来描述下我曾经遭遇过的 RPC 接口设计的痛点，相信不少人有过相同的遭遇。 ** 查询接口过多。** 各种 findBy 方法，加上各自的重载，几乎占据了一个接口 80% 的代码量。这也符合一般人的开发习惯，因为页面需要各式各样的数据格式，加上查询条件差异很大，便造成了：一个查询条件，一个方法的尴尬场景。这样会导致另外一个问题，需要使用某个查询方法时，直接新增了方法，但实际上可能这个方法已经出现过了，隐藏在了令人眼花缭乱的方法中。 ** 难以扩展 **。接口的任何改动，比如新增一个入参，都会导致调用者被迫升级，这也通常是 RPC 设计被诟病的一点，不合理的 RPC 接口设计会放大这个缺点。 ** 升级困难。** 在之前的 “初识 RPC 接口设计”一节中，版本管理的粒度是 project，而不是 module，这意味着：api 即使没有发生变化，app 版本演进，也会造成 api 的被迫升级，因为 project 是一个整体。问题又和上一条一样了，api 一旦发生变化，调用者也得被迫升级，牵一发而动全身。 ** 难以测试 **。接口一多，职责随之变得繁杂，业务场景各异，测试用例难以维护。特别是对于那些有良好习惯编写单元测试的程序员而言，简直是噩梦，用例也得跟着改。 ** 异常设计不合理 **。在既往的工作经历中曾经有一次会议，就 RPC 调用中的异常设计引发了争议，一派人觉得需要有一个业务 CommonResponse，封装异常，每次调用后，优先判断调用结果是否 success，在进行业务逻辑处理；另一派人觉得这比较麻烦，由于 RPC 框架是可以封装异常调用的，所以应当直接 try catch 异常，不需要进行业务包裹。在没有明确规范时，这两种风格的代码同时存在于项目中，十分难看！ 在千米网的三个月中，看了不少最佳实践。加上一次公司内部易永健老师的分享，涉及到了相同的话题，耳濡目染，这些曾经我发觉的痛点也逐渐有了解决之道。 1 单参数接口如果你使用过 springcloud ，可能会不适应 http 通信的限制，因为 @RequestBody 只能使用单一的参数，也就意味着，springcloud 构建的微服务架构下，接口天然是单参数的。而 RPC 方法入参的个数在语法层面是不会受到限制的，但如果强制要求入参为单参数，会解决一部分的痛点。 **1.1 使用 Specification 模式解决查询接口过多的问题 ** 123456public interface StudentApi{ Student findByName(String name); List&lt;Student&gt; findAllByName(String name); Student findByNameAndNo(String name,String no); Student findByIdcard(String Idcard);} 如上的多个查询方法目的都是同一个：根据条件查询出 Student，只不过查询条件有所差异。试想一下，Student 对象假设有 10 个属性，最坏的情况下它们的排列组合都可能作为查询条件，这便是查询接口过多的根源。 12345public interface StudentApi{ Student findBySpec(StudentSpec spec); List&lt;Student&gt; findListBySpec(StudentListSpec spec); Page&lt;Student&gt; findPageBySpec(StudentPageSpec spec);} 上述接口便是最通用的单参接口，三个方法几乎囊括了 99% 的查询条件。所有的查询条件都被封装在了 StudentSpec,StudentListSpec,StudentPageSpec 之中，分别满足了单对象查询，批量查询，分页查询的需求。如果你了解领域驱动设计，会发现这里借鉴了其中 Specification 模式的思想。 **1.2 单参数易于做统一管理 ** 12345public interface SomeProvider { void opA(ARequest request); void opB(BRequest request); CommonResponse&lt;C&gt; opC(CRequest request);} 入参中的入参虽然形态各异，但由于是单个入参，所以可以统一继承 AbstractBaseRequest，即上述的 ARequest，BRequest，CRequest 都是 AbstractBaseRequest 的子类。在千米内部项目中，AbstractBaseRequest 定义了 traceId、clientIp、clientType、operationType 等公共入参，减少了重复命名，我们一致认为，这更加的 OO。 有了 AbstractBaseRequest，我们可以更加轻松地在其之上做 AOP，千米的实践中，大概做了如下的操作： 请求入参统一校验（request.checkParam(); param.checkParam();） 实体变更统一加锁，降低锁粒度 请求分类统一处理（if (request instanceof XxxRequest)） 请求报文统一记日志（log.setRequest(JsonUtil.getJsonString(request))) 操作成功统一发消息 如果不遵守单参数的约定，上述这些功能也并不是无法实现，但所需花费的精力远大于单参数，一个简单的约定带来的优势，我们认为是值得的。 **1.3 单参数入参兼容性强 ** 还记得前面的小节中，我提到了 SpringCloud，在 SpringCloud Feign 中，接口的入参通常会被 @RequestBody 修饰，强制做单参数的限制。千米内部使用了 Dubbo 作为 Rpc 框架，一般而言，为 Dubbo 服务设计的接口是不能直接用作 Feign 接口的（主要是因为 @RequestBody 的限制），但有了单参数的限制，便使之成为了可能。为什么我好端端的 Dubbo 接口需要兼容 Feign 接口？可能会有人发出这样的疑问，莫急，这样做的初衷当然不是为了单纯做接口兼容，而是想充分利用 HTTP 丰富的技术栈以及一些自动化工具。 自动生成 HTTP 接口实现（让服务端同时支持 Dubbo 和 HTTP 两种服务接口） 看过我之前文章的朋友应该了解过一个设计：千米内部支持的是 Dubbo 协议和 HTTP 协议族（如 JSON RPC 协议，Restful 协议），这并不意味着程序员需要写两份代码，我们可以通过 Dubbo 接口自动生成 HTTP 接口，体现了单参数设计的兼容性之强。 通过 Swagger UI 实现对 Dubbo 接口的可视化便捷测试 又是一个兼容 HTTP 技术栈带来的便利，在 Restful 接口的测试中，Swagger 一直是备受青睐的一个工具，但可惜的是其无法对 Dubbo 接口进行测试。兼容 HTTP 后，我们只需要做一些微小的工作，便可以实现 Swagger 对 Dubbo 接口的可视化测试。 有利于 TestNg 集成测试 自动生成 TestNG 集成测试代码和缺省测试用例，这使得服务端接口集成测试变得异常简单，程序员更能集中精力设计业务用例，结合缺省用例、JPA 自动建表和 PowerMock 模拟外部依赖接口实现本机环境。 这块涉及到了公司内部的代码，只做下简单介绍，我们一般通过内部项目 com.qianmi.codegenerator:api-dubbo-2-restful ，com.qianmi.codegenerator:api-request-json 生成自动化的测试用例，方便测试。而这些自动化工具中大量使用了反射，而由于单参数的设计，反射用起来比较方便。 2. 接口异常设计首先肯定一点，RPC 框架是可以封装异常的，Exception 也是返回值的一部分。在 go 语言中可能更习惯于返回 err,res 的组合，但 JAVA 中我个人更偏向于 try catch 的方法捕获异常。RPC 接口设计中的异常设计也是一个注意点。 ** 初始方案 ** 12345public interface ModuleAProvider { void opA(ARequest request); void opB(BRequest request); CommonResponse&lt;C&gt; opC(CRequest request);} 我们假设模块 A 存在上述的 ModuleAProvider 接口，ModuleAProvider 的实现中或多或少都会出现异常，例如可能存在的异常 ModuleAException，调用者实际上并不知道 ModuleAException 的存在，只有当出现异常时，才会知晓。对于 ModuleAException 这种业务异常，我们更希望调用方能够显示的处理，所以 ModuleAException 应该被设计成 Checked Excepition。 ** 正确的异常设计姿势 ** 12345public interface ModuleAProvider { void opA(ARequest request) throws ModuleAException; void opB(BRequest request) throws ModuleAException; CommonResponse&lt;C&gt; opC(CRequest request) throws ModuleAException;} 上述接口中定义的异常实际上也是一种契约，契约的好处便是不需要叙述，调用方自然会想到要去处理 Checked Exception，否则连编译都过不了。 ** 调用方的处理方式 ** 在 ModuleB 中，应当如下处理异常： 12345678910111213141516171819202122public class ModuleBService implements ModuleBProvider { @Reference ModuleAProvider moduleAProvider; @Override public void someOp() throws ModuleBexception{ try{ moduleAProvider.opA(...); }catch(ModuleAException e){ throw new ModuleBException(e.getMessage()); } } @Override public void anotherOp(){ try{ moduleAProvider.opB(...); }catch(ModuleAException e){ // 业务逻辑处理 } }} someOp 演示了一个异常流的传递，ModuleB 暴露出去的异常应当是 ModuleB 的 api 模块中异常类，虽然其依赖了 ModuleA ，但需要将异常进行转换，或者对于那些意料之中的业务异常可以像 anotherOp() 一样进行处理，不再传递。这时如果新增 ModuleC 依赖 ModuleB，那么 ModuleC 完全不需要关心 ModuleA 的异常。 ** 异常与熔断 ** 作为系统设计者，我们应该认识到一点： RPC 调用，失败是常态。通常我们需要对 RPC 接口做熔断处理，比如千米内部便集成了 Netflix 提供的熔断组件 Hystrix。Hystrix 需要知道什么样的异常需要进行熔断，什么样的异常不能够进行熔断。在没有上述的异常设计之前，回答这个问题可能还有些难度，但有了 Checked Exception 的契约，一切都变得明了清晰了。 123456789101112131415161718192021public class ModuleAProviderProxy { @Reference private ModuleAProvider moduleAProvider; @HystrixCommand(ignoreExceptions = {ModuleAException.class}) public void opA(ARequest request) throws ModuleAException { moduleAProvider.opA(request); } @HystrixCommand(ignoreExceptions = {ModuleAException.class}) public void opB(BRequest request) throws ModuleAException { moduleAProvider.oBB(request); } @HystrixCommand(ignoreExceptions = {ModuleAException.class}) public CommonResponse&lt;C&gt; opC(CRequest request) throws ModuleAException { return moduleAProvider.opC(request); } } 如服务不可用等原因引发的多次接口调用超时异常，会触发 Hystrix 的熔断；而对于业务异常，我们则认为不需要进行熔断，因为对于接口 throws 出的业务异常，我们也认为是正常响应的一部分，只不过借助于 JAVA 的异常机制来表达。实际上，和生成自动化测试类的工具一样，我们使用了另一套自动化的工具，可以由 Dubbo 接口自动生成对应的 Hystrix Proxy。我们坚定的认为开发体验和用户体验一样重要，所以公司内部会有非常多的自动化工具。 3. API 版本单独演进引用一段公司内部的真实对话： A：我下载了你们的代码库怎么编译不通过啊，依赖中 xxx-api-1.1.3 版本的 jar 包找不到了，那可都是 RELEASE 版本啊。 B：你不知道我们 nexus 容量有限，只能保存最新的 20 个 RELEASE 版本吗？那个 API 现在最新的版本是 1.1.31 啦。 A：啊，这才几个月就几十个 RELEASE 版本啦？这接口太不稳定啦。 B： 其实接口一行代码没改，我们业务分析是很牛逼的，一直很稳定。但是这个 API 是和我们项目一起打包的，我们需求更新一次，就发布一次，API 就被迫一起升级版本。发生这种事，大家都不想的。 在单体式架构中，版本演进的单位是整个项目。微服务解决的一个关键的痛点便是其做到了每个服务的单独演进，这大大降低了服务间的耦合。正如我文章开始时举得那个例子一样：serviceA 是一个演进的单位，serviceA-api 和 serviceA-app 这两个 Module 从属于 serviceA，这意味着 app 的一次升级，将会引发 api 的升级，因为他们是共生的！而从微服务的使用角度来看，调用者关心的是 api 的结构，而对其实现压根不在乎。所以对于 api 定义未发生变化，其 app 发生变化的那些升级，其实可以做到对调用者无感知。在实践中也是如此 ​ api 版本的演进应该是缓慢的，而 app 版本的演进应该是频繁的。 所以，对于这两个演进速度不一致的模块，我们应该单独做版本管理，他们有自己的版本号。 4. 问题回归 ** 查询接口过多。** 各种 findBy 方法，加上各自的重载，几乎占据了一个接口 80% 的代码量。这也符合一般人的开发习惯，因为页面需要各式各样的数据格式，加上查询条件差异很大，便造成了：一个查询条件，一个方法的尴尬场景。这样会导致另外一个问题，需要使用某个查询方法时，直接新增了方法，但实际上可能这个方法已经出现过了，隐藏在了令人眼花缭乱的方法中。 解决方案：使用单参 +Specification 模式，降低重复的查询方法，大大降低接口中的方法数量。 ** 难以扩展 **。接口的任何改动，比如新增一个入参，都会导致调用者被迫升级，这也通常是 RPC 设计被诟病的一点，不合理的 RPC 接口设计会放大这个缺点。 解决方案：单参设计其实无形中包含了所有的查询条件的排列组合，可以直接在 app 实现逻辑的新增，而不需要对 api 进行改动（如果是参数的新增则必须进行 api 的升级，参数的废弃可以用 @Deprecated 标准）。 ** 升级困难。** 在之前的 “初识 RPC 接口设计”一节中，版本管理的粒度是 project，而不是 module，这意味着：api 即使没有发生变化，app 版本演进，也会造成 api 的被迫升级，因为 project 是一个整体。问题又和上一条一样了，api 一旦发生变化，调用者也得被迫升级，牵一发而动全身。 解决方案：以 module 为版本演进的粒度。api 和 app 单独演进，减少调用者的不必要升级次数。 ** 难以测试 **。接口一多，职责随之变得繁杂，业务场景各异，测试用例难以维护。特别是对于那些有良好习惯编写单元测试的程序员而言，简直是噩梦，用例也得跟着改。 解决方案：单参数设计 + 自动化测试工具，打造良好的开发体验。 ** 异常设计不合理 **。在既往的工作经历中曾经有一次会议，就 RPC 调用中的异常设计引发了争议，一派人觉得需要有一个业务 CommonResponse，封装异常，每次调用后，优先判断调用结果是否 success，在进行业务逻辑处理；另一派人觉得这比较麻烦，由于 RPC 框架是可以封装异常调用的，所以应当直接 try catch 异常，不需要进行业务包裹。在没有明确规范时，这两种风格的代码同时存在于项目中，十分难看！ 解决方案：Checked Exception+ 正确异常处理姿势，使得代码更加优雅，降低了调用方不处理异常带来的风险。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/rpc-interface-design/"},{"title":"深入理解 RPC 之协议篇","text":"协议（Protocol）是个很广的概念，RPC 被称为远程过程调用协议，HTTP 和 TCP 也是大家熟悉的协议，也有人经常拿 RPC 和 RESTFUL 做对比，后者也可以被理解为一种协议… 我个人偏向于把“协议”理解为不同厂家不同用户之间的“约定”，而在 RPC 中，协议的含义也有多层。 Protocol 在 RPC 中的层次关系翻看 dubbo 和 motan 两个国内知名度数一数二的 RPC 框架（或者叫服务治理框架可能更合适）的文档，他们都有专门的一章介绍自身对多种协议的支持。RPC 框架是一个分层结构，从我的这个《深入理解 RPC》系列就可以看出，是按照分层来介绍 RPC 的原理的，前面已经介绍过了传输层，序列化层，动态代理层，他们各自负责 RPC 调用生命周期中的一环，而协议层则是凌驾于它们所有层之上的一层。简单描述下各个层之间的关系： protocol 层主要用于配置 refer（发现服务） 和 exporter（暴露服务） 的实现方式，transport 层定义了传输的方式，codec 层诠释了具体传输过程中报文解析的方式，serialize 层负责将对象转换成字节，以用于传输，proxy 层负责将这些细节屏蔽。 它们的包含关系如下：protocol &gt; transport &gt; codec &gt; serialize motan 的 Protocol 接口可以佐证这一点： 12345public interface Protocol { &lt;T&gt; Exporter&lt;T&gt; export(Provider&lt;T&gt; provider, URL url); &lt;T&gt; Referer&lt;T&gt; refer(Class&lt;T&gt; clz, URL url, URL serviceUrl); void destroy();} 我们都知道 RPC 框架支持多种协议，由于协议处于框架层次的较高位置，任何一种协议的替换，都可能会导致服务发现和服务注册的方式，传输的方式，以及序列化的方式，而不同的协议也给不同的业务场景带来了更多的选择，下面就来看看一些常用协议。 Dubbo 中的协议dubbo://Dubbo 缺省协议采用单一长连接和 NIO 异步通讯，适合于小数据量高并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。 反之，Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 适用场景：常规远程服务方法调用 rmi://RMI 协议采用 JDK 标准的 java.rmi.* 实现，采用阻塞式短连接和 JDK 标准序列化方式。 适用场景：常规远程服务方法调用，与原生 RMI 服务互操作 hessian://Hessian 协议用于集成 Hessian 的服务，Hessian 底层采用 Http 通讯，采用 Servlet 暴露服务，Dubbo 缺省内嵌 Jetty 作为服务器实现。 Dubbo 的 Hessian 协议可以和原生 Hessian 服务互操作，即： 提供者用 Dubbo 的 Hessian 协议暴露服务，消费者直接用标准 Hessian 接口调用 或者提供方用标准 Hessian 暴露服务，消费方用 Dubbo 的 Hessian 协议调用。 Hessian 在之前介绍过，当时仅仅是用它来作为序列化工具，但其本身其实就是一个协议，可以用来做远程通信。 适用场景：页面传输，文件传输，或与原生 hessian 服务互操作 http://基于 HTTP 表单的远程调用协议，采用 Spring 的 HttpInvoker 实现 适用场景：需同时给应用程序和浏览器 JS 使用的服务。 webserivice://基于 WebService 的远程调用协议，基于 Apache CXF 的 frontend-simple 和 transports-http 实现。 可以和原生 WebService 服务互操作，即： 提供者用 Dubbo 的 WebService 协议暴露服务，消费者直接用标准 WebService 接口调用， 或者提供方用标准 WebService 暴露服务，消费方用 Dubbo 的 WebService 协议调用 适用场景：系统集成，跨语言调用 thrift://当前 dubbo 支持的 thrift 协议是对 thrift 原生协议的扩展，在原生协议的基础上添加了一些额外的头信息，比如 service name，magic number 等。 memcached://基于 memcached 实现的 RPC 协议 redis://基于 Redis 实现的 RPC 协议。 dubbo 支持的众多协议详见 http://dubbo.io/books/dubbo-user-book/references/protocol/dubbo.html dubbo 的一个分支 dangdangdotcom/dubbox 扩展了 REST 协议 rest://JAX-RS 是标准的 Java REST API，得到了业界的广泛支持和应用，其著名的开源实现就有很多，包括 Oracle 的 Jersey，RedHat 的 RestEasy，Apache 的 CXF 和 Wink，以及 restlet 等等。另外，所有支持 JavaEE 6.0 以上规范的商用 JavaEE 应用服务器都对 JAX-RS 提供了支持。因此，JAX-RS 是一种已经非常成熟的解决方案，并且采用它没有任何所谓 vendor lock-in 的问题。 JAX-RS 在网上的资料非常丰富，例如下面的入门教程： Oracle 官方的 tutorial：http://docs.oracle.com/javaee/7/tutorial/doc/jaxrs.htm IBM developerWorks 中国站文章：http://www.ibm.com/developerworks/cn/java/j-lo-jaxrs/ 更多的资料请自行 google 或者百度一下。就学习 JAX-RS 来说，一般主要掌握其各种 annotation 的用法即可。 注意：dubbo 是基于 JAX-RS 2.0 版本的，有时候需要注意一下资料或 REST 实现所涉及的版本。 适用场景：跨语言调用 千米网也给 dubbo 贡献了一个扩展协议：https://github.com/dubbo/dubbo-rpc-jsonrpc jsonrpc://Why HTTP在互联网快速迭代的大潮下，越来越多的公司选择 nodejs、django、rails 这样的快速脚本框架来开发 web 端应用 而后端的服务用 Java 又是最合适的，这就产生了大量的跨语言的调用需求。而 http、json 是天然合适作为跨语言的标准，各种语言都有成熟的类库虽然 Dubbo 的异步长连接协议效率很高，但是在脚本语言中，这点效率的损失并不重要。 Why Not RESTfulDubbox 在 RESTful 接口上已经做出了尝试，但是 REST 架构和 dubbo 原有的 RPC 架构是有区别的，区别在于 REST 架构需要有资源 (Resources) 的定义， 需要用到 HTTP 协议的基本操作 GET、POST、PUT、DELETE 对资源进行操作。Dubbox 需要重新定义接口的属性，这对原有的 Dubbo 接口迁移是一个较大的负担。相比之下，RESTful 更合适互联网系统之间的调用，而 RPC 更合适一个系统内的调用，所以我们使用了和 Dubbo 理念较为一致的 JsonRPC JSON-RPC 2.0 规范 和 JAX-RS 一样，也是一个规范，JAVA 对其的支持可参考 jsonrpc4j 适用场景：跨语言调用 Motan 中的协议motan://motan 协议之于 motan，地位等同于 dubbo 协议之于 dubbo，两者都是各自默认的且都是自定义的协议。内部使用 netty 进行通信（旧版本使用 netty3 ，最新版本支持 netty4），默认使用 hessian 作为序列化器。 适用场景：常规远程服务方法调用 injvm://顾名思义，如果 Provider 和 Consumer 位于同一个 jvm，motan 提供了 injvm 协议。这个协议是 jvm 内部调用，不经过本地网络，一般在服务化拆分时，作为过渡方案使用，可以通过开关机制在本地和远程调用之间进行切换，等过渡完成后再去除本地实现的引用。 grpc:// 和 yar://这两个协议的诞生缘起于一定的历史遗留问题，moton 是新浪微博开源的，而其内部有很多 PHP 应用，为解决跨语言问题，这两个协议进而出现了。 适用场景：较为局限的跨语言调用 restful://motan 在 0.3.1 (2017-07-11) 版本发布了 restful 协议的支持（和 dubbo 的 rest 协议本质一样），dubbo 默认使用 jetty 作为 http server，而 motan 使用则是 netty 。主要实现的是 java 对 restful 指定的规范，即 javax.ws.rs 包下的类。 适用场景：跨语言调用 motan2://motan 1.0.0 (2017-10-31) 版本发布了 motan2 协议，用于对跨语言的支持，不同于 restful，jsonrpc 这样的通用协议，motan2 把请求的一些元数据作为单独的部分传输，更适合不同语言解析。 适用场景：跨语言调用 Motan is a cross-language remote procedure call(RPC) framework for rapid development of high performance distributed services. Motan-go is golang implementation. Motan-PHP is PHP client can interactive with Motan server directly or through Motan-go agent. Motan-openresty is a Lua(Luajit) implementation based on Openresty 从 motan 的 changeLog 以及 github 首页的介绍来看，其致力于打造成一个跨语言的服务治理框架，这倒是比较亦可赛艇的事。 面向未来的协议motan 已经支持 motan2://，计划支持 mcq://，kafka:// … 支持更多的协议，以应对复杂的业务场景。对这个感兴趣的朋友，可以参见这篇文章：http://mp.weixin.qq.com/s/XZVCHZZzCX8wwgNKZtsmcA 总结如果仅仅是将 dubbo，motan 作为一个 RPC 框架使用，那大多人会选择其默认的协议（dubbo 协议，motan 协议），而如果是有历史遗留原因，如需要对接异构系统，就需要替换成其他协议了。大多数互联网公司选择自研 RPC 框架，或者改造自己的协议，都是为了适配自身业务的特殊性，协议层的选择非常重要。","link":"/rpc-protocol/"},{"title":"深入理解 RPC 之服务注册与发现篇","text":"在我们之前 RPC 原理的分析中，主要将笔墨集中在 Client 和 Server 端。而成熟的服务治理框架中不止存在这两个角色，一般还会有一个 Registry（注册中心）的角色。一张图就可以解释注册中心的主要职责。 注册中心，用于服务端注册远程服务以及客户端发现服务 服务端，对外提供后台服务，将自己的服务信息注册到注册中心 客户端，从注册中心获取远程服务的注册信息，然后进行远程过程调用 目前主要的注册中心可以借由 zookeeper，eureka，consul，etcd 等开源框架实现。互联网公司也会因为自身业务的特性自研，如美团点评自研的 MNS，新浪微博自研的 vintage。 本文定位是对注册中心有一定了解的读者，所以不过多阐述注册中心的基础概念。 注册中心的抽象借用开源框架中的核心接口，可以帮助我们从一个较为抽象的高度去理解注册中心。例如 motan 中的相关接口： 服务注册接口 123456789101112public interface RegistryService { //1. 向注册中心注册服务 void register(URL url); //2. 从注册中心摘除服务 void unregister(URL url); //3. 将服务设置为可用，供客户端调用 void available(URL url); //4. 禁用服务，客户端无法发现该服务 void unavailable(URL url); //5. 获取已注册服务的集合 Collection&lt;URL&gt; getRegisteredServiceUrls();} 服务发现接口 12345678public interface DiscoveryService { //1. 订阅服务 void subscribe(URL url, NotifyListener listener); //2. 取消订阅 void unsubscribe(URL url, NotifyListener listener); //3. 发现服务列表 List&lt;URL&gt; discover(URL url);} 主要使用的方法是 RegistryService#register(URL) 和 DiscoveryService#discover(URL)。其中这个 URL 参数被传递，显然也是很重要的一个类。 123456789public class URL { private String protocol;// 协议名称 private String host; private int port; // interfaceName, 也代表着路径 private String path; private Map&lt;String, String&gt; parameters; private volatile transient Map&lt;String, Number&gt; numbers;} 注册中心也没那么玄乎，其实可以简单理解为：提供一个存储介质，供服务提供者和服务消费者共同连接，而存储的主要信息就是这里的 URL。但是具体 URL 都包含了什么实际信息，我们还没有一个直观的感受。 注册信息概览以元老级别的注册中心 zookeeper 为例，看看它实际都存储了什么信息以及它是如何持久化上一节的 URL。 为了测试，我创建了一个 RPC 服务接口 com.sinosoft.student.api.DemoApi , 并且在 6666 端口暴露了这个服务的实现类，将其作为服务提供者。在 6667 端口远程调用这个服务，作为服务消费者。两者都连接本地的 zookeeper，本机 ip 为 192.168.150.1。 使用 zkClient.bash 或者 zkClient.sh 作为客户端连接到本地的 zookeeper，执行如下的命令： 12[zk: localhost:2181(CONNECTED) 1] ls /motan/demo_group/com.sinosoft.student.api.DemoApi&gt; [client, server, unavailableServer] zookeeper 有着和 linux 类似的命令和结构，其中 motan，demo_group，com.sinosoft.student.api.DemoApi，client, server, unavailableServer 都是一个个节点。可以从上述命令看出他们的父子关系。 /motan/demo_group/com.sinosoft.student.api.DemoApi 的结构为 / 框架标识 / 分组名 / 接口名，其中的分组是 motan 为了隔离不同组的服务而设置的。这样，接口名称相同，分组不同的服务无法互相发现。如果此时有一个分组名为 demo_group2 的服务，接口名称为 DemoApi2，则 motan 会为其创建一个新的节点 /motan/demo_group2/com.sinosoft.student.api.DemoApi2 而 client，server，unavailableServer 则就是服务注册与发现的核心节点了。我们先看看这些节点都存储了什么信息。 server 节点： 12345[zk: localhost:2181(CONNECTED) 2] ls /motan/demo_group/com.sinosoft.student.api.DemoApi/server&gt; [192.168.150.1:6666][zk: localhost:2181(CONNECTED) 3] get /motan/demo_group/com.sinosoft.student.api.DemoApi/server/192.168.150.1:6666&gt; motan://192.168.150.1:6666/com.sinosoft.student.api.DemoApi?serialization=hessian2&amp;protocol=motan&amp;isDefault=true&amp;maxContentLength=1548576&amp;shareChannel=true&amp;refreshTimestamp=1515122649835&amp;id=motanServerBasicConfig&amp;nodeType=service&amp;export=motan:6666&amp;requestTimeout=9000000&amp;accessLog=false&amp;group=demo_group&amp; client 节点： 1234[zk: localhost:2181(CONNECTED) 4] ls /motan/demo_group/com.sinosoft.student.api.DemoApi/client&gt; [192.168.150.1][zk: localhost:2181(CONNECTED) 5] get /motan/demo_group/com.sinosoft.student.api.DemoApi/client/192.168.150.1&gt; motan://192.168.150.1:0/com.sinosoft.student.api.DemoApi?singleton=true&amp;maxContentLength=1548576&amp;check=false&amp;nodeType=service&amp;version=1.0&amp;throwException=true&amp;accessLog=false&amp;serialization=hessian2&amp;retries=0&amp;protocol=motan&amp;isDefault=true&amp;refreshTimestamp=1515122631758&amp;id=motanClientBasicConfig&amp;requestTimeout=9000&amp;group=demo_group&amp; unavailableServer 节点是一个过渡节点，所以在一切正常的情况下不会存在信息，它的具体作用在下面会介绍。 从这些输出数据可以发现，注册中心承担的一个职责就是存储服务调用中相关的信息，server 向 zookeeper 注册信息，保存在 server 节点，而 client 实际和 server 共享同一个接口，接口名称就是路径名，所以也到达了同样的 server 节点去获取信息。并且同时注册到了 client 节点下（为什么需要这么做在下面介绍）。 注册信息详解Server 节点server 节点承担着最重要的职责，它由服务提供者创建，以供服务消费者获取节点中的信息，从而定位到服务提供者真正网络拓扑位置以及得知如何调用。demo 中我只在本机 [192.168.150.1:6666] 启动了一个实例，所以在 server 节点之下，只存在这么一个节点，继续 get 这个节点，可以获取更详细的信息 1motan://192.168.150.1:6666/com.sinosoft.student.api.DemoApi?serialization=hessian2&amp;protocol=motan&amp;isDefault=true&amp;maxContentLength=1548576&amp;shareChannel=true&amp;refreshTimestamp=1515122649835&amp;id=motanServerBasicConfig&amp;nodeType=service&amp;export=motan:6666&amp;requestTimeout=9000000&amp;accessLog=false&amp;group=demo_group&amp; 作为一个 value 值，它和 http 协议的请求十分相似，不过是以 motan:// 开头，表达的意图也很明确，这是 motan 协议和相关的路径及参数，关于 RPC 中的协议，可以翻看我的上一篇文章《深入理解 RPC 之协议篇》。 serialization 对应序列化方式，protocol 对应协议名称，maxContentLength 对应 RPC 传输中数据报文的最大长度，shareChannel 是传输层用到的参数，netty channel 中的一个属性，group 对应分组名称。 上述的 value 包含了 RPC 调用中所需要的全部信息。 Client 节点在 motan 中使用 zookeeper 作为注册中心时，客户端订阅服务时会向 zookeeper 注册自身，主要是方便对调用方进行统计、管理。但订阅时是否注册 client 不是必要行为，和不同的注册中心实现有关，例如使用 consul 时便没有注册。 由于我们使用 zookeeper，也可以分析下 zookeeper 中都注册了什么信息。 1motan://192.168.150.1:0/com.sinosoft.student.api.DemoApi?singleton=true&amp;maxContentLength=1548576&amp;check=false&amp;nodeType=service&amp;version=1.0&amp;throwException=true&amp;accessLog=false&amp;serialization=hessian2&amp;retries=0&amp;protocol=motan&amp;isDefault=true&amp;refreshTimestamp=1515122631758&amp;id=motanClientBasicConfig&amp;requestTimeout=9000&amp;group=demo_group 和 Server 节点的值类似，但也有客户独有的一些属性，如 singleton 代表服务是否单例，check 检查服务提供者是否存在，retries 代表重试次数，这也是 RPC 中特别需要注意的一点。 UnavailableServer 节点unavailableServer 节点也不是必须存在的一个节点，它主要用来做 server 端的延迟上线，优雅关机。 延迟上线：一般推荐的服务端启动流程为：server 向注册中心的 unavailableServer 注册，状态为 unavailable，此时整个服务处于启动状态，但不对外提供服务，在服务验证通过，预热完毕，此时打开心跳开关，此时正式提供服务。 优雅关机：当需要对 server 方进行维护升级时，如果直接关闭，则会影响到客户端的请求。所以理想的情况应当是首先切断流量，再进行 server 的下线。具体的做法便是：先关闭心跳开关，客户端感知停止调用后，再关闭服务进程。 感知服务的下线服务上线时自然要注册到注册中心，但下线时也得从注册中心中摘除。注册是一个主动的行为，这没有特别要注意的地方，但服务下线却是一个值得思考的问题。服务下线包含了主动下线和系统宕机等异常方式的下线。 临时节点 + 长连接在 zookeeper 中存在持久化节点和临时节点的概念。持久化节点一经创建，只要不主动删除，便会一直持久化存在；临时节点的生命周期则是和客户端的连接同生共死的，应用连接到 zookeeper 时创建一个临时节点，使用长连接维持会话，这样无论何种方式服务发生下线，zookeeper 都可以感知到，进而删除临时节点。zookeeper 的这一特性和服务下线的需求契合的比较好，所以临时节点被广泛应用。 主动下线 + 心跳检测并不是所有注册中心都有临时节点的概念，另外一种感知服务下线的方式是主动下线。例如在 eureka 中，会有 eureka-server 和 eureka-client 两个角色，其中 eureka-server 保存注册信息，地位等同于 zookeeper。当 eureka-client 需要关闭时，会发送一个通知给 eureka-server，从而让 eureka-server 摘除自己这个节点。但这么做最大的一个问题是，如果仅仅只有主动下线这么一个手段，一旦 eureka-client 非正常下线（如断电，断网），eureka-server 便会一直存在一个已经下线的服务节点，一旦被其他服务发现进而调用，便会带来问题。为了避免出现这样的情况，需要给 eureka-server 增加一个心跳检测功能，它会对服务提供者进行探测，比如每隔 30s 发送一个心跳，如果三次心跳结果都没有返回值，就认为该服务已下线。 注册中心对比 Feature Consul zookeeper etcd euerka 服务健康检查 服务状态，内存，硬盘等 (弱) 长连接，keepalive 连接心跳 可配支持 多数据中心 支持 — — — kv 存储服务 支持 支持 支持 — 一致性 raft paxos raft — cap ca cp cp ap 使用接口 (多语言能力) 支持 http 和 dns 客户端 http/grpc http（sidecar） watch 支持 全量 / 支持 long polling 支持 支持 long polling 支持 long polling/ 大部分增量 自身监控 metrics — metrics metrics 安全 acl /https acl https 支持（弱） — spring cloud 集成 已支持 已支持 已支持 已支持 一般而言注册中心的特性决定了其使用的场景，例如很多框架支持 zookeeper，在我自己看来是因为其老牌，易用，但业界也有很多人认为 zookeeper 不适合做注册中心，它本身是一个分布式协调组件，并不是为注册服务而生，server 端注册一个服务节点，client 端并不需要在同一时刻拿到完全一致的服务列表，只要最终一致性即可。在跨 IDC，多数据中心等场景下 consul 发挥了很大的优势，这也是很多互联网公司选择使用 consul 的原因。 eureka 是 ap 注册中心，并且是 spring cloud 默认使用的组件，spring cloud eureka 较为贴近 spring cloud 生态。 总结注册中心主要用于解耦服务调用中的定位问题，是分布式系统必须面对的一个问题。更多专业性的对比，可以期待 spring4all.com 的注册中心专题讨论，相信会有更为细致地对比。","link":"/rpc-registry/"},{"title":"深入理解 RPC 之序列化篇 --Kryo","text":"一年前，笔者刚刚接触 RPC 框架，从单体式应用向分布式应用的变革无疑是让人兴奋的，同时也对 RPC 背后到底做了哪些工作产生了兴趣，但其底层的设计对新手而言并不是很友好，其涉及的一些常用技术点都有一定的门槛。如传输层常常使用的 netty，之前完全没听过，想要学习它，需要掌握前置知识点 nio；协议层，包括了很多自定义的协议，而每个 RPC 框架的实现都有差异；代理层的动态代理技术，如 jdk 动态代理，虽然实战经验不多，但至少还算会用，而 cglib 则又有一个盲区；序列化层倒还算是众多层次中相对简单的一环，但 RPC 为了追求可扩展性，性能等诸多因素，通常会支持多种序列化方式以供使用者插拔使用，一些常用的序列化方案 hessian，kryo，Protobuf 又得熟知… 这个系列打算就 RPC 框架涉及到的一些知识点进行探讨，本篇先从序列化层的一种选择 –kryo 开始进行介绍。 序列化概述大白话介绍下 RPC 中序列化的概念，可以简单理解为对象 –&gt; 字节的过程，同理，反序列化则是相反的过程。为什么需要序列化？因为网络传输只认字节。所以互信的过程依赖于序列化。有人会问，FastJson 转换成字符串算不算序列化？对象持久化到数据库算不算序列化？没必要较真，广义上理解即可。 JDK 序列化 可能你没用过 kryo，没用过 hessian，但你一定用过 jdk 序列化。我最早接触 jdk 序列化，是在大二的 JAVA 大作业中，《XX 管理系统》需要把对象保存到文件中（那时还没学数据库），jdk 原生支持的序列化方式用起来也很方便。 12345678910111213141516171819class Student implements Serializable{ private String name; } class Main{ public static void main(String[] args) throws Exception{ // create a Student Student st = new Student(&quot;kirito&quot;); // serialize the st to student.db file ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(&quot;student.db&quot;)); oos.writeObject(st); oos.close(); // deserialize the object from student.db ObjectInputStream ois = new ObjectInputStream(new FileInputStream(&quot;student.db&quot;)); Student kirito = (Student) ois.readObject(); ois.close(); // assert assert &quot;kirito&quot;.equals(kirito.getName()); } } Student 实体类需要实现 Serializable 接口，以告知其可被序列化。 序列化协议的选择通常有下列一些常用的指标： 通用性。是否只能用于 java 间序列化 / 反序列化，是否跨语言，跨平台。 性能。分为空间开销和时间开销。序列化后的数据一般用于存储或网络传输，其大小是很重要的一个参数；解析的时间也影响了序列化协议的选择，如今的系统都在追求极致的性能。 可扩展性。系统升级不可避免，某一实体的属性变更，会不会导致反序列化异常，也应该纳入序列化协议的考量范围。 易用性。API 使用是否复杂，会影响开发效率。 容易用的模型通常性能不好，性能好的模型通常用起来都比较麻烦。显然，JDK 序列化属于前者。我们不过多介绍它，直接引入今天的主角 kryo 作为它的替代品。 Kryo 入门引入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt; 由于其底层依赖于 ASM 技术，与 Spring 等框架可能会发生 ASM 依赖的版本冲突（文档中表示这个冲突还挺容易出现）所以提供了另外一个依赖以供解决此问题 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo-shaded&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt; 快速入门12345678910111213141516class Student implements Serializable{ private String name; } public class Main { public static void main(String[] args) throws Exception{ Kryo kryo = new Kryo(); Output output = new Output(new FileOutputStream(&quot;student.db&quot;)); Student kirito = new Student(&quot;kirito&quot;); kryo.writeObject(output, kirito); output.close(); Input input = new Input(new FileInputStream(&quot;student.db&quot;)); Student kiritoBak = kryo.readObject(input, Student.class); input.close(); assert &quot;kirito&quot;.equals(kiritoBak.getName()); }} 不需要注释也能理解它的执行流程，和 jdk 序列化差距并不是很大。 三种读写方式Kryo 共支持三种读写方式 如果知道 class 字节码，并且对象不为空 123kryo.writeObject(output, someObject);// ...SomeClass someObject = kryo.readObject(input, SomeClass.class); 快速入门中的序列化 / 反序列化的方式便是这一种。而 Kryo 考虑到 someObject 可能为 null，也会导致返回的结果为 null，所以提供了第二套读写方式。 如果知道 class 字节码，并且对象可能为空 123kryo.writeObjectOrNull(output, someObject);// ...SomeClass someObject = kryo.readObjectOrNull(input, SomeClass.class); 但这两种方法似乎都不能满足我们的需求，在 RPC 调用中，序列化和反序列化分布在不同的端点，对象的类型确定，我们不想依赖于手动指定参数，最好是…emmmmm… 将字节码的信息直接存放到序列化结果中，在反序列化时自行读取字节码信息。Kryo 考虑到了这一点，于是提供了第三种方式。 如果实现类的字节码未知，并且对象可能为 null 123456kryo.writeClassAndObject(output, object);// ...Object object = kryo.readClassAndObject(input);if (object instanceof SomeClass) { // ...} 我们牺牲了一些空间一些性能去存放字节码信息，但这种方式是我们在 RPC 中应当使用的方式。 我们关心的问题继续介绍 Kryo 特性之前，不妨让我们先思考一下，一个序列化工具或者一个序列化协议，应当需要考虑哪些问题。比如，支持哪些类型的序列化？循环引用会不会出现问题？在某个类增删字段之后反序列化会报错吗？等等等等…. 带着我们考虑到的这些疑惑，以及我们暂时没考虑到的，但 Kryo 帮我们考虑到的，来看看 Kryo 到底支持哪些特性。 支持的序列化类型 boolean Boolean byte Byte char Character short Short int Integer long Long float Float double Double byte[] String BigInteger BigDecimal Collection Date Collections.emptyList Collections.singleton Map StringBuilder TreeMap Collections.emptyMap Collections.emptySet KryoSerializable StringBuffer Class Collections.singletonList Collections.singletonMap Currency Calendar TimeZone Enum EnumSet 表格中支持的类型一览无余，这都是其默认支持的。 12Kryo kryo = new Kryo();kryo.addDefaultSerializer(SomeClass.class, SomeSerializer.class); 这样的方式，也可以为一个 Kryo 实例扩展序列化器。 总体而言，Kryo 支持以下的类型： 枚举 集合、数组 子类 / 多态 循环引用 内部类 泛型 但需要注意的是，**Kryo 不支持 Bean 中增删字段 **。如果使用 Kryo 序列化了一个类，存入了 Redis，对类进行了修改，会导致反序列化的异常。 另外需要注意的一点是使用反射创建的一些类序列化的支持。如使用 Arrays.asList(); 创建的 List 对象，会引起序列化异常。 1Exception in thread &quot;main&quot; com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): java.util.Arrays$ArrayList 但 new ArrayList() 创建的 List 对象则不会，使用时需要注意，可以使用第三方库对 Kryo 进行序列化类型的扩展。如 https://github.com/magro/kryo-serializers 所提供的。 ** 不支持包含无参构造器类的反序列化 **，尝试反序列化一个不包含无参构造器的类将会得到以下的异常： 1Exception in thread &quot;main&quot; com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): moe.cnkirito.Xxx 保证每个类具有无参构造器是应当遵守的编程规范，但实际开发中一些第三库的相关类不包含无参构造，的确是有点麻烦。 线程安全Kryo 是线程不安全的，意味着每当需要序列化和反序列化时都需要实例化一次，或者借助 ThreadLocal 来维护以保证其线程安全。 1234567891011private static final ThreadLocal&lt;Kryo&gt; kryos = new ThreadLocal&lt;Kryo&gt;() { protected Kryo initialValue() { Kryo kryo = new Kryo(); // configure kryo instance, customize settings return kryo; };};// Somewhere else, use KryoKryo k = kryos.get();... Kryo 相关配置参数详解每个 Kryo 实例都可以拥有两个配置参数，这值得被拉出来单独聊一聊。 12kryo.setRegistrationRequired(false);// 关闭注册行为kryo.setReferences(true);// 支持循环引用 Kryo 支持对注册行为，如 kryo.register(SomeClazz.class);, 这会赋予该 Class 一个从 0 开始的编号，但 Kryo 使用注册行为最大的问题在于，其不保证同一个 Class 每一次注册的号码想用，这与注册的顺序有关，也就意味着在不同的机器、同一个机器重启前后都有可能拥有不同的编号，这会导致序列化产生问题，所以在分布式项目中，一般关闭注册行为。 第二个注意点在于循环引用，Kryo 为了追求高性能，可以关闭循环引用的支持。不过我并不认为关闭它是一件好的选择，大多数情况下，请保持 kryo.setReferences(true)。 常用 Kryo 工具类1234567891011121314151617181920212223242526272829public class KryoSerializer { public byte[] serialize(Object obj) { Kryo kryo = kryoLocal.get(); ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); Output output = new Output(byteArrayOutputStream);//&lt;1&gt; kryo.writeClassAndObject(output, obj);//&lt;2&gt; output.close(); return byteArrayOutputStream.toByteArray(); } public &lt;T&gt; T deserialize(byte[] bytes) { Kryo kryo = kryoLocal.get(); ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); Input input = new Input(byteArrayInputStream);// &lt;1&gt; input.close(); return (T) kryo.readClassAndObject(input);//&lt;2&gt; } private static final ThreadLocal&lt;Kryo&gt; kryoLocal = new ThreadLocal&lt;Kryo&gt;() {//&lt;3&gt; @Override protected Kryo initialValue() { Kryo kryo = new Kryo(); kryo.setReferences(true);// 默认值为 true, 强调作用 kryo.setRegistrationRequired(false);// 默认值为 false, 强调作用 return kryo; } }; } &lt;1&gt; Kryo 的 Input 和 Output 接收一个 InputStream 和 OutputStream，Kryo 通常完成字节数组和对象的转换，所以常用的输入输出流实现为 ByteArrayInputStream/ByteArrayOutputStream。 &lt;2&gt; writeClassAndObject 和 readClassAndObject 配对使用在分布式场景下是最常见的，序列化时将字节码存入序列化结果中，便可以在反序列化时不必要传入字节码信息。 &lt;3&gt; 使用 ThreadLocal 维护 Kryo 实例，这样减少了每次使用都实例化一次 Kryo 的开销又可以保证其线程安全。 参考文章https://github.com/EsotericSoftware/kryo Kryo 使用指南 序列化与反序列化 更多的序列化方案，和 RPC 其他层次中会涉及到的技术，在后续的文章中进行逐步介绍。","link":"/rpc-serialize-1/"},{"title":"深入理解 RPC 之序列化篇 -- 总结篇","text":"上一篇 《深入理解 RPC 之序列化篇 –Kryo》, 介绍了序列化的基础概念，并且详细介绍了 Kryo 的一系列特性，在这一篇中，简略的介绍其他常用的序列化器，并对它们进行一些比较。序列化篇仅仅由 Kryo 篇和总结篇构成可能有点突兀，等待后续有时间会补充详细的探讨。 定义抽象接口123456public interface Serialization { byte[] serialize(Object obj) throws IOException; &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) throws IOException;} RPC 框架中的序列化实现自然是种类多样，但它们必须遵循统一的规范，于是我们使用 Serialization 作为序列化的统一接口，无论何种方案都需要实现该接口。 Kryo 实现Kryo 篇已经给出了实现代码。 1234567891011121314151617181920212223242526272829303132public class KryoSerialization implements Serialization { @Override public byte[] serialize(Object obj) { Kryo kryo = kryoLocal.get(); ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); Output output = new Output(byteArrayOutputStream); kryo.writeObject(output, obj); output.close(); return byteArrayOutputStream.toByteArray(); } @Override public &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) { Kryo kryo = kryoLocal.get(); ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); Input input = new Input(byteArrayInputStream); input.close(); return (T) kryo.readObject(input, clz); } private static final ThreadLocal&lt;Kryo&gt; kryoLocal = new ThreadLocal&lt;Kryo&gt;() { @Override protected Kryo initialValue() { Kryo kryo = new Kryo(); kryo.setReferences(true); kryo.setRegistrationRequired(false); return kryo; } };} 所需依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt; Hessian 实现1234567891011121314151617public class Hessian2Serialization implements Serialization { @Override public byte[] serialize(Object data) throws IOException { ByteArrayOutputStream bos = new ByteArrayOutputStream(); Hessian2Output out = new Hessian2Output(bos); out.writeObject(data); out.flush(); return bos.toByteArray(); } @Override public &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) throws IOException { Hessian2Input input = new Hessian2Input(new ByteArrayInputStream(bytes)); return (T) input.readObject(clz); }} 所需依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.caucho&lt;/groupId&gt; &lt;artifactId&gt;hessian&lt;/artifactId&gt; &lt;version&gt;4.0.51&lt;/version&gt;&lt;/dependency&gt; 大名鼎鼎的 Hessian 序列化方案经常被 RPC 框架用来作为默认的序列化方案，可见其必然具备一定的优势。其具体的优劣我们放到文末的总结对比中与其他序列化方案一起讨论。而在此，着重提一点 Hessian 使用时的坑点。 **BigDecimal 的反序列化 ** 使用 Hessian 序列化包含 BigDecimal 字段的对象时会导致其值一直为 0，不注意这个 bug 会导致很大的问题，在最新的 4.0.51 版本仍然可以复现。解决方案也很简单，指定 BigDecimal 的序列化器即可，通过添加两个文件解决这个 bug： resources\\META-INF\\hessian\\serializers 1java.math.BigDecimal=com.caucho.hessian.io.StringValueSerializer resources\\META-INF\\hessian\\deserializers 1java.math.BigDecimal=com.caucho.hessian.io.BigDecimalDeserializer Protostuff 实现12345678910111213141516171819202122232425262728public class ProtostuffSerialization implements Serialization { @Override public byte[] serialize(Object obj) throws IOException { Class clz = obj.getClass(); LinkedBuffer buffer = LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE); try { Schema schema = RuntimeSchema.createFrom(clz); return ProtostuffIOUtil.toByteArray(obj, schema, buffer); } catch (Exception e) { throw e; } finally { buffer.clear(); } } @Override public &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) throws IOException { T message = objenesis.newInstance(clz); // &lt;1&gt; Schema&lt;T&gt; schema = RuntimeSchema.createFrom(clz); ProtostuffIOUtil.mergeFrom(bytes, message, schema); return message; } private Objenesis objenesis = new ObjenesisStd(); // &lt;2&gt;} 所需依赖： 1234567891011121314151617&lt;!-- Protostuff --&gt;&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-core&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-runtime&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Objenesis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.objenesis&lt;/groupId&gt; &lt;artifactId&gt;objenesis&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt;&lt;/dependency&gt; Protostuff 可以理解为 google protobuf 序列化的升级版本，protostuff-runtime 无需静态编译，这比较适合 RPC 通信时的特性，很少见到有人直接拿 protobuf 作为 RPC 的序列化器，而 protostuff-runtime 仍然占据一席之地。 &lt;1&gt; 使用 Protostuff 的一个坑点在于其反序列化时需用户自己实例化序列化后的对象，所以才有了 T message = objenesis.newInstance(clz); 这行代码。使用 objenesis 工具实例化一个需要的对象，而后使用 ProtostuffIOUtil 完成赋值操作。 &lt;2&gt; 上述的 objenesis.newInstance(clz) 可以由 clz.newInstance() 代替，后者也可以实例化一个对象，但如果对象缺少无参构造函数，则会报错。借助于 objenesis 可以绕开无参构造器实例化一个对象，且性能优于直接反射创建。所以一般在选择 Protostuff 作为序列化器时，一般配合 objenesis 使用。 Fastjson 实现1234567891011121314151617public class FastJsonSerialization implements Serialization { static final String charsetName = &quot;UTF-8&quot;; @Override public byte[] serialize(Object data) throws IOException { SerializeWriter out = new SerializeWriter(); JSONSerializer serializer = new JSONSerializer(out); serializer.config(SerializerFeature.WriteEnumUsingToString, true);//&lt;1&gt; serializer.config(SerializerFeature.WriteClassName, true);//&lt;1&gt; serializer.write(data); return out.toBytes(charsetName); } @Override public &lt;T&gt; T deserialize(byte[] data, Class&lt;T&gt; clz) throws IOException { return JSON.parseObject(new String(data), clz); }} 所需依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.28&lt;/version&gt;&lt;/dependency&gt; &lt;1&gt; JSON 序列化注意对枚举类型的特殊处理；额外补充类名可以在反序列化时获得更丰富的信息。 序列化对比在我的 PC 上对上述序列化方案进行测试： ** 测试用例：对一个简单 POJO 对象序列化 / 反序列化 100W 次 ** serialize/ms deserialize/ms Fastjson 2832 2242 Kryo 2975 1987 Hessian 4598 3631 Protostuff 2944 2541 ** 测试用例：序列化包含 1000 个简单对象的 List，循环 1000 次 ** serialize/ms deserialize/ms Fastjson 2551 2821 Kryo 1951 1342 Hessian 1828 2213 Protostuff 1409 2813 对于耗时类型的测试需要做到预热 + 平均值等条件，测试后效果其实并不如人意，从我不太严谨的测试来看，并不能明显地区分出他们的性能。另外，Kryo 关闭 Reference 可以加速，Protostuff 支持静态编译加速，Schema 缓存等特性，每个序列化方案都有自身的特殊性，启用这些特性会伴随一些限制。但在 RPC 实际地序列化使用中不会利用到这些特性，所以在测试时并没有特别关照它们。 ** 序列化包含 1000 个简单对象的 List，查看字节数 ** 字节数 /byte Fastjson 120157 Kryo 39134 Hessian 86166 Protostuff 86084 字节数这个指标还是很直观的，Kryo 拥有绝对的优势，只有 Hessian，Protostuff 的一半，而 Fastjson 作为一个文本类型的序列化方案，自然无法和字节类型的序列化方案比较。而字节最终将用于网络传输，是 RPC 框架非常在意的一个性能点。 ** 综合评价 ** 经过个人测试，以及一些官方的测试结果，我觉得在 RPC 场景下，序列化的速度并不是一个很大考量标准，因为各个序列化方案都在有意优化速度，只要不是 jdk 序列化，速度就不会太慢。 Kryo：专为 JAVA 定制的序列化协议，序列化后字节数少，利于网络传输。但不支持跨语言（或支持的代价比较大）。dubbox 扩展中支持了 kryo 序列化协议。github 3018 star。 Hessian：支持跨语言，序列化后字节数适中，API 易用。是国内主流 rpc 框架：dubbo，motan 的默认序列化协议。hessian.caucho.com 未托管在 github Protostuff：提起 Protostuff 不得不说到 Protobuf。Protobuf 可能更出名一些，因为其是 google 的亲儿子，grpc 框架便是使用 protobuf 作为序列化协议，虽然 protobuf 与语言无关平台无关，但需要使用特定的语法编写 .prpto 文件，然后静态编译，这带了一些复杂性。而 protostuff 实际是对 protobuf 的扩展，protostuff-runtime 模块继承了 protobuf 性能，且不需要预编译文件，但与此同时，也失去了跨语言的特性。所以 protostuff 的定位是一个 JAVA 序列化框架，其性能略优于 Hessian。tip ：protostuff 反序列化时需用户自己初始化序列化后的对象，其只负责将该对象进行赋值。github 719 star。 Fastjson：作为一个 json 工具，被拉到 RPC 的序列化方案中似乎有点不妥，但 motan 该 RPC 框架除了支持 hessian 之外，还支持了 fastjson 的序列化。可以将其作为一个跨语言序列化的简易实现方案。github 11.8k star。","link":"/rpc-serialize-2/"},{"title":"深入理解 RPC 之传输篇","text":"RPC 被称为“远程过程调用”，表明了一个方法调用会跨越网络，跨越进程，所以传输层是不可或缺的。一说到网络传输，一堆名词就蹦了出来：TCP、UDP、HTTP，同步 or 异步，阻塞 or 非阻塞，长连接 or 短连接… 本文介绍两种传输层的实现：使用 Socket 和使用 Netty。前者实现的是阻塞式的通信，是一个较为简单的传输层实现方式，借此可以了解传输层的工作原理及工作内容；后者是非阻塞式的，在一般的 RPC 场景下，性能会表现的很好，所以被很多开源 RPC 框架作为传输层的实现方式。 RpcRequest 和 RpcResponse传输层传输的主要对象其实就是这两个类，它们封装了请求 id，方法名，方法参数，返回值，异常等 RPC 调用中需要的一系列信息。 12345678910public class RpcRequest implements Serializable { private String interfaceName; private String methodName; private String parametersDesc; private Object[] arguments; private Map&lt;String, String&gt; attachments; private int retries = 0; private long requestId; private byte rpcProtocolVersion;} 123456789public class RpcResponse implements Serializable { private Object value; private Exception exception; private long requestId; private long processTime; private int timeout; private Map&lt;String, String&gt; attachments;// rpc 协议版本兼容时可以回传一些额外的信息 private byte rpcProtocolVersion;} Socket 传输Server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class RpcServerSocketProvider { public static void main(String[] args) throws Exception { // 序列化层实现参考之前的章节 Serialization serialization = new Hessian2Serialization(); ServerSocket serverSocket = new ServerSocket(8088); ExecutorService executorService = Executors.newFixedThreadPool(10); while (true) { final Socket socket = serverSocket.accept(); executorService.execute(() -&gt; { try { InputStream is = socket.getInputStream(); OutputStream os = socket.getOutputStream(); try { DataInputStream dis = new DataInputStream(is); int length = dis.readInt(); byte[] requestBody = new byte[length]; dis.read(requestBody); // 反序列化 requestBody =&gt; RpcRequest RpcRequest rpcRequest = serialization.deserialize(requestBody, RpcRequest.class); // 反射调用生成响应 并组装成 rpcResponse RpcResponse rpcResponse = invoke(rpcRequest); // 序列化 rpcResponse =&gt; responseBody byte[] responseBody = serialization.serialize(rpcResponse); DataOutputStream dos = new DataOutputStream(os); dos.writeInt(responseBody.length); dos.write(responseBody); dos.flush(); } catch (Exception e) { e.printStackTrace(); } finally { is.close(); os.close(); } } catch (Exception e) { e.printStackTrace(); } finally { try { socket.close(); } catch (Exception e) { e.printStackTrace(); } } }); } } public static RpcResponse invoke(RpcRequest rpcRequest) { // 模拟反射调用 RpcResponse rpcResponse = new RpcResponse(); rpcResponse.setRequestId(rpcRequest.getRequestId()); //... some operation return rpcResponse; }} Client 1234567891011121314151617181920212223242526272829303132public class RpcSocketConsumer { public static void main(String[] args) throws Exception { // 序列化层实现参考之前的章节 Serialization serialization = new Hessian2Serialization(); Socket socket = new Socket(&quot;localhost&quot;, 8088); InputStream is = socket.getInputStream(); OutputStream os = socket.getOutputStream(); // 封装 rpc 请求 RpcRequest rpcRequest = new RpcRequest(); rpcRequest.setRequestId(12345L); // 序列化 rpcRequest =&gt; requestBody byte[] requestBody = serialization.serialize(rpcRequest); DataOutputStream dos = new DataOutputStream(os); dos.writeInt(requestBody.length); dos.write(requestBody); dos.flush(); DataInputStream dis = new DataInputStream(is); int length = dis.readInt(); byte[] responseBody = new byte[length]; dis.read(responseBody); // 反序列化 responseBody =&gt; rpcResponse RpcResponse rpcResponse = serialization.deserialize(responseBody, RpcResponse.class); is.close(); os.close(); socket.close(); System.out.println(rpcResponse.getRequestId()); }} dis.readInt()和 dis.read(byte[] bytes) 决定了使用 Socket 通信是一种阻塞式的操作，报文头 + 报文体的传输格式是一种常见的格式，除此之外，使用特殊的字符如空行也可以划分出报文结构。在示例中，我们使用一个 int（4 字节）来传递报问题的长度，之后传递报文体，在复杂的通信协议中，报文头除了存储报文体还会额外存储一些信息，包括协议名称，版本，心跳标识等。 在网络传输中，只有字节能够被识别，所以我们在开头引入了 Serialization 接口，负责完成 RpcRequest 和 RpcResponse 与字节的相互转换。（Serialization 的工作机制可以参考之前的文章） 使用 Socket 通信可以发现：每次 Server 处理 Client 请求都会从线程池中取出一个线程来处理请求，这样的开销对于一般的 Rpc 调用是不能够接受的，而 Netty 一类的网络框架便派上了用场。 Netty 传输Server 和 ServerHandler 1234567891011121314151617181920212223242526272829303132public class RpcNettyProvider { public static void main(String[] args) throws Exception{ EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { // 创建并初始化 Netty 服务端 Bootstrap 对象 ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup); bootstrap.channel(NioServerSocketChannel.class); bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel channel) throws Exception { ChannelPipeline pipeline = channel.pipeline(); pipeline.addLast(new RpcDecoder(RpcRequest.class)); // 解码 RPC 请求 pipeline.addLast(new RpcEncoder(RpcResponse.class)); // 编码 RPC 响应 pipeline.addLast(new RpcServerHandler()); // 处理 RPC 请求 } }); bootstrap.option(ChannelOption.SO_BACKLOG, 1024); bootstrap.childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = bootstrap.bind(&quot;127.0.0.1&quot;, 8087).sync(); // 关闭 RPC 服务器 future.channel().closeFuture().sync(); } finally { workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); } }} 1234567891011121314151617181920212223public class RpcServerHandler extends SimpleChannelInboundHandler&lt;RpcRequest&gt; { @Override public void channelRead0(final ChannelHandlerContext ctx, RpcRequest request) throws Exception { RpcResponse rpcResponse = invoke(request); // 写入 RPC 响应对象并自动关闭连接 ctx.writeAndFlush(rpcResponse).addListener(ChannelFutureListener.CLOSE); } private RpcResponse invoke(RpcRequest rpcRequest) { // 模拟反射调用 RpcResponse rpcResponse = new RpcResponse(); rpcResponse.setRequestId(rpcRequest.getRequestId()); //... some operation return rpcResponse; } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) { cause.printStackTrace(); ctx.close(); }} Client 和 ClientHandler 1234567891011121314151617181920212223242526272829303132333435public class RpcNettyConsumer { public static void main(String[] args) throws Exception{ EventLoopGroup group = new NioEventLoopGroup(); try { // 创建并初始化 Netty 客户端 Bootstrap 对象 Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group); bootstrap.channel(NioSocketChannel.class); bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel channel) throws Exception { ChannelPipeline pipeline = channel.pipeline(); pipeline.addLast(new RpcEncoder(RpcRequest.class)); // 编码 RPC 请求 pipeline.addLast(new RpcDecoder(RpcResponse.class)); // 解码 RPC 响应 pipeline.addLast(new RpcClientHandler()); // 处理 RPC 响应 } }); bootstrap.option(ChannelOption.TCP_NODELAY, true); // 连接 RPC 服务器 ChannelFuture future = bootstrap.connect(&quot;127.0.0.1&quot;, 8087).sync(); // 写入 RPC 请求数据并关闭连接 Channel channel = future.channel(); RpcRequest rpcRequest = new RpcRequest(); rpcRequest.setRequestId(123456L); channel.writeAndFlush(rpcRequest).sync(); channel.closeFuture().sync(); } finally { group.shutdownGracefully(); } }} 1234567891011121314public class RpcClientHandler extends SimpleChannelInboundHandler&lt;RpcResponse&gt; { @Override public void channelRead0(ChannelHandlerContext ctx, RpcResponse response) throws Exception { System.out.println(response.getRequestId());// 处理响应 } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); }} 使用 Netty 的好处是很方便地实现了非阻塞式的调用，关键部分都给出了注释。上述的代码虽然很多，并且和我们熟悉的 Socket 通信代码大相径庭，但大多数都是 Netty 的模板代码，启动服务器，配置编解码器等。真正的 RPC 封装操作大多集中在 Handler 的 channelRead 方法（负责读取）以及 channel.writeAndFlush 方法（负责写入）中。 12345678910111213141516171819public class RpcEncoder extends MessageToByteEncoder { private Class&lt;?&gt; genericClass; Serialization serialization = new Hessian2Serialization(); public RpcEncoder(Class&lt;?&gt; genericClass) { this.genericClass = genericClass; } @Override public void encode(ChannelHandlerContext ctx, Object in, ByteBuf out) throws Exception { if (genericClass.isInstance(in)) { byte[] data = serialization.serialize(in); out.writeInt(data.length); out.writeBytes(data); } }} 1234567891011121314151617181920212223242526public class RpcDecoder extends ByteToMessageDecoder { private Class&lt;?&gt; genericClass; public RpcDecoder(Class&lt;?&gt; genericClass) { this.genericClass = genericClass; } Serialization serialization = new Hessian2Serialization(); @Override public void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception { if (in.readableBytes() &lt; 4) { return; } in.markReaderIndex(); int dataLength = in.readInt(); if (in.readableBytes() &lt; dataLength) { in.resetReaderIndex(); return; } byte[] data = new byte[dataLength]; in.readBytes(data); out.add(serialization.deserialize(data, genericClass)); }} 使用 Netty 不能保证返回的字节大小，所以需要加上 in.readableBytes()&lt; 4 这样的判断，以及 in.markReaderIndex() 这样的标记，用来区分报文头和报文体。 同步与异步 阻塞与非阻塞这两组传输特性经常被拿来做对比，很多文章声称 Socket 是同步阻塞的，Netty 是异步非阻塞，其实有点问题。 其实这两组并没有必然的联系，同步阻塞，同步非阻塞，异步非阻塞都有可能（同步非阻塞倒是没见过），而大多数使用 Netty 实现的 RPC 调用其实应当是同步非阻塞的（当然一般 RPC 也支持异步非阻塞）。 同步和异步关注的是 ** 消息通信机制 **所谓同步，就是在发出一个 * 调用 * 时，在没有得到结果之前，该 * 调用 * 就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由 * 调用者 * 主动等待这个 * 调用 * 的结果。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在 * 调用 * 发出后，* 被调用者 * 通过状态、通知来通知调用者，或通过回调函数处理这个调用。 如果需要 RPC 调用返回一个结果，该结果立刻被使用，那意味着着大概率需要是一个同步调用。如果不关心其返回值，则可以将其做成异步接口，以提升效率。 阻塞和非阻塞关注的是 ** 程序在等待调用结果（消息，返回值）时的状态 **. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 在上述的例子中可以看出 Socket 通信我们显示声明了一个包含 10 个线程的线程池，每次请求到来，分配一个线程，等待客户端传递报文头和报文体的行为都会阻塞该线程，可以见得其整体是阻塞的。而在 Netty 通信的例子中，每次请求并没有分配一个线程，而是通过 Handler 的方式处理请求（联想 NIO 中 Selector），是非阻塞的。 使用同步非阻塞方式的通信机制并不一定同步阻塞式的通信强，所谓没有最好，只有更合适，而一般的同步非阻塞 通信适用于 1. 网络连接数量多 2. 每个连接的 io 不频繁 的场景，与 RPC 调用较为契合。而成熟的 RPC 框架的传输层和协议层通常也会提供多种选择，以应对不同的场景。 总结本文堆砌了一些代码，而难点主要是对 Socket 的理解，和 Netty 框架的掌握。Netty 的学习有一定的门槛，但实际需要掌握的知识点其实并不多（仅仅针对 RPC 框架所涉及的知识点而言），学习 Netty ，个人推荐《Netty IN ACTION》以及 https://waylau.gitbooks.io/netty-4-user-guide/Getting%20Started/Before%20Getting%20Started.html 该网站的例子。 参考资料： http://javatar.iteye.com/blog/1123915 – 梁飞 https://gitee.com/huangyong/rpc – 黄勇","link":"/rpc-transport/"},{"title":"一文了解 CORS 跨域","text":"作为一个 Web 开发，一定不会对下面的跨域报错陌生。 当一个资源从与该资源本身所在的服务器不同的域或端口请求一个资源时，资源会发起一个跨域 HTTP 请求。例如站点 http://www.aliyun.com 的某 HTML 页面请求 http://www.alibaba.com/image.jpg。 出于安全原因，浏览器限制从页面脚本内发起的跨域请求，有些浏览器不会限制跨域请求的发起，但是会将结果拦截。 这意味着使用这些 API 的 Web 应用只能加载同一个域下的资源，除非使用 CORS 机制（Cross-Origin Resource Sharing 跨源资源共享）获取目标服务器的授权来解决这个问题。 这也是本文将要探讨的主要问题，需要额外强调的是，跨域问题产生的主体是“浏览器”，这也是为什么，当我们使用 curl、postman、各种语言的 HTTP 客户端等工具时，从来没有被跨域问题困扰过。 什么是跨域http://www.aliyun.com 站点访问 http://www.alibaba.com/image.jpg 很容易被判断为一个跨域请求，因为域名不一样，同源策略详细描述如下： 协议相同 域名相同 端口相同 以下是跨域与同源的一些示例 站点 资源访问 跨域 or 同源 http://www.aliyun.com http://www.aliyun.com/hello 同源 http://www.aliyun.com http://aliyun.com/hello 跨域（域名不同，子域名和父域名也属于不同域名） http://www.aliyun.com https://www.aliyun.com/hello 跨域（协议不同） http://www.aliyun.com https://www.aliyun.com:81/hello 跨域（端口不同） 同源策略存在的原因是为了保护用户的安全和隐私，防止恶意网站对其他网站进行攻击或滥用。如果没有同源机制，以下一些常见的跨域攻击方式将会让网站维护者不堪其扰： CSRF（Cross-Site Request Forgery）：攻击者在恶意网站中放置一个含有恶意请求的页面，并诱使用户访问该页面。当用户在其他网站登录时，恶意请求会自动发送给目标网站，以伪装成用户的操作。这样，攻击者可以利用用户已经登录的凭证进行恶意操作，如修改密码、发起交易等。 XSS（Cross-Site Scripting）：攻击者在合法网站的输入框或评论中注入恶意脚本代码。当用户访问包含恶意脚本的页面时，脚本会在用户的浏览器中执行。攻击者可以利用这种方式窃取用户的登录凭证、敏感信息或执行其他恶意操作。 Clickjacking：攻击者通过在一个网页上覆盖一个透明的、恶意的图层，来欺骗用户点击看似无害的内容，实际上是触发了恶意操作，如转账或进行其他敏感操作。 解决跨域问题，常见的方案有： CORS（跨域资源共享）：在服务器端设置响应头部，允许指定的域名访问资源。 JSONP（JSON with Padding）：通过在页面中动态添加 &lt;script&gt; 元素，利用 script 标签的跨域特性来获取数据。 代理服务器：在服务器端设置一个代理服务器，将请求代理转发到目标服务器，绕过浏览器的同源策略。 本文将会主要介绍 CORS 跨域资源共享方案。 CORS 跨域资源共享介绍CORS 被定义在 w3c 规范中：https://fetch.spec.whatwg.org/#http-cors-protocol，这里包含了最详细也最官方的描述。它并不是一个框架或者工具，而是一种机制、契约，当浏览器和后端服务同时遵守 CORS 规范时，跨域访问便成了可能。根据使用经验，我们将 CORS 的机制分成了两种模式：简单请求模式和预检请求模式。 同时符合以下条件，就属于简单请求模式： 使用以下 HTTP 方法之一：GET、POST、HEAD 除了简单请求头之外(例如 content-type)，不能包含自定义请求头(例如通过 XMLHttpRequest.setRequestHeader 设置的请求头) Content-Type 为 application/x-www-form-urlencoded、multipart/form-data 或 text/plain 之一 (application/x-www-form-urlencoded 由于是表单格式，属于简单请求，而 application/json 的请求体需要拆包，不属于简单请求) 对于不符合简单请求模式的请求，浏览器将会启用预检请求模式。 简单请求模式浏览器在出现跨域请求时，会自动给请求携带 Origin 请求头，以下图为例，是 http://edasnext.aliyun.com 发往 http://edas.aliyun.com 的一个跨域请求 服务端如果要正常支持跨域请求，在判断当前请求为跨域请求时，需要在响应中携带 Access-Control-Allow-Origin、Access-Control-Allow-Methods 等相关的响应头。如果 edasnext.aliyun.com 该来源不在服务端的跨域配置列表中，则返回 403 拒绝该请求。浏览器会检查 Access 相关的响应头，如果没有携带，则会出现文章最开始的跨域报错。 Access to XMLHttpRequest at ‘http://edas.aliyun.com/testCors' from origin ‘http://edasnext.aliyun.com' has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. 简单请求模式-快速开始为了更加直观理解 CORS 的简单请求模式，本节快速开始给出了一个由 springboot 构建的 demo，它和大多数业务应用的项目结构类似。 编写 RestController 12345678910@RestControllerpublic class IndexController { @RequestMapping(&quot;/testCors&quot;) public String testCors(HttpServletResponse response) { System.out.println(&quot;hello cors&quot;); return &quot;hello cors&quot;; }} 并配置启动端口为 80。 编写跨域请求前端 123456789101112131415&lt;div id=&quot;test&quot;&gt;&lt;/div&gt;&lt;input type=&quot;button&quot; value=&quot;简单请求&quot; onclick=&quot;simpleRequest()&quot;/&gt;&lt;/body&gt;&lt;script&gt; function simpleRequest() { $.ajax({ url:'http://edas.aliyun.com/testCors', type:'get', success:function (msg) { $(&quot;#test&quot;).html(msg); } }) }&lt;/script&gt; 配置 hosts 12127.0.0.1 edas.aliyun.com127.0.0.1 edasnext.aliyun.com 为了方便在本地复现跨域问题，使用同一个后端，配置了两个域名解析，edasnext.aliyun.com作为前端访问的入口，edas.aliyun.com则作为后端接口的入口，由此构建一个跨域场景。 跨域测试 可以看到，由于当前的 springboot 应用没有进行跨域配置，所以请求失败了。 而如果通过 postman 重放这次请求，请求成功： 这个实验得出了两个结论： 浏览器提示跨域请求失败，服务端可能已经处理完毕，但是由于没有携带 Access 相关响应头，在到达浏览器时，被拒绝了 跨域问题的主体是浏览器，服务端是配合的角色 服务端跨域配置 如果仅仅是应对简单请求模式，完全可以直接给响应添加 Access-Control-Allow-Origin响应头，但实际的跨域全场景，流程比较复杂，springboot 提供了专门的跨域配置解决该问题： 12345678910111213@Configurationpublic class CorsConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/testCors&quot;) .allowedOrigins(&quot;http://edasnext.aliyun.com&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(true) .maxAge(3600); }} 关于上述 Configuration 中的 CorsRegistry 跨域配置的最佳实践，将会在下文中进行详细介绍。 再次发起跨域请求测试成功 至此简单请求模式介绍完毕。 预检请求模式 预检请求模式相比简单请求模式会多出一个 OPTIONS 请求的流程，这个行为也是浏览器自主产生的。预检请求的必要性主要在于更加安全，方便服务端针对复杂跨域请求进行自主的校验，并且减少了不必要的非正常跨域请求，缺点自然是加大了 CORS 的复杂度。 预检请求成功时，浏览器会接受到预检响应中的信息，该信息包含了是否允许携带 cookie 以及预检的缓存时间，这两个参数都是极其有意义的，前者会在下文继续补充，而后者决定了在一段时间内，针对复杂请求是否仍要发送预检请求。 预检请求模式-快速入门编写跨域前端 1234567891011121314151617&lt;div id=&quot;test&quot;&gt;&lt;/div&gt;&lt;input type=&quot;button&quot; value=&quot;非简单请求&quot; onclick=&quot;preflightedRequest()&quot;/&gt;&lt;/body&gt;&lt;script&gt; function preflightedRequest() { $.ajax({ url:'http://edas.aliyun.com/testCors', type: 'post', data: JSON.stringify({}), contentType: 'application/json', success:function (msg) { $(&quot;#test&quot;).html(msg); } }) }&lt;/script&gt; 测试非简单请求 由于服务端已经配置过跨域了，能够配合浏览器正常处理预检，可以看到浏览器先发送了一次预检请求，后发送了实际请求。 CORS 跨域配置的最佳实践以 springboot 提供的 CorsRegistry 跨域配置为例，服务端在处理 CORS 跨域时一般有以下配置： 123456corsRegistry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;http://edasnext.aliyun.com&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(true) .maxAge(3600); 其中 allowedOrigins 和 allowCredentials 字段需要格外关注 allowedOrigins 表明允许哪些跨域来源允许访问该服务端，与 HTTP 请求中的 Origin 请求头对应 allowCredentials 表明跨域请求是否可以携带 cookie 一个跨域配置的误区是配置 allowedOrigins=* 同时配置 allowCredentials=true 1234567// 错误的示例corsRegistry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(true) .maxAge(3600); 这表明允许任何来源可以进行跨域请求，并且允许携带 cookie。 浏览器的同源策略是基于安全考虑而设置的约束，避免了 CSRF、XSS 等常见的低成本的攻击手段，所以并不能简单认为跨域请求不被浏览器拦截就完事大吉了，需要做的是在没有安全漏洞的前提下保证正常跨域请求能够访问成功。 在 springboot 框架下，允许进行上述的配置，但是实际处理请求时，服务端会出现报错： java.lang.IllegalArgumentException: When allowCredentials is true, allowedOrigins cannot contain the special value “*” since that cannot be set on the “Access-Control-Allow-Origin” response header. To allow credentials to a set of origins, list them explicitly or consider using “allowedOriginPatterns” instead.] with root cause 这也不见得是一个最佳实践，更加建议在配置时就给出 ERROR 或者 WARN 配置提示，而不是延迟到运行时报错。 服务端跨域的行为并没有明确地被 w3c 规范规定，这导致每个框架都有自己的配置格式以及表现，也有框架默许这样的错误配置。或许这些框架在正常情况下，跨域请求不会被拦截，这种暴风雨前的宁静让框架使用者感到舒心，但遭遇 CSRF、XSS 等攻击时，没有人会感谢这样的“方便”。 一些跨域配置的最佳实践如下： 允许所有来源进行跨域请求时，不允许携带 cookie： 123456corsRegistry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(false) .maxAge(3600); 允许指定来源进行跨域请求时，携带 cookie： 123456corsRegistry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;http://edasnext.aliyun.com&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(true) .maxAge(3600); 允许泛域名匹配时携带 cookie： 123456corsRegistry.addMapping(&quot;/**&quot;) .allowedOriginPatterns(&quot;http://*.aliyun.com&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(true) .maxAge(3600); 当子域名无法枚举时，可以使用这种匹配方式，但不要配置为 http://*.com或者 * 针对指定的服务端路径进行精细化的跨域配置 123456corsRegistry.addMapping(&quot;/testCors&quot;) .allowedOrigins(&quot;http://edasnext.aliyun.com&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(true) .maxAge(3600); 网关中的跨域配置在稍微复杂的系统架构中，往往会引入网关组件，跨域配置也是非常适合转移到网关的一项配置，可以将跨域的复杂度和安全性保障，收敛到网关这一个单一组件中。 网关处理跨域问题，请求链路为：浏览器 -&gt; 网关 -&gt; 服务端，根据前面的结论，同源策略只存在于浏览器发起的请求，所以网关向服务端的请求时不存在跨域问题，只需要考虑浏览器 -&gt; 网关这一跳。 以 Spring Cloud Gateway 为例，其提供了开箱即用的跨域能力： 123456789101112131415161718192021spring: cloud: gateway: fail-on-route-definition-error: false routes: - id: r-cors predicates: - Path=/testCors uri: http://edas.aliyun.com order: 1000 globalcors: cors-configurations: '[/**]': allowedOrigins: 'http://edasnext.aliyun.com' allowCredentials: true allowedMethods: - '*' allowedHeaders: - '*' maxAge: 3600 add-to-simple-url-handler-mapping: true 在上述示例中，我们配置了一个 r-cors 路由，转发到本地的后端服务，并且配置了全局的跨域策略，和服务端的配置格式类似。 add-to-simple-url-handler-mapping的含义是：如果路由没有配置 OPTIONS 匹配，可以打开此开关，让预检请求成功返回，不会因为跨域的预检而导致路由访问不通。推荐打开此配置，这样在配置路由时，可以只需要关注业务真正的请求方法，而不需要考虑跨域问题。 在网关配置跨域后，后端服务就可以不用处理跨域问题了，去除服务端的 corsRegistry 配置，并且增加请求 Spring Cloud Gateway 网关的测试用例（本地将网关启动在 8080 端口）： 123456789101112131415161718192021222324252627282930&lt;div id=&quot;test&quot;&gt;&lt;/div&gt;&lt;input type=&quot;button&quot; value=&quot;简单请求-网关&quot; onclick=&quot;simpleRequestPassbyGateway()&quot;/&gt;&lt;input type=&quot;button&quot; value=&quot;非简单请求-网关&quot; onclick=&quot;preflightedRequestPassbyGateway()&quot;/&gt;&lt;/body&gt;&lt;script&gt; function simpleRequestPassbyGateway() { $.ajax({ url:'http://127.0.0.1:8080/testCors', type:'get', data:{}, success:function (msg) { $(&quot;#test&quot;).html(msg); } }) } function preflightedRequestPassbyGateway() { $.ajax({ url:'http://127.0.0.1:8080/testCors', type: 'post', data: JSON.stringify({ }), contentType: 'application/json', success:function (msg) { $(&quot;#test&quot;).html(msg); } }) }&lt;/script&gt; simpleRequestPassbyGateway，preflightedRequestPassbyGateway 会经过网关路由转发至后端服务，根据同源策略 origin=127.0.0.1:8080，而前端域名为 http://edasnext.aliyun.com，这同样是一个跨域请求。 至此，只需要在网关进行统一配置跨域，后端服务就不用关注跨域问题了。所以，跨域的支持也是主流网关的常用功能之一。 网关跨域和服务端跨域共存的问题试想一下，如果服务端配置了跨域，同时网关配置跨域，表现会如何呢？这种场景一定不会少，例如一个原本配置了跨域的应用，需要接入到网关，一定会存在两份跨域配置共存的时机。还是延续上述的跨域用例，打开服务端的 CorsRegistry 配置，再发送跨域请求至网关，会得到如下报错： Access to XMLHttpRequest at ‘http://127.0.0.1:8080/testCors' from origin ‘http://edasnext.aliyun.com' has been blocked by CORS policy: The ‘Access-Control-Allow-Origin’ header contains multiple values ‘http://edasnext.aliyun.com, http://edasnext.aliyun.com', but only one is allowed. 这是因为网关和服务端都会给响应追加跨域请求头，导致浏览器无法识别。 一个比较简单的开源解决方案是在网关上配置一个过滤器： 12345spring: cloud: gateway: default-filters: - DedupeResponseHeader=Access-Control-Allow-Credentials Access-Control-Allow-Origin 此方案可以去除重复的跨域响应头。 在共存阶段后完成流量迁移，最后建议还是去除服务端的配置。","link":"/scg-cors/"},{"title":"SpringCloud Gateway 在微服务架构下的最佳实践","text":"前言本文整理自云原生技术实践营广州站 Meetup 的分享，其中的经验来自于我们团队开发的阿里云 CSB 2.0 这款产品，这是一款基于开源 SpringCloud Gateway 为 code base 开发的产品，在完全兼容开源用法的前提下，做了非常多企业级的改造，涉及功能特性、稳定性、安全、性能等方面。 为什么需要微服务网关 从功能角度来看，微服务网关通常用来统一提供认证授权、限流、熔断、协议转换等功能。 从使用场景上来看 南北向流量，需要流量网关和微服务网关配合使用，主要是为了区分外部流量和微服务流量，将内部的微服务能力，以统一的 HTTP 接入点对外提供服务 东西向流量，在一些业务量比较大的系统中，可能会按照业务域隔离出一系列的微服务，在同一业务域内的微服务通信走的是服务发现机制，而跨业务域访问，则建议借助于微服务网关。 微服务网关核心功能微服务架构、微服务/API 网关这些关键词发展至今，早已不是什么新鲜的概念，技术选型者也从出于好奇心关注一个技术，转移到了更加关注这个技术的本质。市场上各类网关产品的功能也逐渐趋于同质化，基本可以用同一张图来概括： 网关选型对比企业在选择使用一款网关产品时，通常会有两个选择，一是基于某一款开源产品做二次开发，二是选择某一款商业化产品开箱即用，无论如何，都应当从稳定性、安全、性能、业务兼容性等方面去进行选型。请相信我今天是站在 SpringCloud Gateway 角度进行的分享，我会尽可能做到客观、公正。 早期 SpringCloud 社区出现过 Zuul 这种产品，时至今日搜索微服务网关的资料，大概率都会出现它的身影，仅其通信模型是同步的线程模型这一条，就不足以支撑其成为企业级的网关产品选型，我会主要对比 SpringCloud Gateway、阿里云 CSB 2.0、Nginx、Kong、Envoy。 严谨来说，这几个网关并不适合对比，因为他们都有其各自适用的场景，表格仅供参考。 SpringCloud Gateway 的优势在于其可以很好地跟 Spring 社区和 SpringCloud 微服务体系打通，这一点跟 Java 语言流行的原因如出一辙，所以如果一个企业的语言体系是 Java 技术栈，并且基于 SpringBoot/ SpringCloud 开发微服务，选型 SpringCloud Gateway 作为微服务网关，会有着得天独厚的优势。 SpringCloud Gateway 选型的优势： SpringCloud Gateway 有很多开箱即用的功能，且扩展点多 适合 Java 技术栈 Spring/SpringCloud 社区生态好 适合跟 SpringBoot/ SpringCloud 微服务生态集成 SpringCloud Gateway 介绍如果你之前没有了解过 SpringCloud Gateway，也不用担心，下面一小部分篇幅会介绍 SpringCloud Gateway 基本用法。这是一段非常基础的 SpringCloud Gateway 路由配置示例 1234567891011121314151617181920212223spring: cloud: gateway: routes: - id: aliyun uri: https://www.aliyun.com predicates: - Host=*.aliyun.com - id: httpbin uri: http://httpbin.org predicates: - Path=/httpbin/** filters: - StripPrefix=1 - id: sca-provider uri: lb://sca-provider predicates: - Path=/sca/** filters: - StripPrefix=1 nacos: discovery: server-addr: mse-xxxxx-p.nacos-ans.mse.aliyuncs.com:8848 该示例介绍了微服务网关常见的几种路由配置示例 Host 路由匹配 前缀 Path 路由匹配 前缀 Path 路由匹配 &amp; 服务发现 SpringCloud Gateway 支持丰富的路由匹配逻辑，以应对各种类型的业务诉求： 其中 Path、Header、Method 这几种断言最为常用。 针对于网关请求路径、参数和后端服务请求路径、参数不一致的场景，SpringCloud Gateway 也提供了诸多开箱即用的 GatewayFilter，以实现对请求和响应的定制。 SpringCloud Gateway 的 user guide 介绍到此为止，如果想要了解 develop guide，建议参考 SpringCloud Gateway 的官方文档。 开源特性 vs 企业级特性需求众所周知，开源产品直接投入企业级生产使用一般是会面临一些挑战的，毕竟场景不同。以扩展性为例，开源产品大多讲究扩展点丰富，以应对开源用户千奇百怪的需求，而企业级产品场景更为单一，性能和稳定性是第一考虑因素，当二者发生 trade off 时，则需要一些取舍了。 开源 SpringCloud Gateway 没有开箱即用地支持一些重要的企业级特性，如果选型 SpringCloud Gateway 构建生产级别可用的微服务网关，那我的建议是需要补足以上这些能力。下面我会花较多的篇幅介绍我们在开源基础上做的一些企业级改造，希望能够抛砖引玉。 白屏化管控 表面看来，SpringCloud Gateway 并没有配套一个管理控制台，深层次一点来看，是 SpringCloud Gateway 还停留在一个开发框架层面，不是那么的产品化，同时它的领域模型也不是划分的那么清晰，说的好听点，这说明 SpringCloud Gateway 有充足的改造空间。 我们的改造原则有两点，一是完全兼容开源的规则及模型，不破坏底层规则的语义，这样我们可以跟随社区的节奏一起演进，将来也有机会贡献给社区，二是区分研发态的领域模型和用户态的产品模型，我们抽象出了路由、服务、来源、消费者、策略、插件等领域对象，这算不上什么创新，实际上网关领域的这些模型早已有了一些约定俗成的规范。 白屏化管控的背后，也意味着一切配置：路由配置、服务配置、策略配置…都是动态的，并且配置的变更都会实时生效。 配置方案重构上文提到了配置实时生效这一改造，有人可能会有疑问，开源不是已经支持将路由配置存储在 Nacos 中了吗？对的，开源支持两种配置方式，一是将路由配置在 application.yaml 中，这样最简单，但对于路由配置的 curd 都需要重启进程，非常繁琐，二是将配置托管到 Nacos 这样的配置中心组件中，实现分布式配置，能够动态刷新，但我们认为这还不足以支持企业级需求，将配置存储在单个 dataId 中这种开源方案有以下痛点： 配置推送慢：配置量大，网络传输慢，万级别配置推送耗时 5 分钟 爆炸半径大：不支持配置拆分，错误配置影响解析流程，导致网关路由整体不可用 配置规模：单个 value 有 10M大小限制，仅支持千级别路由 配置拆分势在必行，但其中困难也很多，例如动态监听的管理，稳定性的保障流程尤为复杂，额外提供的视图层与实际配置中心数据一致性保障等等。方案参考下图： 图中还有一个细节，也是我们优先选择 Nacos 作为配置中心的原因，nacos-client 的 snapshot 机制可以保证在管控以及配置中心组件都不可用时，即使网关 broker 重启了，依旧保证路由不丢失，保证自身可用性。 经过这套方案的改造，我们获得显著的优化效果： 推送时间优化：1w 配置 5 分钟 -&gt; 30 秒 配置量上限提升：1000 -&gt; 10w 确保了配置推送的最终一致性 协议转换 x 服务发现这两个企业级改造放到一起说，在实现上这两个模块也耦合的比较紧密。 协议转换：就以 Java 微服务体系而言，后端服务很有可能会出现 Dubbo 框架或者 GRPC 框架，甚至有些老的业务还会使用 WebService 这类框架，大多数时候我们说的网关都是只对接 HTTP 这一类通信协议，这限制了我们后端服务只能是 SpringBoot 或者 SpringCloud 框架，网关支持后端不同协议类型的能力，我们称之为协议转换。 服务发现：微服务框架离不开服务发现，一般常见的注册中心包括 Nacos、Eureka 等，例如开源 SpringCloud Gateway 便支持对接 Nacos/Eureka 两类注册中心。 这类开源特性的痛点是： SpringCloud Gateway 仅支持 HTTP2HTTP，不支持 HTTP2DUBBO，HTTP2GRPC，HTTP2WEBSERVICE SpringCloud Gateway 仅支持单一注册中心的静态配置 一些常见的企业级诉求： 存在不同类型的微服务架构：SpringCloud、Dubbo、GRPC 网关支持跨环境访问，需要连接多个注册中心或者多个命名空间 针对这些痛点和诉求，分享一些我们改造时遇到的难点以及经验 在支持不同协议时，对应的服务框架可能已经有了对应的 remoting 层和 discovery 层，我们的选择是仅引入该协议的 remoting 二方包解决协议转换问题，对于 discovery 层，应当自行封装，避免使用对应协议的 discovery 层这个误区，因为回归到网关领域，服务发现和协议转换是对等的模块，抽象 ServiceDiscoveryFIlter 负责服务发现，ProtocolTransferFilter 则负责点对点的协议通信。 在服务发现层，为了适配不同注册中心的模型（推和拉），提供了两个实现 PullServiceRegistry、PushServiceRegistry，这些改造是独立于 spring-cloud-loadbalancer 模块实现的，开源的默认实现存在诸多的限制，例如仅支持拉模型 + 缓存服务列表的方案，实际上推模型能够为网关的服务发现提供更高的实时性。 基本流程：服务发现 serviceName -&gt; n x IP，负载均衡 IP n -&gt;1，协议转换 IP 点对点通信 这样一套扩展机制可以在有新的协议类型、注册中心、负载均衡算法需要对接时实现快速扩展。 限流熔断如果仔细阅读过 SpringCloud Gateway 的文档，你会发现，开源对限流熔断的支持是非常有限的，它强依赖一个 Redis 做集群限流，且限流方案是自己实现的，而我们可能会更加信赖 Sentinel 提供的解决方案。事实上，开源 Sentinel 也对 SpringCloud Gateway 提供了一部分开箱即用的能力，使用层面完全没问题，主要是欠缺了一部分可观测性的能力。 在改造中，尤为注意要使用高版本的 Sentinel，即按比例阈值这套模型实现的限流方案，集成 Sentinel 之后，我们按照网关的通用场景提供了两类限流模型：基于慢调用比例的限流熔断和基于响应码比例的限流熔断。借助于 Sentinel 的能力，可惜实现渐进式的恢复。 可观测体系建设可观测性体系的建设，可以说是很多开源产品距离企业级使用的距离，SpringCloud Gateway 亦是如此。 网关通常会需要记录三类可观测性指标。 Metrics：如上图所示，记录请求数、QPS、响应码、P99、P999 等指标 Trace：网关链路能够串联后续微服务体系链路，实现全链路监控 Logging：按类别打印网关日志，常见的日志分类如 accessLog、requestLog、remotingLog 等 开源 SpringCloud Gateway 集成了 micrometer-registry-prometheus，提供了一个开箱即用的大盘：https://docs.spring.io/spring-cloud-gateway/docs/3.1.8/reference/html/gateway-grafana-dashboard.json，需要更加丰富维度的指标则需要自行埋点。 Trace 方案推荐对接 opentelemetry。 Logging 方案则是 SpringCloud Gateway 开源欠缺的，在实际生产中至少应该打印 accessLog 记录请求信息，按需开启 requestLog 记录请求的 payload 信息和响应体信息，以及与后端服务连接的日志，用于排查一些连接问题。日志采集方案我们的实践是将 accessLog 输出到标准输出中，方便在 K8s 架构下配置采集，或者采用日志 agent 的方案进行文件采集。 性能优化除了功能层面的优化与新增，网关的性能也是使用者尤为关注的点。在前文中，我并没有把 SpringCloud Gateway 归为一个性能特别高的网关分类中，主要是基于我们的实践，发现其有不少优化空间。下面的章节我会分享一些基于 SpringCloud Gateway 进行的性能优化。 网关优化道阻且长，为了验证优化效果，建设性能基线不可避免，需要面向 benchmark 进行优化。 一些常用的优化技巧在网关中也同样适用，例如：缓存、懒加载、预分配、算法复杂度优化、CPU 友好操作，减少线程切换。 火焰图通过火焰图观测性能可以从宏观角度分析大的性能损耗点 一个理想的网关火焰图应当是大部分的时间片占用花费在 IO 上，即图中的 netty 相关的损耗，除此之外占用了 CPU 的类，都需要重点关注。通过火焰图，我们也定位到了相当多的性能损耗点，并针对进行了优化。 GlobalFilter 排序优化SpringCloud Gateway 中通过 GlobalFilter、GatewayFilter 对请求进行过滤，在 FilteringWebHandler 中可以看到这段逻辑 1234567891011public Mono&lt;Void&gt; handle(ServerWebExchange exchange) { Route route = exchange.getRequiredAttribute(GATEWAY_ROUTE_ATTR); List&lt;GatewayFilter&gt; gatewayFilters = route.getFilters(); List&lt;GatewayFilter&gt; combined = new ArrayList&lt;&gt;(this.globalFilters); combined.addAll(gatewayFilters); // TODO: needed or cached? AnnotationAwareOrderComparator.sort(combined); return new DefaultGatewayFilterChain(combined).filter(exchange);} 开源实现在每次请求级别都会重新组装出一个 FilterChain，并进行排序，内存分配和排序会占用 CPU，无疑会导致性能下降，通过注释可以看到 Contributor 自己也意识到了这里的性能问题，但一直没有修复。 一个可行的优化手段是在路由或者策略变更时，触发 FilterChain 的更新，这样请求时 FilterChain 就没必要重新构造了。而观测到这一性能问题，正是通过了火焰图中的 FilteringWebHandler.handle 的占用。 路由增量推送之前的企业级特性章节中，我介绍了配置中心改造的方案，其中提及了开源方案爆炸半径大的问题，可以从下面的代码中，窥见一斑： 1234567891011121314151617181920public class RouteDefinitionRouteLocator implements RouteLocator { @Override public Flux&lt;Route&gt; getRoutes() { Flux&lt;Route&gt; routes = this.routeDefinitionLocator.getRouteDefinitions().map(this::convertToRoute); if (!gatewayProperties.isFailOnRouteDefinitionError()) { // instead of letting error bubble up, continue routes = routes.onErrorContinue((error, obj) -&gt; { if (logger.isWarnEnabled()) { logger.warn(&quot;RouteDefinition id &quot; + ((RouteDefinition) obj).getId() + &quot; will be ignored. Definition has invalid configs, &quot; + error.getMessage()); } }); } return routes.map(route -&gt; { return route; }); } 可以见得，SpringCloud Gateway 认为路由配置是一个整体，任意路由的变更，就会导致整个 Route 序列重新构建。并且在默认情况下，如果其中一个路由配置出错了，会导致整个网关路由不可用，除非 isFailOnRouteDefinitionError 被关闭。 我们的改造方案是使用 Map 结构进行改造，配合路由配置的增量推送，实现 Route 的单点更新。 1234567891011121314151617181920212223242526public class DynamicRouteRepository implements Ordered, RouteLocator, ApplicationEventPublisherAware, RouteDefinitionWriter { private RouteConverter routeConverter; static class RouteKey implements Ordered { private String id; private int order; ... } static final Map&lt;RouteKey, Route&gt; ORDERED_ROUTE = new TreeMap&lt;&gt;((o1, o2) -&gt; { int order1 = o1.order; int order2 = o2.order; if (order1 != order2) { return Integer.compare(order1, order2); } return o1.id.compareTo(o2.id); }); private static final Map&lt;String, Integer&gt; ORDER = new HashMap&lt;&gt;(); public Route getRouteById(String id) { return ORDERED_ROUTE.get(new RouteKey(id, ORDER.getOrDefault(id, 0))); } ...} 路由内存优化这个优化来自于我们一次生产问题的排查，起初我们并没有意识到该问题。问题表现为路由数量非常大时，内存占用的消耗超过了我们的预期，经过 dump 发现，同一份路由的配置内容竟然以 3 种形式常驻于内存中 Nacos 配置中心自身的 Cache SpringCloud Gateway 路由定义 RouteDefinition 的占用 SpringCloud Gateway 真实路由 Route 的占用 Nacos 的占用在我们预期之类，但 RouteDefinition 其实仅仅是一个中间变量，如果流程合理，其实是没必要常驻内存的，经过优化，我们去除了一份占用，增加了支持路由的数量。 内存泄漏优化该问题通用来自于生产实践，SpringCloud Gateway 底层依赖 netty 进行 IO 通信，熟悉 netty 的人应当知道其有一个读写缓冲的设计，如果通信内容较小，一般会命中 chunked buffer，而通信内容较大时，例如文件上传，则会触发内存的新分配，而 SpringCloud Gateway 在对接 netty 时存在逻辑缺陷，会导致新分配的池化内存无法完全回收，导致堆外内存泄漏。并且这块堆外内存时 netty 使用 unsafe 自行分配的，通过常规的 JVM 工具还无法观测，非常隐蔽。 处于改造成本考量，我们最终选择的方案是增加一行启动参数 -Dio.netty.allocator.type=unpooled，使得请求未命中 chunked buffer 时，分配的临时内存不进行池化，规避内存性能问题。 可能有人会有疑问，-Dio.netty.allocator.type=unpooled会不会导致性能下降，这个担心完毕没有必要，首先只有大报文才会触发该内存的分配，而网关的最佳实践应该是不允许文件上传这类需求，加上该参数只是为了应对非主流场景的一个兜底行为。 预构建 URI该热点问题由 org.springframework.cloud.client.loadbalancer.LoadBalancerUriTools 贡献，SpringCloud Gateway 引用了 spring-cloud-loadbalancer 解决服务发现和负载均衡的问题， 1234567891011private static URI doReconstructURI(ServiceInstance serviceInstance, URI original) { String host = serviceInstance.getHost(); String scheme = (String)Optional.ofNullable(serviceInstance.getScheme()).orElse(computeScheme(original, serviceInstance)); int port = computePort(serviceInstance.getPort(), scheme); if (Objects.equals(host, original.getHost()) &amp;&amp; port == original.getPort() &amp;&amp; Objects.equals(scheme, original.getScheme())) { return original; } else { boolean encoded = containsEncodedParts(original); return UriComponentsBuilder.fromUri(original).scheme(scheme).host(host).port(port).build(encoded).toUri(); }} 注意最后一行构建，实际是针对不可变对象的一次变更，从而进行了一次深拷贝，重新重构了一个 URI，这样的行为同样发生在调用级别，不要小看这类行为，它会严重占用 CPU。 优化方案便是，对于不可变部分的构造，提前到路由推送时构建，对于可变的调用级别的参数，支持修改。这一点跟路由增量推送的优化是一个道理。 Spring 体系出于契约考虑，大量使用了不可变变量传递契约信息，但某些扩展点中，又的确希望对其进行变更，不得已进行了深拷贝，从而造成了性能下降，企业级应用需要在其中寻找到一个平衡点。 对象缓存尽量避免调用链路中出现 new 关键字，它会加大 CPU 的开销，从而影响 IO，可以使用 ThreadLocal 或者对象池化技术进行对象复用。 如果 new 关键词仅出现在初始化，配置推送等异步场景，通常是一次性的行为，则出于代码可读性的考虑，不做太多要求。 总结今天的分享简单介绍了一些主流的网关的对比，并重点介绍了 SpringCloud Gateway 适用的场景。并分析了 SpringCloud Gateway 如果在企业中投入生产使用，我们认为需要新增&amp;改造的一些能力，最后针对一些常见的性能优化场景，介绍了我们的一些优化方案。这些经验完全来源我们 CSB 2.0 微服务网关基于 SpringCloud Gateway 改造的实践，CSB 2.0 是一款适用于私有化输出的网关产品，在今年，我们也会在公有云 EDAS 中将其进行输出，敬请期待。","link":"/scg-microservice-practise/"},{"title":"从校园到职场，聊聊实习这点事","text":"从我最近发的实习招聘文章就可以知道，我最近在忙春招的事了，本人也非常“荣幸”，担任了我们团队这次春招的负责人。陆陆续续沟通了很多学生，于是在这个周末抽出了一点时间，跟大家聊聊我对校园实习的一些看法。 就在我发完实习招聘的当天下午，一位同学就联系到了我，非常客气地称呼我为“老师”，从申请好友时的备注中得知其是一位名校的学生，希望找一份暑期实习的工作。由于是第一个加我微信的同学，我兴致很高地跟他沟通了番。 说实话，这位同学虽然学历不错，但交流下来，发现在编程实践经验上差了那么一点，但也能理解，毕竟是学生吗，可能还是以学习学校课本上的知识为主。由于我们是中间件岗位，我又继续跟他聊了下中间件，问他对中间件的了解有多少，他说没了解过，但我可以学，说到这儿，我眉头开始一紧，继续追问他，这你也敢投简历？他回答说，阿里一直是我梦寐以求的公司。一时语塞，让我联想到了当初面试阿里时的自己，貌似也说过同样的话。最后我草草结束了交流，还是将他的简历投递进了系统。 接触过这些大三/研二找实习的学生之后，我不禁回忆起了自己的经历。我大三的时候都没有意识到要找一份实习，甚至觉得公司招聘实习生，就是在招一些廉价劳动力，去干一些脏活累活。直到应届毕业之后，才了解到原来国内这些大公司，几乎不招聘应届生，要么是 3 年工作经验起步，要么是校招实习转正，几乎没有第三种情况！ 很多学生没有实习计划，或者出于对工作的恐惧，或者出于对自身能力的不自信，又或者是对实习工资的不屑…也有一大部分同学，觉得春招太早了，希望留着机会等到秋招。我这里有一份据阿里巴巴某部门的公开数据：2020 年转正入职的校招生中 80% 来自于春招，20% 来自于秋招。「好书一本，明日在读」，我不推荐大家错过现在这个最好的时机。 换个角度思考，公司为什么招聘实习生？先说一个好消息，请大家相信我，国内互联网公司绝不会为了招聘廉价劳动力而希望你来参加实习，再说一个坏消息，也请大家相信我，这些公司不是福利机构，给大家提供实习机会，还给大家发工资，绝不是为了帮助学生出国留学，或者方便去 title 更响亮的公司入职。公司招聘实习生，是为了提前物色人才，在实习中培养你，希望你毕业后能够入职，持续补充公司的新鲜血液。 交流过程中，也有同学比较关心实习薪资的问题，通常还都是技术不错的同学，希望能够议价。以我的了解，通常国内大厂的实习工资都是固定的，不会特别高，本科 200，研究生 300 大概这样的标准，还有 2000/ 月的住房补贴。这里并没有贬低同学的意思，但需要认识到一个事实，即便你绩点 5.0、ACM 金牌、挑战赛国一，进入公司后，依旧需要熟悉开发配置环境，熟悉公司流程，熟悉团队所用的技术框架，再到真正的开发，实习这几个月真的很短。 实习期间能够独立完成完成 1 个特性开发已经算是了不起的存在了，公司还需要投入一个 P7 级别的技术专家 1 对 1 带。大可不必计较这几个月的薪资。 对于非名校或非科班的学生，也请不必避讳、不必吝惜你简历的投递。只需要你各类程序竞赛中有较高的名次，专业课上有排名靠前的成绩，有很好的博客积累，有开源项目的贡献经验，有权威机构的论文投递经验，照样可以获得公司的青睐。但也不能盲目乐观，相比名校专业对口的学生，他们真的更有优势，也比双非的同学有更早更多的积累，「没伞的孩子要学会奔跑」。也请同学们理解，我们需要一定的区分度，最终是希望找到可以一起共事的人。同样用一份公开数据说话给这些学生以信心，近几年均有 40% 的学生比例来自 TOP 23 之外的学校。 实习生刚起步，可以塑造成任意的形状，你在学校积累的那些经验不应该完全作为你实习的参考。例如我收到一份机器学习方向研究生的简历，有意向从事中间件的开发，起初我也是不理解，因为机器学习近几年也很火热，但交流过程中学生对于自身择业方向的思考说服了我，学生基于自身专业和意向城市中的互联网公司分布综合考虑，决定了选择中间件方向。有些道理大家都懂 Java 更偏向于业务研发和中间件研发，C/C++ 更偏向于数据库研发，Go 更偏向于云原生研发，Python 更偏向于机器学习 北上杭有阿里；深圳有腾讯；广州有微信；北京有一大波互联网… 语言和方向有绑定关系，城市和公司的选择有指导性，但人没有定型。借助于实习的机会，你可以测试下自己适合什么岗位，什么公司的价值观适合你，什么样的团队氛围适合你成长，这些东西老师和学校帮不了你，我这篇文章帮不了你，但是一次实习机会可以。千万不要以一个熟悉的课题方向，一个对你有影响力的学长，一份职业工资排行就决定了你的方向。很多机会放在你面前，选择一个靠谱的公司，一个你认为能够 cover 的方向即可，真巧，我们“阿里云云原生中间件”部门就十分欢迎你来。 如何准备实习简历？我交流下来发现很多学生是很优秀的，但缺少表达技巧，准备的简历却过于简单。前面我介绍了公司筛选候选学生的流程，学生需要做的是： 突出项目经历：参加学校/公司 xx 项目，采用了 xx 技术，解决了 xx 难题 突出学习经历：xx 专业课成绩 xx，专业排名 top xx；获得 xx 奖学金 突出竞赛经历：参与了 xx 举办的 xx 竞赛，竞赛主要考验了学生 xx 的能力，取得了 xx/xx 的名次，收获了 xx 的技能；xx 网站解题量 xx。 突出开源经历：参与 xx 开源项目，该项目是 xx。我参与贡献了 xx 模块的代码，附带 issue/pr 突出优秀论文：攻克了 xx 领域的难题，并且在 xx 发表，突出影响力 突出博客积累：文章主要体现了我在 xx 领域的思考，以及学习、编码经验的积累，附上博客链接，uv 或阅读量 如果你没有上述的任意一项，说明你的积累相比同时期的学生已经稍有落后了。但是此时，你仍然可以表现出对技术的追求，对技术的热情，大家都很乐意帮助那些努力上进的同学，面试官也希望在同学身上看到自驱力，按照经验，这类同学的成长速度也会很快。 最后，我当然也希望你有自己的思考，择业是一件非常私人的事，没有人可以替你做选择，综合考虑自身的学习经历，不要自恃过高，也不要妄自菲薄，非常期待能和你共事。如果你也恰好有意向从事中间件的开发，随时欢迎你与我【微信 id：xiayimiaoshenghua】咨询，我这里是【阿里云云原生中间件】团队，我们正在针对 22 届的学生进行春季实习的意向沟通。","link":"/school-internship/"},{"title":"gson 替换 fastjson 引发的线上问题分析","text":"前言Json 序列化框架存在的安全漏洞一直以来都是程序员们挂在嘴边调侃的一个话题，尤其是这两年 fastjson 由于被针对性研究，更是频频地的报出漏洞，出个漏洞不要紧，可安全团队总是用邮件催着线上应用要进行依赖升级，这可就要命了，我相信很多小伙伴也是不胜其苦，考虑了使用其他序列化框架替换 fastjson。这不，最近我们就有一个项目将 fastjson 替换为了 gson，引发了一个线上的问题。分享下这次的经历，以免大家踩到同样的坑，在此警示大家，规范千万条，安全第一条，升级不规范，线上两行泪。 问题描述线上一个非常简单的逻辑，将对象序列化成 fastjson，再使用 HTTP 请求将字符串发送出去。原本工作的好好的，在将 fastjson 替换为 gson 之后，竟然引发了线上的 OOM。经过内存 dump 分析，发现竟然发送了一个 400 M+ 的报文，由于 HTTP 工具没有做发送大小的校验，强行进行了传输，直接导致了线上服务整体不可用。 问题分析为什么同样是 Json 序列化，fastjson 没出过问题，而换成 gson 之后立马就暴露了呢？通过分析内存 dump 的数据，发现很多字段的值都是重复的，再结合我们业务数据的特点，一下子定位到了问题 – gson 序列化重复对象存在严重的缺陷。 直接用一个简单的例子，来说明当时的问题。模拟线上的数据特性，使用 List&lt;Foo&gt; 添加进同一个引用对象 1234567891011121314Foo foo = new Foo();Bar bar = new Bar();List&lt;Foo&gt; foos = new ArrayList&lt;&gt;();for(int i=0;i&lt;3;i++){ foos.add(foo);}bar.setFoos(foos);Gson gson = new Gson();String gsonStr = gson.toJson(bar);System.out.println(gsonStr);String fastjsonStr = JSON.toJSONString(bar);System.out.println(fastjsonStr); 观察打印结果： gson： 1{&quot;foos&quot;:[{&quot;a&quot;:&quot;aaaaa&quot;},{&quot;a&quot;:&quot;aaaaa&quot;},{&quot;a&quot;:&quot;aaaaa&quot;}]} fastjson： 1{&quot;foos&quot;:[{&quot;a&quot;:&quot;aaaaa&quot;},{&quot;$ref&quot;:&quot;$.foos[0]&quot;},{&quot;$ref&quot;:&quot;$.foos[0]&quot;}]} 可以发现 gson 处理重复对象，是对每个对象都进行了序列化，而 fastjson 处理重复对象，是将除第一个对象外的其他对象使用引用符号 $ref 进行了标记。 当单个重复对象的数量非常多，以及单个对象的提交较大时，两种不同的序列化策略会导致一个质变，我们不妨来针对特殊的场景进行下对比。 压缩比测试 序列化对象：包含大量的属性。以模拟线上的业务数据。 重复次数：200。即 List 中包含 200 个同一引用的对象，以模拟线上复杂的对象结构，扩大差异性。 序列化方式：gson、fastjson、Java、Hessian2。额外引入了 Java 和 Hessian2 的对照组，方便我们了解各个序列化框架在这个特殊场景下的表现。 主要观察各个序列化方式压缩后的字节大小，因为这关系到网络传输时的大小；次要观察反序列后 List 中还是不是同一个对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Main { public static void main(String[] args) throws IOException, ClassNotFoundException { Foo foo = new Foo(); Bar bar = new Bar(); List&lt;Foo&gt; foos = new ArrayList&lt;&gt;(); for(int i=0;i&lt;200;i++){ foos.add(foo); } bar.setFoos(foos); // gson Gson gson = new Gson(); String gsonStr = gson.toJson(bar); System.out.println(gsonStr.length()); Bar gsonBar = gson.fromJson(fastjsonStr, Bar.class); System.out.println(gsonBar.getFoos().get(0) == gsonBar.getFoos().get(1)); // fastjson String fastjsonStr = JSON.toJSONString(bar); System.out.println(fastjsonStr.length()); Bar fastjsonBar = JSON.parseObject(fastjsonStr, Bar.class); System.out.println(fastjsonBar.getFoos().get(0) == fastjsonBar.getFoos().get(1)); // java ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(byteArrayOutputStream); oos.writeObject(bar); oos.close(); System.out.println(byteArrayOutputStream.toByteArray().length); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(byteArrayOutputStream.toByteArray())); Bar javaBar = (Bar) ois.readObject(); ois.close(); System.out.println(javaBar.getFoos().get(0) == javaBar.getFoos().get(1)); // hessian2 ByteArrayOutputStream hessian2Baos = new ByteArrayOutputStream(); Hessian2Output hessian2Output = new Hessian2Output(hessian2Baos); hessian2Output.writeObject(bar); hessian2Output.close(); System.out.println(hessian2Baos.toByteArray().length); ByteArrayInputStream hessian2Bais = new ByteArrayInputStream(hessian2Baos.toByteArray()); Hessian2Input hessian2Input = new Hessian2Input(hessian2Bais); Bar hessian2Bar = (Bar) hessian2Input.readObject(); hessian2Input.close(); System.out.println(hessian2Bar.getFoos().get(0) == hessian2Bar.getFoos().get(1)); }} 输出结果： 123456789101112131415gson:62810falsefastjson:4503trueJava:1540trueHessian2:686true 结论分析：由于单个对象序列化后的体积较大，采用引用表示的方式可以很好的缩小体积，可以发现 gson 并没有采取这种序列化优化策略，导致体积膨胀。甚至一贯不被看好的 Java 序列化都比其优秀的多，而 Hessian2 更是夸张，直接比 gson 优化了 2个数量级。并且反序列化后，gson 并不能将原本是同一引用的对象还原回去，而其他的序列化框架均可以实现这一点。 吞吐量测试除了关注序列化之后数据量的大小，各个序列化的吞吐量也是我们关心的一个点。使用基准测试可以精准地测试出各个序列化方式的吞吐量。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@BenchmarkMode({Mode.Throughput})@State(Scope.Benchmark)public class MicroBenchmark { private Bar bar; @Setup public void prepare() { Foo foo = new Foo(); Bar bar = new Bar(); List&lt;Foo&gt; foos = new ArrayList&lt;&gt;(); for(int i=0;i&lt;200;i++){ foos.add(foo); } bar.setFoos(foos); } Gson gson = new Gson(); @Benchmark public void gson(){ String gsonStr = gson.toJson(bar); gson.fromJson(gsonStr, Bar.class); } @Benchmark public void fastjson(){ String fastjsonStr = JSON.toJSONString(bar); JSON.parseObject(fastjsonStr, Bar.class); } @Benchmark public void java() throws Exception { ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(byteArrayOutputStream); oos.writeObject(bar); oos.close(); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(byteArrayOutputStream.toByteArray())); Bar javaBar = (Bar) ois.readObject(); ois.close(); } @Benchmark public void hessian2() throws Exception { ByteArrayOutputStream hessian2Baos = new ByteArrayOutputStream(); Hessian2Output hessian2Output = new Hessian2Output(hessian2Baos); hessian2Output.writeObject(bar); hessian2Output.close(); ByteArrayInputStream hessian2Bais = new ByteArrayInputStream(hessian2Baos.toByteArray()); Hessian2Input hessian2Input = new Hessian2Input(hessian2Bais); Bar hessian2Bar = (Bar) hessian2Input.readObject(); hessian2Input.close(); } public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(MicroBenchmark.class.getSimpleName()) .build(); new Runner(opt).run(); }} 吞吐量报告： 12345Benchmark Mode Cnt Score Error UnitsMicroBenchmark.fastjson thrpt 25 6724809.416 ± 1542197.448 ops/sMicroBenchmark.gson thrpt 25 1508825.440 ± 194148.657 ops/sMicroBenchmark.hessian2 thrpt 25 758643.567 ± 239754.709 ops/sMicroBenchmark.java thrpt 25 734624.615 ± 66892.728 ops/s 是不是有点出乎意料，fastjson 竟然独领风骚，文本类序列化的吞吐量相比二进制序列化的吞吐量要高出一个数量级，分别是每秒百万级和每秒十万级的吞吐量。 整体测试结论 fastjson 序列化过后带有 $ 的引用标记也能够被 gson 正确的反序列化，但笔者并没有找到让 gson 序列化时转换成引用的配置 fastjson、hessian、java 均支持循环引用的解析；gson 不支持 fastjson 可以设置 DisableCircularReferenceDetect，关闭循环引用和重复引用的检测 gson 反序列化之前的同一个引用的对象，在经历了序列化再反序列化回来之后，不会被认为是同一个对象，可能会导致内存对象数量的膨胀；而 fastjson、java、hessian2 等序列化方式由于记录的是引用标记，不存在该问题 以笔者的测试 case 为例，hessian2 具有非常强大的序列化压缩比，适合大报文序列化后供网络传输的场景使用 以笔者的测试 case 为例，fastjson 具有非常高的吞吐量，对得起它的 fast，适合需要高吞吐的场景使用 序列化还需要考虑到是否支持循环引用，是否支持循环对象优化，是否支持枚举类型、集合、数组、子类、多态、内部类、泛型等综合场景，以及是否支持可视化等比较的场景，增删字段后的兼容性等等特性。综合来看，笔者比较推荐 hessian2 和 fastjson 两种序列化方式 总结大家都知道 fastjson 为了快，做了相对一些较为 hack 的逻辑，这也导致其漏洞较多，但我认为编码都是在 trade off 之中进行的，如果有一个完美的框架，那其他竞品框架早就不会存在了。笔者对各个序列化框架的研究也不深，可能你会说 jackson 更加优秀，我只能说能解决你的场景遇到的问题，那就是合适的框架。 最后，想要替换序列化框架时一定要慎重，了解清楚替代框架的特性，可能原先框架解决的问题，新的框架不一定能很好的 cover。","link":"/serialize-practice/"},{"title":"聊聊服务治理中的路由设计","text":"前言路由（Route）的设计广泛存在于众多领域，以 RPC 框架 Dubbo 为例，就有标签路由、脚本路由、权重路由、同机房路由等实现。 在框架设计层面，路由层往往位于负载均衡层之前，在进行选址时，路由完成的是 N 选 M（M &lt;= N），而负载均衡完成的是 M 选一，共同影响选址逻辑，最后触发调用。 在业务层面，路由往往是为了实现一定的业务语义，对流量进行调度，所以服务治理框架通常提供的都是基础的路由扩展能力，使用者根据业务场景进行扩展。 今天这篇文章将会围绕路由层该如何设计展开。 路由的抽象建模先参考 Dubbo 2.7 的实现，进行第一个版本的路由设计，该版本也最直观，非常容易理解。 123public interface Router { List&lt;Invoker&gt; route(List&lt;Invoker&gt; invokers, Invocation invocation);} Invoker：服务提供方地址的抽象 Invocation：调用的抽象 上述的 route 方法实现的便是 N 选 M 的逻辑。 接下来，以业务上比较常见的同机房路由为例继续建模。顾名思义，在部署时，提供者采用多机房部署，起到容灾的效果，同机房路由最简单的版本即过滤筛选出跟调用方同一机房的地址。 伪代码实现如下： 12345678910List&lt;Invoker&gt; route(List&lt;Invoker&gt; invokers, Invocation invocation) { String site = invocation.getSite(); List&lt;Invoker&gt; result = new ArrayList&lt;&gt;(); for (Invoker invoker: invokers) { if (invoker.getSite().equals(site)) { result.add(invoker); } } return result;} Dubbo 在较新的 2.7 版本中，也是采用了这样的实现方式。这种实现的弊端也是非常明显的：每一次调用，都需要对全量的地址进行一次循环遍历！注意，这是调用级别！在超大规模的集群下，开销之大，可想而知。 路由的改进方案基于之前路由的抽象建模，可以直观地理解路由选址的过程，其实也就是 2 步： 根据流量特性与路由规则特性选出对应的路由标。 根据路由标过滤对应的服务端地址列表 纵观整个调用过程： 第一步：一定是动态的，Invocation 可能来自于不同的机房，自然会携带不同的机房标。 第二步：根据路由标过滤对应的服务地址列表，完全是可以优化的，因为服务端的地址列表基本是固定的（在不发生上下线时），可以提前计算好每个机房的地址列表，这样就完成了算法复杂度从 O(N) 到 O(1) 的优化。 基于这个优化思路继续完善，路由选址的过程不应该发生在调用级别，而应该发生在下面两个场景： 地址列表变化时。需要重新计算路由地址列表。 路由规则发生变化时。例如路由规则不再是静态的，可以接受动态配置的推送，此时路由地址列表也需要重新计算。 但无论是哪个场景，相比调用级别的计算量，都是九牛一毛的存在。 优化过后的路由方案，伪代码如下： 12345678910111213141516171819202122232425Map&lt;String, List&lt;Invoker&gt;&gt; invokerMap = new ArrayList&lt;&gt;();String originRule;List&lt;Invoker&gt; originInvokers;void generateRoute(List&lt;Invoker&gt; invokers, String rule) { // 不同路由有不同的路由地址列表计算方式 invokerMap = calculate(invokers, rule);}// 地址推送void addressNotify(List&lt;Invoker&gt; invokers) { originInvokers = invokers; generateRoute(originInvokers, originRule);}// 规则变化void ruleChange(String rule) { originRule = rule; generateRoute(originInvokers, originRule);}List&lt;Invoker&gt; route(Invocation invocation) { String site = invocation.getSite(); return invokerMap.get(site);} 这份伪代码仅供参考，如果需要实现，仍然需要考虑非常多的细节，例如： 下一级路由如何触发构建 如何确保路由的可观测性 优化过后的方案，路由过程如下： 对比之前，主要是两个变化： 路由的代码组织结构从 pipeline 的链式结构，变成树型结构 建树的过程发生在地址 notify 和规则推送时，在 invocation 级别无需计算 静态路由和动态路由上述的新方案，并不是特别新奇的概念，正是我们熟知的”打表“。这里也要进行说明，并不是所有的路由场景都可以提前打表，如果某一个路由的实现中，服务地址列表的切分依赖了调用时的信息，自然需要将 N 选 M 的过程延迟到调用时。但根据我个人的经验，大多数的路由实现，基本都是标的匹配过程，无非是路由标的类型，计算标的逻辑不一样而已。 对于这类可以提前打表的路由实现，我们不妨称之为静态路由；而必须在调用级别计算的路由实现，可以称之为动态路由。 上述的优化方案，适用于静态路由场景，并且在真实业务场景中，几乎 90% 的路由实现都是静态路由。 总结本文以 Dubbo2.7 为例，在其基础上提出了一种静态路由策略的优化方案，可以大大减少路由过程中的计算量。这里也给大家卖个关子，Dubbo 3.0 有没有对这块进行优化呢，采取的是不是本文的静态路由方案呢，背后会不会有其他的思考呢？嘿嘿，本文先不给结论，有知道的小伙伴可以留言告诉大家哦。","link":"/service-governance-route/"},{"title":"聊聊服务发现的推拉模型","text":"前言过去一年，我的工作重心投入到了 API 网关（阿里云 CSB）中，这对于我来说是一个新的领域，但和之前接触的微服务治理方向又密不可分。API 网关适配微服务场景需要完成一些基础能力的建设，其一便是对接注册中心，从而作为微服务的入口流量，例如 Zuul、SpringCloud Gateway 都实现了这样的功能。实际上很多开源网关在这一特性上均存在较大的局限性，本文暂不讨论这些局限性，而是针对服务发现这一通用的场景，分享我对它的一些思考。 概念澄清服务发现这个词说实话还是有点抽象的，在微服务这一特定的领域具象化讨论才有意义。“服务发现”指的是“服务消费者获取服务提供者服务地址”的这一过程，而“服务”这一名词在不同微服务框架中代指也可能有所不同，不过大多数都是代指的应用、接口等信息。 SpringCloud 以应用维度表示服务 Dubbo2.x 以接口维度表示服务；Dubbo3.x 以应用维度表示服务 服务从 Provider -&gt; Registry -&gt; Consumer 的这一流动过程便是本文重点讨论的内容。 数据传递的两种方式：推模型和拉模型，一直是老生常谈的话题，在服务发现中也不妨一谈。 先不要急着回答服务发现这一场景中，推拉到底谁好的问题，让我们先看看一些微服务框架中的服务发现是如何实现的。 微服务框架中的服务发现这一节以 Dubbo 和 SpringCloud 两个微服务框架为引子，看看它们的服务发现到底使用的是拉模型还是推模型。 Dubbo 服务发现123456789101112131415public interface RegistryService { void register(URL url); void unregister(URL url); /** * Subscribe to eligible registered data and automatically push when the registered data is changed. */ void subscribe(URL url, NotifyListener listener); void unsubscribe(URL url, NotifyListener listener); List&lt;URL&gt; lookup(URL url);} Dubbo 管理服务发现的核心接口 RegistryService 直接给出了答案，通过 subscribe 和 notify 这些关键字便可以猜测到 Dubbo 使用的是推模型。 上图是一个推模型的工作流程。 SpringCloud 服务发现1234567891011121314151617public interface DiscoveryClient extends Ordered { int DEFAULT_ORDER = 0; String description(); List&lt;ServiceInstance&gt; getInstances(String serviceId); List&lt;String&gt; getServices(); default void probe() { this.getServices(); } default int getOrder() { return 0; }} DiscoveryClient 是 SpringCloud 中一个核心服务发现的接口，通过 getInstances 基本可以看出，SpringCloud 使用的是拉模型。 上图是一个拉模型的工作流程。 尽管我们还没有详细领略到两个模型背后的优化和实现细节，但从事实来看，Dubbo 和 SpringCloud 使用了不同的服务发现机制，都能让微服务玩转起来。 此时，如果你心里已经有了 Dubbo 是推模型，SpringCloud 是拉模型的认知，不妨再继续看下一节，可能这样的认知又会有了动摇。 注册中心中的推拉上一节站在微服务框架的角度，介绍了服务发现的推拉模型，这一节则是站在注册中心的角度来分析。说到底，无论是 Dubbo 还是 SpringCloud，总得对接一款注册中心才可以获得服务发现的能力，可以是 Zookeeper，可以是 Nacos，可以是 Eureka，也可以是任意的其他提供了服务发现能力的组件。 我就先以 Nacos 为例介绍下它的推拉模型。先看 Nacos 的 Naming 模块提供的核心接口： 123456789101112131415public interface NamingService { void registerInstance(String serviceName, String ip, int port) throws NacosException; void deregisterInstance(String serviceName, String ip, int port) throws NacosException; List&lt;Instance&gt; getAllInstances(String serviceName, boolean subscribe) throws NacosException; List&lt;Instance&gt; selectInstances(String serviceName, String groupName, boolean healthy, boolean subscribe) throws NacosException; void subscribe(String serviceName, EventListener listener) throws NacosException; void unsubscribe(String serviceName, List&lt;String&gt; clusters, EventListener listener) throws NacosException; } 为了方便阅读，我删除了大部分重载的接口以及非核心的接口。可以发现，从 API 角度，Nacos 是同时提供了推模型和拉模型两套接口的，这样也是方便其被微服务框架集成，有兴趣的读者，可以自行去阅读下 Dubbo/SpringCloud Alibaba 集成 Nacos 的代码，Dubbo 使用的便是 subscribe 这一套推模型的接口，SpringCloud Alibaba 则是使用的 selectInstances 这一套拉模型的接口。 那是否说”Nacos 是一个推拉模型结合的注册中心”呢，不够严谨。且看 getAllInstances，selectInstances 这两个方法都有一个 subscribe 入参，跟一下源码探究一下 1234567891011121314public List&lt;Instance&gt; selectInstances(String serviceName, String groupName, List&lt;String&gt; clusters, boolean healthy, boolean subscribe) throws NacosException { ServiceInfo serviceInfo; if (subscribe) { serviceInfo = hostReactor.getServiceInfo(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); } else { serviceInfo = hostReactor .getServiceInfoDirectlyFromServer(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); } return selectInstances(serviceInfo, healthy);} 可以发现 subscribe 这个参数控制的是是否直接从注册中心拉取服务，subscribe=false 时，实际是从 Nacos 自身维护的一块本地缓存中获取到的服务，大多数情况下，获取服务使用的是 subscribe=true 的重载方法。所以，selectInstance 看起来是在拉服务，subscribe 看起来是在推服务，实际上 Nacos 内核维护缓存的方式我们并未得知。 从上述 subscribe 的提示中，我们可以得出结论，我们并不能直接通过个别接口就得出该注册中心使用的是推模型还是拉模型，究竟何种模型，还是要看 client 端是如何从 server 端加载/更新服务信息的。 那么，真实情况下，Nacos client 究竟是如何从 server 端获取到服务列表的呢？也不卖关子了，直接给结论： 在 Nacos 1.x 中，Nacos 采用的是定时拉 + udp 推送的机制。客户端会启动一个定时器，每 10s 拉取一次服务，确保服务端服务版本一致，为了解决 10s 间隔内服务更新了，客户端却没有及时收到通知的这一问题，Nacos 还在服务端服务更新时，触发了一次 udp 推送。 在 Nacos 2.x 中，Nacos 采用的是服务端 tcp 推送的机制。客户端启动时会跟服务端建立一条 tcp 长连接，服务端服务变更后，会复用客户端建立的这条连接进行数据推送。 所以在回答，Nacos 到底是推模型还是拉模型时，需要区分版本来回答。 结论：Nacos 1.x 是拉模型；Nacos 2.x 是推模型 不知道有没有读者好奇 Nacos 为什么这么设计，我简单用一些 QA 快速解答一些可能的疑问： Q：为什么 Nacos 1.x 使用了 udp 推送，却把 Nacos 1.x 定义为拉模型？ A：Nacos 1.x 中 udp 推送主要是为了降低服务更新延时而设计的，并且在复杂网络部署架构中，例如 client 与 server 只能单向访问，或者有 SLB 中间介质时，udp 就会失效；且 udp 本身就是不稳定的，Nacos 尝试两次失败后就会放弃推送。所以主要还是在用拉模式来保障。 Q：为什么 Nacos 1.x 一开始不使用 Nacos 2.x 中的架构，使用 tcp 推送？ A：个人猜测是因为拉模型实现起来简单，Nacos 2.x 才引入了 grpc 实现长连接 Q：为什么 Nacos 1.x 的服务发现使用的是短轮询，不像配置中心那样使用长轮询？ A：在服务发现场景中，服务端比较在意内存消耗，长轮询虽然不会占用线程，但服务端依旧会 hold 住 request/response，造成不必要的内存浪费。 一些常见注册中心的推拉模型： Zookeeper：推模型 Nacos 1.x：拉模型 Nacos 2.x：推模型 Eureka：拉模型 好，介绍完了注册中心视角的服务发现推拉模型了，再回过头来看一个问题：如果使用了 SpringCloud Alibaba + Nacos 2.1 版本，那它的服务发现就是走的哪种模型呢？ 正确答案：在微服务框架视角，sca 走的是拉模型这种同步拉取服务的机制；在注册中心视角，应用作为客户端是使用的推模型在接收服务变更的推送。 那有一部分比较帅的人可能会有问了，到底什么模型比较好呢？哎，下面就容我对比下二者。 推拉对比实时性服务推送的实时性是服务发现主要的 SLA 指标，其指的是当服务地址发生变化时，客户端感知到变化的延迟。试想一下，服务端正在发布，IP 地址发生了变化，但是由于地址推送不及时，客户端过了 10 分钟还在调用旧的服务地址，这是多么可怕的一件事。 拉模型感知服务变化的延迟便是短轮询间隔+拉取服务的耗时，在 Nacos 中，SLA 为 10s。 推模型感知服务变化的延迟则为服务端推送服务的耗时，在 Nacos 中，SLA 为 1s。 在实时性上，推模型有较大优势。 压力拉模型和推模型都会对服务端造成压力，但是二者的时机不同。 拉模型的压力是固定的，取决于轮询间隔。 推模型的压力取决于服务变更的频率。 用两个场景来做对比： 常态化场景。日常运行时，服务列表一般不会变化，拉模型会导致不必要的开销，对服务端造成较大压力。 快上快下场景。机器快速扩容或者缩容，导致服务地址频繁变更，推送量会瞬时变大，对服务端和客户端造成较大压力。 针对快上快下场景，也可以进行一系列的优化，例如推送合并，增量推送，数据分离等等，目前 Nacos 支持了推送合并这一优化。 代码复杂度码龄越大，越发意识到代码复杂度是一个非常重要的技术选型指标，越简单的代码越容易维护，也具备持久的生命力，架构师需要在代码复杂度和性能的 trade off 中，找到一个平衡点，不得不承认，推模型的复杂度往往要比拉模型高出很多，例如多出了长连接的状态管理这一环节。 拉模型完全胜出。 总结这篇文章不希望大家陷入到字眼中，判断某一个框架或者工具是推或者拉模型，而是希望能介绍清楚服务发现中推拉模型的工作流程，方便大家对这些微服务框架也好，注册中心也好，有一个更深的理解。 总结一下，主流的微服务框架和注册中心的服务发现机制中，推模型和拉模型均有使用，具体如何选择，如何优化，可以根据自身服务的特点，以及服务的规模去选择使用。 在我负责的 API 网关（阿里云 CSB）中，采用了一套独立的服务发现机制，同时支持拉模型和推模型，以适配部分仅支持推模型或者仅支持拉模型的注册中心。","link":"/service-pull-push/"},{"title":"JAVA 拾遗 -- 关于 SPI 机制","text":"JDK 提供的 SPI(Service Provider Interface)机制，可能很多人不太熟悉，因为这个机制是针对厂商或者插件的，也可以在一些框架的扩展中看到。其核心类 java.util.ServiceLoader 可以在 jdk1.8 的文档中看到详细的介绍。虽然不太常见，但并不代表它不常用，恰恰相反，你无时无刻不在用它。玄乎了，莫急，思考一下你的项目中是否有用到第三方日志包，是否有用到数据库驱动？其实这些都和 SPI 有关。再来思考一下，现代的框架是如何加载日志依赖，加载数据库驱动的，你可能会对 class.forName(“com.mysql.jdbc.Driver”)这段代码不陌生，这是每个 java 初学者必定遇到过的，但如今的数据库驱动仍然是这样加载的吗？你还能找到这段代码吗？这一切的疑问，将在本篇文章结束后得到解答。 首先介绍 SPI 机制是个什么东西 实现一个自定义的 SPI1 项目结构 invoker 是我们的用来测试的主项目。 interface 是针对厂商和插件商定义的接口项目，只提供接口，不提供实现。 good-printer,bad-printer 分别是两个厂商对 interface 的不同实现，所以他们会依赖于 interface 项目。 这个简单的 demo 就是让大家体验，在不改变 invoker 代码，只更改依赖的前提下，切换 interface 的实现厂商。 2 interface 模块2.1 moe.cnkirito.spi.api.Printer 123public interface Printer { void print();} interface 只定义一个接口，不提供实现。规范的制定方一般都是比较牛叉的存在，这些接口通常位于 java，javax 前缀的包中。这里的 Printer 就是模拟一个规范接口。 3 good-printer 模块3.1 good-printer\\pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 规范的具体实现类必然要依赖规范接口 3.2 moe.cnkirito.spi.api.GoodPrinter 12345public class GoodPrinter implements Printer { public void print() { System.out.println(&quot;你是个好人 ~&quot;); }} 作为 Printer 规范接口的实现一 3.3 resources\\META-INF\\services\\moe.cnkirito.spi.api.Printer 1moe.cnkirito.spi.api.GoodPrinter 这里需要重点说明，每一个 SPI 接口都需要在自己项目的静态资源目录中声明一个 services 文件，文件名为实现规范接口的类名全路径，在此例中便是 moe.cnkirito.spi.api.Printer，在文件中，则写上一行具体实现类的全路径，在此例中便是 moe.cnkirito.spi.api.GoodPrinter。 这样一个厂商的实现便完成了。 4 bad-printer 模块我们在按照和 good-printer 模块中定义的一样的方式，完成另一个厂商对 Printer 规范的实现。 4.1 bad-printer\\pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.2 moe.cnkirito.spi.api.BadPrinter 123456public class BadPrinter implements Printer { public void print() { System.out.println(&quot;我抽烟，喝酒，蹦迪，但我知道我是好女孩 ~&quot;); }} 4.3 resources\\META-INF\\services\\moe.cnkirito.spi.api.Printer 1moe.cnkirito.spi.api.BadPrinter 这样，另一个厂商的实现便完成了。 **5 invoker 模块 ** 这里的 invoker 便是我们自己的项目了。如果一开始我们想使用厂商 good-printer 的 Printer 实现，是需要将其的依赖引入。 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;good-printer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; **5.1 编写调用主类 ** 12345678910public class MainApp { public static void main(String[] args) { ServiceLoader&lt;Printer&gt; printerLoader = ServiceLoader.load(Printer.class); for (Printer printer : printerLoader) { printer.print(); } }} ServiceLoader 是 java.util 提供的用于加载固定类路径下文件的一个加载器，正是它加载了对应接口声明的实现类。 5.2 打印结果 1 1你是个好人 ~ 如果在后续的方案中，想替换厂商的 Printer 实现，只需要将依赖更换 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;bad-printer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 调用主类无需变更代码，这符合开闭原则 5.3 打印结果 2 1我抽烟，喝酒，蹦迪，但我知道我是好女孩 ~ 是不是很神奇呢？这一切对于调用者来说都是透明的，只需要切换依赖即可！ SPI 在实际项目中的应用先总结下有什么新知识，resources/META-INF/services 下的文件似乎我们之前没怎么接触过，ServiceLoader 也没怎么接触过。那么现在我们打开自己项目的依赖，看看有什么发现。 在 mysql-connector-java-xxx.jar 中发现了 META-INF\\services\\java.sql.Driver 文件，里面只有两行记录： 12com.mysql.jdbc.Drivercom.mysql.fabric.jdbc.FabricMySQLDriver 我们可以分析出，java.sql.Driver 是一个规范接口，com.mysql.jdbc.Drivercom.mysql.fabric.jdbc.FabricMySQLDriver 则是 mysql-connector-java-xxx.jar 对这个规范的实现接口。 在 jcl-over-slf4j-xxxx.jar 中发现了 META-INF\\services\\org.apache.commons.logging.LogFactory 文件，里面只有一行记录： 1org.apache.commons.logging.impl.SLF4JLogFactory 相信不用我赘述，大家都能理解这是什么含义了 更多的还有很多，有兴趣可以自己翻一翻项目路径下的那些 jar 包 既然说到了数据库驱动，索性再多说一点，还记得一道经典的面试题：class.forName(“com.mysql.jdbc.Driver”) 到底做了什么事？ 先思考下：自己会怎么回答？ 都知道 class.forName 与类加载机制有关，会触发执行 com.mysql.jdbc.Driver 类中的静态方法，从而使主类加载数据库驱动。如果再追问，为什么它的静态块没有自动触发？可答：因为数据库驱动类的特殊性质，JDBC 规范中明确要求 Driver 类必须向 DriverManager 注册自己，导致其必须由 class.forName 手动触发，这可以在 java.sql.Driver 中得到解释。完美了吗？还没，来到最新的 DriverManager 源码中，可以看到这样的注释, 翻译如下： DriverManager 类的方法 getConnection 和 getDrivers 已经得到提高以支持 Java Standard Edition Service Provider 机制。 JDBC 4.0 Drivers 必须包括 META-INF/services/java.sql.Driver 文件。此文件包含 java.sql.Driver 的 JDBC 驱动程序实现的名称。例如，要加载 my.sql.Driver 类，META-INF/services/java.sql.Driver 文件需要包含下面的条目： my.sql.Driver 应用程序不再需要使用 Class.forName() 显式地加载 JDBC 驱动程序。当前使用 Class.forName() 加载 JDBC 驱动程序的现有程序将在不作修改的情况下继续工作。 可以发现，Class.forName 已经被弃用了，所以，这道题目的最佳回答，应当是和面试官牵扯到 JAVA 中的 SPI 机制，进而聊聊加载驱动的演变历史。 java.sql.DriverManager 1234567891011121314public Void run() { ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); try{ while(driversIterator.hasNext()) { driversIterator.next(); } } catch(Throwable t) { // Do nothing } return null;} 当然那，本节的内容还是主要介绍 SPI，驱动这一块这是引申而出，如果不太理解，可以多去翻一翻 jdk1.8 中 Driver 和 DriverManager 的源码，相信会有不小的收获。 SPI 在扩展方面的应用SPI 不仅仅是为厂商指定的标准，同样也为框架扩展提供了一个思路。框架可以预留出 SPI 接口，这样可以在不侵入代码的前提下，通过增删依赖来扩展框架。前提是，框架得预留出核心接口，也就是本例中 interface 模块中类似的接口，剩下的适配工作便留给了开发者。 例如我的上一篇文章 https://www.cnkirito.moe/2017/11/07/spring-cloud-sleuth/ 中介绍的 motan 中 Filter 的扩展，便是采用了 SPI 机制，熟悉这个设定之后再回头去了解一些框架的 SPI 扩展就不会太陌生了。","link":"/spi/"},{"title":"警惕 Spring Boot Actuator 引发的安全漏洞","text":"前言一年一度的 HW 行动开始了，最近也是被各种安全漏洞搞的特别闹心，一周能收到几十封安全团队扫描出来的漏洞邮件，这其中有一类漏洞很容易被人忽视，但影响面却极广，危害也极大，我说出它的名字你应该也不会感到陌生，正是 Spring Boot Actuator 。 写这篇文章前，我跟我的朋友做了一个小调查，问他们对 Spring Boot Actuator 的了解，结果惊人的一致，大家都知道 Spring Boot 提供了 spring-boot-starter-actuator 的自动配置，但却很少有人真正用到它相关的特性。在继续往下面看这篇文章时，大家也可以先思考下几个问题： 检查下你开发的项目中有引入 spring-boot-starter-actuator 依赖吗？ 你在项目中有真正用到 spring-boot-starter-actuator 的有关功能吗？ 你知道 spring-boot-starter-actuator 的安全风险和正确配置方式吗？ Spring Boot Actuator 是什么？好久没翻过 spring 的文档了，为了解释这个还算陌生的名词 Actutor [ˈæktjuˌeɪtər]，我特地去翻了下它的文档，找到了官方的定义 Definition of ActuatorAn actuator is a manufacturing term that refers to a mechanical device for moving or controlling something. Actuators can generate a large amount of motion from a small change. 好家伙，看了等于白看，以我 CET-6 的水平，理解这段话着实有点难度，希望能有英语比较好的同学帮我翻译下。只能按照我个人对 Spring Boot Actuator 功能的理解来意译下了：我们可以借助于 Spring Boot Actuator 来对 Spring Boot 应用的健康状态、环境配置、Metrics、Trace、Spring 上下文等信息进行查看，除了一系列查看功能之外，它还实现了 Spring Boot 应用的上下线和内存 dump 功能。 Quick Start第一步 引入依赖 tips：spring-boot-starter-actuator 在不同版本 Spring Boot 中有一定的配置差异，本文采用的是目前最新的 2.4.4 版本 pom.xml12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;version&gt;2.4.4&lt;/version&gt;&lt;/dependency&gt; 第二步 了解 endpointendpoint 是我们使用 Spring Boot Actuator 最需要关心的对象，列举一些你可能感兴趣的 endpoint ID Description beans 查看 Spring 容器中的所有对象 configprops 查看被 @ConfigurationProperties 修饰的对象列表 env 查看 application.yaml 配置的环境配置信息 health 健康检查端点 info 应用信息 metrics 统计信息 mappings 服务契约 @RequestMapping 相关的端点 shutdown 优雅下线 例如 health，只需要访问如下 endpoint 即可获取应用的状态 info1curl &quot;localhost:8080/actuator/health&quot; 第三步 了解 endpoint 的 enable 和 exposure 状态Spring Boot Actuator 针对于所有 endpoint 都提供了两种状态的配置 enabled 启用状态。默认情况下除了 shutdown 之外，其他 endpoint 都是启用状态。这也很好理解，其他 endpoint 基本都是查看行为，shutdown 却会影响应用的运行状态。 exposure 暴露状态。endpoint 的 enabled 设置为 true 后，还需要暴露一次，才能够被访问，默认情况下只有 health 和 info 是暴露的。 enabled 不启用时，相关的 endpoint 的代码完全不会被 Spring 上下文加载，所以 enabled 为 false 时，exposure 配置了也无济于事。 几个典型的配置示例如下 启用并暴露所有 endpoint 暴露所有 endpoint12345678management: endpoints: web: exposure: include: &quot;*&quot; endpoint: shutdown: enabled: true 只启用并暴露指定 endpoint 只启用并暴露指定 endpoint12345678910management: endpoints: enabled-by-default: false endpoint: info: enabled: true endpoints: web: exposure: include: &quot;info&quot; 禁用所有 endpoint 禁用所有 endpoint123management: endpoints: enabled-by-default: false 或者，去除掉 spring-boot-starter-actuator 依赖！ 了解 Spring Boot Actuator 的安全风险从上文的介绍可知，有一些 Spring Boot Actuator 提供的 endpoint 是会将应用重要的信息暴露出去的，以 env 为例来感受下一个典型的 application.yaml 的示例。 application.yaml123456789101112131415server: port: 8080spring: datasource: url: jdbc:mysql://testDbHost:3306/kirito username: kirito password: 123456kirito: ak: kirito@xxx_ak sk: kirito@xxx_skmanagement: endpoints: web: exposure: include: &quot;*&quot; 上面的配置再经典不过，我们看看访问 localhost:8080/actuator/env 之后的返回值 localhost:8080/actuator/env12345678910111213141516171819202122232425262728293031323334353637383940414243444546{ &quot;activeProfiles&quot;: [], &quot;propertySources&quot;: [ { &quot;name&quot;: &quot;server.ports&quot;, &quot;properties&quot;: { &quot;local.server.port&quot;: { &quot;value&quot;: 8080 } } }, { &quot;name&quot;: &quot;Config resource 'class path resource [application.yaml]' via location 'optional:classpath:/'&quot;, &quot;properties&quot;: { &quot;server.port&quot;: { &quot;value&quot;: 8080, &quot;origin&quot;: &quot;class path resource [application.yaml] - 2:9&quot; }, &quot;spring.datasource.url&quot;: { &quot;value&quot;: &quot;jdbc:mysql://testDbHost:3306/kirito&quot;, &quot;origin&quot;: &quot;class path resource [application.yaml] - 5:44&quot; }, &quot;spring.datasource.username&quot;: { &quot;value&quot;: &quot;kirito&quot;, &quot;origin&quot;: &quot;class path resource [application.yaml] - 6:15&quot; }, &quot;spring.datasource.password&quot;: { &quot;value&quot;: &quot;******&quot;, &quot;origin&quot;: &quot;class path resource [application.yaml] - 7:15&quot; }, &quot;kirito.ak&quot;: { &quot;value&quot;: &quot;kirito@xxx_ak&quot;, &quot;origin&quot;: &quot;class path resource [application.yaml] - 10:7&quot; }, &quot;kirito.sk&quot;: { &quot;value&quot;: &quot;kirito@xxx_sk&quot;, &quot;origin&quot;: &quot;class path resource [application.yaml] - 11:7&quot; }, &quot;management.endpoints.web.exposure.include&quot;: { &quot;value&quot;: &quot;*&quot;, &quot;origin&quot;: &quot;class path resource [application.yaml] - 17:18&quot; } } } ]} 可以发现，对于内置的敏感配置信息 spring.datasource.password，Spring Boot Actuator 是进行了脱敏的，但是对于自定义的一些敏感配置，如 kirito.ak 和 kirito.sk 却被暴露出来了。 可能有的读者会立马提出质疑：我们的机器都部署内网，并且一般都是通过反向代理对外暴露的服务，这类 endpoint 是不会被外部用户访问到的。那我只能说太天真了，例如以下情况都是导致安全漏洞的真实 case： 反向代理误配置了根节点，将 actuator 的 endpoint 和 web 服务一起暴露了出去 线上配置没问题，测试环境部署时开通了公网 SLB，导致 actuator 的 endpoint 暴露了出去 同一环境中某台机器被攻陷，导致应用配置信息泄露 安全建议针对 Spring Boot Actuator 提供的 endpoint，采取以下几种措施，可以尽可能降低被安全攻击的风险 最小粒度暴露 endpoint。只开启并暴露真正用到的 endpoint，而不是配置： management.endpoints.web.exposure.include=*。 为 endpoint 配置独立的访问端口，从而和 web 服务的端口分离开，避免暴露 web 服务时，误将 actuator 的 endpoint 也暴露出去。例：management.port=8099。 引入 spring-boot-starter-security 依赖，为 actuator 的 endpoint 配置访问控制。 慎重评估是否需要引入 spring-boot-stater-actuator。以我个人的经验，我至今还没有遇到什么需求是一定需要引入spring-boot-stater-actuator 才能解决，如果你并不了解上文所述的安全风险，我建议你先去除掉该依赖。 你使用 Spring Boot Actuator 了吗？","link":"/spring-boot-actuator-notes/"},{"title":"Spring 揭秘 -- 寻找遗失的 web.xml","text":"今天我们来放松下心情，不聊分布式，云原生，来聊一聊初学者接触的最多的 java web 基础。几乎所有人都是从 servlet，jsp，filter 开始编写自己的第一个 hello world 工程。那时，还离不开 web.xml 的配置，在 xml 文件中编写繁琐的 servlet 和 filter 的配置。随着 spring 的普及，配置逐渐演变成了两种方式—java configuration 和 xml 配置共存。现如今，springboot 的普及，java configuration 成了主流，xml 配置似乎已经“灭绝”了。不知道你有没有好奇过，这中间都发生了哪些改变，web.xml 中的配置项又是被什么替代项取代了？ servlet3.0 以前的时代为了体现出整个演进过程，还是来回顾下 n 年前我们是怎么写 servlet 和 filter 代码的。 项目结构（本文都采用 maven 项目结构） 12345678910111213141516.├── pom.xml├── src ├── main │ ├── java │ │ └── moe │ │ └── cnkirito │ │ ├── filter │ │ │ └── HelloWorldFilter.java │ │ └── servlet │ │ └── HelloWorldServlet.java │ └── resources │ └── WEB-INF │ └── web.xml └── test └── java 12345678910public class HelloWorldServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.setContentType(&quot;text/plain&quot;); PrintWriter out = resp.getWriter(); out.println(&quot;hello world&quot;); }} 123456789101112131415161718public class HelloWorldFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(&quot;触发 hello world 过滤器...&quot;); filterChain.doFilter(servletRequest,servletResponse); } @Override public void destroy() { }} 别忘了在 web.xml 中配置 servlet 和 filter 123456789101112131415161718192021222324252627&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_4_0.xsd&quot; version=&quot;4.0&quot;&gt; &lt;servlet&gt; &lt;servlet-name&gt;HelloWorldServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;moe.cnkirito.servlet.HelloWorldServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloWorldServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/hello&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;HelloWorldFilter&lt;/filter-name&gt; &lt;filter-class&gt;moe.cnkirito.filter.HelloWorldFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HelloWorldFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/hello&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;/web-app&gt; 这样，一个 java web hello world 就完成了。当然，本文不是 servlet 的入门教程，只是为了对比。 servlet3.0 新特性 Servlet 3.0 作为 Java EE 6 规范体系中一员，随着 Java EE 6 规范一起发布。该版本在前一版本（Servlet 2.5）的基础上提供了若干新特性用于简化 Web 应用的开发和部署。其中一项新特性便是提供了无 xml 配置的特性。 servlet3.0 首先提供了 @WebServlet，@WebFilter 等注解，这样便有了抛弃 web.xml 的第一个途径，凭借注解声明 servlet 和 filter 来做到这一点。 除了这种方式，servlet3.0 规范还提供了更强大的功能，可以在运行时动态注册 servlet ，filter，listener。以 servlet 为例，过滤器与监听器与之类似。ServletContext 为动态配置 Servlet 增加了如下方法： ServletRegistration.Dynamic addServlet(String servletName,Class&lt;? extends Servlet&gt; servletClass) ServletRegistration.Dynamic addServlet(String servletName, Servlet servlet) ServletRegistration.Dynamic addServlet(String servletName, String className) T createServlet(Class clazz) ServletRegistration getServletRegistration(String servletName) Map&lt;String,? extends ServletRegistration&gt; getServletRegistrations() 其中前三个方法的作用是相同的，只是参数类型不同而已；通过 createServlet()方法创建的 Servlet，通常需要做一些自定义的配置，然后使用 addServlet() 方法来将其动态注册为一个可以用于服务的 Servlet。两个 getServletRegistration() 方法主要用于动态为 Servlet 增加映射信息，这等价于在 web.xml 中使用 标签为存在的 Servlet 增加映射信息。 以上 ServletContext 新增的方法要么是在 ServletContextListener 的 contexInitialized 方法中调用，要么是在 ServletContainerInitializer 的 onStartup() 方法中调用。 ServletContainerInitializer 也是 Servlet 3.0 新增的一个接口，容器在启动时使用 JAR 服务 API(JAR Service API) 来发现 ServletContainerInitializer 的实现类，并且容器将 WEB-INF/lib 目录下 JAR 包中的类都交给该类的 onStartup()方法处理，我们通常需要在该实现类上使用 @HandlesTypes 注解来指定希望被处理的类，过滤掉不希望给 onStartup() 处理的类。 一个典型的 servlet3.0+ 的 web 项目结构如下： 123456789101112131415161718.├── pom.xml└── src ├── main │ ├── java │ │ └── moe │ │ └── cnkirito │ │ ├── CustomServletContainerInitializer.java │ │ ├── filter │ │ │ └── HelloWorldFilter.java │ │ └── servlet │ │ └── HelloWorldServlet.java │ └── resources │ └── META-INF │ └── services │ └── javax.servlet.ServletContainerInitializer └── test └── java 我并未对 HelloWorldServlet 和 HelloWorldFilter 做任何改动，而是新增了一个 CustomServletContainerInitializer , 它实现了 javax.servlet.ServletContainerInitializer 接口，用来在 web 容器启动时加载指定的 servlet 和 filter，代码如下： 123456789101112131415161718192021222324252627public class CustomServletContainerInitializer implements ServletContainerInitializer { private final static String JAR_HELLO_URL = &quot;/hello&quot;; @Override public void onStartup(Set&lt;Class&lt;?&gt;&gt; c, ServletContext servletContext) { System.out.println(&quot;创建 helloWorldServlet...&quot;); ServletRegistration.Dynamic servlet = servletContext.addServlet( HelloWorldServlet.class.getSimpleName(), HelloWorldServlet.class); servlet.addMapping(JAR_HELLO_URL); System.out.println(&quot;创建 helloWorldFilter...&quot;); FilterRegistration.Dynamic filter = servletContext.addFilter( HelloWorldFilter.class.getSimpleName(), HelloWorldFilter.class); EnumSet&lt;DispatcherType&gt; dispatcherTypes = EnumSet.allOf(DispatcherType.class); dispatcherTypes.add(DispatcherType.REQUEST); dispatcherTypes.add(DispatcherType.FORWARD); filter.addMappingForUrlPatterns(dispatcherTypes, true, JAR_HELLO_URL); }} 对上述代码进行一些解读。ServletContext 我们称之为 servlet 上下文，它维护了整个 web 容器中注册的 servlet，filter，listener，以 servlet 为例，可以使用 servletContext.addServlet 等方法来添加 servlet。而方法入参中 Set&lt;Class> c 和 @HandlesTypes 注解在 demo 中我并未使用，感兴趣的朋友可以 debug 看看到底获取了哪些 class ，一般正常的流程是使用 @HandlesTypes 指定需要处理的 class，而后对 Set","link":"/servlet-explore/"},{"title":"使用 Spring Cloud Sleuth 实现链路监控","text":"在服务比较少的年代，一个系统的接口响应缓慢通常能够迅速被发现，但如今的微服务模块，大多具有规模大，依赖关系复杂等特性，错综复杂的网状结构使得我们不容易定位到某一个执行缓慢的接口。分布式的服务跟踪组件就是为了解决这一个问题。其次，它解决了另一个难题，在没有它之前，我们客户会一直询问：你们的系统有监控吗？你们的系统有监控吗？你们的系统有监控吗？现在，谢天谢地，他们终于不问了。是有点玩笑的成分，但可以肯定的一点是，实现全链路监控是保证系统健壮性的关键因子。 介绍 Spring Cloud Sleuth 和 Zipkin 的文章在网上其实并不少，所以我打算就我目前的系统来探讨一下，如何实现链路监控。全链路监控这个词意味着只要是不同系统模块之间的调用都应当被监控，这就包括了如下几种常用的交互方式： 1 Http 协议，如 RestTemplate，Feign，Okhttp3，HttpClient… 2 Rpc 远程调用，如 Motan，Dubbo，GRPC… 3 分布式 Event，如 RabbitMq，Kafka… 而我们项目目前混合使用了 Http 协议，Motan Rpc 协议，所以本篇文章会着墨于实现这两块的链路监控。 项目结构 上面的项目结构是本次 demo 的核心结构，其中 zipkin-server 作为服务跟踪的服务端，记录各个模块发送而来的调用请求，最终形成调用链路的报告。 order,goods 两个模块为用来做测试的业务模块，分别实现了 http 形式和 rpc 形式的远程调用，最终我们会在 zipkin-server 的 ui 页面验证他们的调用记录。 interface 存放了 order 和 goods 模块的公用接口，rpc 调用需要一个公用的接口。 filter-opentracing 存放了自定义的 motan 扩展代码，用于实现 motan rpc 调用的链路监控。 Zipkin 服务端** 添加依赖 ** 全部依赖 ** 核心依赖 ** 12345678910111213&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-storage-mysql&lt;/artifactId&gt; &lt;version&gt;1.28.0&lt;/version&gt;&lt;/dependency&gt; zipkin-autoconfigure-ui 提供了默认了 UI 页面，zipkin-storage-mysql 选择将链路调用信息存储在 mysql 中，更多的选择可以有 elasticsearch，cassandra。 zipkin-server/src/main/resources/application.yml 12345678910111213spring: application: name: zipkin-server datasource: url: jdbc:mysql://localhost:3306/zipkin username: root password: root driver-class-name: com.mysql.jdbc.Driverzipkin: storage: type: mysqlserver: port: 9411 ** 创建启动类 ** 1234567891011121314@SpringBootApplication@EnableZipkinServerpublic class ZipkinServerApp { @Bean public MySQLStorage mySQLStorage(DataSource datasource) { return MySQLStorage.builder().datasource(datasource).executor(Runnable::run).build(); } public static void main(String[] args) { SpringApplication.run(ZipkinServerApp.class, args); }} 当前版本在手动配置数据库之后才不会启动报错，可能与版本有关。mysql 相关的脚本可以在此处下载：mysql 初始化脚本。 zipkin-server 单独启动后，就可以看到链路监控页面了，此时由于没有收集到任何链路调用记录，显示如下： HTTP 链路监控编写 order 和 goods 两个服务，在 order 暴露一个 http 端口，在 goods 中使用 RestTemplate 远程调用，之后查看在 zipkin 服务端查看调用信息。 首先添加依赖，让普通的应用具备收集和发送报告的能力，这一切在 spring cloud sleuth 的帮助下都变得很简单 ** 添加依赖 ** 全部依赖 ** 核心依赖 ** 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; spring-cloud-starter-zipkin 依赖内部包含了两个依赖，等于同时引入了 spring-cloud-starter-sleuth，spring-cloud-sleuth-zipkin 两个依赖。名字特别像，注意区分。 以 order 为例介绍配置文件 order/src/main/resources/application.yml 1234567891011spring: application: name: order # 1 zipkin: base-url: http://localhost:9411 # 2 sleuth: enabled: true sampler: percentage: 1 # 3server: port: 8060 &lt;1&gt; 指定项目名称可以方便的标记应用，在之后的监控页面可以看到这里的配置名称 &lt;2&gt; 指定 zipkin 的服务端，用于发送链路调用报告 &lt;3&gt; 采样率，值为 [0,1] 之间的任意实数，顾名思义，这里代表 100% 采集报告。 ** 编写调用类 ** 服务端 order 123456789101112@RestController@RequestMapping(&quot;/api&quot;)public class OrderController { Logger logger = LoggerFactory.getLogger(OrderController.class); @RequestMapping(&quot;/order/{id}&quot;) public MainOrder getOrder(@PathVariable(&quot;id&quot;) String id) { logger.info(&quot;order invoking ...&quot;); //&lt;1&gt; return new MainOrder(id, new BigDecimal(200D), new Date()); }} 客户端 goods 12345public MainOrder test(){ ResponseEntity&lt;MainOrder&gt; mainOrderResponseEntity = restTemplate.getForEntity(&quot;http://localhost:8060/api/order/1144&quot;, MainOrder.class); MainOrder body = mainOrderResponseEntity.getBody(); return body;} &lt;1&gt; 首先观察这一行日志在控制台是如何输出的 12017-11-08 09:54:00.633 INFO [order,d251f40af64361d2,e46132755dc395e1,true] 2780 --- [nio-8060-exec-1] m.c.sleuth.order.web.OrderController : order invoking ... 比没有引入 sleuth 之前多了一些信息，其中 order,d251f40af64361d2,e46132755dc395e1,true 分别代表了应用名称，traceId，spanId，当前调用是否被采集，关于 trace，span 这些专业词语，强烈建议去看看 Dapper 这篇论文，有很多中文翻译版本，并不是想象中的学术范，非常容易理解，很多链路监控文章中的截图都来自于这篇论文，我在此就不再赘述概念了。 紧接着，回到 zipkin-server 的监控页面，查看变化 到这里，Http 监控就已经完成了，如果你的应用使用了其他的 Http 工具，如 okhttp3，也可以去 [opentracing，zipkin 相关的文档中寻找依赖。 RPC 链路监控虽说 spring cloud 是大势所趋，其推崇的 http 调用方式也是链路监控的主要对象，但不得不承认目前大多数的系统内部调用仍然是 RPC 的方式，至少我们内部的系统是如此，由于我们内部采用的 RPC 框架是 weibo 开源的 motan，这里以此为例，介绍 RPC 的链路监控。motan 使用 SPI 机制，实现了对链路监控的支持，https://github.com/weibocom/motan/issues/304 这条 issue 中可以得知其加入了 opentracing 标准化追踪。但目前只能通过自己添加组件的方式才能配合 spring-cloud-sleuth 使用，下面来看看实现步骤。 filter-opentracing 实现思路：引入 SleuthTracingFilter，作为全局的 motan 过滤器，给每一次 motan 的调用打上 traceId 和 spanId，并编写一个 SleuthTracingContext，持有一个 SleuthTracerFactory 工厂，用于适配不同的 Tracer 实现。 具体的实现可以参考文末的地址 order/src/main/resources/META-INF/services/com.weibo.api.motan.filter.Filter 1com.weibo.api.motan.filter.sleuth.SleuthTracingFilter 添加一行过滤器的声明，使得项目能够识别 ** 配置 SleuthTracingContext** 123456789101112@BeanSleuthTracingContext sleuthTracingContext(@Autowired(required = false) org.springframework.cloud.sleuth.Tracer tracer){ SleuthTracingContext context = new SleuthTracingContext(); context.setTracerFactory(new SleuthTracerFactory() { @Override public org.springframework.cloud.sleuth.Tracer getTracer() { return tracer; } }); return context;} 使用 spring-cloud-sleuth 的 Tracer 作为 motan 调用的收集器 ** 为服务端和客户端配置过滤器 ** 123basicServiceConfigBean.setFilter(&quot;sleuth-tracing&quot;);basicRefererConfigBean.setFilter(&quot;sleuth-tracing&quot;); ** 编写调用测试类 ** order 作为客户端 12345678@MotanRefererGoodsApi goodsApi;@RequestMapping(&quot;/goods&quot;)public String getGoodsList() { logger.info(&quot;getGoodsList invoking ...&quot;); return goodsApi.getGoodsList();} goods 作为服务端 1234567891011@MotanServicepublic class GoodsApiImpl implements GoodsApi { Logger logger = LoggerFactory.getLogger(GoodsApiImpl.class); @Override public String getGoodsList() { logger.info(&quot;GoodsApi invoking ...&quot;); return &quot;success&quot;; }} ** 查看调用关系 ** 第一张图中，使用前缀 http 和 motan 来区别调用的类型，第二张图中，依赖变成了双向的，因为一开始的 http 调用 goods 依赖于 order，而新增了 motan rpc 调用之后 order 又依赖于 goods。 总结系统间交互的方式除了 http，rpc，还有另外的方式如 mq，以后还可能会有更多的方式，但实现的监控的思路都是一致的，即如何无侵入式地给调用打上标签，记录报告。Dapper 给实现链路监控提供了一个思路，而 OpenTracing 为各个框架不同的调用方式提供了适配接口….Spring Cloud Sleuth 则是遵循了 Spring 一贯的风格，整合了丰富的资源，为我们的系统集成链路监控提供了很大的便捷性。 关于 motan 具体实现链路监控的代码由于篇幅限制，将源码放在了我的 github 中，如果你的系统使用了 motan，可以用于参考：https://github.com/lexburner/sleuth-starter 参考《Spring Cloud 微服务实战》– 翟永超 黄桂钱老师的指导","link":"/spring-cloud-sleuth/"},{"title":"Spring Data Redis（一）-- 解析 RedisTemplate","text":"谈及系统优化，缓存一直是不可或缺的一点。在缓存中间件层面，我们有 MemCache，Redis 等选择；在系统分层层面，又需要考虑多级缓存；在系统可用性层面，又要考虑到缓存雪崩，缓存穿透，缓存失效等常见的缓存问题… 缓存的使用与优化值得我们花费一定的精力去深入理解。《Spring Data Redis》这个系列打算围绕 spring-data-redis 来进行分析，从 hello world 到源码分析，夹杂一些不多实战经验（经验有限），不止限于 spring-data-redis 本身，也会扩展谈及缓存这个大的知识点。 至于为何选择 redis，相信不用我赘述，redis 如今非常流行，几乎成了项目必备的组件之一。而 spring-boot-starter-data-redis 模块又为我们在 spring 集成的项目中提供了开箱即用的功能，更加便捷了我们开发。系列的第一篇便是简单介绍下整个组件最常用的一个工具类：RedisTemplate。 1 引入依赖1234567891011121314151617&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; springboot 的老用户会发现 redis 依赖名称发生了一点小的变化，在 springboot1.4 之前，redis 依赖的名称为：spring-boot-starter-redis，而在之后较新的版本中，使用 spring-boot-starter-redis 依赖，则会在项目启动时得到一个过期警告。意味着，我们应该彻底放弃旧的依赖。spring-data 这个项目定位为 spring 提供一个统一的数据仓库接口，如（spring-boot-starter-data-jpa,spring-boot-starter-data-mongo,spring-boot-starter-data-rest），将 redis 纳入后，改名为了 spring-boot-starter-data-redis。 2 配置 redis 连接resources/application.yml 123456spring: redis: host: 127.0.0.1 database: 0 port: 6379 password: 本机启动一个单点的 redis 即可，使用 redis 的 0 号库作为默认库（默认有 16 个库），在生产项目中一般会配置 redis 集群和哨兵保证 redis 的高可用，同样可以在 application.yml 中修改，非常方便。 3 编写测试类1234567891011121314151617181920import org.assertj.core.api.Assertions;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.test.context.junit4.SpringRunner;@RunWith(SpringRunner.class)@SpringBootTestpublic class ApplicationTests { @Autowired private RedisTemplate redisTemplate;// &lt;1&gt; @Test public void test() throws Exception { redisTemplate.opsForValue().set(&quot;student:1&quot;, &quot;kirito&quot;); // &lt;2&gt; Assertions.assertThat(redisTemplate.opsForValue().get(&quot;student:1&quot;)).isEqualTo(&quot;kirito&quot;); }} &lt;1&gt; 引入了 RedisTemplate，这个类是 spring-starter-data-redis 提供给应用直接访问 redis 的入口。从其命名就可以看出，其是模板模式在 spring 中的体现，与 restTemplate，jdbcTemplate 类似，而 springboot 为我们做了自动的配置，具体会在下文详解。 &lt;2&gt; redisTemplate 通常不直接操作键值，而是通过 opsForXxx() 访问，在本例中，key 和 value 均为字符串类型。绑定字符串在实际开发中也是最为常用的操作类型。 4 详解 RedisTemplate 的 APIRedisTemplate 为我们操作 Redis 提供了丰富的 API，可以将他们简单进行下归类。 4.1 常用数据操作这一类 API 也是我们最常用的一类。 众所周知，redis 存在 5 种数据类型： 字符串类型（string），散列类型（hash），列表类型（list），集合类型（set），有序集合类型（zset） 而 redisTemplate 实现了 RedisOperations 接口，在其中，定义了一系列与 redis 相关的基础数据操作接口，数据类型分别于下来 API 对应： 123456789101112// 非绑定 key 操作ValueOperations&lt;K, V&gt; opsForValue();&lt;HK, HV&gt; HashOperations&lt;K, HK, HV&gt; opsForHash();ListOperations&lt;K, V&gt; opsForList();SetOperations&lt;K, V&gt; opsForSet();ZSetOperations&lt;K, V&gt; opsForZSet();// 绑定 key 操作BoundValueOperations&lt;K, V&gt; boundValueOps(K key);&lt;HK, HV&gt; BoundHashOperations&lt;K, HK, HV&gt; boundHashOps(K key);BoundListOperations&lt;K, V&gt; boundListOps(K key);BoundSetOperations&lt;K, V&gt; boundSetOps(K key);BoundZSetOperations&lt;K, V&gt; boundZSetOps(K key); 若以 bound 开头，则意味着在操作之初就会绑定一个 key，后续的所有操作便默认认为是对该 key 的操作，算是一个小优化。 4.2 对原生 Redis 指令的支持Redis 原生指令中便提供了一些很有用的操作，如设置 key 的过期时间，判断 key 是否存在等等… 常用的 API 列举： RedisTemplate API 原生 Redis 指令 说明 public void delete(K key) DEL key [key …] 删除给定的一个或多个 key public Boolean hasKey(K key) EXISTS key 检查给定 key 是否存在 public Boolean expire/expireAt(…) EXPIRE key seconds 为给定 key 设置生存时间，当 key 过期时 (生存时间为 0)，它会被自动删除。 public Long getExpire(K key) TTL key 以秒为单位，返回给定 key 的剩余生存时间 (TTL, time to live)。 更多的原生 Redis 指令支持可以参考 javadoc 4.3 CAS 操作CAS（Compare and Swap）通常有 3 个操作数，内存值 V，旧的预期值 A，要修改的新值 B。当且仅当预期值 A 和内存值 V 相同时，将内存值 V 修改为 B，否则什么都不做。CAS 也通常与并发，乐观锁，非阻塞，机器指令等关键词放到一起讲解。可能会有很多朋友在秒杀场景的架构设计中见到了 Redis，本质上便是利用了 Redis 分布式共享内存的特性以及一系列的 CAS 指令。还记得在 4.1 中通过 redisTemplate.opsForValue()或者 redisTemplate.boundValueOps() 可以得到一个 ValueOperations 或 BoundValueOperations 接口 (以值为字符串的操作接口为例)，这些接口除了提供了基础操作外，还提供了一系列 CAS 操作，也可以放到 RedisTemplate 中一起理解。 常用的 API 列举： ValueOperations API 原生 Redis 指令 说明 Boolean setIfAbsent(K key, V value) SETNX key value 将 key 的值设为 value ，当且仅当 key 不存在。设置成功，返回 1 ， 设置失败，返回 0 。 V getAndSet(K key, V value) GETSET key value 将给定 key 的值设为 value ，并返回 key 的旧值 (old value)。 Long increment(K key, long delta)/Double increment(K key, double delta) INCR/INCRBY/INCRBYFLOAT 将 key 所储存的值加上增量 increment 。 如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行 INCR/INCRBY/INCRBYFLOAT 命令。线程安全的 + 关于 CAS 的理解可以参考我之前的文章 java 并发实践 –CAS 或者其他博文。 4.4 发布订阅redis 之所以被冠以银弹，万金油的称号，关键在于其实现的功能真是太多了，甚至实现了一部分中间件队列的功能，其内置的 channel 机制，可以用于实现分布式的队列和广播。 RedisTemplate 提供了 convertAndSend()功能，用于发送消息，与 RedisMessageListenerContainer 配合接收，便实现了一个简易的发布订阅。如果想要使用 Redis 实现发布订阅，可以参考我之前的文章。 浅析分布式下的事件驱动机制 4.5 Lua 脚本RedisTemplate 中包含了这样一个 Lua 执行器，意味着我们可以使用 RedisTemplate 执行 Lua 脚本。 1private ScriptExecutor&lt;K&gt; scriptExecutor; Lua 这门语言也非常有意思，小巧而精悍，有兴趣的朋友可以去了解一下 nginx+lua 开发，使用 openResty 框架。而 Redis 内置了 Lua 的解析器，由于 Redis 单线程的特性（不严谨），可以使用 Lua 脚本，完成一些线程安全的符合操作（CAS 操作仅仅只能保证单个操作的线程安全，无法保证复合操作，如果你有这样的需求，可以考虑使用 Redis+Lua 脚本）。 123public &lt;T&gt; T execute(RedisScript&lt;T&gt; script, List&lt;K&gt; keys, Object... args) { return scriptExecutor.execute(script, keys, args);} 上述操作便可以完成对 Lua 脚本的调用。这儿有一个简单的示例，使用 Redis+Lua 脚本实现分布式的应用限流。分布式限流 5 总结Spring Data Redis 系列的第一篇，介绍了 spring-data 对 redis 操作的封装，顺带了解 redis 具备的一系列特性，如果你对 redis 的理解还仅仅停留在它是一个分布式的 key-value 数据库，那么相信现在你一定会感叹其竟然如此强大。后续将会对缓存在项目中的应用以及 spring-boot-starter-data-redis 进一步解析。","link":"/spring-data-redis-1/"},{"title":"Spring Data Redis（二）-- 序列化","text":"默认序列化方案在上一篇文章《Spring Data Redis（一）》中，我们执行了这样一个操作： 1redisTemplate.opsForValue().set(&quot;student:1&quot;,&quot;kirito&quot;); 试图使用 RedisTemplate 在 Redis 中存储一个键为“student:1”，值为“kirito”的 String 类型变量（redis 中通常使用‘:’作为键的分隔符）。那么是否真的如我们所预想的那样，在 Redis 中存在这样的键值对呢？ 这可以说是 Redis 中最基础的操作了，但严谨起见，还是验证一下为妙，使用 RedisDesktopManager 可视化工具，或者 redis-cli 都可以查看 redis 中的数据。 emmmmm，大概能看出是我们的键值对，但前面似乎多了一些奇怪的 16 进制字符，在不了解 RedisTemplate 工作原理的情况下，自然会对这个现象产生疑惑。 首先看看 springboot 如何帮我们自动完成 RedisTemplate 的配置： 12345678910111213@Configurationprotected static class RedisConfiguration { @Bean @ConditionalOnMissingBean(name = &quot;redisTemplate&quot;) public RedisTemplate&lt;Object, Object&gt; redisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;Object, Object&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; }} 没看出什么特殊的设置，于是我们进入 RedisTemplate 自身的源码中一窥究竟。 首先是在类开头声明了一系列的序列化器： 123456789private boolean enableDefaultSerializer = true;// 配置默认序列化器private RedisSerializer&lt;?&gt; defaultSerializer;private ClassLoader classLoader;private RedisSerializer keySerializer = null;private RedisSerializer valueSerializer = null;private RedisSerializer hashKeySerializer = null;private RedisSerializer hashValueSerializer = null;private RedisSerializer&lt;String&gt; stringSerializer = new StringRedisSerializer(); 看到了我们关心的 keySerializer 和 valueSerializer，在 RedisTemplate.afterPropertiesSet() 方法中，可以看到，默认的序列化方案: 12345678910111213141516171819202122232425262728public void afterPropertiesSet() { super.afterPropertiesSet(); boolean defaultUsed = false; if (defaultSerializer == null) { defaultSerializer = new JdkSerializationRedisSerializer( classLoader != null ? classLoader : this.getClass().getClassLoader()); } if (enableDefaultSerializer) { if (keySerializer == null) { keySerializer = defaultSerializer; defaultUsed = true; } if (valueSerializer == null) { valueSerializer = defaultSerializer; defaultUsed = true; } if (hashKeySerializer == null) { hashKeySerializer = defaultSerializer; defaultUsed = true; } if (hashValueSerializer == null) { hashValueSerializer = defaultSerializer; defaultUsed = true; } } ... initialized = true;} 默认的方案是使用了 JdkSerializationRedisSerializer，所以导致了前面的结果，注意：字符串和使用 jdk 序列化之后的字符串是两个概念。 我们可以查看 set 方法的源码： 12345678910public void set(K key, V value) { final byte[] rawValue = rawValue(value); execute(new ValueDeserializingRedisCallback(key) { protected byte[] inRedis(byte[] rawKey, RedisConnection connection) { connection.set(rawKey, rawValue); return null; } }, true);} 最终与 Redis 交互使用的是原生的 connection，键值则全部是字节数组，意味着所有的序列化都依赖于应用层完成，Redis 只认字节！这也是引出本节介绍的初衷，序列化是与 Redis 打交道很关键的一个环节。 StringRedisSerializer在我不长的使用 Redis 的时间里，其实大多数操作是字符串操作，键值均为字符串，String.getBytes() 即可满足需求。spring-data-redis 也考虑到了这一点，其一，提供了 StringRedisSerializer 的实现，其二，提供了 StringRedisTemplate，继承自 RedisTemplate。 12345678910public class StringRedisTemplate extends RedisTemplate&lt;String, String&gt;{ public StringRedisTemplate() { RedisSerializer&lt;String&gt; stringSerializer = new StringRedisSerializer(); setKeySerializer(stringSerializer); setValueSerializer(stringSerializer); setHashKeySerializer(stringSerializer); setHashValueSerializer(stringSerializer); } ...} 即只能存取字符串。尝试执行如下的代码： 1234@AutowiredStringRedisTemplate stringRedisTemplate;stringRedisTemplate.opsForValue().set(&quot;student:2&quot;, &quot;SkYe&quot;); 再同样观察 RedisDesktopManager 中的变化： 由于更换了序列化器，我们得到的结果也不同了。 项目中序列化器使用的注意点理论上，字符串（本质是字节）其实是万能格式，是否可以使用 StringRedisTemplate 将复杂的对象存入 Redis 中，答案当然是肯定的。可以在应用层手动将对象序列化成字符串，如使用 fastjson，jackson 等工具，反序列化时也是通过字符串还原出原来的对象。而如果是用 redisTemplate.opsForValue().set(&quot;student:3&quot;,new Student(3,&quot;kirito&quot;)); 便是依赖于内部的序列化器帮我们完成这样的一个流程，和使用 stringRedisTemplate.opsForValue().set(&quot;student:3&quot;,JSON.toJSONString(new Student(3,&quot;kirito&quot;))); 其实是一个等价的操作。但有两点得时刻记住两点: Redis 只认字节。 使用什么样的序列化器序列化，就必须使用同样的序列化器反序列化。 曾经在 review 代码时发现，项目组的两位同事操作 redis，一个使用了 RedisTemplate，一个使用了 StringRedisTemplate，当他们操作同一个键时，key 虽然相同，但由于序列化器不同，导致无法获取成功。差异虽小，但影响是非常可怕的。 另外一点是，微服务不同模块连接了同一个 Redis，在共享内存中交互数据，可能会由于版本升级，模块差异，导致相互的序列化方案不一致，也会引起问题。如果项目中途切换了序列化方案，也可能会引起 Redis 中老旧持久化数据的反序列化异常，同样需要引起注意。最优的方案自然是在项目初期就统一好序列化方案，所有模块引用同一份依赖，避免不必要的麻烦（或者干脆全部使用默认配置）。 序列化接口 RedisSerializer无论是 RedisTemplate 中默认使用的 JdkSerializationRedisSerializer，还是 StringRedisTemplate 中使用的 StringRedisSerializer 都是实现自统一的接口 RedisSerializer 1234public interface RedisSerializer&lt;T&gt; { byte[] serialize(T t) throws SerializationException; T deserialize(byte[] bytes) throws SerializationException;} 在 spring-data-redis 中提供了其他的默认实现，用于替换默认的序列化方案。 GenericToStringSerializer 依赖于内部的 ConversionService，将所有的类型转存为字符串 GenericJackson2JsonRedisSerializer 和 Jackson2JsonRedisSerializer 以 JSON 的形式序列化对象 OxmSerializer 以 XML 的形式序列化对象 我们可能出于什么样的目的修改序列化器呢？按照个人理解可以总结为以下几点： 各个工程间约定了数据格式，如使用 JSON 等通用数据格式，可以让异构的系统接入 Redis 同样也能识别数据，而 JdkSerializationRedisSerializer 则不具备这样灵活的特性 数据的可视化，在项目初期我曾经偏爱 JSON 序列化，在运维时可以清晰地查看各个 value 的值，非常方便。 效率问题，如果需要将大的对象存入 Value 中，或者 Redis IO 非常频繁，替换合适的序列化器便可以达到优化的效果。 替换默认的序列化器可以将全局的 RedisTemplate 覆盖，也可以在使用时在局部实例化一个 RedisTemplate 替换（不依赖于 IOC 容器）需要根据实际的情况选择替换的方式，以 Jackson2JsonRedisSerializer 为例介绍全局替换的方式： 123456789101112131415161718@Beanpublic RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate redisTemplate = new RedisTemplate(); redisTemplate.setConnectionFactory(redisConnectionFactory); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper();// &lt;1&gt; objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); redisTemplate.setKeySerializer(new StringRedisSerializer()); // &lt;2&gt; redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); // &lt;2&gt; redisTemplate.afterPropertiesSet(); return redisTemplate;} &lt;1&gt; 修改 Jackson 序列化时的默认行为 &lt;2&gt; 手动指定 RedisTemplate 的 Key 和 Value 的序列化器 然后使用 RedisTemplate 进行保存： 12345678910@AutowiredStringRedisTemplate stringRedisTemplate;public void test() { Student student3 = new Student(); student3.setName(&quot;kirito&quot;); student3.setId(&quot;3&quot;); student3.setHobbies(Arrays.asList(&quot;coding&quot;,&quot;write blog&quot;,&quot;eat chicken&quot;)); redisTemplate.opsForValue().set(&quot;student:3&quot;,student3);} 紧接着，去 RedisDesktopManager 中查看结果： 标准的 JSON 格式 实现 Kryo 序列化我们也可以考虑根据自己项目和需求的特点，扩展序列化器，这是非常方便的。比如前面提到的，为了追求性能，可能考虑使用 Kryo 序列化器替换缓慢的 JDK 序列化器，如下是一个参考实现（为了 demo 而写，未经过生产验证） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class KryoRedisSerializer&lt;T&gt; implements RedisSerializer&lt;T&gt; { private final static Logger logger = LoggerFactory.getLogger(KryoRedisSerializer.class); private static final ThreadLocal&lt;Kryo&gt; kryos = new ThreadLocal&lt;Kryo&gt;() { protected Kryo initialValue() { Kryo kryo = new Kryo(); return kryo; }; }; @Override public byte[] serialize(Object obj) throws SerializationException { if (obj == null) { throw new RuntimeException(&quot;serialize param must not be null&quot;); } Kryo kryo = kryos.get(); Output output = new Output(64, -1); try { kryo.writeClassAndObject(output, obj); return output.toBytes(); } finally { closeOutputStream(output); } } @Override public T deserialize(byte[] bytes) throws SerializationException { if (bytes == null) { return null; } Kryo kryo = kryos.get(); Input input = null; try { input = new Input(bytes); return (T) kryo.readClassAndObject(input); } finally { closeInputStream(input); } } private static void closeOutputStream(OutputStream output) { if (output != null) { try { output.flush(); output.close(); } catch (Exception e) { logger.error(&quot;serialize object close outputStream exception&quot;, e); } } } private static void closeInputStream(InputStream input) { if (input != null) { try { input.close(); } catch (Exception e) { logger.error(&quot;serialize object close inputStream exception&quot;, e); } } }} 由于 Kyro 是线程不安全的，所以使用了一个 ThreadLocal 来维护，也可以挑选其他高性能的序列化方案如 Hessian，Protobuf…","link":"/spring-data-redis-2/"},{"title":"Spring Security(一)--Architecture Overview","text":"一直以来我都想写一写 Spring Security 系列的文章，但是整个 Spring Security 体系强大却又繁杂。陆陆续续从最开始的 guides 接触它，到项目中看了一些源码，到最近这个月为了写一写这个系列的文章，阅读了好几遍文档，最终打算尝试一下，写一个较为完整的系列文章。 较为简单或者体量较小的技术，完全可以参考着 demo 直接上手，但系统的学习一门技术则不然。以我的认知，一般的文档大致有两种风格：Architecture First 和 Code First。前者致力于让读者先了解整体的架构，方便我们对自己的认知有一个宏观的把控，而后者以特定的 demo 配合讲解，可以让读者在解决问题的过程中顺便掌握一门技术。关注过我博客或者公众号的朋友会发现，我之前介绍技术的文章，大多数是 Code First，提出一个需求，介绍一个思路，解决一个问题，分析一下源码，大多如此。而学习一个体系的技术，我推荐 Architecture First，正如本文标题所言，这篇文章是我 Spring Security 系列的第一篇，主要是根据 Spring Security 文档选择性 ~~ 翻译 ~~ 整理而成的一个架构概览，配合自己的一些注释方便大家理解。写作本系列文章时，参考版本为 Spring Security 4.2.3.RELEASE。 [TOC] 1 核心组件这一节主要介绍一些在 Spring Security 中常见且核心的 Java 类，它们之间的依赖，构建起了整个框架。想要理解整个架构，最起码得对这些类眼熟。 1.1 SecurityContextHolderSecurityContextHolder 用于存储安全上下文（security context）的信息。当前操作的用户是谁，该用户是否已经被认证，他拥有哪些角色权限… 这些都被保存在 SecurityContextHolder 中。SecurityContextHolder 默认使用 ThreadLocal 策略来存储认证信息。看到 ThreadLocal 也就意味着，这是一种与线程绑定的策略。Spring Security 在用户登录时自动绑定认证信息到当前线程，在用户退出时，自动清除当前线程的认证信息。但这一切的前提，是你在 web 场景下使用 Spring Security，而如果是 Swing 界面，Spring 也提供了支持，SecurityContextHolder 的策略则需要被替换，鉴于我的初衷是基于 web 来介绍 Spring Security，所以这里以及后续，非 web 的相关的内容都一笔带过。 获取当前用户的信息因为身份信息是与线程绑定的，所以可以在程序的任何地方使用静态方法获取用户信息。一个典型的获取当前登录用户的姓名的例子如下所示： 1234567Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();if (principal instanceof UserDetails) {String username = ((UserDetails)principal).getUsername();} else {String username = principal.toString();} getAuthentication()返回了认证信息，再次 getPrincipal() 返回了身份信息，UserDetails 便是 Spring 对身份信息封装的一个接口。Authentication 和 UserDetails 的介绍在下面的小节具体讲解，本节重要的内容是介绍 SecurityContextHolder 这个容器。 1.2 Authentication先看看这个接口的源码长什么样： 123456789101112131415package org.springframework.security.core;// &lt;1&gt;public interface Authentication extends Principal, Serializable { // &lt;1&gt; Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); // &lt;2&gt; Object getCredentials();// &lt;2&gt; Object getDetails();// &lt;2&gt; Object getPrincipal();// &lt;2&gt; boolean isAuthenticated();// &lt;2&gt; void setAuthenticated(boolean var1) throws IllegalArgumentException;} &lt;1&gt; Authentication 是 spring security 包中的接口，直接继承自 Principal 类，而 Principal 是位于 java.security 包中的。可以见得，Authentication 在 spring security 中是最高级别的身份 / 认证的抽象。 &lt;2&gt; 由这个顶级接口，我们可以得到用户拥有的权限信息列表，密码，用户细节信息，用户身份信息，认证信息。 还记得 1.1 节中，authentication.getPrincipal() 返回了一个 Object，我们将 Principal 强转成了 Spring Security 中最常用的 UserDetails，这在 Spring Security 中非常常见，接口返回 Object，使用 instanceof 判断类型，强转成对应的具体实现类。接口详细解读如下： getAuthorities()，权限信息列表，默认是 GrantedAuthority 接口的一些实现类，通常是代表权限信息的一系列字符串。 getCredentials()，密码信息，用户输入的密码字符串，在认证过后通常会被移除，用于保障安全。 getDetails()，细节信息，web 应用中的实现接口通常为 WebAuthenticationDetails，它记录了访问者的 ip 地址和 sessionId 的值。 getPrincipal()，敲黑板！！！最重要的身份信息，大部分情况下返回的是 UserDetails 接口的实现类，也是框架中的常用接口之一。UserDetails 接口将会在下面的小节重点介绍。 Spring Security 是如何完成身份认证的？1 用户名和密码被过滤器获取到，封装成 Authentication, 通常情况下是 UsernamePasswordAuthenticationToken 这个实现类。 2 AuthenticationManager 身份管理器负责验证这个 Authentication 3 认证成功后，AuthenticationManager 身份管理器返回一个被填充满了信息的（包括上面提到的权限信息，身份信息，细节信息，但密码通常会被移除）Authentication 实例。 4 SecurityContextHolder 安全上下文容器将第 3 步填充了信息的 Authentication，通过 SecurityContextHolder.getContext().setAuthentication(…) 方法，设置到其中。 这是一个抽象的认证流程，而整个过程中，如果不纠结于细节，其实只剩下一个 AuthenticationManager 是我们没有接触过的了，这个身份管理器我们在后面的小节介绍。将上述的流程转换成代码，便是如下的流程： 12345678910111213141516171819202122232425262728293031323334353637383940public class AuthenticationExample {private static AuthenticationManager am = new SampleAuthenticationManager();public static void main(String[] args) throws Exception { BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); while(true) { System.out.println(&quot;Please enter your username:&quot;); String name = in.readLine(); System.out.println(&quot;Please enter your password:&quot;); String password = in.readLine(); try { Authentication request = new UsernamePasswordAuthenticationToken(name, password); Authentication result = am.authenticate(request); SecurityContextHolder.getContext().setAuthentication(result); break; } catch(AuthenticationException e) { System.out.println(&quot;Authentication failed:&quot; + e.getMessage()); } } System.out.println(&quot;Successfully authenticated. Security context contains:&quot; + SecurityContextHolder.getContext().getAuthentication());}}class SampleAuthenticationManager implements AuthenticationManager {static final List&lt;GrantedAuthority&gt; AUTHORITIES = new ArrayList&lt;GrantedAuthority&gt;();static { AUTHORITIES.add(new SimpleGrantedAuthority(&quot;ROLE_USER&quot;));}public Authentication authenticate(Authentication auth) throws AuthenticationException { if (auth.getName().equals(auth.getCredentials())) { return new UsernamePasswordAuthenticationToken(auth.getName(), auth.getCredentials(), AUTHORITIES); } throw new BadCredentialsException(&quot;Bad Credentials&quot;);}} 注意：上述这段代码只是为了让大家了解 Spring Security 的工作流程而写的，不是什么源码。在实际使用中，整个流程会变得更加的复杂，但是基本思想，和上述代码如出一辙。 1.3 AuthenticationManager初次接触 Spring Security 的朋友相信会被 AuthenticationManager，ProviderManager ，AuthenticationProvider … 这么多相似的 Spring 认证类搞得晕头转向，但只要稍微梳理一下就可以理解清楚它们的联系和设计者的用意。AuthenticationManager（接口）是认证相关的核心接口，也是发起认证的出发点，因为在实际需求中，我们可能会允许用户使用用户名 + 密码登录，同时允许用户使用邮箱 + 密码，手机号码 + 密码登录，甚至，可能允许用户使用指纹登录（还有这样的操作？没想到吧），所以说 AuthenticationManager 一般不直接认证，AuthenticationManager 接口的常用实现类 ProviderManager 内部会维护一个 List&lt;AuthenticationProvider&gt; 列表，存放多种认证方式，实际上这是委托者模式的应用（Delegate）。也就是说，核心的认证入口始终只有一个：AuthenticationManager，不同的认证方式：用户名 + 密码（UsernamePasswordAuthenticationToken），邮箱 + 密码，手机号码 + 密码登录则对应了三个 AuthenticationProvider。这样一来四不四就好理解多了？熟悉 shiro 的朋友可以把 AuthenticationProvider 理解成 Realm。在默认策略下，只需要通过一个 AuthenticationProvider 的认证，即可被认为是登录成功。 只保留了关键认证部分的 ProviderManager 源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ProviderManager implements AuthenticationManager, MessageSourceAware, InitializingBean { // 维护一个 AuthenticationProvider 列表 private List&lt;AuthenticationProvider&gt; providers = Collections.emptyList(); public Authentication authenticate(Authentication authentication) throws AuthenticationException { Class&lt;? extends Authentication&gt; toTest = authentication.getClass(); AuthenticationException lastException = null; Authentication result = null; // 依次认证 for (AuthenticationProvider provider : getProviders()) { if (!provider.supports(toTest)) { continue; } try { result = provider.authenticate(authentication); if (result != null) { copyDetails(authentication, result); break; } } ... catch (AuthenticationException e) { lastException = e; } } // 如果有 Authentication 信息，则直接返回 if (result != null) { if (eraseCredentialsAfterAuthentication &amp;&amp; (result instanceof CredentialsContainer)) { // 移除密码 ((CredentialsContainer) result).eraseCredentials(); } // 发布登录成功事件 eventPublisher.publishAuthenticationSuccess(result); return result; } ... // 执行到此，说明没有认证成功，包装异常信息 if (lastException == null) { lastException = new ProviderNotFoundException(messages.getMessage( &quot;ProviderManager.providerNotFound&quot;, new Object[] { toTest.getName() }, &quot;No AuthenticationProvider found for {0}&quot;)); } prepareException(lastException, authentication); throw lastException; }} ProviderManager 中的 List，会依照次序去认证，认证成功则立即返回，若认证失败则返回 null，下一个 AuthenticationProvider 会继续尝试认证，如果所有认证器都无法认证成功，则 ProviderManager 会抛出一个 ProviderNotFoundException 异常。 到这里，如果不纠结于 AuthenticationProvider 的实现细节以及安全相关的过滤器，认证相关的核心类其实都已经介绍完毕了：身份信息的存放容器 SecurityContextHolder，身份信息的抽象 Authentication，身份认证器 AuthenticationManager 及其认证流程。姑且在这里做一个分隔线。下面来介绍下 AuthenticationProvider 接口的具体实现。 1.4 DaoAuthenticationProviderAuthenticationProvider 最最最常用的一个实现便是 DaoAuthenticationProvider。顾名思义，Dao 正是数据访问层的缩写，也暗示了这个身份认证器的实现思路。由于本文是一个 Overview，姑且只给出其 UML 类图： 按照我们最直观的思路，怎么去认证一个用户呢？用户前台提交了用户名和密码，而数据库中保存了用户名和密码，认证便是负责比对同一个用户名，提交的密码和保存的密码是否相同便是了。在 Spring Security 中。提交的用户名和密码，被封装成了 UsernamePasswordAuthenticationToken，而根据用户名加载用户的任务则是交给了 UserDetailsService，在 DaoAuthenticationProvider 中，对应的方法便是 retrieveUser，虽然有两个参数，但是 retrieveUser 只有第一个参数起主要作用，返回一个 UserDetails。还需要完成 UsernamePasswordAuthenticationToken 和 UserDetails 密码的比对，这便是交给 additionalAuthenticationChecks 方法完成的，如果这个 void 方法没有抛异常，则认为比对成功。比对密码的过程，用到了 PasswordEncoder 和 SaltSource，密码加密和盐的概念相信不用我赘述了，它们为保障安全而设计，都是比较基础的概念。 如果你已经被这些概念搞得晕头转向了，不妨这么理解 DaoAuthenticationProvider：它获取用户提交的用户名和密码，比对其正确性，如果正确，返回一个数据库中的用户信息（假设用户信息被保存在数据库中）。 1.5 UserDetails 与 UserDetailsService上面不断提到了 UserDetails 这个接口，它代表了最详细的用户信息，这个接口涵盖了一些必要的用户信息字段，具体的实现类对它进行了扩展。 12345678910111213141516public interface UserDetails extends Serializable { Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); String getPassword(); String getUsername(); boolean isAccountNonExpired(); boolean isAccountNonLocked(); boolean isCredentialsNonExpired(); boolean isEnabled();} 它和 Authentication 接口很类似，比如它们都拥有 username，authorities，区分他们也是本文的重点内容之一。Authentication 的 getCredentials()与 UserDetails 中的 getPassword() 需要被区分对待，前者是用户提交的密码凭证，后者是用户正确的密码，认证器其实就是对这两者的比对。Authentication 中的 getAuthorities()实际是由 UserDetails 的 getAuthorities() 传递而形成的。还记得 Authentication 接口中的 getUserDetails() 方法吗？其中的 UserDetails 用户详细信息便是经过了 AuthenticationProvider 之后被填充的。 123public interface UserDetailsService { UserDetails loadUserByUsername(String username) throws UsernameNotFoundException;} UserDetailsService 和 AuthenticationProvider 两者的职责常常被人们搞混，关于他们的问题在文档的 FAQ 和 issues 中屡见不鲜。记住一点即可，敲黑板！！！UserDetailsService 只负责从特定的地方（通常是数据库）加载用户信息，仅此而已，记住这一点，可以避免走很多弯路。UserDetailsService 常见的实现类有 JdbcDaoImpl，InMemoryUserDetailsManager，前者从数据库加载用户，后者从内存中加载用户，也可以自己实现 UserDetailsService，通常这更加灵活。 1.6 架构概览图为了更加形象的理解上述我介绍的这些核心类，附上一张按照我的理解，所画出 Spring Security 的一张非典型的 UML 图 如果对 Spring Security 的这些概念感到理解不能，不用担心，因为这是 Architecture First 导致的必然结果，先过个眼熟。后续的文章会秉持 Code First 的理念，陆续详细地讲解这些实现类的使用场景，源码分析，以及最基本的：如何配置 Spring Security，在后面的文章中可以不时翻看这篇文章，找到具体的类在整个架构中所处的位置，这也是本篇文章的定位。另外，一些 Spring Security 的过滤器还未囊括在架构概览中，如将表单信息包装成 UsernamePasswordAuthenticationToken 的过滤器，考虑到这些虽然也是架构的一部分，但是真正重写他们的可能性较小，所以打算放到后面的章节讲解。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-security-1/"},{"title":"Spring Security(二)--Guides","text":"上一篇文章《Spring Security(一)–Architecture Overview》，我们介绍了 Spring Security 的基础架构，这一节我们通过 Spring 官方给出的一个 guides 例子，来了解 Spring Security 是如何保护我们的应用的，之后会对进行一个解读。 [TOC] 2 Spring Security Guides2.1 引入依赖1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 由于我们集成了 springboot，所以不需要显示的引入 Spring Security 文档中描述 core，config 依赖，只需要引入 spring-boot-starter-security 即可。 2.2 创建一个不受安全限制的 web 应用这是一个首页，不受安全限制 src/main/resources/templates/home.html 1234567891011&lt;!DOCTYPE html&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:sec=&quot;http://www.thymeleaf.org/thymeleaf-extras-springsecurity3&quot;&gt; &lt;head&gt; &lt;title&gt;Spring Security Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome!&lt;/h1&gt; &lt;p&gt;Click &lt;a th:href=&quot;@{/hello}&quot;&gt;here&lt;/a&gt; to see a greeting.&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 这个简单的页面上包含了一个链接，跳转到 “/hello”。对应如下的页面 src/main/resources/templates/hello.html 12345678910&lt;!DOCTYPE html&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:sec=&quot;http://www.thymeleaf.org/thymeleaf-extras-springsecurity3&quot;&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello world!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 接下来配置 Spring MVC，使得我们能够访问到页面。 123456789101112@Configurationpublic class MvcConfig extends WebMvcConfigurerAdapter { @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(&quot;/home&quot;).setViewName(&quot;home&quot;); registry.addViewController(&quot;/&quot;).setViewName(&quot;home&quot;); registry.addViewController(&quot;/hello&quot;).setViewName(&quot;hello&quot;); registry.addViewController(&quot;/login&quot;).setViewName(&quot;login&quot;); }} 2.3 配置 Spring Security一个典型的安全配置如下所示： 12345678910111213141516171819202122232425@Configuration@EnableWebSecurity &lt;1&gt;public class WebSecurityConfig extends WebSecurityConfigurerAdapter { &lt;1&gt; @Override protected void configure(HttpSecurity http) throws Exception { http &lt;2&gt; .authorizeRequests() .antMatchers(&quot;/&quot;, &quot;/home&quot;).permitAll() .anyRequest().authenticated() .and() .formLogin() .loginPage(&quot;/login&quot;) .permitAll() .and() .logout() .permitAll(); } @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception { auth &lt;3&gt; .inMemoryAuthentication() .withUser(&quot;admin&quot;).password(&quot;admin&quot;).roles(&quot;USER&quot;); }} &lt;1&gt; @EnableWebSecurity 注解使得 SpringMVC 集成了 Spring Security 的 web 安全支持。另外，WebSecurityConfig 配置类同时集成了 WebSecurityConfigurerAdapter，重写了其中的特定方法，用于自定义 Spring Security 配置。整个 Spring Security 的工作量，其实都是集中在该配置类，不仅仅是这个 guides，实际项目中也是如此。 &lt;2&gt; configure(HttpSecurity) 定义了哪些 URL 路径应该被拦截，如字面意思所描述：”/“, “/home” 允许所有人访问，”/login” 作为登录入口，也被允许访问，而剩下的 “/hello” 则需要登陆后才可以访问。 &lt;3&gt; configureGlobal(AuthenticationManagerBuilder) 在内存中配置一个用户，admin/admin 分别是用户名和密码，这个用户拥有 USER 角色。 我们目前还没有登录页面，下面创建登录页面： 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:sec=&quot;http://www.thymeleaf.org/thymeleaf-extras-springsecurity3&quot;&gt; &lt;head&gt; &lt;title&gt;Spring Security Example &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div th:if=&quot;${param.error}&quot;&gt; Invalid username and password. &lt;/div&gt; &lt;div th:if=&quot;${param.logout}&quot;&gt; You have been logged out. &lt;/div&gt; &lt;form th:action=&quot;@{/login}&quot; method=&quot;post&quot;&gt; &lt;div&gt;&lt;label&gt; User Name : &lt;input type=&quot;text&quot; name=&quot;username&quot;/&gt; &lt;/label&gt;&lt;/div&gt; &lt;div&gt;&lt;label&gt; Password: &lt;input type=&quot;password&quot; name=&quot;password&quot;/&gt; &lt;/label&gt;&lt;/div&gt; &lt;div&gt;&lt;input type=&quot;submit&quot; value=&quot;Sign In&quot;/&gt;&lt;/div&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 这个 Thymeleaf 模板提供了一个用于提交用户名和密码的表单, 其中 name=”username”，name=”password” 是默认的表单值，并发送到“/ login”。 在默认配置中，Spring Security 提供了一个拦截该请求并验证用户的过滤器。 如果验证失败，该页面将重定向到“/ login?error”，并显示相应的错误消息。 当用户选择注销，请求会被发送到“/ login?logout”。 最后，我们为 hello.html 添加一些内容，用于展示用户信息。 12345678910111213&lt;!DOCTYPE html&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:sec=&quot;http://www.thymeleaf.org/thymeleaf-extras-springsecurity3&quot;&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 th:inline=&quot;text&quot;&gt;Hello [[${#httpServletRequest.remoteUser}]]!&lt;/h1&gt; &lt;form th:action=&quot;@{/logout}&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;Sign Out&quot;/&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 我们使用 Spring Security 之后，HttpServletRequest#getRemoteUser() 可以用来获取用户名。 登出请求将被发送到“/ logout”。 成功注销后，会将用户重定向到“/ login?logout”。 2.4 添加启动类12345678@SpringBootApplicationpublic class Application { public static void main(String[] args) throws Throwable { SpringApplication.run(Application.class, args); }} 2.5 测试访问首页 http://localhost:8080/: 点击 here，尝试访问受限的页面：/hello, 由于未登录，结果被强制跳转到登录也 /login： 输入正确的用户名和密码之后，跳转到之前想要访问的 /hello: 点击 Sign out 退出按钮，访问:/logout, 回到登录页面: 2.6 总结本篇文章没有什么干货，基本算是翻译了 Spring Security Guides 的内容，稍微了解 Spring Security 的朋友都不会对这个翻译感到陌生。考虑到受众的问题，一个入门的例子是必须得有的，方便后续对 Spring Security 的自定义配置进行讲解。下一节，以此 guides 为例，讲解这些最简化的配置背后，Spring Security 都帮我们做了什么工作。 本节所有的代码，可以直接在 Spring 的官方仓库下载得到，git clone https://github.com/spring-guides/gs-securing-web.git。不过，建议初学者根据文章先一步步配置，出了问题，再与 demo 进行对比。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-security-2/"},{"title":"Spring Security(三)-- 核心配置解读","text":"上一篇文章《Spring Security(二)–Guides》，通过 Spring Security 的配置项了解了 Spring Security 是如何保护我们的应用的，本篇文章对上一次的配置做一个分析。 [TOC] 3 核心配置解读3.1 功能介绍这是 Spring Security 入门指南中的配置项： 1234567891011121314151617181920212223242526@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(&quot;/&quot;, &quot;/home&quot;).permitAll() .anyRequest().authenticated() .and() .formLogin() .loginPage(&quot;/login&quot;) .permitAll() .and() .logout() .permitAll(); } @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception { auth .inMemoryAuthentication() .withUser(&quot;admin&quot;).password(&quot;admin&quot;).roles(&quot;USER&quot;); }} 当配置了上述的 javaconfig 之后，我们的应用便具备了如下的功能： 除了“/”,”/home”(首页),”/login”(登录),”/logout”(注销), 之外，其他路径都需要认证。 指定“/login”该路径为登录页面，当未认证的用户尝试访问任何受保护的资源时，都会跳转到“/login”。 默认指定“/logout”为注销页面 配置一个内存中的用户认证器，使用 admin/admin 作为用户名和密码，具有 USER 角色 防止 CSRF 攻击 Session Fixation protection(可以参考我之前讲解 Spring Session 的文章，防止别人篡改 sessionId) Security Header(添加一系列和 Header 相关的控制) HTTP Strict Transport Security for secure requests 集成 X-Content-Type-Options 缓存控制 集成 X-XSS-Protection X-Frame-Options integration to help prevent Clickjacking(iframe 被默认禁止使用) 为 Servlet API 集成了如下的几个方法 HttpServletRequest#getRemoteUser() HttpServletRequest.html#getUserPrincipal() HttpServletRequest.html#isUserInRole(java.lang.String) HttpServletRequest.html#login(java.lang.String, java.lang.String) HttpServletRequest.html#logout() 3.2 @EnableWebSecurity我们自己定义的配置类 WebSecurityConfig 加上了 @EnableWebSecurity 注解，同时继承了 WebSecurityConfigurerAdapter。你可能会在想谁的作用大一点，毫无疑问 @EnableWebSecurity 起到决定性的配置作用，它其实是个组合注解。 1234567@Import({ WebSecurityConfiguration.class, // &lt;2&gt; SpringWebMvcImportSelector.class }) // &lt;1&gt;@EnableGlobalAuthentication // &lt;3&gt;@Configurationpublic @interface EnableWebSecurity { boolean debug() default false;} @Import 是 springboot 提供的用于引入外部的配置的注解，可以理解为：@EnableWebSecurity 注解激活了 @Import 注解中包含的配置类。 &lt;1&gt; SpringWebMvcImportSelector 的作用是判断当前的环境是否包含 springmvc，因为 spring security 可以在非 spring 环境下使用，为了避免 DispatcherServlet 的重复配置，所以使用了这个注解来区分。 &lt;2&gt; WebSecurityConfiguration 顾名思义，是用来配置 web 安全的，下面的小节会详细介绍。 &lt;3&gt; @EnableGlobalAuthentication 注解的源码如下： 1234@Import(AuthenticationConfiguration.class)@Configurationpublic @interface EnableGlobalAuthentication {} 注意点同样在 @Import 之中，它实际上激活了 AuthenticationConfiguration 这样的一个配置类，用来配置认证相关的核心类。 也就是说：@EnableWebSecurity 完成的工作便是加载了 WebSecurityConfiguration，AuthenticationConfiguration 这两个核心配置类，也就此将 spring security 的职责划分为了配置安全信息，配置认证信息两部分。 WebSecurityConfiguration在这个配置类中，有一个非常重要的 Bean 被注册了。 12345678910@Configurationpublic class WebSecurityConfiguration { //DEFAULT_FILTER_NAME = &quot;springSecurityFilterChain&quot; @Bean(name = AbstractSecurityWebApplicationInitializer.DEFAULT_FILTER_NAME) public Filter springSecurityFilterChain() throws Exception { ... } } 在未使用 springboot 之前，大多数人都应该对“springSecurityFilterChain”这个名词不会陌生，他是 spring security 的核心过滤器，是整个认证的入口。在曾经的 XML 配置中，想要启用 spring security，需要在 web.xml 中进行如下配置： 12345678910&lt;!-- Spring Security --&gt; &lt;filter&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 而在 springboot 集成之后，这样的 XML 被 java 配置取代。WebSecurityConfiguration 中完成了声明 springSecurityFilterChain 的作用，并且最终交给 DelegatingFilterProxy 这个代理类，负责拦截请求（注意 DelegatingFilterProxy 这个类不是 spring security 包中的，而是存在于 web 包中，spring 使用了代理模式来实现安全过滤的解耦）。 AuthenticationConfiguration123456789101112131415@Configuration@Import(ObjectPostProcessorConfiguration.class)public class AuthenticationConfiguration { @Bean public AuthenticationManagerBuilder authenticationManagerBuilder( ObjectPostProcessor&lt;Object&gt; objectPostProcessor) { return new AuthenticationManagerBuilder(objectPostProcessor); } public AuthenticationManager getAuthenticationManager() throws Exception { ... }} AuthenticationConfiguration 的主要任务，便是负责生成全局的身份认证管理者 AuthenticationManager。还记得在《Spring Security(一)–Architecture Overview》中，介绍了 Spring Security 的认证体系，AuthenticationManager 便是最核心的身份认证管理器。 3.3 WebSecurityConfigurerAdapter适配器模式在 spring 中被广泛的使用，在配置中使用 Adapter 的好处便是，我们可以选择性的配置想要修改的那一部分配置，而不用覆盖其他不相关的配置。WebSecurityConfigurerAdapter 中我们可以选择自己想要修改的内容，来进行重写，而其提供了三个 configure 重载方法，是我们主要关心的： 由参数就可以知道，分别是对 AuthenticationManagerBuilder，WebSecurity，HttpSecurity 进行个性化的配置。 HttpSecurity 常用配置1234567891011121314151617181920212223242526272829@Configuration@EnableWebSecuritypublic class CustomWebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(&quot;/resources/**&quot;, &quot;/signup&quot;, &quot;/about&quot;).permitAll() .antMatchers(&quot;/admin/**&quot;).hasRole(&quot;ADMIN&quot;) .antMatchers(&quot;/db/**&quot;).access(&quot;hasRole('ADMIN') and hasRole('DBA')&quot;) .anyRequest().authenticated() .and() .formLogin() .usernameParameter(&quot;username&quot;) .passwordParameter(&quot;password&quot;) .failureForwardUrl(&quot;/login?error&quot;) .loginPage(&quot;/login&quot;) .permitAll() .and() .logout() .logoutUrl(&quot;/logout&quot;) .logoutSuccessUrl(&quot;/index&quot;) .permitAll() .and() .httpBasic() .disable(); }} 上述是一个使用 Java Configuration 配置 HttpSecurity 的典型配置，其中 http 作为根开始配置，每一个 and()对应了一个模块的配置（等同于 xml 配置中的结束标签），并且 and() 返回了 HttpSecurity 本身，于是可以连续进行配置。他们配置的含义也非常容易通过变量本身来推测， authorizeRequests() 配置路径拦截，表明路径访问所对应的权限，角色，认证信息。 formLogin() 对应表单认证相关的配置 logout() 对应了注销相关的配置 httpBasic() 可以配置 basic 登录 etc 他们分别代表了 http 请求相关的安全配置，这些配置项无一例外的返回了 Configurer 类，而所有的 http 相关配置可以通过查看 HttpSecurity 的主要方法得知： 需要对 http 协议有一定的了解才能完全掌握所有的配置，不过，springboot 和 spring security 的自动配置已经足够使用了。其中每一项 Configurer（e.g.FormLoginConfigurer,CsrfConfigurer）都是 HttpConfigurer 的细化配置项。 WebSecurityBuilder12345678910@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override public void configure(WebSecurity web) throws Exception { web .ignoring() .antMatchers(&quot;/resources/**&quot;); }} 以笔者的经验，这个配置中并不会出现太多的配置信息。 AuthenticationManagerBuilder1234567891011@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth .inMemoryAuthentication() .withUser(&quot;admin&quot;).password(&quot;admin&quot;).roles(&quot;USER&quot;); }} 想要在 WebSecurityConfigurerAdapter 中进行认证相关的配置，可以使用 configure(AuthenticationManagerBuilder auth) 暴露一个 AuthenticationManager 的建造器：AuthenticationManagerBuilder 。如上所示，我们便完成了内存中用户的配置。 细心的朋友会发现，在前面的文章中我们配置内存中的用户时，似乎不是这么配置的，而是： 1234567891011@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception { auth .inMemoryAuthentication() .withUser(&quot;admin&quot;).password(&quot;admin&quot;).roles(&quot;USER&quot;); }} 如果你的应用只有唯一一个 WebSecurityConfigurerAdapter，那么他们之间的差距可以被忽略，从方法名可以看出两者的区别：使用 @Autowired 注入的 AuthenticationManagerBuilder 是全局的身份认证器，作用域可以跨越多个 WebSecurityConfigurerAdapter，以及影响到基于 Method 的安全控制；而 protected configure() 的方式则类似于一个匿名内部类，它的作用域局限于一个 WebSecurityConfigurerAdapter 内部。关于这一点的区别，可以参考我曾经提出的 issuespring-security#issues4571。官方文档中，也给出了配置多个 WebSecurityConfigurerAdapter 的场景以及 demo，将在该系列的后续文章中解读。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-security-3/"},{"title":"Spring Security(四)-- 核心过滤器源码分析","text":"前面的部分，我们关注了 Spring Security 是如何完成认证工作的，但是另外一部分核心的内容：过滤器，一直没有提到，我们已经知道 Spring Security 使用了 springSecurityFillterChian 作为了安全过滤的入口，这一节主要分析一下这个过滤器链都包含了哪些关键的过滤器，并且各自的使命是什么。 4 过滤器详解4.1 核心过滤器概述由于过滤器链路中的过滤较多，即使是 Spring Security 的官方文档中也并未对所有的过滤器进行介绍，在之前，《Spring Security(二)–Guides》入门指南中我们配置了一个表单登录的 demo，以此为例，来看看这过程中 Spring Security 都帮我们自动配置了哪些过滤器。 123456789101112Creating filter chain: o.s.s.web.util.matcher.AnyRequestMatcher@1, [o.s.s.web.context.SecurityContextPersistenceFilter@8851ce1, o.s.s.web.header.HeaderWriterFilter@6a472566, o.s.s.web.csrf.CsrfFilter@61cd1c71, o.s.s.web.authentication.logout.LogoutFilter@5e1d03d7, o.s.s.web.authentication.UsernamePasswordAuthenticationFilter@122d6c22, o.s.s.web.savedrequest.RequestCacheAwareFilter@5ef6fd7f, o.s.s.web.servletapi.SecurityContextHolderAwareRequestFilter@4beaf6bd, o.s.s.web.authentication.AnonymousAuthenticationFilter@6edcad64, o.s.s.web.session.SessionManagementFilter@5e65afb6, o.s.s.web.access.ExceptionTranslationFilter@5b9396d3, o.s.s.web.access.intercept.FilterSecurityInterceptor@3c5dbdf8] 上述的 log 信息是我从 springboot 启动的日志中 CV 所得，spring security 的过滤器日志有一个特点：log 打印顺序与实际配置顺序符合，也就意味着 SecurityContextPersistenceFilter 是整个过滤器链的第一个过滤器，而 FilterSecurityInterceptor 则是末置的过滤器。另外通过观察过滤器的名称，和所在的包名，可以大致地分析出他们各自的作用，如 UsernamePasswordAuthenticationFilter 明显便是与使用用户名和密码登录相关的过滤器，而 FilterSecurityInterceptor 我们似乎看不出它的作用，但是其位于 web.access 包下，大致可以分析出他与访问限制相关。第四篇文章主要就是介绍这些常用的过滤器，对其中关键的过滤器进行一些源码分析。先大致介绍下每个过滤器的作用： SecurityContextPersistenceFilter 两个主要职责：请求来临时，创建 SecurityContext 安全上下文信息，请求结束时清空 SecurityContextHolder。 HeaderWriterFilter (文档中并未介绍，非核心过滤器) 用来给 http 响应添加一些 Header, 比如 X-Frame-Options, X-XSS-Protection*，X-Content-Type-Options. CsrfFilter 在 spring4 这个版本中被默认开启的一个过滤器，用于防止 csrf 攻击，了解前后端分离的人一定不会对这个攻击方式感到陌生，前后端使用 json 交互需要注意的一个问题。 LogoutFilter 顾名思义，处理注销的过滤器 UsernamePasswordAuthenticationFilter 这个会重点分析，表单提交了 username 和 password，被封装成 token 进行一系列的认证，便是主要通过这个过滤器完成的，在表单认证的方法中，这是最最关键的过滤器。 RequestCacheAwareFilter (文档中并未介绍，非核心过滤器) 内部维护了一个 RequestCache，用于缓存 request 请求 SecurityContextHolderAwareRequestFilter 此过滤器对 ServletRequest 进行了一次包装，使得 request 具有更加丰富的 API AnonymousAuthenticationFilter 匿名身份过滤器，这个过滤器个人认为很重要，需要将它与 UsernamePasswordAuthenticationFilter 放在一起比较理解，spring security 为了兼容未登录的访问，也走了一套认证流程，只不过是一个匿名的身份。 SessionManagementFilter 和 session 相关的过滤器，内部维护了一个 SessionAuthenticationStrategy，两者组合使用，常用来防止 session-fixation protection attack，以及限制同一用户开启多个会话的数量 ExceptionTranslationFilter 直译成异常翻译过滤器，还是比较形象的，这个过滤器本身不处理异常，而是将认证过程中出现的异常交给内部维护的一些类去处理，具体是那些类下面详细介绍 FilterSecurityInterceptor 这个过滤器决定了访问特定路径应该具备的权限，访问的用户的角色，权限是什么？访问的路径需要什么样的角色和权限？这些判断和处理都是由该类进行的。 其中加粗的过滤器可以被认为是 Spring Security 的核心过滤器，将在下面，一个过滤器对应一个小节来讲解。 4.2 SecurityContextPersistenceFilter试想一下，如果我们不使用 Spring Security，如果保存用户信息呢，大多数情况下会考虑使用 Session 对吧？在 Spring Security 中也是如此，用户在登录过一次之后，后续的访问便是通过 sessionId 来识别，从而认为用户已经被认证。具体在何处存放用户信息，便是第一篇文章中提到的 SecurityContextHolder；认证相关的信息是如何被存放到其中的，便是通过 SecurityContextPersistenceFilter。在 4.1 概述中也提到了，SecurityContextPersistenceFilter 的两个主要作用便是请求来临时，创建 SecurityContext 安全上下文信息和请求结束时清空 SecurityContextHolder。顺带提一下：微服务的一个设计理念需要实现服务通信的无状态，而 http 协议中的无状态意味着不允许存在 session，这可以通过 setAllowSessionCreation(false) 实现，这并不意味着 SecurityContextPersistenceFilter 变得无用，因为它还需要负责清除用户信息。在 Spring Security 中，虽然安全上下文信息被存储于 Session 中，但我们在实际使用中不应该直接操作 Session，而应当使用 SecurityContextHolder。 源码分析org.springframework.security.web.context.SecurityContextPersistenceFilter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class SecurityContextPersistenceFilter extends GenericFilterBean { static final String FILTER_APPLIED = &quot;__spring_security_scpf_applied&quot;; // 安全上下文存储的仓库 private SecurityContextRepository repo; public SecurityContextPersistenceFilter() { //HttpSessionSecurityContextRepository 是 SecurityContextRepository 接口的一个实现类 // 使用 HttpSession 来存储 SecurityContext this(new HttpSessionSecurityContextRepository()); } public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; if (request.getAttribute(FILTER_APPLIED) != null) { // ensure that filter is only applied once per request chain.doFilter(request, response); return; } request.setAttribute(FILTER_APPLIED, Boolean.TRUE); // 包装 request，response HttpRequestResponseHolder holder = new HttpRequestResponseHolder(request, response); // 从 Session 中获取安全上下文信息 SecurityContext contextBeforeChainExecution = repo.loadContext(holder); try { // 请求开始时，设置安全上下文信息，这样就避免了用户直接从 Session 中获取安全上下文信息 SecurityContextHolder.setContext(contextBeforeChainExecution); chain.doFilter(holder.getRequest(), holder.getResponse()); } finally { // 请求结束后，清空安全上下文信息 SecurityContext contextAfterChainExecution = SecurityContextHolder .getContext(); SecurityContextHolder.clearContext(); repo.saveContext(contextAfterChainExecution, holder.getRequest(), holder.getResponse()); request.removeAttribute(FILTER_APPLIED); if (debug) { logger.debug(&quot;SecurityContextHolder now cleared, as request processing completed&quot;); } } }} 过滤器一般负责核心的处理流程，而具体的业务实现，通常交给其中聚合的其他实体类，这在 Filter 的设计中很常见，同时也符合职责分离模式。例如存储安全上下文和读取安全上下文的工作完全委托给了 HttpSessionSecurityContextRepository 去处理，而这个类中也有几个方法可以稍微解读下，方便我们理解内部的工作流程 org.springframework.security.web.context.HttpSessionSecurityContextRepository 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class HttpSessionSecurityContextRepository implements SecurityContextRepository { // 'SPRING_SECURITY_CONTEXT' 是安全上下文默认存储在 Session 中的键值 public static final String SPRING_SECURITY_CONTEXT_KEY = &quot;SPRING_SECURITY_CONTEXT&quot;; ... private final Object contextObject = SecurityContextHolder.createEmptyContext(); private boolean allowSessionCreation = true; private boolean disableUrlRewriting = false; private String springSecurityContextKey = SPRING_SECURITY_CONTEXT_KEY; private AuthenticationTrustResolver trustResolver = new AuthenticationTrustResolverImpl(); // 从当前 request 中取出安全上下文，如果 session 为空，则会返回一个新的安全上下文 public SecurityContext loadContext(HttpRequestResponseHolder requestResponseHolder) { HttpServletRequest request = requestResponseHolder.getRequest(); HttpServletResponse response = requestResponseHolder.getResponse(); HttpSession httpSession = request.getSession(false); SecurityContext context = readSecurityContextFromSession(httpSession); if (context == null) { context = generateNewContext(); } ... return context; } ... public boolean containsContext(HttpServletRequest request) { HttpSession session = request.getSession(false); if (session == null) { return false; } return session.getAttribute(springSecurityContextKey) != null; } private SecurityContext readSecurityContextFromSession(HttpSession httpSession) { if (httpSession == null) { return null; } ... // Session 存在的情况下，尝试获取其中的 SecurityContext Object contextFromSession = httpSession.getAttribute(springSecurityContextKey); if (contextFromSession == null) { return null; } ... return (SecurityContext) contextFromSession; } // 初次请求时创建一个新的 SecurityContext 实例 protected SecurityContext generateNewContext() { return SecurityContextHolder.createEmptyContext(); }} SecurityContextPersistenceFilter 和 HttpSessionSecurityContextRepository 配合使用，构成了 Spring Security 整个调用链路的入口，为什么将它放在最开始的地方也是显而易见的，后续的过滤器中大概率会依赖 Session 信息和安全上下文信息。 4.3 UsernamePasswordAuthenticationFilter表单认证是最常用的一个认证方式，一个最直观的业务场景便是允许用户在表单中输入用户名和密码进行登录，而这背后的 UsernamePasswordAuthenticationFilter，在整个 Spring Security 的认证体系中则扮演着至关重要的角色。 上述的时序图，可以看出 UsernamePasswordAuthenticationFilter 主要肩负起了调用身份认证器，校验身份的作用，至于认证的细节，在前面几章花了很大篇幅进行了介绍，到这里，其实 Spring Security 的基本流程就已经走通了。 源码分析org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter#attemptAuthentication 123456789101112131415public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException { // 获取表单中的用户名和密码 String username = obtainUsername(request); String password = obtainPassword(request); ... username = username.trim(); // 组装成 username+password 形式的 token UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken( username, password); // Allow subclasses to set the &quot;details&quot; property setDetails(request, authRequest); // 交给内部的 AuthenticationManager 去认证，并返回认证信息 return this.getAuthenticationManager().authenticate(authRequest);} UsernamePasswordAuthenticationFilter 本身的代码只包含了上述这么一个方法，非常简略，而在其父类 AbstractAuthenticationProcessingFilter 中包含了大量的细节，值得我们分析： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public abstract class AbstractAuthenticationProcessingFilter extends GenericFilterBean implements ApplicationEventPublisherAware, MessageSourceAware { // 包含了一个身份认证器 private AuthenticationManager authenticationManager; // 用于实现 remeberMe private RememberMeServices rememberMeServices = new NullRememberMeServices(); private RequestMatcher requiresAuthenticationRequestMatcher; // 这两个 Handler 很关键，分别代表了认证成功和失败相应的处理器 private AuthenticationSuccessHandler successHandler = new SavedRequestAwareAuthenticationSuccessHandler(); private AuthenticationFailureHandler failureHandler = new SimpleUrlAuthenticationFailureHandler(); public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; ... Authentication authResult; try { // 此处实际上就是调用 UsernamePasswordAuthenticationFilter 的 attemptAuthentication 方法 authResult = attemptAuthentication(request, response); if (authResult == null) { // 子类未完成认证，立刻返回 return; } sessionStrategy.onAuthentication(authResult, request, response); } // 在认证过程中可以直接抛出异常，在过滤器中，就像此处一样，进行捕获 catch (InternalAuthenticationServiceException failed) { // 内部服务异常 unsuccessfulAuthentication(request, response, failed); return; } catch (AuthenticationException failed) { // 认证失败 unsuccessfulAuthentication(request, response, failed); return; } // 认证成功 if (continueChainBeforeSuccessfulAuthentication) { chain.doFilter(request, response); } // 注意，认证成功后过滤器把 authResult 结果也传递给了成功处理器 successfulAuthentication(request, response, chain, authResult); } } 整个流程理解起来也并不难，主要就是内部调用了 authenticationManager 完成认证，根据认证结果执行 successfulAuthentication 或者 unsuccessfulAuthentication，无论成功失败，一般的实现都是转发或者重定向等处理，不再细究 AuthenticationSuccessHandler 和 AuthenticationFailureHandler，有兴趣的朋友，可以去看看两者的实现类。 4.4 AnonymousAuthenticationFilter匿名认证过滤器，可能有人会想：匿名了还有身份？我自己对于 Anonymous 匿名身份的理解是 Spirng Security 为了整体逻辑的统一性，即使是未通过认证的用户，也给予了一个匿名身份。而 AnonymousAuthenticationFilter 该过滤器的位置也是非常的科学的，它位于常用的身份认证过滤器（如 UsernamePasswordAuthenticationFilter、BasicAuthenticationFilter、RememberMeAuthenticationFilter）之后，意味着只有在上述身份过滤器执行完毕后，SecurityContext 依旧没有用户信息，AnonymousAuthenticationFilter 该过滤器才会有意义 —- 基于用户一个匿名身份。 源码分析org.springframework.security.web.authentication.AnonymousAuthenticationFilter 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class AnonymousAuthenticationFilter extends GenericFilterBean implements InitializingBean { private AuthenticationDetailsSource&lt;HttpServletRequest, ?&gt; authenticationDetailsSource = new WebAuthenticationDetailsSource(); private String key; private Object principal; private List&lt;GrantedAuthority&gt; authorities; // 自动创建一个 &quot;anonymousUser&quot; 的匿名用户, 其具有 ANONYMOUS 角色 public AnonymousAuthenticationFilter(String key) { this(key, &quot;anonymousUser&quot;, AuthorityUtils.createAuthorityList(&quot;ROLE_ANONYMOUS&quot;)); } /** * * @param key key 用来识别该过滤器创建的身份 * @param principal principal 代表匿名用户的身份 * @param authorities authorities 代表匿名用户的权限集合 */ public AnonymousAuthenticationFilter(String key, Object principal, List&lt;GrantedAuthority&gt; authorities) { Assert.hasLength(key, &quot;key cannot be null or empty&quot;); Assert.notNull(principal, &quot;Anonymous authentication principal must be set&quot;); Assert.notNull(authorities, &quot;Anonymous authorities must be set&quot;); this.key = key; this.principal = principal; this.authorities = authorities; } ... public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException { // 过滤器链都执行到匿名认证过滤器这儿了还没有身份信息，塞一个匿名身份进去 if (SecurityContextHolder.getContext().getAuthentication()== null) { SecurityContextHolder.getContext().setAuthentication( createAuthentication((HttpServletRequest) req)); } chain.doFilter(req, res); } protected Authentication createAuthentication(HttpServletRequest request) { // 创建一个 AnonymousAuthenticationToken AnonymousAuthenticationToken auth = new AnonymousAuthenticationToken(key, principal, authorities); auth.setDetails(authenticationDetailsSource.buildDetails(request)); return auth; } ...} 其实对比 AnonymousAuthenticationFilter 和 UsernamePasswordAuthenticationFilter 就可以发现一些门道了，UsernamePasswordAuthenticationToken 对应 AnonymousAuthenticationToken，他们都是 Authentication 的实现类，而 Authentication 则是被 SecurityContextHolder(SecurityContext) 持有的，一切都被串联在了一起。 4.5 ExceptionTranslationFilterExceptionTranslationFilter 异常转换过滤器位于整个 springSecurityFilterChain 的后方，用来转换整个链路中出现的异常，将其转化，顾名思义，转化以意味本身并不处理。一般其只处理两大类异常：AccessDeniedException 访问异常和 AuthenticationException 认证异常。 这个过滤器非常重要，因为它将 Java 中的异常和 HTTP 的响应连接在了一起，这样在处理异常时，我们不用考虑密码错误该跳到什么页面，账号锁定该如何，只需要关注自己的业务逻辑，抛出相应的异常便可。如果该过滤器检测到 AuthenticationException，则将会交给内部的 AuthenticationEntryPoint 去处理，如果检测到 AccessDeniedException，需要先判断当前用户是不是匿名用户，如果是匿名访问，则和前面一样运行 AuthenticationEntryPoint，否则会委托给 AccessDeniedHandler 去处理，而 AccessDeniedHandler 的默认实现，是 AccessDeniedHandlerImpl。所以 ExceptionTranslationFilter 内部的 AuthenticationEntryPoint 是至关重要的，顾名思义：认证的入口点。 源码分析1234567891011121314151617181920212223242526272829public class ExceptionTranslationFilter extends GenericFilterBean { // 处理异常转换的核心方法 private void handleSpringSecurityException(HttpServletRequest request, HttpServletResponse response, FilterChain chain, RuntimeException exception) throws IOException, ServletException { if (exception instanceof AuthenticationException) { // 重定向到登录端点 sendStartAuthentication(request, response, chain, (AuthenticationException) exception); } else if (exception instanceof AccessDeniedException) { Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); if (authenticationTrustResolver.isAnonymous(authentication) || authenticationTrustResolver.isRememberMe(authentication)) { // 重定向到登录端点 sendStartAuthentication( request, response, chain, new InsufficientAuthenticationException( &quot;Full authentication is required to access this resource&quot;)); } else { // 交给 accessDeniedHandler 处理 accessDeniedHandler.handle(request, response, (AccessDeniedException) exception); } } }} 剩下的便是要搞懂 AuthenticationEntryPoint 和 AccessDeniedHandler 就可以了。 选择了几个常用的登录端点，以其中第一个为例来介绍，看名字就能猜到是认证失败之后，让用户跳转到登录页面。还记得我们一开始怎么配置表单登录页面的吗？ 12345678910111213141516171819@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(&quot;/&quot;, &quot;/home&quot;).permitAll() .anyRequest().authenticated() .and() .formLogin()//FormLoginConfigurer .loginPage(&quot;/login&quot;) .permitAll() .and() .logout() .permitAll(); }} 我们顺着 formLogin 返回的 FormLoginConfigurer 往下找，看看能发现什么，最终在 FormLoginConfigurer 的父类 AbstractAuthenticationFilterConfigurer 中有了不小的收获： 12345678public abstract class AbstractAuthenticationFilterConfigurer extends ...{ ... //formLogin 不出所料配置了 AuthenticationEntryPoint private LoginUrlAuthenticationEntryPoint authenticationEntryPoint; // 认证失败的处理器 private AuthenticationFailureHandler failureHandler; ...} 具体如何配置的就不看了，我们得出了结论，formLogin() 配置了之后最起码做了两件事，其一，为 UsernamePasswordAuthenticationFilter 设置了相关的配置，其二配置了 AuthenticationEntryPoint。 登录端点还有 Http401AuthenticationEntryPoint，Http403ForbiddenEntryPoint 这些都是很简单的实现，有时候我们访问受限页面，又没有配置登录，就看到了一个空荡荡的默认错误页面，上面显示着 401,403，就是这两个入口起了作用。 还剩下一个 AccessDeniedHandler 访问决策器未被讲解，简单提一下：AccessDeniedHandlerImpl 这个默认实现类会根据 errorPage 和状态码来判断，最终决定跳转的页面 org.springframework.security.web.access.AccessDeniedHandlerImpl#handle 1234567891011121314151617181920public void handle(HttpServletRequest request, HttpServletResponse response, AccessDeniedException accessDeniedException) throws IOException, ServletException { if (!response.isCommitted()) { if (errorPage != null) { // Put exception into request scope (perhaps of use to a view) request.setAttribute(WebAttributes.ACCESS_DENIED_403, accessDeniedException); // Set the 403 status code. response.setStatus(HttpServletResponse.SC_FORBIDDEN); // forward to error page. RequestDispatcher dispatcher = request.getRequestDispatcher(errorPage); dispatcher.forward(request, response); } else { response.sendError(HttpServletResponse.SC_FORBIDDEN, accessDeniedException.getMessage()); } }} 4.6 FilterSecurityInterceptor想想整个认证安全控制流程还缺了什么？我们已经有了认证，有了请求的封装，有了 Session 的关联… 还缺一个：由什么控制哪些资源是受限的，这些受限的资源需要什么权限，需要什么角色… 这一切和访问控制相关的操作，都是由 FilterSecurityInterceptor 完成的。 FilterSecurityInterceptor 的工作流程用笔者的理解可以理解如下：FilterSecurityInterceptor 从 SecurityContextHolder 中获取 Authentication 对象，然后比对用户拥有的权限和资源所需的权限。前者可以通过 Authentication 对象直接获得，而后者则需要引入我们之前一直未提到过的两个类：SecurityMetadataSource，AccessDecisionManager。理解清楚决策管理器的整个创建流程和 SecurityMetadataSource 的作用需要花很大一笔功夫，这里，暂时只介绍其大概的作用。 在 JavaConfig 的配置中，我们通常如下配置路径的访问控制： 12345678910111213141516@Overrideprotected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(&quot;/resources/**&quot;, &quot;/signup&quot;, &quot;/about&quot;).permitAll() .antMatchers(&quot;/admin/**&quot;).hasRole(&quot;ADMIN&quot;) .antMatchers(&quot;/db/**&quot;).access(&quot;hasRole('ADMIN') and hasRole('DBA')&quot;) .anyRequest().authenticated() .withObjectPostProcessor(new ObjectPostProcessor&lt;FilterSecurityInterceptor&gt;() { public &lt;O extends FilterSecurityInterceptor&gt; O postProcess( O fsi) { fsi.setPublishAuthorizationSuccess(true); return fsi; } });} 在 ObjectPostProcessor 的泛型中看到了 FilterSecurityInterceptor，以笔者的经验，目前并没有太多机会需要修改 FilterSecurityInterceptor 的配置。 总结本篇文章在介绍过滤器时，顺便进行了一些源码的分析，目的是方便理解整个 Spring Security 的工作流。伴随着整个过滤器链的介绍，安全框架的轮廓应该已经浮出水面了，下面的章节，主要打算通过自定义一些需求，再次分析其他组件的源码，学习应该如何改造 Spring Security，为我们所用。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-security-4/"},{"title":"Spring Security(五)-- 动手实现一个 IP_Login","text":"在开始这篇文章之前，我们似乎应该思考下为什么需要搞清楚 Spring Security 的内部工作原理？按照第二篇文章中的配置，一个简单的表单认证不就达成了吗？更有甚者，为什么我们不自己写一个表单认证，用过滤器即可完成，大费周章引入 Spring Security，看起来也并没有方便多少。对的，在引入 Spring Security 之前，我们得首先想到，是什么需求让我们引入了 Spring Security，以及为什么是 Spring Security，而不是 shiro 等等其他安全框架。我的理解是有如下几点： 1 在前文的介绍中，Spring Security 支持防止 csrf 攻击，session-fixation protection，支持表单认证，basic 认证，rememberMe… 等等一些特性，有很多是开箱即用的功能，而大多特性都可以通过配置灵活的变更，这是它的强大之处。 2 Spring Security 的兄弟的项目 Spring Security SSO，OAuth2 等支持了多种协议，而这些都是基于 Spring Security 的，方便了项目的扩展。 3 SpringBoot 的支持，更加保证了 Spring Security 的开箱即用。 4 为什么需要理解其内部工作原理? 一个有自我追求的程序员都不会满足于浅尝辄止，如果一个开源技术在我们的日常工作中十分常用，那么我偏向于阅读其源码，这样可以让我们即使排查不期而至的问题，也方便日后需求扩展。 5 Spring 及其子项目的官方文档是我见过的最良心的文档！~~ 相比较于 Apache 的部分文档 ~~ 这一节，为了对之前分析的 Spring Security 源码和组件有一个清晰的认识，介绍一个使用 IP 完成登录的简单 demo。 5 动手实现一个 IP_Login5.1 定义需求在表单登录中，一般使用数据库中配置的用户表，权限表，角色表，权限组表… 这取决于你的权限粒度，但本质都是借助了一个持久化存储，维护了用户的角色权限，而后给出一个 /login 作为登录端点，使用表单提交用户名和密码，而后完成登录后可自由访问受限页面。 在我们的 IP 登录 demo 中，也是类似的，使用 IP 地址作为身份，内存中的一个 ConcurrentHashMap 维护 IP 地址和权限的映射，如果在认证时找不到相应的权限，则认为认证失败。 实际上，在表单登录中，用户的 IP 地址已经被存放在 Authentication.getDetails() 中了，完全可以只重写一个 AuthenticationProvider 认证这个 IP 地址即可，但是，本 demo 是为了厘清 Spring Security 内部工作原理而设置，为了设计到更多的类，我完全重写了 IP 过滤器。 5.2 设计概述我们的参考完全是表单认证，在之前章节中，已经了解了表单认证相关的核心流程，将此图再贴一遍： 在 IP 登录的 demo 中，使用 IpAuthenticationProcessingFilter 拦截 IP 登录请求，同样使用 ProviderManager 作为全局 AuthenticationManager 接口的实现类，将 ProviderManager 内部的 DaoAuthenticationProvider 替换为 IpAuthenticationProvider，而 UserDetailsService 则使用一个 ConcurrentHashMap 代替。更详细一点的设计： IpAuthenticationProcessingFilter–&gt;UsernamePasswordAuthenticationFilter IpAuthenticationToken–&gt;UsernamePasswordAuthenticationToken ProviderManager–&gt;ProviderManager IpAuthenticationProvider–&gt;DaoAuthenticationProvider ConcurrentHashMap–&gt;UserDetailsService 5.3 IpAuthenticationToken123456789101112131415161718192021222324252627282930313233343536public class IpAuthenticationToken extends AbstractAuthenticationToken { private String ip; public String getIp() { return ip; } public void setIp(String ip) { this.ip = ip; } public IpAuthenticationToken(String ip) { super(null); this.ip = ip; super.setAuthenticated(false);// 注意这个构造方法是认证时使用的 } public IpAuthenticationToken(String ip, Collection&lt;? extends GrantedAuthority&gt; authorities) { super(authorities); this.ip = ip; super.setAuthenticated(true);// 注意这个构造方法是认证成功后使用的 } @Override public Object getCredentials() { return null; } @Override public Object getPrincipal() { return this.ip; }} 两个构造方法需要引起我们的注意，这里设计的用意是模仿的 UsernamePasswordAuthenticationToken，第一个构造器是用于认证之前，传递给认证器使用的，所以只有 IP 地址，自然是未认证；第二个构造器用于认证成功之后，封装认证用户的信息，此时需要将权限也设置到其中，并且 setAuthenticated(true)。这样的设计在诸多的 Token 类设计中很常见。 5.4 IpAuthenticationProcessingFilter1234567891011121314public class IpAuthenticationProcessingFilter extends AbstractAuthenticationProcessingFilter { // 使用 /ipVerify 该端点进行 ip 认证 IpAuthenticationProcessingFilter() { super(new AntPathRequestMatcher(&quot;/ipVerify&quot;)); } @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException { // 获取 host 信息 String host = request.getRemoteHost(); // 交给内部的 AuthenticationManager 去认证，实现解耦 return getAuthenticationManager().authenticate(new IpAuthenticationToken(host)); }} AbstractAuthenticationProcessingFilter 这个过滤器在前面一节介绍过，是 UsernamePasswordAuthenticationFilter 的父类，我们的 IpAuthenticationProcessingFilter 也继承了它 构造器中传入了 /ipVerify 作为 IP 登录的端点 attemptAuthentication() 方法中加载请求的 IP 地址，之后交给内部的 AuthenticationManager 去认证 5.5 IpAuthenticationProvider123456789101112131415161718192021222324252627282930public class IpAuthenticationProvider implements AuthenticationProvider { final static Map&lt;String, SimpleGrantedAuthority&gt; ipAuthorityMap = new ConcurrenHashMap(); // 维护一个 ip 白名单列表，每个 ip 对应一定的权限 static { ipAuthorityMap.put(&quot;127.0.0.1&quot;, new SimpleGrantedAuthority(&quot;ADMIN&quot;)); ipAuthorityMap.put(&quot;10.236.69.103&quot;, new SimpleGrantedAuthority(&quot;ADMIN&quot;)); ipAuthorityMap.put(&quot;10.236.69.104&quot;, new SimpleGrantedAuthority(&quot;FRIEND&quot;)); } @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { IpAuthenticationToken ipAuthenticationToken = (IpAuthenticationToken) authentication; String ip = ipAuthenticationToken.getIp(); SimpleGrantedAuthority simpleGrantedAuthority = ipAuthorityMap.get(ip); // 不在白名单列表中 if (simpleGrantedAuthority == null) { return null; } else { // 封装权限信息，并且此时身份已经被认证 return new IpAuthenticationToken(ip, Arrays.asList(simpleGrantedAuthority)); } } // 只支持 IpAuthenticationToken 该身份 @Override public boolean supports(Class&lt;?&gt; authentication) { return (IpAuthenticationToken.class .isAssignableFrom(authentication)); }} return new IpAuthenticationToken(ip, Arrays.asList(simpleGrantedAuthority)); 使用了 IpAuthenticationToken 的第二个构造器，返回了一个已经经过认证的 IpAuthenticationToken。 5.6 配置 WebSecurityConfigAdapter1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter { //ip 认证者配置 @Bean IpAuthenticationProvider ipAuthenticationProvider() { return new IpAuthenticationProvider(); } // 配置封装 ipAuthenticationToken 的过滤器 IpAuthenticationProcessingFilter ipAuthenticationProcessingFilter(AuthenticationManager authenticationManager) { IpAuthenticationProcessingFilter ipAuthenticationProcessingFilter = new IpAuthenticationProcessingFilter(); // 为过滤器添加认证器 ipAuthenticationProcessingFilter.setAuthenticationManager(authenticationManager); // 重写认证失败时的跳转页面 ipAuthenticationProcessingFilter.setAuthenticationFailureHandler(new SimpleUrlAuthenticationFailureHandler(&quot;/ipLogin?error&quot;)); return ipAuthenticationProcessingFilter; } // 配置登录端点 @Bean LoginUrlAuthenticationEntryPoint loginUrlAuthenticationEntryPoint(){ LoginUrlAuthenticationEntryPoint loginUrlAuthenticationEntryPoint = new LoginUrlAuthenticationEntryPoint (&quot;/ipLogin&quot;); return loginUrlAuthenticationEntryPoint; } @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(&quot;/&quot;, &quot;/home&quot;).permitAll() .antMatchers(&quot;/ipLogin&quot;).permitAll() .anyRequest().authenticated() .and() .logout() .logoutSuccessUrl(&quot;/&quot;) .permitAll() .and() .exceptionHandling() .accessDeniedPage(&quot;/ipLogin&quot;) .authenticationEntryPoint(loginUrlAuthenticationEntryPoint()) ; // 注册 IpAuthenticationProcessingFilter 注意放置的顺序 这很关键 http.addFilterBefore(ipAuthenticationProcessingFilter(authenticationManager()), UsernamePasswordAuthenticationFilter.class); } @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.authenticationProvider(ipAuthenticationProvider()); }} WebSecurityConfigAdapter 提供了我们很大的便利，不需要关注 AuthenticationManager 什么时候被创建，只需要使用其暴露的 configure(AuthenticationManagerBuilder auth) 便可以添加我们自定义的 ipAuthenticationProvider。剩下的一些细节，注释中基本都写了出来。 5.7 配置 SpringMVC1234567891011121314@Configurationpublic class MvcConfig extends WebMvcConfigurerAdapter { @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(&quot;/home&quot;).setViewName(&quot;home&quot;); registry.addViewController(&quot;/&quot;).setViewName(&quot;home&quot;); registry.addViewController(&quot;/hello&quot;).setViewName(&quot;hello&quot;); registry.addViewController(&quot;/ip&quot;).setViewName(&quot;ipHello&quot;); registry.addViewController(&quot;/ipLogin&quot;).setViewName(&quot;ipLogin&quot;); }} 页面的具体内容和表单登录基本一致，可以在文末的源码中查看。 5.8 运行效果成功的流程 http://127.0.0.1:8080/ 访问首页，其中 here 链接到的地址为：http://127.0.0.1:8080/hello 点击 here，由于 http://127.0.0.1:8080/hello 是受保护资源，所以跳转到了校验 IP 的页面。此时若点击 Sign In by IP 按钮，将会提交到 /ipVerify 端点，进行 IP 的认证。 登录校验成功之后，页面被成功重定向到了原先访问的 失败的流程 注意此时已经注销了上次的登录，并且，使用了 localhost(localhost 和 127.0.0.1 是两个不同的 IP 地址，我们的内存中只有 127.0.0.1 的用户, 没有 localhost 的用户) 点击 here 后，由于没有认证过，依旧跳转到登录页面 此时，我们发现使用 localhost，并没有认证成功，符合我们的预期 5.9 总结一个简单的使用 Spring Security 来进行验证 IP 地址的登录 demo 就已经完成了，这个 demo 主要是为了更加清晰地阐释 Spring Security 内部工作的原理设置的，其本身没有实际的项目意义，认证 IP 其实也不应该通过 Spring Security 的过滤器去做，退一步也应该交给 Filter 去做（这个 Filter 不存在于 Spring Security 的过滤器链中），而真正项目中，如果真正要做黑白名单这样的功能，一般选择在网关层或者 nginx 的扩展模块中做。再次特地强调下，怕大家误解。 最后祝大家国庆玩的开心 ~ 本节的代码可以在 github 中下载源码：https://github.com/lexburner/spring-security-ipLogin ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-security-5/"},{"title":"该如何设计你的 PasswordEncoder?","text":"缘起前端时间将一个集成了 spring-security-oauth2 的旧项目改造了一番，将 springboot 升级成了 springboot 2.0，众所周知 springboot 2.0 依赖的是 spring5，并且许多相关的依赖都发生了较大的改动，与本文相关的改动罗列如下，有兴趣的同学可以看看：Spring Security 5.0 New Features ，增强了 oauth2 集成的功能以及和一个比较有意思的改动—重构了密码编码器的实现（Password Encoding，由于大多数 PasswordEncoder 相关的算法是 hash 算法，所以本文将 PasswordEncoder 翻译成‘密码编码器’和并非‘密码加密器’）官方称之为 Modernized Password Encoding — 现代化的密码编码方式 另外，springboot2.0 的自动配置也做了一些调整，其中也有几点和 spring-security 相关，戳这里看所有细节 springboot2.0 迁移指南 一开始，我仅仅修改了依赖，将 12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.4.RELEASE&lt;/version&gt;&lt;/parent&gt; 升级成了 12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;&lt;/parent&gt; 不出意料出现了兼容性的问题，我在尝试登陆时，出现了如下的报错 1java.lang.IllegalArgumentException: There is no PasswordEncoder mapped for the id &quot;null&quot; 原因也很明显，正如 spring security 的更新文档中描述的那样，spring security 5 对 PasswordEncoder 做了相关的重构，原先默认配置的 PlainTextPasswordEncoder（明文密码）被移除了。这引起了我的兴趣，spring security 在新版本中对于 passwordEncoder 进行了哪些改造，这些改造背后又是出于什么样的目的呢？卖个关子，先从远古时期的案例来一步步演化出所谓的“现代化密码编码方式”。 密码存储演进史自从互联网有了用户的那一刻起，存储用户密码这件事便成为了一个健全的系统不得不面对的一件事。远古时期，明文存储密码可能还不被认为是一个很大的系统缺陷（事实上这是一件很恐怖的事）。提及明文存储密码，我立刻联想到的是 CSDN 社区在 2011 年末发生的 600 万用户密码泄露的事件，谁也不会想到这个和程序员密切相关的网站会犯如此低级的错误。明文存储密码使得恶意用户可以通过 sql 注入等攻击方式来获取用户名和密码，虽然安全框架和良好的编码规范可以规避很多类似的攻击，但依旧避免不了系统管理员，DBA 有途径获取用户密码这一事实。事实上，不用明文存储存储密码，程序员们早在 n 多年前就已经达成了共识。 不能明文存储，一些 hash 算法便被广泛用做密码的编码器，对密码进行单向 hash 处理后存储数据库，当用户登录时，计算用户输入的密码的 hash 值，将两者进行比对。单向 hash 算法，顾名思义，它无法（或者用不能轻易更为合适）被反向解析还原出原密码。这杜绝了管理员直接获取密码的途径，可仅仅依赖于普通的 hash 算法（如 md5，sha256）是不合适的，他主要有 3 个特点： 同一密码生成的 hash 值一定相同 不同密码的生成的 hash 值可能相同（md5 的碰撞问题相比 sha256 还要严重） 计算速度快。 以上三点结合在一起，破解此类算法成了不是那么困难的一件事，尤其是第三点，会在下文中再次提到，多快才算非常快？按照相关资料的说法： modern hardware perform billions of hash calculations a second. 考虑到大多数用户使用的密码多为数字 + 字母 + 特殊符号的组合，攻击者将常用的密码进行枚举，甚至通过排列组合来暴力破解，这被称为 rainbow table。算法爱好者能够立刻看懂到上述的方案，这被亲切地称之为—打表，一种暴力美学，这张表是可以被复用的。 虽然仅仅依赖于传统 hash 算法的思路被否决了，但这种 hash 后比对的思路，几乎被后续所有的优化方案继承。 hash 方案迎来的第一个改造是对引入一个“随机的因子”来掺杂进明文中进行 hash 计算，这样的随机因子通常被称之为盐 （salt）。salt 一般是用户相关的，每个用户持有各自的 salt。此时狗蛋和二丫的密码即使相同，由于 salt 的影响，存储在数据库中的密码也是不同的，除非…为每个用户单独建议一张 rainbow table。很明显 salted hash 相比普通的单向 hash 方案加大了 hacker 攻击的难度。但了解过 GPU 并行计算能力之强大的童鞋，都能够意识到，虽然破解 salted hash 比较麻烦，却并非不可行，勤劳勇敢的安全专家似乎也对这个方案不够满意。 为解决上述 salted hash 仍然存在的问题，一些新型的单向 hash 算法被研究了出来。其中就包括：Bcrypt，PBKDF2，Scrypt，Argon2。为什么这些 hash 算法能保证密码存储的安全性？因为他们足够慢，恰到好处的慢。这么说不严谨，只是为了给大家留个深刻的映像：慢。这类算法有一个特点，存在一个影响因子，可以用来控制计算强度，这直接决定了破解密码所需要的资源和时间，直观的体会可以见下图，在一年内破解如下算法所需要的硬件资源花费（折算成美元） 这使得破解成了一件极其困难的事，并且，其中的计算强度因子是可控的，这样，即使未来量子计算机的计算能力爆表，也可以通过其控制计算强度以防破解。注意，普通的验证过程只需要计算一次 hash 计算，使用此类 hash 算法并不会影响到用户体验。 慢 hash 算法真的安全吗？Bcrypt，Scrypt，PBKDF2 这些慢 hash 算法是目前最为推崇的 password encoding 方式，好奇心驱使我思考了这样一个问题：慢 hash 算法真的安全吗？ 我暂时还没有精力仔细去研究他们中每一个算法的具体实现，只能通过一些文章来拾人牙慧，简单看看这几个算法的原理和安全性。 PBKDF2 被设计的很简单，它的基本原理是通过一个伪随机函数（例如 HMAC 函数），把明文和一个盐值作为输入参数，然后按照设置的计算强度因子重复进行运算，并最终产生密钥。这样的重复 hash 已经被认为足够安全，但也有人提出了不同意见，此类算法对于传统的 CPU 来说的确是足够安全，但 GPU 被搬了出来，前文提到过 GPU 的并行计算能力非常强大。 Bcrypt 强大的一点在于，其不仅仅是 CPU 密集型，还是 RAM 密集型！双重的限制因素，导致 GPU，ASIC（专用集成电路）无法应对 Bcrypt 带来的破解困境。 然后…看了 Scrypt 的相关资料之后我才意识到这个坑有多深。一个熟悉又陌生的词出现在了我面前：FPGA（现场可编程逻辑门阵列），这货就比较厉害了。现成的芯片指令结构如传统的 CPU，GPU，ASIC 都无法破解 Bcrypt，但是 FPGA 支持烧录逻辑门（如 AND、OR、XOR、NOT），通过编程的方式烧录指令集的这一特性使得可以定制硬件来破解 Bcrypt。尽管我不认为懂这个技术的人会去想办法破解真正的系统，但，只要这是一个可能性，就总有方法会被发明出来与之对抗。Scrypt 比 Bcrypt 额外考虑到的就是大规模的 自定义硬件攻击 ，从而刻意设计需要大量内存运算。 理论终归是理论，实际上 Bcrypt 算法被发明至今 18 年，使用范围广，且从未因为安全问题而被修改，其有限性是已经被验证过的，相比之下 Scrypt 据我看到的文章显示是 9 年的历史，没有 Bcrypt 使用的广泛。从破解成本和权威性的角度来看，Bcrypt 用作密码编码器是不错的选择。 spring security 废弃的接口回到文档中，spring security 5 对 PasswordEncoder 做了相关的重构，原先默认配置的 PlainTextPasswordEncoder（明文密码）被移除了，想要做到明文存储密码，只能使用一个过期的类来过渡 1234@BeanPasswordEncoder passwordEncoder(){ return NoOpPasswordEncoder.getInstance();} 实际上，spring security 提供了 BCryptPasswordEncoder 来进行密码编码，并作为了相关配置的默认配置，只不过没有暴露为全局的 Bean。使用明文存储的风险在文章一开始就已经强调过，NoOpPasswordEncoder 只能存在于 demo 中。 1234@BeanPasswordEncoder passwordEncoder(){ return new BCryptPasswordEncoder();} 别忘了对你数据库中的密码进行同样的编码，否则无法对应。 更深层的思考实际上，spring security 5 的另一个设计是促使我写成本文的初衷。 不知道有没有读者产生跟我相同的困扰： 如果我要设计一个 QPS 很高的登录系统，使用 spring security 推荐的 BCrypt 会不会存在性能问题？ spring security 怎么这么坑，原来的密码编码器都给改了，我需要怎么迁移旧密码编码的应用程序？ 万一以后出了更高效的加密算法，这种笨重的硬编码方式配置密码编码器是不是不够灵活？ 在 spring security 5 提供了这样一个思路，应该将密码编码之后的 hash 值和加密方式一起存储，并提供了一个 DelegatingPasswordEncoder 来作为众多密码密码编码方式的集合。 1234@BeanPasswordEncoder passwordEncoder(){ return PasswordEncoderFactories.createDelegatingPasswordEncoder();} 负责生产 DelegatingPasswordEncoder 的工厂方法： 123456789101112131415161718192021public class PasswordEncoderFactories { public static PasswordEncoder createDelegatingPasswordEncoder() { String encodingId = &quot;bcrypt&quot;; Map&lt;String, PasswordEncoder&gt; encoders = new HashMap&lt;&gt;(); encoders.put(encodingId, new BCryptPasswordEncoder()); encoders.put(&quot;ldap&quot;, new LdapShaPasswordEncoder()); encoders.put(&quot;MD4&quot;, new Md4PasswordEncoder()); encoders.put(&quot;MD5&quot;, new MessageDigestPasswordEncoder(&quot;MD5&quot;)); encoders.put(&quot;noop&quot;, NoOpPasswordEncoder.getInstance()); encoders.put(&quot;pbkdf2&quot;, new Pbkdf2PasswordEncoder()); encoders.put(&quot;scrypt&quot;, new SCryptPasswordEncoder()); encoders.put(&quot;SHA-1&quot;, new MessageDigestPasswordEncoder(&quot;SHA-1&quot;)); encoders.put(&quot;SHA-256&quot;, new MessageDigestPasswordEncoder(&quot;SHA-256&quot;)); encoders.put(&quot;sha256&quot;, new StandardPasswordEncoder()); return new DelegatingPasswordEncoder(encodingId, encoders); } private PasswordEncoderFactories(){}} 如此注入 PasswordEncoder 之后，我们在数据库中需要这么存储数据： 12345{bcrypt}$2a$10$dXJ3SW6G7P50lGmMkkmwe.20cQQubK3.HZWzG3YB1tlRy.fqvM/BG {noop}password {pbkdf2}5d923b44a6d129f3ddf3e3c8d29412723dcbde72445e8ef6bf3b508fbf17fa4ed4d6b99ca763d8dc {scrypt}$e0801$8bWJaSu2IKSn9Z9kM+TPXfOc/9bdYSrN1oD9qfVThWEwdRTnO7re7Ei+fUZRJ68k9lTyuTeUp4of4g24hHnazw==$OAOec05+bXxvuu/1qZ6NUR+xQYvYv7BeL1QxwRpY5Pc= {sha256}97cde38028ad898ebc02e690819fa220e88c62e0699403e94fff291cfffaf8410849f27605abcbc0 还记得文章开始的报错吗？ 1java.lang.IllegalArgumentException: There is no PasswordEncoder mapped for the id &quot;null&quot; 这个 id 就是因为我们没有为数据库中的密码添加 {bcrypt} 此类的前缀导致的。 你会不会担心密码泄露后，{bcrypt}，{pbkdf2}，{scrypt}，{sha256} 此类前缀会直接暴露密码的编码方式？其实这个考虑是多余的，因为密码存储的依赖算法并不是一个秘密。大多数能搞到你密码的 hacker 都可以轻松的知道你用的是什么算法，例如，bcrypt 算法通常以 $2a$ 开头 稍微思考下，前面的三个疑问就可以迎刃而解，这就是文档中所谓的：** 能够自适应服务器性能的现代化密码编码方案 **。 参考Password Hashing: PBKDF2, Scrypt, Bcrypt core-services-password-encoding show me the codespring security oauth2 的 github 代码示例，体会下 spring security 4 -&gt; spring security 5 的相关变化。 https://github.com/lexburner/oauth2-demo ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-security-6/"},{"title":"Re：从零开始的 Spring Session(一)","text":"Session 和 Cookie 这两个概念，在学习 java web 开发之初，大多数人就已经接触过了。最近在研究跨域单点登录的实现时，发现对于 Session 和 Cookie 的了解，并不是很深入，所以打算写两篇文章记录一下自己的理解。在我们的应用集成 Spring Session 之前，先补充一点 Session 和 Cookie 的关键知识。 Session 与 Cookie 基础由于 http 协议是无状态的协议，为了能够记住请求的状态，于是引入了 Session 和 Cookie 的机制。我们应该有一个很明确的概念，那就是 Session 是存在于服务器端的，在单体式应用中，他是由 tomcat 管理的，存在于 tomcat 的内存中，当我们为了解决分布式场景中的 session 共享问题时，引入了 redis，其共享内存，以及支持 key 自动过期的特性，非常契合 session 的特性，我们在企业开发中最常用的也就是这种模式。但是只要你愿意，也可以选择存储在 JDBC，Mongo 中，这些，spring 都提供了默认的实现，在大多数情况下，我们只需要引入配置即可。而 Cookie 则是存在于客户端，更方便理解的说法，可以说存在于浏览器。Cookie 并不常用，至少在我不长的 web 开发生涯中，并没有什么场景需要我过多的关注 Cookie。http 协议允许从服务器返回 Response 时携带一些 Cookie，并且同一个域下对 Cookie 的数量有所限制，之前说过 Session 的持久化依赖于服务端的策略，而 Cookie 的持久化则是依赖于本地文件。虽然说 Cookie 并不常用，但是有一类特殊的 Cookie 却是我们需要额外关注的，那便是与 Session 相关的 sessionId，他是真正维系客户端和服务端的桥梁。 代码示例用户发起请求，服务器响应请求，并做一些用户信息的处理，随后返回响应给用户；用户再次发起请求，携带 sessionId，服务器便能够识别，这个用户就是之前请求的那个。 使用 Springboot 编写一个非常简单的服务端，来加深对其的理解。需求很简单，当浏览器访问 localhost:8080/test/cookie?browser=xxx 时，如果没有获取到 session，则将 request 中的 browser 存入 session；如果获取到 session，便将 session 中的 browser 值输出。顺便将 request 中的所有 cookie 打印出来。 12345678910111213141516171819202122@Controllerpublic class CookieController { @RequestMapping(&quot;/test/cookie&quot;) public String cookie(@RequestParam(&quot;browser&quot;) String browser, HttpServletRequest request, HttpSession session) { // 取出 session 中的 browser Object sessionBrowser = session.getAttribute(&quot;browser&quot;); if (sessionBrowser == null) { System.out.println(&quot;不存在 session，设置 browser=&quot; + browser); session.setAttribute(&quot;browser&quot;, browser); } else { System.out.println(&quot;存在 session，browser=&quot; + sessionBrowser.toString()); } Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0) { for (Cookie cookie : cookies) { System.out.println(cookie.getName() + &quot;:&quot; + cookie.getValue()); } } return &quot;index&quot;; }} 我们没有引入其他任何依赖，看看原生的 session 机制是什么。 1 使用 chrome 浏览器，访问 localhost:8080/test/cookie?browser=chrome, 控制台输出如下： 1Session Info: 不存在 session，设置 browser=chrome 既没有 session，也没有 cookie，我们将 browser=chrome 设置到 session 中。 再次访问同样的端点，控制台输出如下： 12Session Info: 存在 session，browser=chromeCookie Info: JSESSIONID : 4CD1D96E04FC390EA6C60E8C40A636AF 多次访问之后，控制台依旧打印出同样的信息。 稍微解读下这个现象，可以验证一些结论。当服务端往 session 中保存一些数据时，Response 中自动添加了一个 Cookie：JSESSIONID：xxxx, 再后续的请求中，浏览器也是自动的带上了这个 Cookie，服务端根据 Cookie 中的 JSESSIONID 取到了对应的 session。这验证了一开始的说法，客户端服务端是通过 JSESSIONID 进行交互的，并且，添加和携带 key 为 JSESSIONID 的 Cookie 都是 tomcat 和浏览器自动帮助我们完成的，这很关键。 2 使用 360 浏览器，访问 localhost:8080/test/cookie?browser=360 第一次访问： 1Session Info: 不存在 session，设置 browser=360 后续访问： 12Session Info: 存在 session，browser=360Cookie Info: JSESSIONID : 320C21A645A160C4843D076204DA2F40 为什么要再次使用另一个浏览器访问呢？先卖个关子，我们最起码可以得出结论，不同浏览器，访问是隔离的，甚至重新打开同一个浏览器，JSESSIONID 也是不同的。另外可以尝试把保存 session 的操作注视掉，则可以发现 Response 中就不会返回 JSESSIONID 了，即这是一次无状态的请求。 安全问题其实上述的知识点，都是非常浅显的，之所以啰嗦一句，是为了引出这一节的内容，以及方便观察后续我们引入 Spring Session 之后的发生的变化。 还记得上一节的代码示例中，我们使用了两个浏览器： chrome 浏览器访问时，JSESSIONID 为 4CD1D96E04FC390EA6C60E8C40A636AF，后端 session 记录的值为：browser=chrome。 360 浏览器访问时，JSESSIONID 为 320C21A645A160C4843D076204DA2F40, 后端 session 记录的值为：browser=360。 我们使用 chrome 插件 Edit this Cookie，将 chrome 浏览器中的 JSESSIONID 修改为 360 浏览器中的值 同样访问原来的端点：localhost:8080/test/cookie?browser=chrome，得到的输出如下： 12存在 session，browser=360JSESSIONID : 320C21A645A160C4843D076204DA2F40 证实了一点，存放在客户端的 Cookie 的确是存在安全问题的，我们使用 360 的 JSESSIONID“骗”过了服务器。毕竟，服务器只能通过 Cookie 中的 JSESSIONID 来辨别身份。（这提示我们不要在公共场合保存 Cookie 信息，现在的浏览器在保存 Cookie 时通常会让你确定一次） 下一篇文章，将正式讲解如何在应用中集成 Spring Session。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-session-1/"},{"title":"Spring Security(六)—SpringSecurityFilterChain 加载流程深度解析","text":"SpringSecurityFilterChain 作为 SpringSecurity 的核心过滤器链在整个认证授权过程中起着举足轻重的地位，每个请求到来，都会经过该过滤器链，前文 《Spring Security( 四)– 核心过滤器源码分析》 中我们分析了 SpringSecurityFilterChain 的构成，但还有很多疑问可能没有解开： 这个 SpringSecurityFilterChain 是怎么注册到 web 环境中的？ 有读者发出这样的疑问：”SpringSecurityFilterChain 的实现类到底是什么，我知道它是一个 Filter，但是在很多配置类中看到了 BeanName=SpringSecurityFilterChain 相关的类，比如 DelegatingFilterProxy，FilterChainProxy，SecurityFilterChain，他们的的名称实在太相似了，到底哪个才是真正的实现，SpringSecurity 又为什么要这么设计？“ 我们貌似一直在配置 WebSecurity ，但没有对 SpringSecurityFilterChain 进行什么配置，WebSecurity 相关配置是怎么和 SpringSecurityFilterChain 结合在一起的？ 以上是个人 YY 的一些 SpringSecurityFilterChain 相关的问题，因为我当初研究了一段时间 SpringSecurity 源码，依旧没有理清这么多错综复杂的类。那么本文就主要围绕 SpringSecurityFilterChain 展开我们的探索。 ###6.1 SpringSecurityFilterChain 是怎么注册的？ 这个问题并不容易解释，因为 SpringSecurity 仅仅在 web 环境下（SpringSecurity 还支持非 web 环境）就有非常多的支持形式： **Java 配置方式 ** 作为独立的 SpringSecurity 依赖提供给朴素的 java web 项目使用，并且项目不使用 Spring！没错，仅仅使用 servlet，jsp 的情况下也是可以集成 SpringSecurity 的。 提供给包含 SpringMVC 项目使用。 提供给具备 Servlet3.0+ 的 web 项目使用。 SpringBoot 内嵌容器环境下使用 SpringSecurity，并且包含了一定程度的自动配置。 **XML 配置方式 ** 使用 XML 中的命名空间配置 SpringSecurity。 注意，以上条件可能存在交集，比如我的项目是一个使用 servlet3.0 的 web 项目同时使用了 SpringMVC；也有可能使用了 SpringBoot 同时配合 SpringMVC；还有可能使用了 SpringBoot，却打成了 war 包，部署在外置的支持 Servlet3.0+ 规范的应用容器中… 各种组合方式会导致配置 SpringSecurityFilterChain 的注册方式产生差异，所以，这个问题说复杂还真有点，需要根据你的环境来分析。我主要分析几种较为常见的注册方式。 SpringSecurityFilterChain 抽象概念里最重要的三个类：DelegatingFilterProxy，FilterChainProxy 和 SecurityFilterChain，对这三个类的源码分析和设计将会贯彻本文。不同环境下 DelegatingFilterProxy 的注册方式区别较大，但 FilterChainProxy 和 SecurityFilterChain 的差异不大，所以重点就是分析 DelegatingFilterProxy 的注册方式。它们三者的分析会放到下一节中。 ####6.1.1 servlet3.0+ 环境下 SpringSecurity 的 java config 方式 这是一个比较常见的场景，你可能还没有使用 SpringBoot 内嵌的容器，将项目打成 war 包部署在外置的应用容器中，比如最常见的 tomcat，一般很少 web 项目低于 servlet3.0 版本的，并且该场景摒弃了 XML 配置。 123456import org.springframework.security.web.context.*;public class SecurityWebApplicationInitializer extends AbstractSecurityWebApplicationInitializer {} 主要自定义一个 SecurityWebApplicationInitializer 并且让其继承自 AbstractSecurityWebApplicationInitializer 即可。如此简单的一个继承背后又经历了 Spring 怎样的封装呢？自然要去 AbstractSecurityWebApplicationInitializer 中去一探究竟。经过删减后的源码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public abstract class AbstractSecurityWebApplicationInitializer implements WebApplicationInitializer {//&lt;1&gt; public static final String DEFAULT_FILTER_NAME = &quot;springSecurityFilterChain&quot;; // &lt;1&gt; 父类 WebApplicationInitializer 的加载入口 public final void onStartup(ServletContext servletContext) throws ServletException { beforeSpringSecurityFilterChain(servletContext); if (this.configurationClasses != null) { AnnotationConfigWebApplicationContext rootAppContext = new AnnotationConfigWebApplicationContext(); rootAppContext.register(this.configurationClasses); servletContext.addListener(new ContextLoaderListener(rootAppContext)); } if (enableHttpSessionEventPublisher()) { servletContext.addListener( &quot;org.springframework.security.web.session.HttpSessionEventPublisher&quot;); } servletContext.setSessionTrackingModes(getSessionTrackingModes()); insertSpringSecurityFilterChain(servletContext);//&lt;2&gt; afterSpringSecurityFilterChain(servletContext); } // &lt;2&gt; 在这儿初始化了关键的 DelegatingFilterProxy private void insertSpringSecurityFilterChain(ServletContext servletContext) { String filterName = DEFAULT_FILTER_NAME; // &lt;2&gt; 该方法中最关键的一个步骤，DelegatingFilterProxy 在此被创建 DelegatingFilterProxy springSecurityFilterChain = new DelegatingFilterProxy( filterName); String contextAttribute = getWebApplicationContextAttribute(); if (contextAttribute != null) { springSecurityFilterChain.setContextAttribute(contextAttribute); } registerFilter(servletContext, true, filterName, springSecurityFilterChain); } // &lt;3&gt; 使用 servlet3.0 的新特性，动态注册 springSecurityFilterChain(实际上注册的是 springSecurityFilterChain 代理类) private final void registerFilter(ServletContext servletContext, boolean insertBeforeOtherFilters, String filterName, Filter filter) { Dynamic registration = servletContext.addFilter(filterName, filter); registration.setAsyncSupported(isAsyncSecuritySupported()); EnumSet&lt;DispatcherType&gt; dispatcherTypes = getSecurityDispatcherTypes(); registration.addMappingForUrlPatterns(dispatcherTypes, !insertBeforeOtherFilters, &quot;/*&quot;); }} &lt;1&gt;&lt;3&gt; 放在一起讲，因为他们都和 servlet3.0 新特性以及 spring 对 servlet3.0 的支持相关，这也是为什么在场景描述中我特地强调了需要 servlet3.0 环境。如果你对 servlet3.0 的新特性不了解，这儿准备了一篇详细的介绍为你阐述 《Spring 揭秘 – 寻找遗失的 web.xml》 。得益于 Spring 的封装，在 servlet3.0 环境下，web 容器启动时会自行去寻找类路径下所有实现了 WebApplicationInitializer 接口的 Initializer 实例，并调用他们的 onStartup 方法。所以，我们只需要继承 AbstractSecurityWebApplicationInitializer ，便可以自动触发 web 容器的加载，进而配置和 SpringSecurityFilterChain 第一个密切相关的类，第 &lt;2&gt; 步中的 DelegatingFilterProxy。 &lt;2&gt; DelegatingFilterProxy 在此被实例化出来。在第 &lt;3&gt; 步中，它作为一个 Filter 正式注册到了 web 容器中。 6.1.2 XML 配置这个真的是简单易懂，因为它是被指名道姓配置成一个 Filter 的。 web.xml 123456789&lt;filter&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; web.xml 的存在注定了其无所谓当前环境是不是 servlet3.0+，虽然我个人不太喜欢 xml 的配置方式，但不得不说，这样真的很简单粗暴。 6.1.3 SpringBoot 内嵌应用容器并且使用自动配置《Spring 揭秘 – 寻找遗失的 web.xml》 中我曾经得出一个结论，内嵌容器是完全不会使用 SPI 机制加载 servlet3.0 新特性的那些 Initializer 的，springboot 又推崇 java configuration，所以上述两种方案完全被抛弃了。那么 SpringBoot 如何注册 DelegatingFilterProxy 呢？ 12345678910111213141516171819202122232425262728@Configuration@ConditionalOnWebApplication@EnableConfigurationProperties@ConditionalOnClass({ AbstractSecurityWebApplicationInitializer.class, SessionCreationPolicy.class })@AutoConfigureAfter(SecurityAutoConfiguration.class)public class SecurityFilterAutoConfiguration { private static final String DEFAULT_FILTER_NAME = AbstractSecurityWebApplicationInitializer.DEFAULT_FILTER_NAME;//springSecurityFilterChain // &lt;1&gt; @Bean @ConditionalOnBean(name = DEFAULT_FILTER_NAME) public DelegatingFilterProxyRegistrationBean securityFilterChainRegistration( SecurityProperties securityProperties) { DelegatingFilterProxyRegistrationBean registration = new DelegatingFilterProxyRegistrationBean( DEFAULT_FILTER_NAME); registration.setOrder(securityProperties.getFilterOrder()); registration.setDispatcherTypes(getDispatcherTypes(securityProperties)); return registration; } @Bean @ConditionalOnMissingBean public SecurityProperties securityProperties() { return new SecurityProperties(); }} &lt;1&gt; DelegatingFilterProxyRegistrationBean 的分析在之前那篇文章中也有详细的介绍，其作用便是在 SpringBoot 环境下通过 TomcatStarter 等内嵌容器启动类来注册一个 DelegatingFilterProxy。这下，和前面两种配置方式都对应上了。 ###SpringSecurityFilterChain 三个核心类的源码分析 理解 SpringSecurityFilterChain 的工作流程必须搞懂三个类：org.springframework.web.filter.DelegatingFilterProxy，org.springframework.security.web.FilterChainProxy ， org.springframework.security.web.SecurityFilterChain DelegatingFilterProxy上面一节主要就是介绍 DelegatingFilterProxy 在不同环境下的注册方式，可以很明显的发现，DelegatingFilterProxy 是 SpringSecurity 的“门面”，注意它的包结构：org.springframework.web.filter，它本身是 Spring Web 包中的类，并不是 SpringSecurity 中的类。因为 Spring 考虑到了多种使用场景，自然希望将侵入性降到最低，所以使用了这个委托代理类来代理真正的 SpringSecurityFilterChain。DelegatingFilterProxy 实现了 javax.servlet.Filter 接口，使得它可以作为一个 java web 的标准过滤器，其职责也很简单，只负责调用真正的 SpringSecurityFilterChain。 删减掉非重要代码后的 DelegatingFilterProxy： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class DelegatingFilterProxy extends GenericFilterBean { private WebApplicationContext webApplicationContext; // springSecurityFilterChain private String targetBeanName; // &lt;1&gt; 关键点 private volatile Filter delegate; private final Object delegateMonitor = new Object(); public DelegatingFilterProxy(String targetBeanName, WebApplicationContext wac) { Assert.hasText(targetBeanName, &quot;Target Filter bean name must not be null or empty&quot;); this.setTargetBeanName(targetBeanName); this.webApplicationContext = wac; if (wac != null) { this.setEnvironment(wac.getEnvironment()); } } @Override protected void initFilterBean() throws ServletException { synchronized (this.delegateMonitor) { if (this.delegate == null) { if (this.targetBeanName == null) { this.targetBeanName = getFilterName(); } // Fetch Spring root application context and initialize the delegate early, // if possible. If the root application context will be started after this // filter proxy, we'll have to resort to lazy initialization. WebApplicationContext wac = findWebApplicationContext(); if (wac != null) { this.delegate = initDelegate(wac); } } } } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws ServletException, IOException { // 过滤器代理支持懒加载 Filter delegateToUse = this.delegate; if (delegateToUse == null) { synchronized (this.delegateMonitor) { delegateToUse = this.delegate; if (delegateToUse == null) { WebApplicationContext wac = findWebApplicationContext(); delegateToUse = initDelegate(wac); } this.delegate = delegateToUse; } } // 让代理过滤器执行实际的过滤行为 invokeDelegate(delegateToUse, request, response, filterChain); } // 初始化过滤器代理 // &lt;2&gt; protected Filter initDelegate(WebApplicationContext wac) throws ServletException { Filter delegate = wac.getBean(getTargetBeanName(), Filter.class); if (isTargetFilterLifecycle()) { delegate.init(getFilterConfig()); } return delegate; } // 调用代理过滤器 protected void invokeDelegate( Filter delegate, ServletRequest request, ServletResponse response, FilterChain filterChain) throws ServletException, IOException { delegate.doFilter(request, response, filterChain); }} &lt;1&gt; 可以发现整个 DelegatingFilterProxy 的逻辑就是为了调用 private volatile Filter delegate; 那么问题来了，这个 delegate 的真正实现是什么呢？ &lt;2&gt; 可以看到，DelegatingFilterProxy 尝试去容器中获取名为 targetBeanName 的类，而 targetBeanName 的默认值便是 Filter 的名称，也就是 springSecurityFilterChain！也就是说，DelegatingFilterProxy 只是名称和 targetBeanName 叫 springSecurityFilterChain，真正容器中的 Bean(name=”springSecurityFilterChain”) 其实另有其人（这里 springboot 稍微有点区别，不过不影响理解，我们不纠结这个细节了）。通过 debug，我们发现了真正的 springSecurityFilterChain — FilterChainProxy。 FilterChainProxy 和 SecurityFilterChainorg.springframework.security.web.FilterChainProxy 已经是 SpringSecurity 提供的类了，原来它才是真正的 springSecurityFilterChain，我们来看看它的源码（有删减，不影响理解）。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class FilterChainProxy extends GenericFilterBean { // &lt;1&gt; 包含了多个 SecurityFilterChain private List&lt;SecurityFilterChain&gt; filterChains; public FilterChainProxy(SecurityFilterChain chain) { this(Arrays.asList(chain)); } public FilterChainProxy(List&lt;SecurityFilterChain&gt; filterChains) { this.filterChains = filterChains; } @Override public void afterPropertiesSet() { filterChainValidator.validate(this); } public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { doFilterInternal(request, response, chain); } private void doFilterInternal(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { FirewalledRequest fwRequest = firewall .getFirewalledRequest((HttpServletRequest) request); HttpServletResponse fwResponse = firewall .getFirewalledResponse((HttpServletResponse) response); // &lt;1&gt; List&lt;Filter&gt; filters = getFilters(fwRequest); if (filters == null || filters.size() == 0) { fwRequest.reset(); chain.doFilter(fwRequest, fwResponse); return; } VirtualFilterChain vfc = new VirtualFilterChain(fwRequest, chain, filters); vfc.doFilter(fwRequest, fwResponse); } /** * &lt;1&gt; 可能会有多个过滤器链，返回第一个和请求 URL 匹配的过滤器链 */ private List&lt;Filter&gt; getFilters(HttpServletRequest request) { for (SecurityFilterChain chain : filterChains) { if (chain.matches(request)) { return chain.getFilters(); } } return null; }} 看 FilterChainProxy 的名字就可以发现，它依旧不是真正实施过滤的类，它内部维护了一个 SecurityFilterChain，这个过滤器链才是请求真正对应的过滤器链，并且同一个 Spring 环境下，可能同时存在多个安全过滤器链，如 private List filterChains 所示，需要经过 chain.matches(request) 判断到底哪个过滤器链匹配成功，每个 request 最多只会经过一个 SecurityFilterChain。为何要这么设计？因为 Web 环境下可能有多种安全保护策略，每种策略都需要有自己的一条链路，比如我曾经设计过 Oauth2 服务，在极端条件下，可能同一个服务本身既是资源服务器，又是认证服务器，还需要做 Web 安全！ 如上图，4 个 SecurityFilterChain 存在于 FilterChainProxy 中，值得再次强调：实际每次请求，最多只有一个安全过滤器链被返回。 SecurityFilterChain 才是真正意义上的 SpringSecurityFilterChain： 123456789101112public final class DefaultSecurityFilterChain implements SecurityFilterChain { private final RequestMatcher requestMatcher; private final List&lt;Filter&gt; filters; public List&lt;Filter&gt; getFilters() { return filters; } public boolean matches(HttpServletRequest request) { return requestMatcher.matches(request); }} 其中的 List filters 就是我们在 《Spring Security( 四)– 核心过滤器源码分析》 中分析的诸多核心过滤器，包含了 UsernamePasswordAuthenticationFilter，SecurityContextPersistenceFilter，FilterSecurityInterceptor 等之前就介绍过的 Filter。 ###SecurityFilterChain 的注册过程 还记得 DelegatingFilterProxy 从 Spring 容器中寻找了一个 targetBeanName=springSecurityFilterChain 的 Bean 吗？我们通过 debug 直接定位到了其实现是 SecurityFilterChain，但它又是什么时候被放进去的呢？ 这就得说到老朋友 WebSecurity 了，还记得一般我们都会选择使用 @EnableWebSecurity 和 WebSecurityConfigurerAdapter 来进行 web 安全配置吗，来到 WebSecurity 的源码： 123456789101112131415161718192021222324252627public final class WebSecurity extends AbstractConfiguredSecurityBuilder&lt;Filter, WebSecurity&gt; implements SecurityBuilder&lt;Filter&gt;, ApplicationContextAware { @Override protected Filter performBuild() throws Exception { int chainSize = ignoredRequests.size()+ securityFilterChainBuilders.size(); List&lt;SecurityFilterChain&gt; securityFilterChains = new ArrayList&lt;SecurityFilterChain&gt;( chainSize); for (RequestMatcher ignoredRequest : ignoredRequests) { securityFilterChains.add(new DefaultSecurityFilterChain(ignoredRequest)); } for (SecurityBuilder&lt;? extends SecurityFilterChain&gt; securityFilterChainBuilder : securityFilterChainBuilders) { securityFilterChains.add(securityFilterChainBuilder.build()); } // &lt;1&gt; FilterChainProxy 由 WebSecurity 构建 FilterChainProxy filterChainProxy = new FilterChainProxy(securityFilterChains); if (httpFirewall != null) { filterChainProxy.setFirewall(httpFirewall); } filterChainProxy.afterPropertiesSet(); Filter result = filterChainProxy; postBuildAction.run(); return result; }} &lt;1&gt; 最终定位到 WebSecurity 的 performBuild 方法，我们之前配置了一堆参数的 WebSecurity 最终帮助我们构建了 FilterChainProxy。 并且，最终在 org.springframework.security.config.annotation.web.configuration.WebSecurityConfiguration 中被注册为默认名称为 SpringSecurityFilterChain。 总结一个名称 SpringSecurityFilterChain，借助于 Spring 的 IOC 容器，完成了 DelegatingFilterProxy 到 FilterChainProxy 的连接，并借助于 FilterChainProxy 内部维护的 List 中的某一个 SecurityFilterChain 来完成最终的过滤。 ** 推荐阅读 ** https://www.cnkirito.moe/spring-security-1/ Spring Security( 一)–Architecture Overview https://www.cnkirito.moe/spring-security-2/ Spring Security( 二)–Guides https://www.cnkirito.moe/spring-security-3/ Spring Security( 三)– 核心配置解读 https://www.cnkirito.moe/spring-security-4/ Spring Security( 四)– 核心过滤器源码分析 https://www.cnkirito.moe/spring-security-5/ Spring Security( 五)– 动手实现一个 IP_Login https://www.cnkirito.moe/spring-security-6/ 该如何设计你的 PasswordEncoder?","link":"/spring-security-7/"},{"title":"Re：从零开始的 Spring Session(二)","text":"上一篇文章介绍了一些 Session 和 Cookie 的基础知识，这篇文章开始正式介绍 Spring Session 是如何对传统的 Session 进行改造的。官网这么介绍 Spring Session： Spring Session provides an API and implementations for managing a user’s session information. It also provides transparent integration with: HttpSession - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral way. Additional features include: Clustered Sessions - Spring Session makes it trivial to support clustered sessions without being tied to an application container specific solution. Multiple Browser Sessions - Spring Session supports managing multiple users’ sessions in a single browser instance (i.e. multiple authenticated accounts similar to Google). RESTful APIs - Spring Session allows providing session ids in headers to work with RESTful APIs WebSocket - provides the ability to keep the HttpSession alive when receiving WebSocket messages 其具体的特性非常之多，具体的内容可以从文档中了解到，笔者做一点自己的总结，Spring Session 的特性包括但不限于以下： 使用 GemFire 来构建 C/S 架构的 httpSession（不关注） 使用第三方仓储来实现集群 session 管理，也就是常说的分布式 session 容器，替换应用容器（如 tomcat 的 session 容器）。仓储的实现，Spring Session 提供了三个实现（redis，mongodb，jdbc），其中 redis 使我们最常用的。程序的实现，使用 AOP 技术，几乎可以做到透明化地替换。（核心） 可以非常方便的扩展 Cookie 和自定义 Session 相关的 Listener，Filter。 可以很方便的与 Spring Security 集成，增加诸如 findSessionsByUserName，rememberMe，限制同一个账号可以同时在线的 Session 数（如设置成 1，即可达到把前一次登录顶掉的效果）等等 介绍完特性，下面开始一步步集成 Spring Session 使用 Redis 集成 Spring Session 引入依赖，Spring Boot 的版本采用 1.5.4 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置 配置类开启 Redis Http Session 12345@Configuration@EnableRedisHttpSessionpublic class HttpSessionConfig {} 基本是 0 配置，只需要让主配置扫描到 @EnableRedisHttpSession 即可 配置文件 application.yml，配置连接的 redis 信息 12345spring: redis: host: localhost port: 6379 database: 0 编写测试 Controller，以便于观察 Spring Session 的特性，和前一篇文章使用同样的代码 12345678910111213141516171819202122@Controllerpublic class CookieController { @RequestMapping(&quot;/test/cookie&quot;) public String cookie(@RequestParam(&quot;browser&quot;) String browser, HttpServletRequest request, HttpSession session) { // 取出 session 中的 browser Object sessionBrowser = session.getAttribute(&quot;browser&quot;); if (sessionBrowser == null) { System.out.println(&quot;不存在 session，设置 browser=&quot; + browser); session.setAttribute(&quot;browser&quot;, browser); } else { System.out.println(&quot;存在 session，browser=&quot; + sessionBrowser.toString()); } Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0) { for (Cookie cookie : cookies) { System.out.println(cookie.getName() + &quot;:&quot; + cookie.getValue()); } } return &quot;index&quot;; }} 启动类省略，下面开始测试。 在浏览器中访问如下端点：http://localhost:8080/test/cookie?browser=chrome，下面是连续访问 4 次的结果 12345671 不存在 session，设置 browser=chrome2 存在 session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a1593 存在 session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a1594 存在 session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a159 如果还记得上一篇文章中运行结果的话，会发现和原生的 session 管理是有一些差别，原先的信息中我们记得 Cookie 中记录的 Key 值是 JSESSIONID，而替换成 RedisHttpSession 之后变成了 SESSION。接着观察 redis 中的变化： 解析一下这个 redis store，如果不纠结于细节，可以跳过，不影响使用。 ​1 spring:session 是默认的 Redis HttpSession 前缀（redis 中，我们常用 ‘:’ 作为分割符）。 2 每一个 session 都会有三个相关的 key，第三个 key 最为重要，它是一个 HASH 数据结构，将内存中的 session 信息序列化到了 redis 中。如上文的 browser，就被记录为 sessionAttr:browser=chrome, 还有一些 meta 信息，如创建时间，最后访问时间等。 3 另外两个 key，expirations:1504446540000 和 sessions:expires:7079… 我发现大多数的文章都没有对其分析，前者是一个 SET 类型，后者是一个 STRING 类型，可能会有读者发出这样的疑问，redis 自身就有过期时间的设置方式 TTL，为什么要额外添加两个 key 来维持 session 过期的特性呢？这需要对 redis 有一定深入的了解才能想到这层设计。当然这不是本节的重点，简单提一下：redis 清除过期 key 的行为是一个异步行为且是一个低优先级的行为，用文档中的原话来说便是，可能会导致 session 不被清除。于是引入了专门的 expiresKey，来专门负责 session 的清除，包括我们自己在使用 redis 时也需要关注这一点。在开发层面，我们仅仅需要关注第三个 key 就行了。 总结本节主要讲解了 Spring Boot 如何集成 Spring Session，下一节将介绍更加复杂的特性。包括自定义 Cookie 序列化策略，与 Spring Security 的集成，根据用户名查找 session 等特性以及使用注意点。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-session-2/"},{"title":"Re：从零开始的 Spring Session(三)","text":"上一篇文章中，我们使用 Redis 集成了 Spring Session。大多数的配置都是 Spring Boot 帮我们自动配置的，这一节我们介绍一点 Spring Session 较为高级的特性。 集成 Spring Security之所以把 Spring Session 和 Spring Security 放在一起讨论，是因为我们的应用在集成 Spring Security 之后，用户相关的认证与 Session 密不可分，如果不注意一些细节，会引发意想不到的问题。 与 Spring Session 相关的依赖可以参考上一篇文章，这里给出增量的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 我们引入依赖后，就已经自动配置了 Spring Security，我们在 application.yml 添加一个内存中的用户： 1234security: user: name: admin password: admin 测试登录点沿用上一篇文章的端点，访问 http://localhost:8080/test/cookie?browser=chrome 端点后会出现 http basic 的认证框，我们输入 admin/admin，即可获得结果，也遇到了第一个坑点，我们会发现每次请求，sessionId 都会被刷新，这显然不是我们想要的结果。 这个现象笔者研究了不少源码，但并没有得到非常满意的解释，只能理解为 SecurityAutoConfiguration 提供的默认配置，没有触发到响应的配置，导致了 session 的不断刷新（如果读者有合理的解释可以和我沟通）。Spring Session 之所以能够替换默认的 tomcat httpSession 是因为配置了 springSessionRepositoryFilter 这个过滤器，且提供了非常高的优先级，这归功于 AbstractSecurityWebApplicationInitializer ，AbstractHttpSessionApplicationInitializer 这两个初始化器，当然，也保证了 Spring Session 会在 Spring Security 之前起作用。 而解决上述的诡异现象也比较容易（但原理不清），我们使用 @EnableWebSecurity 对 Spring Security 进行一些配置，即可解决这个问题。 1234567891011121314151617181920212223242526@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter { // @formatter:off @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(&quot;/resources/**&quot;).permitAll() .anyRequest().authenticated() .and() .httpBasic()//&lt;1&gt; .and() .logout().permitAll(); } // @formatter:on // @formatter:off @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception { auth .inMemoryAuthentication() .withUser(&quot;admin&quot;).password(&quot;admin&quot;).roles(&quot;USER&quot;);//&lt;2&gt; } // @formatter:on} &lt;1&gt; 不想大费周章写一个登录页面，于是开启了 http basic 认证 &lt;2&gt; 配置了 security config 之后，springboot 的 autoConfig 就会失效，于是需要手动配置用户。 再次请求，可以发现 SessionId 返回正常，@EnableWebSecurity 似乎触发了相关的配置，当然了，我们在使用 Spring Security 时不可能使用 autoconfig，但是这个现象的确是一个疑点。 使用自定义 CookieSerializer12345678@Beanpublic CookieSerializer cookieSerializer() { DefaultCookieSerializer serializer = new DefaultCookieSerializer(); serializer.setCookieName(&quot;JSESSIONID&quot;); serializer.setCookiePath(&quot;/&quot;); serializer.setDomainNamePattern(&quot;^.+?\\\\.(\\\\w+\\\\.[a-z]+)$&quot;); return serializer;} 使用上述配置后，我们可以将 Spring Session 默认的 Cookie Key 从 SESSION 替换为原生的 JSESSIONID。而 CookiePath 设置为根路径且配置了相关的正则表达式，可以达到同父域下的单点登录的效果，在未涉及跨域的单点登录系统中，这是一个非常优雅的解决方案。如果我们的当前域名是 moe.cnkirito.moe，该正则会将 Cookie 设置在父域 cnkirito.moe 中，如果有另一个相同父域的子域名 blog.cnkirito.moe 也会识别这个 Cookie，便可以很方便的实现同父域下的单点登录。 根据用户名查找用户归属的 SESSION这个特性听起来非常有意思，你可以在一些有趣的场景下使用它，如知道用户名后即可删除其 SESSION。一直以来我们都是通过线程绑定的方式，让用户操作自己的 SESSION，包括获取用户名等操作。但如今它提供了一个反向的操作，根据用户名获取 SESSION，恰巧，在一些项目中真的可以使用到这个特性，最起码，当别人问起你，或者讨论到和 SESSION 相关的知识时，你可以明晰一点，这是可以做到的。 我们使用 Redis 作为 Session Store 还有一个好处，就是其实现了 FindByIndexNameSessionRepository 接口，下面让我们来见证这一点。 123456789101112@Controllerpublic class CookieController { @Autowired FindByIndexNameSessionRepository&lt;? extends ExpiringSession&gt; sessionRepository; @RequestMapping(&quot;/test/findByUsername&quot;) @ResponseBody public Map findByUsername(@RequestParam String username) { Map&lt;String, ? extends ExpiringSession&gt; usersSessions = sessionRepository.findByIndexNameAndIndexValue(FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME, username); return usersSessions; }} 由于一个用户可能拥有多个 Session，所以返回的是一个 Map 信息，而这里的 username，则就是与 Spring Security 集成之后的用户名，最令人感动 Spring 厉害的地方，是这一切都是自动配置好的。我们在内存中配置的用户的 username 是 admin，于是我们访问这个端点, 可以看到如下的结果 连同我们存入 session 中的 browser=chrome，browser=360 都可以看见（只有键名）。 总结Spring Session 对各种场景下的 Session 管理提供一套非常完善的实现。笔者所介绍的，仅仅是 Spring Session 常用的一些特性，更多的知识点可以在 spring.io 的文档中一览无余，以及本文中作者存在的一个疑惑，如有兴趣可与我沟通。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-session-3/"},{"title":"从 Spring-Session 源码看 Session 机制的实现细节","text":"去年我曾经写过几篇和 Spring Session 相关的文章，从一个未接触过 Spring Session 的初学者视角介绍了 Spring Session 如何上手，如果你未接触过 Spring Session，推荐先阅读下「从零开始学习 Spring Session」系列（https://www.cnkirito.moe/categories/Spring-Session/） Spring Session 主要解决了分布式场景下 Session 的共享问题，本文将从 Spring Session 的源码出发，来讨论一些 Session 设计的细节。 Spring Session 数据结构解读想象一个场景，现在一到面试题呈现在你面前，让你从零开始设计一个 Session 存储方案，你会怎么回答？ 说白了就是让你设计一套数据结构存储 Session，并且我相信提出这个问题时，大多数读者脑海中会浮现出 redis，设计一个 map，使用 ttl 等等，但没想到的细节可能会更多。先来预览一下 Spring Session 的实际数据结构是什么样的（使用 spring-session-redis 实现），当我们访问一次集成了 Spring Session 的 web 应用时 12345@RequestMapping(&quot;/helloworld&quot;)public String hello(HttpSession session){ session.setAttribute(&quot;name&quot;,&quot;xu&quot;); return &quot;hello.html&quot;;} 可以在 Redis 中看到如下的数据结构： 12345A) &quot;spring:session:sessions:39feb101-87d4-42c7-ab53-ac6fe0d91925&quot;B) &quot;spring:session:expirations:1523934840000&quot;C) &quot;spring:session:sessions:expires:39feb101-87d4-42c7-ab53-ac6fe0d91925&quot; 这三种键职责的分析将会贯彻全文，为了统一叙述，在此将他们进行编号，后续简称为 A 类型键，B 类型键，C 类型键。先简单分析下他们的特点 他们公用的前缀是 spring:session A 类型键的组成是前缀 +”sessions”+sessionId，对应的值是一个 hash 数据结构。在我的 demo 中，其值如下 123456{ &quot;lastAccessedTime&quot;: 1523933008926,/*2018/4/17 10:43:28*/ &quot;creationTime&quot;: 1523933008926, /*2018/4/17 10:43:28*/ &quot;maxInactiveInterval&quot;: 1800, &quot;sessionAttr:name&quot;: &quot;xu&quot;} 其中 creationTime（创建时间），lastAccessedTime（最后访问时间），maxInactiveInterval（session 失效的间隔时长） 等字段是系统字段，sessionAttr:xx 可能会存在多个键值对，用户存放在 session 中的数据如数存放于此。 A 类型键对应的默认 TTL 是 35 分钟。 B 类型键的组成是前缀 +”expirations”+ 时间戳，无需纠结这个时间戳的含义，先卖个关子。其对应的值是一个 set 数据结构，这个 set 数据结构中存储着一系列的 C 类型键。在我的 demo 中，其值如下 123[ &quot;expires:39feb101-87d4-42c7-ab53-ac6fe0d91925&quot;] B 类型键对应的默认 TTL 是 30 分钟 C 类型键的组成是前缀 +”sessions:expires”+sessionId，对应一个空值，它仅仅是 sessionId 在 redis 中的一个引用，具体作用继续卖关子。 C 类型键对应的默认 TTL 是 30 分钟。 kirito-session 的天使轮方案介绍完 Spring Session 的数据结构，我们先放到一边，来看看如果我们自己设计一个 Session 方案，拟定为 kirito-session 吧，该如何设计。 kirito 的心路历程是这样的：“使用 redis 存 session 数据，对，session 需要有过期机制，redis 的键可以自动过期，肯定很方便。” 于是 kirito 设计出了 spring-session 中的 A 类型键，复用它的数据结构： 123456{ &quot;lastAccessedTime&quot;: 1523933008926, &quot;creationTime&quot;: 1523933008926, &quot;maxInactiveInterval&quot;: 1800, key/value...} 然后对 A 类型的键设置 ttl A 30 分钟，这样 30 分钟之后 session 过期，0-30 分钟期间如果用户持续操作，那就根据 sessionId 找到 A 类型的 key，刷新 lastAccessedTime 的值，并重新设置 ttl，这样就完成了「续签」的特性。 显然 Spring Session 没有采用如此简练的设计，为什么呢？翻看 Spring Session 的文档 One problem with relying on Redis expiration exclusively is that Redis makes no guarantee of when the expired event will be fired if the key has not been accessed. Specifically the background task that Redis uses to clean up expired keys is a low priority task and may not trigger the key expiration. For additional details see Timing of expired events section in the Redis documentation. 大致意思是说，redis 的键过期机制不“保险”，这和 redis 的设计有关，不在此拓展开，研究这个的时候翻了不少资料，得出了如下的总结： redis 在键实际过期之后不一定会被删除，可能会继续存留，但具体存留的时间我没有做过研究，可能是 1~2 分钟，可能会更久。 具有过期时间的 key 有两种方式来保证过期，一是这个键在过期的时候被访问了，二是后台运行一个定时任务自己删除过期的 key。划重点：** 这启发我们在 key 到期后只需要访问一下 key 就可以确保 redis 删除该过期键 ** 如果没有指令持续关注 key，并且 redis 中存在许多与 TTL 关联的 key，则 key 真正被删除的时间将会有显著的延迟！显著的延迟！显著的延迟！ 天使轮计划惨遭破产，看来单纯依赖于 redis 的过期时间是不可靠的，秉持着力求严谨的态度，迎来了 A 轮改造。 A 轮改造—引入 B 类型键确保 session 的过期机制redis 的官方文档启发我们，可以启用一个后台定时任务，定时去删除那些过期的键，配合上 redis 的自动过期，这样可以双重保险。第一个问题来了，我们将这些过期键存在哪儿呢？不找个合适的地方存起来，定时任务到哪儿去删除这些应该过期的键呢？总不能扫描全库吧！来解释我前面卖的第一个关子，看看 B 类型键的特点： 1spring:session:expirations:1523934840000 时间戳的含义1523934840000 这明显是个 Unix 时间戳，它的含义是存放着这一分钟内应该过期的键，所以它是一个 set 数据结构。解释下这个时间戳是怎么计算出来的 org.springframework.session.data.redis.RedisSessionExpirationPolicy#roundUpToNextMinute 12345678static long roundUpToNextMinute(long timeInMs) { Calendar date = Calendar.getInstance(); date.setTimeInMillis(timeInMs); date.add(Calendar.MINUTE, 1); date.clear(Calendar.SECOND); date.clear(Calendar.MILLISECOND); return date.getTimeInMillis(); } 还记得 lastAccessedTime=1523933008926，maxInactiveInterval=1800 吧，lastAccessedTime 转换成北京时间是: 2018/4/17 10:43:28，向上取整是 2018/4/17 10:44:00，再次转换为 Unix 时间戳得到 1523932980000，单位是 ms，1800 是过期时间的间隔，单位是 s，二者相加 1523932980000+1800*1000=1523934840000。这样 B 类型键便作为了一个「桶」，存放着这一分钟应当过期的 session 的 key。 后台定时任务org.springframework.session.data.redis.RedisSessionExpirationPolicy#cleanupExpiredSessions 1234@Scheduled(cron = &quot;${spring.session.cleanup.cron.expression:0 * * * * *}&quot;)public void cleanupExpiredSessions() { this.expirationPolicy.cleanExpiredSessions();} 后台提供了定时任务去“删除”过期的 key，来补偿 redis 到期未删除的 key。方案再描述下，方便大家理解：取得当前时间的时间戳作为 key，去 redis 中定位到 spring:session:expirations:{当前时间戳} ，这个 set 里面存放的便是所有过期的 key 了。 续签的影响每次 session 的续签，需要将旧桶中的数据移除，放到新桶中。验证这一点很容易。 在第一分钟访问一次 http://localhost:8080/helloworld 端点，得到的 B 类型键为：spring:session:expirations:1523934840000；第二分钟再访问一次 http://localhost:8080/helloworld 端点，A 类型键的 lastAccessedTime 得到更新，并且 spring:session:expirations:1523934840000 这个桶被删除了，新增了 spring:session:expirations:1523934900000 这个桶。当众多用户活跃时，桶的增删和以及 set 中数据的增删都是很频繁的。对了，没提到的一点，对应 key 的 ttl 时间也会被更新。 kirito-session 方案貌似比之前严谨了，目前为止使用了 A 类型键和 B 类型键解决了 session 存储和 redis 键到期不删除的两个问题，但还是存在问题的。 B 轮改造—优雅地解决 B 类型键的并发问题引入 B 类型键看似解决了问题，却也引入了一个新的问题：并发问题。 来看看一个场景： 假设存在一个 sessionId=1 的会话，初始时间戳为 1420656360000 12spring:session:expirations:1420656360000 -&gt; [1]spring:session:session:1 -&gt; &lt;session&gt; 接下来迎来了并发访问，（用户可能在浏览器中多次点击）： 线程 1 在第 2 分钟请求，产生了续签，session:1 应当从 1420656360000 这个桶移动到 142065642000 这个桶 线程 2 在第 3 分钟请求，也产生了续签，session:1 本应当从 1420656360000 这个桶移动到 142065648000 这个桶 如果上两步按照次序执行，自然不会有问题。但第 3 分钟的请求可能已经执行完毕了，第 2 分钟才刚开始执行。 像下面这样： 线程 2 从第一分钟的桶中移除 session:1，并移动到第三分钟的桶中 123spring:session:expirations:1420656360000 -&gt; []spring:session:session:1 -&gt; &lt;session&gt;spring:session:expirations:1420656480000 -&gt; [1] 线程 1 完成相同的操作，它也是基于第一分钟来做的，但会移动到第二分钟的桶中 123spring:session:expirations:1420656360000 -&gt; []spring:session:session:1 -&gt; &lt;session&gt;spring:session:expirations:1420656420000 -&gt; [1] 最后 redis 中键的情况变成了这样： 1234spring:session:expirations:1420656360000 -&gt; []spring:session:session:1 -&gt; &lt;session&gt;spring:session:expirations:1420656480000 -&gt; [1]spring:session:expirations:1420656420000 -&gt; [1] 后台定时任务会在第 32 分钟扫描到 spring:session:expirations:1420656420000 桶中存在的 session，这意味着，本应该在第 33 分钟才会过期的 key，在第 32 分钟就会被删除！ 一种简单的方法是用户的每次 session 续期加上分布式锁，这显然不能被接受。来看看 Spring Session 是怎么巧妙地应对这个并发问题的。 org.springframework.session.data.redis.RedisSessionExpirationPolicy#cleanExpiredSessions 1234567891011121314151617181920212223242526272829303132public void cleanExpiredSessions() { long now = System.currentTimeMillis(); long prevMin = roundDownMinute(now); if (logger.isDebugEnabled()) { logger.debug(&quot;Cleaning up sessions expiring at&quot; + new Date(prevMin)); } // 获取到 B 类型键 String expirationKey = getExpirationKey(prevMin); // 取出当前这一分钟应当过期的 session Set&lt;Object&gt; sessionsToExpire = this.redis.boundSetOps(expirationKey).members(); // 注意：这里删除的是 B 类型键，不是删除 session 本身！ this.redis.delete(expirationKey); for (Object session : sessionsToExpire) { String sessionKey = getSessionKey((String) session); // 遍历一下 C 类型的键 touch(sessionKey); }}/** * By trying to access the session we only trigger a deletion if it the TTL is * expired. This is done to handle * https://github.com/spring-projects/spring-session/issues/93 * * @param key the key */private void touch(String key) { // 并不是删除 key，而只是访问 key this.redis.hasKey(key);} 这里面逻辑主要是拿到过期键的集合（实际上是 C 类型的 key，但这里可以理解为 sessionId，C 类型我下面会介绍），此时这个集合里面存在三种类型的 sessionId。 已经被 redis 删除的过期键。万事大吉，redis 很靠谱的及时清理了过期的键。 已经过期，但是还没来得及被 redis 清除的 key。还记得前面 redis 文档里面提到的一个技巧吗？我们在 key 到期后只需要访问一下 key 就可以确保 redis 删除该过期键，所以 redis.hasKey(key); 该操作就是为了触发 redis 的自己删除。 并发问题导致的多余数据，实际上并未过期。如上所述，第 32 分钟的桶里面存在的 session:1 实际上并不应该被删除，使用 touch 的好处便是我只负责检测，删不删交给 redis 判断。session:1 在第 32 分钟被 touch 了一次，并未被删除，在第 33 分钟时应当被 redis 删除，但可能存在延时，这个时候 touch 一次，确保删除。 所以，源码里面特别强调了一下：要用 touch 去触发 key 的删除，而不能直接 del key。 参考 https://github.com/spring-projects/spring-session/issues/93 C 轮改造—增加 C 类型键完善过期通知事件虽然引入了 B 类型键，并且在后台加了定时器去确保 session 的过期，但似乎…emmmmm… 还是不够完善。在此之前，kirito-session 的设计方案中，存储 session 实际内容的 A 类型键和用于定时器确保删除的桶 B 类型键过期时间都是 30 分钟 (key 的 TTL 是 30 分钟)，注意一个细节，spring-session 中 A 类型键的过期时间是 35 分钟，比实际的 30 分钟多了 5 分钟，这意味着即便 session 已经过期，我们还是可以在 redis 中有 5 分钟间隔来操作过期的 session。于此同时，spring-session 引入了 C 类型键来作为 session 的引用。 解释下之前卖的第二个关子，C 类型键的组成为前缀 +”sessions:expires”+sessionId，对应一个空值，同时也是 B 类型键桶中存放的 session 引用，ttl 为 30 分钟，具体作用便是在自身过期后触发 redis 的 keyspace notifications (http://redis.io/topics/notifications)，具体如何监听 redis 的过期事件简单介绍下：org.springframework.session.data.redis.config.ConfigureNotifyKeyspaceEventsAction 该类配置了相关的过期监听，并使用 SessionExpiredEvent 事件发放 session 的过期事件。为什么引入 C 类型键？keyspace notifications 只会告诉我们哪个键过期了，不会告诉我们内容是什么。** 关键就在于如果 session 过期后监听器可能想要访问 session 的具体内容，然而自身都过期了，还怎么获取内容 **。所以，C 类型键存在的意义便是解耦 session 的存储和 session 的过期，并且使得 server 获取到过期通知后可以访问到 session 真实的值。对于用户来说，C 类型键过期后，意味着登录失效，而对于服务端而言，真正的过期其实是 A 类型键过期，这中间会有 5 分钟的误差。 一点点想法，担忧，疑惑本文大概介绍了 Spring Session 的三种 key 的原因，理清楚其中的逻辑花了不少时间，项目改造正好涉及到相关的缓存值过期这一需求，完全可以参考 Spring Session 的方案。但担忧也是有的，如果真的只是 1~2 两分钟的延迟过期（对应 A 轮改造中遇到的问题），以及 1 分钟的提前删除（对应 B 轮改造中的并发问题）其实个人感觉没必要计较。从产品体验上来说，用户应该不会在意 32 分钟自动退出和 30 分钟退出，可以说 Spring Session 是为了严谨而设计了这一套方案，但引入了定时器和很多辅助的键值对，无疑对内存消耗和 cpu 消耗都是一种浪费。如果在生产环境大量使用 Spring Session，最好权衡下本文提及的相关问题。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-session-4/"},{"title":"spring 中的懒加载与事务 -- 排坑记录","text":"案例描述本文主要描述了开发中常见的几个与 spring 懒加载和事务相关的案例，描述常见的使用场景，以及如何规避他们，给出具体的代码。 在新的线程中，访问某个持久化对象的懒加载属性。 在 quartz 定时任务中，访问某个持久化对象的懒加载属性。 在 dubbo，motan 一类 rpc 框架中，远程调用时服务端 session 关闭的问题。 上面三个案例，其实核心都是一个问题，就是牵扯到 spring 对事务的管理，而懒加载这个技术，只是比较容易体现出事务出错的一个实践，主要用它来引发问题，进而对问题进行思考。 前期准备为了能直观的暴露出第一个案例的问题，我新建了一个项目，采用传统的 mvc 分层，一个 student.java 实体类，一个 studentDao.java 持久层，一个 studentService.java 业务层，一个 studentController 控制层。 12345678910@Entity@Table(name = &quot;student&quot;)public class Student { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Integer id; private String name; getter..setter..} 持久层使用 springdata，框架自动扩展出 CURD 方法 12public interface StudentDao extends JpaRepository&lt;Student, Integer&gt;{} service 层，先给出普通的调用方法。用于错误演示。 1234567891011@Servicepublic class StudentService { @Autowired StudentDao studentDao; public void testNormalGetOne(){ Student student = studentDao.getOne(1); System.out.println(student.getName()); }} 注意：getOne 和 findOne 都是 springdata 提供的根据 id 查找单个实体的方法，区别是前者是懒加载，后者是立即加载。我们使用 getOne 来进行懒加载的实验，就不用大费周章去写懒加载属性，设置多个实体类了。 controller 层，不是简简单单的调用，而是在新的线程中调用。使用 controller 层来代替单元测试（实际项目中，通常使用 controller 调用 service，然后在浏览器或者 http 工具中调用触发，较为方便） 1234567891011@RequestMapping(&quot;/testNormalGetOne&quot;)@ResponseBodypublic String testNormalGetOne() { new Thread(new Runnable() { @Override public void run() { studentService.testNormalGetOne(); } }).start(); return &quot;testNormalGetOne&quot;;} 启动项目后，访问 localhost:8080/testNormalGetOne 报错如下： 1Exception in thread &quot;Thread-6&quot; org.hibernate.LazyInitializationException: could not initialize proxy - no Session 问题分析no session 说明了什么？道理很简单，因为 spring 的 session 是和线程绑定的，在整个 model-&gt;dao-&gt;service-&gt;controller 的调用链中，这种事务和线程绑定的机制非常契合。而我们出现的问题正式由于新开启了一个线程，这个线程与调用链的线程不是同一个。 问题解决我们先使用一种不太优雅的方式解决这个问题。在新的线程中，手动打开 session。 12345678910public void testNormalGetOne() { EntityManagerFactory entityManagerFactory = ApplicationContextProvider.getApplicationContext().getBean(EntityManagerFactory.class); EntityManager entityManager = entityManagerFactory.createEntityManager(); EntityManagerHolder entityManagerHolder = new EntityManagerHolder(entityManager); TransactionSynchronizationManager.bindResource(entityManagerFactory, entityManagerHolder); Student student = studentDao.getOne(1); System.out.println(student.getName()); TransactionSynchronizationManager.unbindResource(entityManagerFactory); EntityManagerFactoryUtils.closeEntityManager(entityManager);} 由于我们使用了 JPA，所以事务是由 EntityManagerFactory 这个工厂类生成的 EntityManager 来管理的。TransactionSynchronizationManager.bindResource(entityManagerFactory, entityManagerHolder); 这个方法使用事务管理器绑定 session。而 ApplicationContextProvider 这个工具类是用来获取 spring 容器中的 EntityManagerFactory 的，为什么不用注入的方式，下文讲解。它的代码如下： 12345678910111213public class ApplicationContextProvider implements ApplicationContextAware { private static ApplicationContext context = null; public static ApplicationContext getApplicationContext() { return context; } @Override public void setApplicationContext(ApplicationContext ac) throws BeansException { context = ac; }} 问题暂时得到了解决。 问题再思考我们一般情况下使用懒加载属性，为什么没有出现 no session 的问题呢？相信大家都知道 @Transactional 这个注解，他会帮我们进行事务包裹，当然也会绑定 session；以及大家熟知的 hiberbate 中的 OpenSessionInterceptor 和 OpenSessionInViewFilter 以及 jpa 中的 OpenEntityManagerInViewInterceptor 都是在没有 session 的情况下，打开 session 的过滤器。这种方法开始前依赖事务开启，方法结束后回收资源的操作，非常适合用过滤器拦截器处理，后续的两个未讲解的案例，其实都是使用了特殊的过滤器。 看一下官方文档如何描述这个 jpa 中的过滤器的： 29.3.4 Open EntityManager in View If you are running a web application, Spring Boot will by default register OpenEntityManagerInViewInterceptor to apply the “Open EntityManager in View” pattern, i.e. to allow for lazy loading in web views. If you don’t want this behavior you should set spring.jpa.open-in-view to false in your application.properties. 我们尝试着关闭这个过滤器：配置 application.properties/application.yml 文件 1spring.jpa.open-in-view=false 再使用正常的方式访问懒加载属性（而不是在一个新的线程中）： 1234567891011 @RequestMapping(&quot;/testNormalGetOne&quot;) @ResponseBody public String testNormalGetOne() {// new Thread(new Runnable() {// @Override// public void run() { studentService.testNormalGetOne();// }// }).start(); return &quot;testNormalGetOne&quot;; } 报错如下： 1{&quot;timestamp&quot;:1498194914012,&quot;status&quot;:500,&quot;error&quot;:&quot;Internal Server Error&quot;,&quot;exception&quot;:&quot;org.hibernate.LazyInitializationException&quot;,&quot;message&quot;:&quot;could not initialize proxy - no Session&quot;,&quot;path&quot;:&quot;/testNormalGetOne&quot;} 是的，我们使用 spring 的 controller 作为单元测试时，以及我们平时在直接使用 jpa 的懒加载属性时没有太关注这个 jpa 的特性，因为 springboot 帮我们默认开启了这个过滤器。这也解释了，为什么在新的线程中，定时任务线程中，rpc 远程调用时 session 没有打开的原因，因为这些流程没有经过 springboot 的 web 调用链。 另外两个实战案例上文已经阐释了，为什么 quartz 定时任务中访问懒加载属性，rpc 框架服务端访问懒加载属性（注意不是客户端，客户端访问懒加载属性那是一种作死的行为，因为是代理对象）为出现问题。我们仿照 spring 打开 session 的思路（这取决于你使用 hibernate 还是 jpa，抑或是 mybatis），来编写我们的过滤器。 quartz 中打开 session：使用 quartz 提供的 JobListenerSupport 支持，编写一个任务过滤器，用于在每次任务执行时打开 session 1234567891011121314151617181920212223242526272829303132public class OpenEntityManagerJobListener extends JobListenerSupport implements ApplicationContextAware { @Override public String getName() { return &quot;OpenEntityManagerJobListener&quot;; } EntityManagerFactory entityManagerFactory; @Override public void jobToBeExecuted(JobExecutionContext context) { entityManagerFactory = applicationContext.getBean(EntityManagerFactory.class); EntityManager entityManager = entityManagerFactory.createEntityManager(); EntityManagerHolder emHolder = new EntityManagerHolder(entityManager); TransactionSynchronizationManager.bindResource(entityManagerFactory, emHolder); } @Override public void jobWasExecuted(JobExecutionContext context, JobExecutionException jobException) { EntityManagerHolder emHolder = (EntityManagerHolder) TransactionSynchronizationManager.unbindResource(entityManagerFactory); EntityManagerFactoryUtils.closeEntityManager(emHolder.getEntityManager()); } ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext; if(this.applicationContext ==null) throw new RuntimeException(&quot;applicationContext is null&quot;); }} ** 配置调度工厂：** 12345678// 调度工厂 @Bean public SchedulerFactoryBean schedulerFactoryBean() { SchedulerFactoryBean factoryBean = new SchedulerFactoryBean(); factoryBean.setTriggers(triggerFactoryBeans().getObject()); factoryBean.setGlobalJobListeners(openEntityManagerJobListener()); return factoryBean; } 也可以参考我的另一篇描述更为细致的文章 (解决 Quartz 定时器中查询懒加载数据 no session 的问题)，那是我还是刚刚参加工作，可能有些许疏漏之处，不过参考是够了。 Motan（我现在使用的 rpc 框架）服务端打开 session利用了 motan 对 spi 扩展的支持，编写了一个 Filter，主要参考了 motan 的 spi 过滤器写法和 springdata 打开 session/entityManager 的思路。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158@SpiMeta(name = &quot;openjpasession&quot;)@Activation(sequence = 100)public class OpenEntityManagerInMotanFilter implements Filter { private Logger logger = LoggerFactory.getLogger(OpenEntityManagerInMotanFilter.class); /** * Default EntityManagerFactory bean name: &quot;entityManagerFactory&quot;. * Only applies when no &quot;persistenceUnitName&quot; param has been specified. * * @see #setEntityManagerFactoryBeanName * @see #setPersistenceUnitName */ public static final String DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME = &quot;entityManagerFactory&quot;; private String entityManagerFactoryBeanName; private String persistenceUnitName; private volatile EntityManagerFactory entityManagerFactory; /** * Set the bean name of the EntityManagerFactory to fetch from Spring's * root application context. * &lt;p&gt;Default is &quot;entityManagerFactory&quot;. Note that this default only applies * when no &quot;persistenceUnitName&quot; param has been specified. * * @see #setPersistenceUnitName * @see #DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME */ public void setEntityManagerFactoryBeanName(String entityManagerFactoryBeanName) { this.entityManagerFactoryBeanName = entityManagerFactoryBeanName; } /** * Return the bean name of the EntityManagerFactory to fetch from Spring's * root application context. */ protected String getEntityManagerFactoryBeanName() { return this.entityManagerFactoryBeanName; } /** * Set the name of the persistence unit to access the EntityManagerFactory for. * &lt;p&gt;This is an alternative to specifying the EntityManagerFactory by bean name, * resolving it by its persistence unit name instead. If no bean name and no persistence * unit name have been specified, we'll check whether a bean exists for the default * bean name &quot;entityManagerFactory&quot;; if not, a default EntityManagerFactory will * be retrieved through finding a single unique bean of type EntityManagerFactory. * * @see #setEntityManagerFactoryBeanName * @see #DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME */ public void setPersistenceUnitName(String persistenceUnitName) { this.persistenceUnitName = persistenceUnitName; } /** * Return the name of the persistence unit to access the EntityManagerFactory for, if any. */ protected String getPersistenceUnitName() { return this.persistenceUnitName; } /** * Look up the EntityManagerFactory that this filter should use. * &lt;p&gt;The default implementation looks for a bean with the specified name * in Spring's root application context. * * @return the EntityManagerFactory to use * @see #getEntityManagerFactoryBeanName */ protected EntityManagerFactory lookupEntityManagerFactory() { String emfBeanName = getEntityManagerFactoryBeanName(); String puName = getPersistenceUnitName(); if (StringUtils.hasLength(emfBeanName)) { return ApplicationContextProvider.getApplicationContext().getBean(emfBeanName, EntityManagerFactory.class); } else if (!StringUtils.hasLength(puName) &amp;&amp; ApplicationContextProvider.getApplicationContext().containsBean(DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME)) { return ApplicationContextProvider.getApplicationContext().getBean(DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME, EntityManagerFactory.class); } else { // Includes fallback search for single EntityManagerFactory bean by type. return EntityManagerFactoryUtils.findEntityManagerFactory(ApplicationContextProvider.getApplicationContext(), puName); } } /** * Create a JPA EntityManager to be bound to a request. * &lt;p&gt;Can be overridden in subclasses. * * @param emf the EntityManagerFactory to use * @see javax.persistence.EntityManagerFactory#createEntityManager() */ protected EntityManager createEntityManager(EntityManagerFactory emf) { return emf.createEntityManager(); } @Override public Response filter(Caller&lt;?&gt; caller, Request request) { if (!(caller instanceof Provider)) { return caller.call(request); } EntityManagerFactory emf = null; try { emf = lookupEntityManagerFactory(); } catch (Exception e) { logger.error(e.getMessage(), e); } // 可能没有启用 openjpa if (emf == null) { return caller.call(request); } try { // 如果没有绑定，绑定到当前线程 if (TransactionSynchronizationManager.getResource(emf) == null) { EntityManager em = createEntityManager(emf); EntityManagerHolder emHolder = new EntityManagerHolder(em); TransactionSynchronizationManager.bindResource(emf, emHolder); } } catch (Exception e) { logger.error(e.getLocalizedMessage(), e); } try { return caller.call(request); } finally { // 解除绑定 closeManager(emf); } } /** * 关闭 emf * * @param emf */ private void closeManager(EntityManagerFactory emf) { if (emf == null || TransactionSynchronizationManager.getResource(emf) == null) { return; } EntityManagerHolder emHolder = null; try { emHolder = (EntityManagerHolder) TransactionSynchronizationManager.unbindResource(emf); } catch (IllegalStateException e) { logger.error(e.getLocalizedMessage(), e); } try { if (emHolder != null) { EntityManagerFactoryUtils.closeEntityManager(emHolder.getEntityManager()); } } catch (Exception e) { logger.error(e.getLocalizedMessage(), e); } }} 总结springboot 中的事务管理做的永远比我们想的多，事务管理器的使用场景，@Transactional 究竟起了哪些作用，以及 spring-data 这个对 DDD 最佳的阐释，以及 mybatis 一类的非 j2ee 规范在微服务的地位中是否高于 jpa，各个层次之间的实体传输，消息传递… 都是值得思考的。","link":"/spring-transation-1/"},{"title":"使用 spring validation 完成数据后端校验","text":"前言数据的校验是交互式网站一个不可或缺的功能，前端的 js 校验可以涵盖大部分的校验职责，如用户名唯一性，生日格式，邮箱格式校验等等常用的校验。但是为了避免用户绕过浏览器，使用 http 工具直接向后端请求一些违法数据，服务端的数据校验也是必要的，可以防止脏数据落到数据库中，如果数据库中出现一个非法的邮箱格式，也会让运维人员头疼不已。我在之前保险产品研发过程中，系统对数据校验要求比较严格且追求可变性及效率，曾使用 drools 作为规则引擎，兼任了校验的功能。而在一般的应用，可以使用本文将要介绍的 validation 来对数据进行校验。 简述 JSR303/JSR-349，hibernate validation，spring validation 之间的关系。JSR303 是一项标准,JSR-349 是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如 @Null，@NotNull，@Pattern，他们位于 javax.validation.constraints 包下，只提供规范不提供实现。而 hibernate validation 是对这个规范的实践（不要将 hibernate 和数据库 orm 框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如 @Email，@Length，@Range 等等，他们位于 org.hibernate.validator.constraints 包下。而万能的 spring 为了给开发者提供便捷，对 hibernate validation 进行了二次封装，显示校验 validated bean 时，你可以使用 spring validation 或者 hibernate validation，而 spring validation 另一个特性，便是其在 springmvc 模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的 web 开发。本文主要介绍在 springmvc 中自动校验的机制。 引入依赖我们使用 maven 构建 springboot 应用来进行 demo 演示。 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 我们只需要引入 spring-boot-starter-web 依赖即可，如果查看其子依赖，可以发现如下的依赖： 12345678&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt; 验证了我之前的描述，web 模块使用了 hibernate-validation，并且 databind 模块也提供了相应的数据绑定功能。 构建启动类无需添加其他注解，一个典型的启动类 1234567@SpringBootApplicationpublic class ValidateApp { public static void main(String[] args) { SpringApplication.run(ValidateApp.class, args); }} 创建需要被校验的实体类1234567891011121314151617public class Foo { @NotBlank private String name; @Min(18) private Integer age; @Pattern(regexp = &quot;^1(3|4|5|7|8)\\\\d{9}$&quot;,message = &quot;手机号码格式错误&quot;) @NotBlank(message = &quot;手机号码不能为空&quot;) private String phone; @Email(message = &quot;邮箱格式错误&quot;) private String email; //... getter setter} 使用一些比较常用的校验注解，还是比较浅显易懂的，字段上的注解名称即可推断出校验内容，每一个注解都包含了 message 字段，用于校验失败时作为提示信息，特殊的校验注解，如 Pattern（正则校验），还可以自己添加正则表达式。 在 @RestController 中校验数据springmvc 为我们提供了自动封装表单参数的功能，一个添加了参数校验的典型 controller 如下所示。 123456789101112131415@RestControllerpublic class FooController { @RequestMapping(&quot;/foo&quot;) public String foo(@Validated Foo foo &lt;1&gt;, BindingResult bindingResult &lt;2&gt;) { if(bindingResult.hasErrors()){ for (FieldError fieldError : bindingResult.getFieldErrors()) { //... } return &quot;fail&quot;; } return &quot;success&quot;; }} 值得注意的地方： &lt;1&gt; 参数 Foo 前需要加上 @Validated 注解，表明需要 spring 对其进行校验，而校验的信息会存放到其后的 BindingResult 中。注意，必须相邻，如果有多个参数需要校验，形式可以如下。foo(@Validated Foo foo, BindingResult fooBindingResult ，@Validated Bar bar, BindingResult barBindingResult); 即一个校验类对应一个校验结果。 &lt;2&gt; 校验结果会被自动填充，在 controller 中可以根据业务逻辑来决定具体的操作，如跳转到错误页面。 一个最基本的校验就完成了，总结下框架已经提供了哪些校验：**JSR 提供的校验注解 **: 12345678910111213@Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) 被注释的元素必须符合指定的正则表达式 **Hibernate Validator 提供的校验注解 **： 12345@NotBlank(message =) 验证字符串非 null，且长度必须大于 0 @Email 被注释的元素必须是电子邮箱地址 @Length(min=,max=) 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range(min=,max=,message=) 被注释的元素必须在合适的范围内 校验实验我们对上面实现的校验入口进行一次测试请求：访问 http://localhost:8080/foo?name=xujingfeng&amp;email=000&amp;age=19 可以得到如下的 debug 信息： 实验告诉我们，校验结果起了作用。并且，可以发现当发生多个错误，spring validation 不会在第一个错误发生后立即停止，而是继续试错，告诉我们所有的错误。debug 可以查看到更多丰富的错误信息，这些都是 spring validation 为我们提供的便捷特性，基本适用于大多数场景。 你可能不满足于简单的校验特性，下面进行一些补充。 分组校验如果同一个类，在不同的使用场景下有不同的校验规则，那么可以使用分组校验。未成年人是不能喝酒的，而在其他场景下我们不做特殊的限制，这个需求如何体现同一个实体，不同的校验规则呢？ 改写注解，添加分组： 12345678Class Foo{ @Min(value = 18,groups = {Adult.class}) private Integer age; public interface Adult{} public interface Minor{}} 这样表明，只有在 Adult 分组下，18 岁的限制才会起作用。 Controller 层改写： 123456789101112131415161718192021@RequestMapping(&quot;/drink&quot;)public String drink(@Validated({Foo.Adult.class}) Foo foo, BindingResult bindingResult) { if(bindingResult.hasErrors()){ for (FieldError fieldError : bindingResult.getFieldErrors()) { //... } return &quot;fail&quot;; } return &quot;success&quot;;}@RequestMapping(&quot;/live&quot;)public String live(@Validated Foo foo, BindingResult bindingResult) { if(bindingResult.hasErrors()){ for (FieldError fieldError : bindingResult.getFieldErrors()) { //... } return &quot;fail&quot;; } return &quot;success&quot;;} drink 方法限定需要进行 Adult 校验，而 live 方法则不做限制。 自定义校验业务需求总是比框架提供的这些简单校验要复杂的多，我们可以自定义校验来满足我们的需求。自定义 spring validation 非常简单，主要分为两步。 1 自定义校验注解我们尝试添加一个“字符串不能包含空格”的限制。 123456789101112131415161718192021222324@Target({METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER})@Retention(RUNTIME)@Documented@Constraint(validatedBy = {CannotHaveBlankValidator.class})&lt;1&gt;public @interface CannotHaveBlank { // 默认错误消息 String message() default &quot;不能包含空格&quot;; // 分组 Class&lt;?&gt;[] groups() default {}; // 负载 Class&lt;? extends Payload&gt;[] payload() default {}; // 指定多个时使用 @Target({FIELD, METHOD, PARAMETER, ANNOTATION_TYPE}) @Retention(RUNTIME) @Documented @interface List { CannotHaveBlank[] value(); }} 我们不需要关注太多东西，使用 spring validation 的原则便是便捷我们的开发，例如 payload，List ，groups，都可以忽略。 &lt;1&gt; 自定义注解中指定了这个注解真正的验证者类。 2 编写真正的校验者类 1234567891011121314151617181920212223public class CannotHaveBlankValidator implements &lt;1&gt; ConstraintValidator&lt;CannotHaveBlank, String&gt; { @Override public void initialize(CannotHaveBlank constraintAnnotation) { } @Override public boolean isValid(String value, ConstraintValidatorContext context &lt;2&gt;) { //null 时不进行校验 if (value != null &amp;&amp; value.contains(&quot; &quot;)) { &lt;3&gt; // 获取默认提示信息 String defaultConstraintMessageTemplate = context.getDefaultConstraintMessageTemplate(); System.out.println(&quot;default message :&quot; + defaultConstraintMessageTemplate); // 禁用默认提示信息 context.disableDefaultConstraintViolation(); // 设置提示语 context.buildConstraintViolationWithTemplate(&quot;can not contains blank&quot;).addConstraintViolation(); return false; } return true; }} &lt;1&gt; 所有的验证者都需要实现 ConstraintValidator 接口，它的接口也很形象，包含一个初始化事件方法，和一个判断是否合法的方法。 1234public interface ConstraintValidator&lt;A extends Annotation, T&gt; { void initialize(A constraintAnnotation); boolean isValid(T value, ConstraintValidatorContext context);} &lt;2&gt; ConstraintValidatorContext 这个上下文包含了认证中所有的信息，我们可以利用这个上下文实现获取默认错误提示信息，禁用错误提示信息，改写错误提示信息等操作。 &lt;3&gt; 一些典型校验操作，或许可以对你产生启示作用。 值得注意的一点是，自定义注解可以用在 METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER 之上，ConstraintValidator 的第二个泛型参数 T，是需要被校验的类型。 手动校验可能在某些场景下需要我们手动校验，即使用校验器对需要被校验的实体发起 validate，同步获得校验结果。理论上我们既可以使用 Hibernate Validation 提供 Validator，也可以使用 Spring 对其的封装。在 spring 构建的项目中，提倡使用经过 spring 封装过后的方法，这里两种方法都介绍下： Hibernate Validation： 123456789Foo foo = new Foo();foo.setAge(22);foo.setEmail(&quot;000&quot;);ValidatorFactory vf = Validation.buildDefaultValidatorFactory();Validator validator = vf.getValidator();Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = validator.validate(foo);for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) { System.out.println(constraintViolation.getMessage());} 由于依赖了 Hibernate Validation 框架，我们需要调用 Hibernate 相关的工厂方法来获取 validator 实例，从而校验。 在 spring framework 文档的 Validation 相关章节，可以看到如下的描述： Spring provides full support for the Bean Validation API. This includes convenient support for bootstrapping a JSR-303/JSR-349 Bean Validation provider as a Spring bean. This allows for a javax.validation.ValidatorFactory or javax.validation.Validator to be injected wherever validation is needed in your application. Use the LocalValidatorFactoryBean to configure a default Validator as a Spring bean: bean id=”validator” class=”org.springframework.validation.beanvalidation.LocalValidatorFactoryBean” The basic configuration above will trigger Bean Validation to initialize using its default bootstrap mechanism. A JSR-303/JSR-349 provider, such as Hibernate Validator, is expected to be present in the classpath and will be detected automatically. 上面这段话主要描述了 spring 对 validation 全面支持 JSR-303、JSR-349 的标准，并且封装了 LocalValidatorFactoryBean 作为 validator 的实现。值得一提的是，这个类的责任其实是非常重大的，他兼容了 spring 的 validation 体系和 hibernate 的 validation 体系，也可以被开发者直接调用，代替上述的从工厂方法中获取的 hibernate validator。由于我们使用了 springboot，会触发 web 模块的自动配置，LocalValidatorFactoryBean 已经成为了 Validator 的默认实现，使用时只需要自动注入即可。 12345678910111213141516@AutowiredValidator globalValidator; &lt;1&gt;@RequestMapping(&quot;/validate&quot;)public String validate() { Foo foo = new Foo(); foo.setAge(22); foo.setEmail(&quot;000&quot;); Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = globalValidator.validate(foo);&lt;2&gt; for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) { System.out.println(constraintViolation.getMessage()); } return &quot;success&quot;;} &lt;1&gt; 真正使用过 Validator 接口的读者会发现有两个接口，一个是位于 javax.validation 包下，另一个位于 org.springframework.validation 包下，注意我们这里使用的是前者 javax.validation，后者是 spring 自己内置的校验接口，LocalValidatorFactoryBean 同时实现了这两个接口。 &lt;2&gt; 此处校验接口最终的实现类便是 LocalValidatorFactoryBean。 基于方法校验12345678910111213141516171819202122@RestController@Validated &lt;1&gt;public class BarController { @RequestMapping(&quot;/bar&quot;) public @NotBlank &lt;2&gt; String bar(@Min(18) Integer age &lt;3&gt;) { System.out.println(&quot;age :&quot; + age); return &quot;&quot;; } @ExceptionHandler(ConstraintViolationException.class) public Map handleConstraintViolationException(ConstraintViolationException cve){ Set&lt;ConstraintViolation&lt;?&gt;&gt; cves = cve.getConstraintViolations();&lt;4&gt; for (ConstraintViolation&lt;?&gt; constraintViolation : cves) { System.out.println(constraintViolation.getMessage()); } Map map = new HashMap(); map.put(&quot;errorCode&quot;,500); return map; }} &lt;1&gt; 为类添加 @Validated 注解 &lt;2&gt; &lt;3&gt; 校验方法的返回值和入参 &lt;4&gt; 添加一个异常处理器，可以获得没有通过校验的属性相关信息 基于方法的校验，个人不推荐使用，感觉和项目结合的不是很好。 使用校验框架的一些想法理论上 spring validation 可以实现很多复杂的校验，你甚至可以使你的 Validator 获取 ApplicationContext，获取 spring 容器中所有的资源，进行诸如数据库校验，注入其他校验工具，完成组合校验（如前后密码一致）等等操作，但是寻求一个易用性和封装复杂性之间的平衡点是我们作为工具使用者应该考虑的，我推崇的方式，是仅仅使用自带的注解和自定义注解，完成一些简单的，可复用的校验。而对于复杂的校验，则包含在业务代码之中，毕竟如用户名是否存在这样的校验，仅仅依靠数据库查询还不够，为了避免并发问题，还是得加上唯一索引之类的额外工作，不是吗？","link":"/spring-validation/"},{"title":"Spring 中的 XML schema 扩展机制","text":"前言很久没有写关于 Spring 的文章了，最近在系统梳理 Dubbo 代码的过程中发现了 XML schema 这个被遗漏的知识点。由于工作中使用 SpringBoot 比较多的原因，几乎很少接触 XML，此文可以算做是亡羊补牢，另一方面，也为后续的 Dubbo 源码解析做个铺垫。 XML schema 扩展机制是啥？这并不是一块很大的知识点，翻阅一下 Spring 的文档，我甚至没找到一个贯穿上下文的词来描述这个功能，XML Schema Authoring 是文档中对应的标题，简单来说： Spring 为基于 XML 构建的应用提供了一种扩展机制，用于定义和配置 Bean。 它允许使用者编写自定义的 XML bean 解析器，并将解析器本身以及最终定义的 Bean 集成到 Spring IOC 容器中。 Dubbo 依赖了 Spring，并提供了一套自定义的 XML 标签，&lt;dubbo:application&gt; ,&lt;dubbo:registry&gt; ,&lt;dubbo:protocol&gt;,&lt;dubbo:service&gt;。作为使用者，大多数人只需要关心这些参数如何配置，但不知道有没有人好奇过，它们是如何加载进入 Spring 的 IOC 容器中被其他组件使用的呢？这便牵扯出了今天的主题：Spring 对 XML schema 的扩展支持。 自定义 XML 扩展为了搞懂 Spring 的 XML 扩展机制，最直接的方式便是实现一个自定义的扩展。实现的步骤也非常简单，分为四步： 编写一个 XML schema 文件描述的你节点元素。 编写一个 NamespaceHandler 的实现类 编写一个或者多个 BeanDefinitionParser 的实现 (关键步骤). 注册上述的 schema 和 handler。 我们的目的便是想要实现一个 kirito XML schema，我们的项目中可以自定义 kirito.xml，在其中会以 kirito 为标签来定义不同的类，并在最终的测试代码中验证这些声明在 kirito.xml 的类是否被 Spring 成功加载。大概像这样，是不是和 dubbo.xml 的格式很像呢？ 动手实现有了明确的目标，我们逐步开展自己的工作。 1 编写 kirito.xsdresources/META-INF/kirito.xsd 123456789101112131415161718192021222324252627282930&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;xsd:schema xmlns=&quot;http://www.cnkirito.moe/schema/kirito&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:beans=&quot;http://www.springframework.org/schema/beans&quot; targetNamespace=&quot;http://www.cnkirito.moe/schema/kirito&quot;&gt; ① &lt;xsd:import namespace=&quot;http://www.springframework.org/schema/beans&quot;/&gt; &lt;xsd:element name=&quot;application&quot;&gt; ② &lt;xsd:complexType&gt; &lt;xsd:complexContent&gt; &lt;xsd:extension base=&quot;beans:identifiedType&quot;&gt; &lt;xsd:attribute name=&quot;name&quot; type=&quot;xsd:string&quot; use=&quot;required&quot;/&gt; &lt;/xsd:extension&gt; &lt;/xsd:complexContent&gt; &lt;/xsd:complexType&gt; &lt;/xsd:element&gt; &lt;xsd:element name=&quot;service&quot;&gt; ② &lt;xsd:complexType&gt; &lt;xsd:complexContent&gt; &lt;xsd:extension base=&quot;beans:identifiedType&quot;&gt; &lt;xsd:attribute name=&quot;name&quot; type=&quot;xsd:string&quot; use=&quot;required&quot;/&gt; &lt;/xsd:extension&gt; &lt;/xsd:complexContent&gt; &lt;/xsd:complexType&gt; &lt;/xsd:element&gt;&lt;/xsd:schema&gt; ① 注意这里的 targetNamespace=&quot;http://www.cnkirito.moe/schema/kirito&quot; 这便是之后 kirito 标签的关键点。 ② kirito.xsd 定义了两个元素： application 和 service，出于简单考虑，都只有一个 name 字段。 schema 的意义在于它可以和 eclipse/IDEA 这样智能化的集成开发环境形成很好的搭配，在编辑 XML 的过程中，用户可以获得告警和提示。 如果配置得当，可以使用自动完成功能让用户在事先定义好的枚举类型中进行选择。 2 编写 KiritoNamespaceHandler123456789public class KiritoNamespaceHandler extends NamespaceHandlerSupport { @Override public void init() { super.registerBeanDefinitionParser(&quot;application&quot;, new KiritoBeanDefinitionParser(ApplicationConfig.class)); super.registerBeanDefinitionParser(&quot;service&quot;, new KiritoBeanDefinitionParser(ServiceBean.class)); }} 完成 schema 之后，还需要一个 NamespaceHandler 来帮助 Spring 解析 XML 中不同命名空间的各类元素。 123&lt;kirito:application name=&quot;kirito&quot;/&gt;&lt;dubbo:application name=&quot;dubbo&quot;/&gt;&lt;motan:application name=&quot;motan&quot;/&gt; 不同的命名空间需要不同的 NamespaceHandler 来处理，在今天的示例中，我们使用 KiritoNamespaceHandler 来解析 kirito 命名空间。KiritoNamespaceHandler 继承自 NamespaceHandlerSupport 类，并在其 init() 方法中注册了两个 BeanDefinitionParser ，用于解析 kirito 命名空间 /kirito.xsd 约束中定义的两个元素：application，service。BeanDefinitionParser 是下一步的主角，我们暂且跳过，将重心放在父类 NamespaceHandlerSupport 之上。 12345public interface NamespaceHandler { void init(); BeanDefinition parse(Element element, ParserContext parserContext); BeanDefinitionHolder decorate(Node source, BeanDefinitionHolder definition, ParserContext parserContext);} NamespaceHandlerSupport 是 NamespaceHandler 命名空间处理器的抽象实现，我粗略看了 NamespaceHandler 的几个实现类，parse 和 decorate 方法可以完成元素节点的组装并通过 ParserContext 注册到 Ioc 容器中，但实际我们并没有调用这两个方法，而是通过 init() 方法注册 BeanDefinitionParser 来完成解析节点以及注册 Bean 的工作，所以对于 NamespaceHandler，我们主要关心 init 中注册的两个 BeanDefinitionParser 即可。 3 编写 KiritoBeanDefinitionParser在文章开始我们便标记到 BeanDefinitionParser 是最为关键的一环，每一个 BeanDefinitionParser 实现类都负责一个映射，将一个 XML 节点解析成 IOC 容器中的一个实体类。 123456789101112131415161718192021222324public class KiritoBeanDefinitionParser implements BeanDefinitionParser { private final Class&lt;?&gt; beanClass; public KiritoBeanDefinitionParser(Class&lt;?&gt; beanClass) { this.beanClass = beanClass; } private static BeanDefinition parse(Element element, ParserContext parserContext, Class&lt;?&gt; beanClass) { RootBeanDefinition beanDefinition = new RootBeanDefinition(); beanDefinition.setBeanClass(beanClass); beanDefinition.setLazyInit(false); String name = element.getAttribute(&quot;name&quot;); beanDefinition.getPropertyValues().addPropertyValue(&quot;name&quot;, name); parserContext.getRegistry().registerBeanDefinition(name, beanDefinition); return beanDefinition; } @Override public BeanDefinition parse(Element element, ParserContext parserContext) { return parse(element, parserContext, beanClass); }} 由于我们的实体类是非常简单的，所以不存在很复杂的解析代码，而实际项目中，往往需要大量的解析步骤。parse 方法会解析一个个 XML 中的元素，使用 RootBeanDefinition 组装成对象，并最终通过 parserContext 注册到 IOC 容器中。 至此，我们便完成了 XML 文件中定义的对象到 IOC 容器的映射。 4 注册 schema 和 handler最后一步还需要通知 Spring，告知其自定义 schema 的所在之处以及对应的处理器。 resources/META-INF/spring.handlers 1http\\://www.cnkirito.moe/schema/kirito=moe.cnkirito.sample.xsd.KiritoNamespaceHandler resources/META-INF/spring.schemas 1http\\://www.cnkirito.moe/schema/kirito/kirito.xsd=META-INF/kirito.xsd 没有太多可以说的，需要遵守 Spring 的约定。 至此一个自定义的 XML schema 便扩展完成了，随后来验证一下。 验证扩展我们首先定义好 kirito.xml 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:kirito=&quot;http://www.cnkirito.moe/schema/kirito&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.cnkirito.moe/schema/kirito http://www.cnkirito.moe/schema/kirito/kirito.xsd&quot;&gt; &lt;kirito:application name=&quot;kirito-demo-application&quot;/&gt; &lt;kirito:service name=&quot;kirito-demo-service&quot;/&gt;&lt;/beans&gt; 使用 Spring 去加载它，并验证 IOC 容器中是否存在注册成功的 Bean。 123456789101112@SpringBootApplication@ImportResource(locations = {&quot;classpath:kirito.xml&quot;})public class XmlSchemaAuthoringSampleApplication { public static void main(String[] args) { ConfigurableApplicationContext applicationContext = SpringApplication.run(XmlSchemaAuthoringSampleApplication.class, args); ServiceBean serviceBean = applicationContext.getBean(ServiceBean.class); System.out.println(serviceBean.getName()); ApplicationConfig applicationConfig = applicationContext.getBean(ApplicationConfig.class); System.out.println(applicationConfig.getName()); }} 观察控制台的输出： kirito-demo-servicekirito-demo-application 一个基础的基于 XML schema 的扩展便完成了。 Dubbo 中的 XML schema 扩展最后我们以 Dubbo 为例，看看一个成熟的 XML schema 扩展是如何被应用的。 刚好对应了四个标准的扩展步骤，是不是对 XML 配置下的 Dubbo 应用有了更好的理解了呢？ 顺带一提，仅仅完成 Bean 的注册还是不够的，在“注册”的同时，Dubbo 还进行了一系列其他操作如：暴露端口，开启服务器，完成注册中心的注册，生成代理对象等等行为，由于不在本文的范围内，后续的 Dubbo 专题会专门介绍这些细节，本文便是了解 Dubbo 加载流程的前置文章了。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。**","link":"/spring-xsd/"},{"title":"浅析 SpringMVC 中返回对象的循环引用问题","text":"问题发现@RestController、@ResponseBody 等注解是我们在写 Web 应用时打交道最多的注解了，我们经常有这样的需求：返回一个对象给前端，SpringMVC 帮助我们序列化成 JSON 对象。而今天我要分享的话题也不是什么高深的内容，可能大家多多少少也都遇到过，那就是返回对象中存在循环引用时的问题，分享我的一些思考。 该问题非常简单容易复现，直接上代码。 准备两个循环引用的对象： 1234567891011@Datapublic class Person { private String name; private IdCard idCard;}@Datapublic class IdCard { private String id; private Person person;} 在 SpringMVC 的 controller 中直接返回存在循环引用的对象： 1234567891011121314151617@RestControllerpublic class HelloController { @RequestMapping(&quot;/hello&quot;) public Person hello() { Person person = new Person(); person.setName(&quot;kirito&quot;); IdCard idCard = new IdCard(); idCard.setId(&quot;xxx19950102xxx&quot;); person.setIdCard(idCard); idCard.setPerson(person); return person; }} 执行 curl localhost:8080/hello 发现，直接报了一个 StackOverFlowError： 问题剖析不难理解这中间发生了什么，从堆栈和常识中都应当了解到一个事实，SpringMVC 默认使用了 jackson 作为 HttpMessageConverter，这样当我们返回对象时，会经过 jackson 的 serializer 序列化成 json 串，而另一个事实便是 jackson 是无法解析 java 中的循环引用的，套娃式的解析，最终导致了 StackOverFlowError。 有人会说，为什么你会有循环引用呢？天知道业务场景有多奇葩，既然 Java 没有限制循环引用的存在，那就肯定会有某一合理的场景存在该可能性，如果你在线上的一个接口一直平稳运行着，知道有一天，碰到了一个包含循环引用的对象，你看着打印出来的 StackOverFlowError 的堆栈，开始怀疑人生，是哪个小（大）可（S）爱（B）干的这种事！ 我们先假设循环引用存在的合理性，如何解决该问题呢？最简单的解法：单向维护关联，参考 Hibernate 中的 OneToMany 关联中单向映射的思想，这需要干掉 IdCard 中的 Person 成员变量。或者，借助于 jackson 提供的注解，指定忽略循环引用的字段，例如这样： 123456@Datapublic class IdCard { private String id; @JsonIgnore private Person person;} 当然，我也翻阅了一些资料，尝试寻求 jackson 更优雅的解决方式，例如这两个注解： 12@JsonManagedReference@JsonBackReference 但在我看来，似乎他们并没有什么大用场。 当然，你如果不嫌弃经常出安全漏洞的 fastjson，也可以选择使用 FastJsonHttpMessageConverter 替换掉 jackson 的默认实现，像下面这样： 1234567891011121314151617181920212223242526272829303132333435363738@Beanpublic HttpMessageConverters fastJsonHttpMessageConverters() { //1、定义一个convert转换消息的对象 FastJsonHttpMessageConverter fastConverter = new FastJsonHttpMessageConverter(); //2、添加fastjson的配置信息 FastJsonConfig fastJsonConfig = new FastJsonConfig(); SerializerFeature[] serializerFeatures = new SerializerFeature[]{ // 输出key是包含双引号 // SerializerFeature.QuoteFieldNames, // 是否输出为null的字段,若为null 则显示该字段 // SerializerFeature.WriteMapNullValue, // 数值字段如果为null，则输出为0 SerializerFeature.WriteNullNumberAsZero, // List字段如果为null,输出为[],而非null SerializerFeature.WriteNullListAsEmpty, // 字符类型字段如果为null,输出为&quot;&quot;,而非null SerializerFeature.WriteNullStringAsEmpty, // Boolean字段如果为null,输出为false,而非null SerializerFeature.WriteNullBooleanAsFalse, // Date的日期转换器 SerializerFeature.WriteDateUseDateFormat, // 循环引用 //SerializerFeature.DisableCircularReferenceDetect, }; fastJsonConfig.setSerializerFeatures(serializerFeatures); fastJsonConfig.setCharset(Charset.forName(&quot;UTF-8&quot;)); //3、在convert中添加配置信息 fastConverter.setFastJsonConfig(fastJsonConfig); //4、将convert添加到converters中 HttpMessageConverter&lt;?&gt; converter = fastConverter; return new HttpMessageConverters(converter);} 你可以自定义一些 json 转换时的 feature，当然我今天主要关注 SerializerFeature.DisableCircularReferenceDetect 这一属性，只要不显示开启该特性，fastjson 默认就能处理循环引用的问题。 如上配置后，让我们看看效果： 1{&quot;idCard&quot;:{&quot;id&quot;:&quot;xxx19950102xxx&quot;,&quot;person&quot;:{&quot;$ref&quot;:&quot;..&quot;}},&quot;name&quot;:&quot;kirito&quot;} 已经正常返回了，fastjson 使用了&quot;$ref&quot;:&quot;..&quot; 这样的标识，解决了循环引用的问题，如果继续使用 fastjson 反序列化，依旧可以解析成同一对象，其实我在之前的文章中已经介绍过这一特性了《gson 替换 fastjson 引发的线上问题分析》。 使用 FastJsonHttpMessageConverter 可以彻底规避掉循环引用的问题，这对于返回类型不固定的场景十分有帮助，而 @JsonIgnore 只能作用于那些固定结构的循环引用对象上。 问题思考值得一提的是，为什么一般标准的 JSON 类库并没有如此关注循环引用的问题呢？fastjson 看起来反而是个特例，我觉得主要还是 JSON 这种序列化的格式就是为了通用而存在的，$ref 这样的契约信息，并没有被 JSON 的规范去定义，fastjson 可以确保 $ref 在序列化、反序列化时能够正常解析，但如果是跨框架、跨系统、跨语言等场景，这一切都是个未知数了。说到底，这还是 Java 语言的循环引用和 JSON 通用规范不包含这一概念之间的 gap（可能 JSON 规范描述了这一特性，但我没有找到，如有问题，烦请指正）。 我到底应该选择 @JsonIgnore 还是使用 FastJsonHttpMessageConverter 呢？经历了上面的思考，我觉得各位看官应该能够根据自己的场景选择合适的方案了。 总结下，如果选择 FastJsonHttpMessageConverter ，改动较大，如果有较多的存量接口，建议做好回归，以确认解决循环引用问题的同时，带来其他不兼容的改动，并且，需要确认你的使用场景，如果出现了循环引用，fastjson 会使用 $ref 来记录引用信息，请确认你的前端或者接口方能够识别该信息，因为这可能并不是标准的 JSON 规范。你也可以选择 @JsonIgnore 来实现最小改动，但也同时需要注意，如果根据序列化的结果再次反序列化，引用信息可不会自动恢复。","link":"/springmvc-fastjson/"},{"title":"聊聊算法在面试中的地位","text":"前段时间，有一位好友找到我，向我打听阿里社招笔试是否看重算法题的考察，我给予了肯定的答复。他表现的有些沮丧，表示自己工程底子很扎实，框架源码也研究地很透彻，唯独算法能力不行，leetcode 上的简单题做起来都有点吃力。以至于面试一些公司时，基本都是前几面和面试官聊工程，相聊甚欢，一到笔试就 GG。鉴于我个人在学生时代有过 ACM 经历，对算法还是相当感冒的，个人算法能力不算出众，也不算弱，最好成绩是省赛金牌，区域赛铜牌（主要还是抱得队友的大腿），后来实在是写不动 C++ 了，中途转了 Java，借这个机会跟大家聊一聊，分享下个人对算法的一些认识。 我发现很多人有的一个观念是刷算法并不能很好地帮助他工作，他们中有些人是有了很多学校或者公司的项目经验，有些则是在数据库、RPC、大数据等某个垂直领域有了比较长时间的沉淀，他们会觉得刻意地刷算法题比较偏门，没有太大的价值。一方面有些人会比较自信，不认为需要靠算法来证明自己的价值，另一方面，有些人会认为刷算法题是应届生面试才需要考察的技能，对于社招来说，公司应该更注重考察项目经验和系统设计层面的技能。以我个人经验来看，面试互联网公司时，算法题几乎都是必考的一个环节，从公司的考察点出发，就可以佐证出，算法不重要这个观点的确是有待商榷的。还有一些人轻视算法，是觉得只有大厂才看重算法，一些小公司的面试根本不 care 算法，而且特别是像我文章开头提到那个朋友一样的人，有着比较强的工程能力，我相信在面试中一定可以凭借着这个优势，赢得面试官的好感，那我不妨再反问一句，为什么要让算法成为你的软肋呢？ 我已经表露了我对面试中算法重要程度的态度，而且我也认为面试中考察算法能力是非常重要的一环。在公司里做项目，我们往往需要花费数个月去落地，而面试中完成算法题最多只限制在半小时内，虽然时间区间不同，但本质上都是在考察一个人在一个固定的时间内完成某个任务的能力。读题考察了候选人的理解能力，期间我会与候选人沟通，以确保他正确理解的题意，并且在码字之前，我会要求对方先讲解题思路，这考察了沟通能力，有的候选人可能没有经历过刷题训练，缺少一些常见的算法思维，但经过提示后，如果能快速地完成 coding，在笔试中或许也能够通过。所以你看，其实考察算法题其实和也是借此考验了你的工作能力，它要求你在短短的半个小时之内做到 Buf Free，一定程度上这比做工程更难，因为没有人为你测试，而你要想通过这一环节，是需要额外花费精力去训练的。 虽然我认为面试中算法很重要，推荐大家准备面试时多去刷刷题，但我也确实抵制一些偏题、怪题。以我的刷题经验和工作经验结合来看，推荐的难度为 leetcode 简单、中等题，ACM 铜牌、银牌题，仅供参考。记得有一次瞄了一眼阿里的校招在线笔试题，具体是哪个部门不清楚，那个难度估计得是地狱难度了，这类情况仅仅是小概率会发生，至少在我们大部门不会出现特别难的算法题。 很多人说面试造火箭，入职拧螺丝，以此来讽刺面试中算法面是不必要的，我是不赞同的。抛开面试，算法能力也的确是工作中帮助了我。简单举几个例子吧，我通过算法题接触到了欧拉函数、GCD 等数论知识，让我可以非常好地理解 RSA 加密的原理和实现过程，而 RSA 加密是很有可能在工程中被使用到的一种非对称加密方式；通过解决常见的数据结构类算法题，我了解到了跳表的实现，这方便了我去理解 Redis 的 Set 结构；熟练地解决贪心和 DP 等问题，也潜移默化地影响着我在工程项目中的代码逻辑。 字节跳动可以说是业内有名的看重算法面的公司了，但鉴于本人并不了解实际的情况，只能跟大家聊聊阿里的算法面试。分成两部分：实习生面试和社招面试。我这里的经验主要都是基于我所了解的情况，在阿里其他部门（非阿里云）可能情况就不一样了。先说实习生面试吧，算法主要考察的是简单题，主要以贪心、数据结构、模拟为主，可以说非常友好了，主要考验学生对于基础知识的掌握程度，但也要求候选人能够在较短时间内完成，否则很难在整体面试中获得 A 评价。而社招，算法面试的地位肯定是要低于工程能力的考核的，但是对于能不能发 offer 又起着决定性的作用，等于说，即使你的工作履历很 match 岗位，工程经验也很丰富，但算法面一塌糊涂，往往用人部门只能忍痛割爱了。 如果你正在准备面试，我是建议准备下算法，刷一些题目找下手感，leetcode 和各种在线 OJ 都是不错的选择，B 站也有很多视频，具体的刷题列表，我这儿没准备，相信你可以在网上找到很多的。","link":"/talk-about-algorithm/"},{"title":"Kirito 的博客崩了，这次是因为...","text":"Hello 大家好呀，最近 Kirito 的博客因为没有备案的原因，给禁止访问了，折腾了一天才恢复，借着这个机会跟大家闲聊一下个人博客、域名那点事。 从 CSDN 到个人博客很多人有写作的习惯，只不过有人喜欢写在 QQ 空间，有人喜欢写在专门的博客论坛，也有人喜欢写在个人博客。最开始，我也是在 CSDN 上进行写作的 在 2017 年的时候，喜欢折腾的我，开始在个人博客 www.cnkirito.moe 上更新文章。 没有什么特别的原因，只是觉得，这很酷。 我一直觉得个人博客就像程序员的名片一样，界面的风格和文章的内容都能彰显出一个程序的内心世界。 域名的由来有些小伙伴很好奇，为什么我的域名：www.cnkirito.moe 是 .moe 结尾的，相比常见的 .com，.moe 的确名不见经传。moe 是日文“萌え”的罗马字写法，这也是汉语中“萌”一词的正确英文写法，当时喜欢二次元的我，购买了 moe 这个顶级域名。至于 cnkirito，懂得都懂，也是二次元的梗。 网站域名解析被封的过程但申请这个域名时，万万没有想到，5 年后的今天，着实被它坑了一把。由于 moe 顶级域名的提供商是国外的域名提供商，这就给备案带来了很大的问题，按照信安部门的要求，域名解析到国内（香港除外）的服务器，都需要备案，否则就会被封禁。 我的服务器是之前打比赛的时候，华为云送的 3 年的 4c8g 的机器，我用来搭建一个个人博客那不是绰绰有余吗。结果谁曾想，收到了华为云的通知，让我进行备案。 备案指的是域名解析到某个服务器的过程，并不仅针对域名或者某一服务器。 但我也不是不想备案呀，我的域名是从国外域名商买来的，压根没有备案这个功能。所以摆在我面前的，只有两条路。 域名转入。缴纳一笔钱，从国外域名商转入到国内域名商，例如阿里云、腾讯云都有这样的服务。 买一台香港的服务器。重新搭建一个博客。 我自然是不想浪费我的服务器，毕竟这年头买个服务器也不便宜。先尝试走方案一，一般转入域名花费在 100~300 不等，这个费用完全可以接受，毕竟是一次性的，但一番调研之后，却发现此路不通，没有任何一个国内的域名商支持转入 moe 类型的域名！好吧，只怪当年年少，选择了这个小众的域名。 无奈之下，只能选择购买香港的服务器了，可惜了我 4c8g 的云服务器，只能当做实验机了。 搭建博客的建议有了我上面的教训，如果大家有搭建个人博客的计划，建议当然是购买国内的服务器，使用国内的域名提供商了。 关于服务器的配置，如果是静态博客，像我使用的就是 hexo 搭建的，1c1g 固定带宽 1M 的服务器也完全够用，建议在大促的时候购买或者使用新人/学生等优惠条件，对比下各个云厂商的价格，300400 完全够撑一年，优惠粒度较大时，几十块都能搞定。域名的费用则是 100200 不等。 SEO 优化。路径需要足够短，正例：https://www.cnkirito.moe/learn-mmap，反例：https://www.cnkirito.moe/2021/11/26/learn-mmap，前者有利于搜索引擎的排名。 配置 HTTPS。有不少个人博客图省事，没有搞 HTTPS，我习惯会配置上 HTTPS。一方面小绿锁会提升个人博客的整体格调，另一方面，HTTPS 也会提高搜索引擎的排名（据说。配置也不麻烦，我就是用的七牛云免费的 SSL 证书，再加上 nginx 的 2 行配置而已，并不麻烦。 图床托管。我的图片都是存储在七牛云上，七牛云有一定的免费额度，我的博客只有在某几个人搜索量比较大，超过了额度，付了一定的费用，大多数时候，图片托管基本不会产生费用。 好，以上就是全部的内容啦，Kirito 的博客和公众号会保证同步更新。 PC 端建议访问博客，获得更好的阅读体验：https://www.cnkirito.moe 移动端建议访问微信公众号：Kirito的技术分享 （除了有恰饭广告之外，和博客没有区别 Orz","link":"/talk-about-blog/"},{"title":"谈谈中间件开发","text":"最近频繁地在跟实习生候选人打交道，每次新接触一个候选人，都要花上一定的时间去介绍我们团队，候选人问的最多的一个问题就是「中间件部门一般是干嘛的？」，恰好我之前也接触过一些想从事中间件开发的小伙伴，问过我「现在转行做中间件开发还来得及吗？」诸如此类的问题，索性就写一篇文章，聊聊我个人这些年做中间件开发的感受吧。 什么是中间件开发？我大四实习时，在一个 20 多人的软件开发团队第一次接触了中间件，当时项目的架构师引入了微博开源的 RPC 框架 Motan，借助于这个框架，我们迅速构建起了一个基于微服务架构的内部电商系统。接着在项目中，由于业务需求，我们又引入了 ActiveMQ…在这个阶段，我已经在使用中间件了，但似乎没有接触到中间件开发，更多的是使用层面上的接触。 我毕业后的第一份工作，公司有几百号研发，当时的 leader 看我对中间件比较感兴趣，有意把我分配在了基础架构团队，我第一次真正意义上成为了一名”中间件研发“，平时主要的工作，是基于开源的 Kong 和 Dubbo，进行一些内部的改造，以提供给业务团队更好地使用。这个阶段，做的事还是比较杂的，业务团队对某些中间件有定制化的需求，都需要去了解这个中间件，熟悉源码。这段时间，也是我成长最快的一个时期，我是在这个期间学会了 Docker、Neo4j、Kong 等以前从来没接触过的技术，并且更加深入地了解 Dubbo 这类 RPC 框架的原理。可能坐在我旁边的就是一个订单部门的同学，抛了一个功能点让我来改造。 现在，我供职于阿里云云原生中间件，相较于上一份中间件研发工作，阿里云这类互联网大公司，任意一个中间件都有少则数人，多则数十人负责，中间件部门和业务部门之间也有着明确的界限。在这里，中间件团队的职责可以细分为三个方向： 中间件团队会被业务团队的需求所驱动，为集团内部提供定制化的解决方案，俗称「自研」。所以你可能并不了解到 HSF、Diamond 这些阿里内部的中间件。 中间件团队会从事开源，花费大量的精力提升中间件的极致性能，提升开源影响力，引领技术先进性。这部分中间件则比较为人所熟知，例如 Dubbo、Spring Cloud Alibaba、RocketMQ、Nacos。 中间件会在阿里云输出商业化产品，相比开源产品，提供更高的 SLA、更强大的功能、更友好的交互。这部分商业化产品诸如：微服务引擎 MSE、消息队列 RocketMQ、分布式应用链路追踪 ARMS。 我的这三段经历，正好反应了不同规模的公司对中间件开发的不同需求。小公司使用中间件，例如 RPC、MQ、缓存等，基本是由业务开发人员自己维护的。但如果后台研发达到数百人，基本就会组建自己的中间件团队，或者选择使用阿里云等云厂商提供的中间件产品。 中间件开发和业务开发的区别在我看来，中间件开发和业务开发并没有什么高下之分，非要说区别的话，有点像游戏里面的不同转职，有人选择的是魔法师，有人选择的是战士。在职场的练级过程中，每个人的总技能点数是一样的，业务开发有点向全能战士的方向去发展，各个点都涉猎一点，但每个方向能够分配到技能点自然就少了；中间件开发就像《因为太怕痛，就全点防御力了》里面的主角，把技能点都分配到了一个方向。 假设你是在一个小公司工作，现阶段并没有专门的中间件团队，大家都是业务开发，此时我们做一个假设：公司即将成立一个中间件团队或者叫基础架构部，那么会是哪一类人容易被选中呢？一定是那些技术功底扎实，对中间件感兴趣，研究过源码的人。这个假设并非凭空捏造，很多互联网公司的中间件团队都是这么一点点壮大起来的。我想说什么呢？业务开发和中间件开发一开始并没有明确的界限，因此，不用顾忌你现在是不是在从事业务开发，只要你对中间件感兴趣，有过源码级别的研究，就可以成为一个中间件开发。 中间件开发需要具备哪些素质？越是大的公司，大的中间件团队，责任分工就越垂直。基本在大公司，一个中间件开发可能花几年时间在某一个垂直方向深耕。以下是一些常见的中间件方向，当然，这个分类在各个公司可能由于组织架构的原因，略有不同。 微服务治理。例如 RPC 相关中间件，注册中心，配置中心，限流熔断，链路追踪等等。开源产品例如：Dubbo、SpringCloud、Nacos、Zookeeper、Sentinel、Hystrix、Zipkin。 消息队列。微服务一般强调的同步通信，消息队列单独列出来，主要是因为其异步的机制。开源产品例如：RocketMQ、Kafka 存储中间件。例如缓存，数据库等等，例如 Mysql、Redis。值得一提的是，由于存储相关的系统一般都非常复杂，特别是在分布式存储领域，体系更是繁杂，在阿里内部一般将数据库和缓存这种存储类型的产品当成是和中间件平级的存在，所以如果有人说 Mysql 和 Redis 不是中间件，也没有啥好争吵的。 存储 Proxy。典型的如 ShardingSphere。 网关。例如 Spring Cloud Gateway、Kong、Nginx。 ServiceMesh。Envoy、Istio 在阿里这边也被划分在中间件部门。 其实可以发现，中间件其实并没有一个明确的定义，到底哪些开源产品可以是中间件，哪些又不是。 列举完这些典型的中间件，继续讨论这一节的主题，一个中间件开发者需要具备哪些素质？ 语言基础。从 Java 程序员的角度，基础通常就是：集合，并发，JVM，常用工具类。 操作系统基础。中间件开发人员经常和操作系统打交道，所以计算机基础也必不可少，我列举一些关键词，供各位参考 文件 IO。例如 pageCache，mmap，direct IO 等概念。 进程线程。例如 green thread，协程等概念。 内存/CPU。例如 cgroup，cache line，bound core 等概念。 网络基础。可以发现上述的每一个中间件都离不开网络通信，一定需要对 TCP 和 HTTP 的原理烂熟于心，框架层面需要熟悉 NIO、Netty、GRPC、HttpClient 等常用的网络框架/工具。 分布式相关知识。了解 CAP， paxos，raft，zab，2pc/3pc，base 等理论知识，例如我看到有一些应届生简历中的一个项目经历就很有意思：根据 MIT 课程 Lab 实现 Raft 协议的 POC。 源码阅读能力。我认为源码阅读能力是一个中间件开发者必备的素质，网上经常能看到各种源码分析文章，通过阅读开源中间件的源码，可以借鉴别人的设计理念，提升自身的编码水准。 保持技术热情，拥抱变化。中间件技术日新月异，可能一个打败一个中间件的不是同类的产品，而是整体的大环境，例如近几年云原生大火，所有中间件几乎都在拥抱变化，主动向 K8s 对齐，在这个大背景下，就需要中间件开发者拥有 K8s 的基础认知能力，熟悉 pod、service、deployment、statefulSet、operator 这些 K8s 的基本概念。 如何成为中间件开发看完上述这些要求，可能会有一些同学开始咋舌了，但其实也没那么可怕，这跟最早学习 Java 基础是一样的，很多东西一开始没有接触过，觉得很难，但熟悉之后会发现，也就那么回事。 我的技术交流群中经常会有同学抱怨说，平时只能接触到 CRUD，根本接触不到这些”高大上“的技术。我想说的是，机会都是自己找的。我这里有几个切实可行的建议： 参与开源社区的项目，贡献代码。了解一个中间件最好的方式就是贡献它，带着问题有的放矢地研究源码，是我比较推荐的方式，你所需要做的是寻找一个合适的 issue，解决它。不断重复这个过程，你其实就是一个中间件开发了！ 多动手做实验。很多上面提到的中间件开发应用的素质都可以通过动手做实验的方式来学习，例如动手实现一个简易的 RPC 框架，实现一个 Raft 协议的 POC，通过 benchmark 对比 FileChannel 和 MMAP 的性能，相信我，这比看书、看视频、看博客有用的多的多。 参与中间件挑战赛。最早是阿里会举办一年一度的中间件性能挑战赛，后来也有一些其他公司如华为开始效仿这类比赛，参与这些挑战赛也可以积累非常多的经验，同时你还可以借着组队，结交非常多的朋友。 对于上述的那些要求，我筛选了我之前写过的一些不需要有任何门槛就可以阅读的文章，可能会对你有所帮助： 用了这么久配置中心，还不知道长轮询是什么？ 一文探讨堆外内存的监控与回收 一种心跳，两种设计 聊聊 TCP 长连接和心跳那些事 文件 IO 操作的最佳实践 以 Dubbo 为例，聊聊如何为开源项目做贡献 中间件欢迎你如果你是一个业务开发，想从事中间件方向的研发，正在纠结，那我想说：选择中间件，最合适的时间是毕业时，其次是现在，Why Not？ 如果你是一个学生，正在找一份实习工作，我还是挺推荐你选择中间件方向的工作的，这个方向非常技术范，无论如何对你的成长都有帮助。最后，你应该猜到我要说什么了吧？如果你有意向从事中间件的开发，随时欢迎你与我【微信 id：xiayimiaoshenghua】咨询，我这里是【阿里云云原生中间件】团队，我们正在针对 22 届的学生进行春季实习的意向沟通。 团队介绍 &amp; JD 请戳阅读原文","link":"/talk-about-middleware-develop/"},{"title":"华为云 TaurusDB 性能挑战赛赛题总结","text":"1 前言 回顾第一次参加性能挑战赛 – 第四届阿里中间件性能挑战赛，那时候真的是什么都不会，只有一腔热情，借着比赛学会了 Netty、学会了文件 IO 的最佳实践，到了这次华为云举办的 TaurusDB 性能挑战赛，已经是第三次参加比赛了，同时也是最“坎坷”的一次比赛。经过我和某位不愿意透露姓名的 96 年小迷妹的不懈努力，最终跑分排名为第 3 名。 如果要挑选一个词来概括这次比赛的核心内容，那非”计算存储分离“莫属了，通过这次比赛，自己也对计算存储分离架构有了比较直观的感受。为了比较直观的体现计算存储分离的优势，以看电影来举个例子：若干年前，我总是常备一块大容量的硬盘存储小电影，但自从家里带宽升级到 100mpbs 之后，我从来不保存电影了，要看直接下载 / 缓冲，基本几分钟就好了。这在几年前还不可想象，如今是触手可及的事实，归根到底是随着互联网的发展，网络 IO 已经不再是瓶颈了。 计算存储分离架构相比传统本地存储架构而言，具有更加灵活、成本更低等特性，但架构的复杂性也会更高，也会更加考验选手的综合能力。 计算存储分离架构的含义： 存储端有状态，只存储数据，不处理业务逻辑。 计算端无状态，只处理逻辑，不持久化存储数据。 2 赛题概览比赛整体分成了初赛和复赛两个部分，初赛要求实现一个简化、高效的本地 kv 存储引擎，复赛在初赛的基础上增加了计算存储分离的架构，计算节点需要通过网络传输将数据递交给存储节点存储。 12345678public interface KVStoreRace { public boolean init(final String dir, final int thread_num) throws KVSException; public long set(final String key, final byte[] value) throws KVSException; public long get(final String key, final Ref&lt;byte[]&gt; val) throws KVSException;} 计算节点和存储节点共用上述的接口，评测程序分为 2 个阶段： ** 正确性评测 ** 此阶段评测程序会并发写入随机数据（key 8B、value 4KB），写入数据过程中进行任意次进程意外退出测试，引擎需要保证异常中止不影响已经写入的数据正确性。异常中止后，重启引擎，验证已经写入数据正确性和完整性，并继续写入数据，重复此过程直至数据写入完毕。只有通过此阶段测试才会进入下一阶段测试。 ** 性能评测 ** 随机写入：16 个线程并发随机写入，每个线程使用 Set 各写 400 万次随机数据（key 8B、value 4KB）顺序读取：16 个线程并发按照写入顺序逐一读取，每个线程各使用 Get 读取 400 万次随机数据热点读取：16 个线程并发读取，每个线程按照写入顺序热点分区，随机读取 400 万次数据，读取范围覆盖全部写入数据。热点的逻辑为：按照数据的写入顺序按 10MB 数据粒度分区，分区逆序推进，在每个 10MB 数据分区内随机读取。随机读取次数会增加约 10%。 ** 语言限定 ** CPP &amp; Java，一起排名 3 赛题剖析看过我之前《PolarDB 数据库性能大赛 Java 选手分享》的朋友应该对题目不会感到陌生，基本可以看做是在 PolarDB 数据库性能挑战赛上增加一个网络通信的部分，所以重头戏基本是在复赛网络通信的比拼上。初赛主要是文件 IO 和存储架构的设计，如果对文件 IO 常识不太了解，可以先行阅读 《文件 IO 操作的一些最佳实践》。 3.1 架构设计计算节点只负责生成数据，在实际生产中计算节点还承担额外的计算开销，由于计算节点是无状态的，所以不能够聚合数据写入、落盘等操作，但可以在 Get 触发网络 IO 时一次读取大块数据用作缓存，减少网络 IO 次数。 存储节点负责存储数据，考验了选手对磁盘 IO 和缓存的设计，可以一次使用缓存写入 / 读取大块数据，减少磁盘 IO 次数。 所以选手们将会围绕网络 IO、磁盘 IO 和缓存设计来设计整体架构。 3.2 正确性检测赛题明确表示会进行 kill -9 并验证数据的一致性，正确性检测主要影响的是写入阶段。 存储节点负责存储数据，需要保证 kill -9 不丢失数据，但并不要求断电不丢失，这间接地阐释了一点：我们可以使用 PageCache 来做写入缓存；正确性检测对于计算节点与存储节点之间通信影响便是：每次写入操作都必须 ack，所以选手必须保证同步通信，类似于 ping/pong 模型。 3.3 性能评测性能评测由随机写、顺序读、热点读（随机读取热点数据）三部分构成。 随机写阶段与 PolarDB 的评测不同，TaurusDB 随机写入 key 的 16 个线程是隔离的，即 A 线程写入的数据只会由 A 线程读出，可以认为是彼此独立的 16 个实例在执行评测，这大大简化了我们的架构。 顺序读阶段的描述也很容易理解，需要注意的是这里的顺序是按照写入顺序，而不是 Key 的字典序，所以随机写可以转化为顺序写，也方便了选手去设计顺序读的架构。 热点读阶段有点故弄玄虚了，其实就是按照 10M 数据为一个分区进行逆序读，同时在 10M 数据范围内掺杂一些随机读，由于操作系统的预读机制只会顺序预读，无法逆序预读，PageCache 将会在这个环节会失效，考验了选手自己设计磁盘 IO 缓存的能力。 4 架构详解4.1 全局架构 计算存储分离架构自然会分成计算节点和存储节点两部分来介绍。计算节点会在内存维护数据的索引表；存储节点负责存储持久化数据，包括索引文件和数据文件；计算节点与存储节点之间的读写都会经过网络 IO。 4.2 随机写架构 随机写阶段，评测程序调用计算节点的 set 接口，发起网络 IO，存储节点接受到数据后不会立刻落盘，针对 data 和 index 的处理也会不同。针对 data 部分，会使用一块缓冲区（如图：Mmap Merge IO）承接数据，由于 Mmap 的特性，会形成 Merge File 文件，一个数据缓冲区可以聚合 16 个数据，当缓冲区满后，将缓冲区的数据追加到数据文件后，并清空 Merge File；针对 index 部分，使用 Mmap 直接追加到索引文件中。 F: 1. data 部分为什么搞这么复杂，需要聚合 16 个数据再刷盘？ Q: 针对此次比赛的数据盘，实测下来 16 个数据刷盘可以打满 IO。 F: 2. 为什么使用 Mmap Merge IO 而不直接使用内存 Merge IO？ Q: 正确性检测阶段，存储节点可能会被随机 kill，Mmap 做缓存的好处是操作系统会帮我们落盘，不会丢失数据 F: 3. 为什么 index 部分直接使用 Mmap，而不和 data 部分一样处理？ Q: 这需要追溯到 Mmap 的特点，Mmap 适合直接写索引这种小数据，所以不需要聚合。 4.3 热点读 &amp; 顺序读架构 热点读取阶段 &amp; 顺序读取阶段 ，这两个阶段其实可以认为是一种策略，只不过一个正序，一个逆序，这里以热点读为例介绍。我们采取了贪心的思想，一次读取操作本应该只会返回 4kb 的数据，但为了做预读缓存，我们决定会存储节点返回 10M 的数据，并缓存在计算节点中，模拟了一个操作系统预读的机制，同时为了能够让计算节点精确知道缓存是否命中，会同时返回索引数据，并在计算节点的内存中维护索引表，这样便减少了成吨的网络 IO 次数。 4.4 存储设计 站在每个线程的视角，可以发现在我们的架构中，每个线程都是独立的。评测程序会对每个线程写入 400w 数据，最终形成 16 * 16G 的数据文件和 16 * 32M 左右的索引文件。 数据文件不停追加 MergeFile，相当于一次落盘单位是 64K（16 个数据），由于自行聚合了数据，所以可以采用 Direct IO，减少操作系统的 overhead。 索引文件由小数据构成，所以采用 Mmap 方式直接追加写 计算节点由于无状态的特性，只能在内存中维护索引结构。 4.5 网络通信设计 我们都知道 Java 中有 BIO（阻塞 IO）和 NIO（非阻塞 IO）之分，并且大多数人可能会下意识觉得：NIO 就是比 BIO 快。而这次比赛恰恰是要告诉大家，这两种 IO 方式没有绝对的快慢之分，只有在合适的场景中选择合适的 IO 方式才能发挥出最佳性能。 稍微分析下这次比赛的通信模型，写入阶段由于需要保证每次 set 不受 kill 的影响，所以需要等到同步返回后才能进行下一次 set，而 get 本身依赖于返回值进行数据校验，所以从通信模型上看只能是同步 ping/pong 模型；从线程数上来看，只有固定的 16 个线程进行收发消息。以上两个因素暗示了 BIO 将会非常契合这次比赛。 在很多人的刻板印象中，阻塞就意味着慢，非阻塞就意味着快，这种理解是完全错误的，快慢取决于通信模型、系统架构、带宽、网卡等因素。我测试了 NIO + CountDownLatch 和 BIO 的差距，前者会比后者整体慢 100s ~ 130s。 5 细节优化点5.1 最大化磁盘吞吐量但凡是涉及到磁盘 IO 的比赛，首先需要测试便是在 Direct IO 下，一次读写多大的块能够打满 IO，在此基础上，才能进行写入缓冲设计和读取缓存设计，否则在这种争分夺秒的性能挑战赛中不可能取得较好的名次。测试方法也很简单，如果能够买到对应的机器，直接使用 iostat 观察不同刷盘大小下的 iops 即可，如果比赛没有机器，只能祭出调参大法，不停提交了，这次 TaurusDB 的盘实测下来 64k、128K 都可以获得最大的吞吐量。 5.2 批量回传数据计算节点设计缓存是一个比较容易想到的优化点，按照常规的思路，索引应该是维护在存储节点，但这样做的话，计算节点在 get 数据时就无法判断是否命中缓存，所以在前文的架构介绍中，我们将索引维护在了计算节点之上，在第一次 get 时，顺便恢复索引。批量返回数据的优势在于增加了缓存命中率、降低总网络 IO 次数、减少上行网络 IO 数据量，是整个比赛中分量较重的一个优化点。 5.3 流控 在比赛中容易出现的一个问题，在批量返回 10M 数据时经常会出现网络卡死的情况，一时间无法定位到问题，以为是代码 BUG，但有时候又能跑出分数，不得以尝试过一次返回较少的数据量，就不会报错。最后还是机智的小迷妹定位到问题是 CPU 和 IO 速率不均等导致的，解决方案便是在一次 pong 共计返回 10M 的基础上，将报文拆分成 64k 的小块，中间插入额外的 CPU 操作，最终保证了程序稳定性的同时，也保障了最佳性能。 额外的 CPU 操作例如：for(int i=0;i&lt;700;i++)，不要小看这个微不足道的一个 for 循环哦。 流控其实也是计算存储分离架构一个常见设计点，存储节点与计算节点的写入速度需要做一个平衡，避免直接打垮存储节点，也有一种”滑动窗口“机制专门应对这种问题，不在此赘述了。 5.4 预分配文件在 Cpp 中可以使用 fallocate 预先分配好文件大小，会使得写入速度提升 2s。在 Java 中没有 fallocate 机制，但是可以利用评测程序的漏洞，在 static 块中事先写好 16 * 16G 的文件，同样可以获得 fallocate 的效果。 5.5 合理设计索引结构get 时需要根据 key 查询到文件偏移量，这显示是一个 Map 结构，在这个 Map 上也有几个点需要注意。以 Java 为例，使用 HashMap 是否可行呢？当然可以，但是缺点也很明显，其会占用比较大的内存，而且存取性能不好，可以使用 LongIntHashMap 来代替，看过我之前文章的朋友应该不会对这个数据结构感到陌生，它是专门为基础数据类型设计的 Map 容器。 每个线程 400w 数据，每个线程独享一个索引 Map，为了避免出现扩容，需要合理的设置扩容引子和初始化容量：new LongIntHashMap(410_0000, 0.99); 5.6 Direct IO最终进入决赛的，有三支 Java 队伍，相比较 Cpp 得天独厚的对操作系统的灵活控制性，Java 选手更像是带着镣铐在舞蹈，幸好有了上次 PolarDB 比赛的经验，我提前封装好了 Java 的 Direct IO 类库：https://github.com/lexburner/kdio，相比 FileChannel，它能够使得磁盘 IO 效率更高。得知有 Java 选手真的在比赛中使用了我的 Direct IO 类库，也是比赛中实实切切的乐趣之一。 6 失败的优化点6.1 预读线程先行考虑到网络 IO 还是比本地磁盘 IO 要慢的，一个本以为可行的方案是单独使用预读线程进行存储节点的磁盘 IO，设计一个 RingBuffer，不断往前预读，直到环满，计算阶段 get 时会消费 RingBuffer 的一格缓存，从而使得网络 IO 和磁盘 IO 不会相互等待。实际测试下来，发现瓶颈主要还是在于网络 IO，这样的优化徒增了不少代码，不利于进行其他的优化尝试，最终放弃。 6.2 计算节点聚合写入缓冲既然在 get 阶段时存储节点批量返回数据给计算节点可以提升性能，那 set 阶段聚合批量的数据再发送给存储节点按理来说也能提升性能吧？的确如此，如果不考虑正确性检测，这的确是一个不错的优化点，但由于 kill 的特性使得我们不得不每一次 set 都进行 ACK。但是！可以对将 4/8/16 个线程编为一组进行聚合呀！通过调整参数来确定该方案是否可行。 然后事与愿违，该方案并没有取得成效。 7 聊聊比赛吧之前此类工程性质的性能挑战赛只有阿里一家互联网公司承办过，作为热衷于中间件性能优化的参赛选手而言，非常高兴华为也能够举办这样性质的比赛。虽然比赛中出现了诸多的幺蛾子，但毕竟是第一次承办比赛，我也就不表了。 如果你同样也是性能挑战赛的爱好者，想要在下一次中间件性能挑战赛中有一群小伙伴一起解题、组队，体验冲分的乐趣，欢迎关注我的微信公众号：【Kirito 的技术分享】，也欢迎加入微信技术交流群进行交流 ~","link":"/taurusdb-race/"},{"title":"聊聊 TCP 长连接和心跳那些事","text":"前言可能很多 Java 程序员对 TCP 的理解只有一个三次握手，四次握手的认识，我觉得这样的原因主要在于 TCP 协议本身稍微有点抽象（相比较于应用层的 HTTP 协议）；其次，非框架开发者不太需要接触到 TCP 的一些细节。其实我个人对 TCP 的很多细节也并没有完全理解，这篇文章主要针对微信交流群里有人提出的长连接，心跳问题，做一个统一的整理。 在 Java 中，使用 TCP 通信，大概率会涉及到 Socket、Netty，本文将借用它们的一些 API 和设置参数来辅助介绍。 长连接与短连接**TCP 本身并没有长短连接的区别 **，长短与否，完全取决于我们怎么用它。 短连接：每次通信时，创建 Socket；一次通信结束，调用 socket.close()。这就是一般意义上的短连接，短连接的好处是管理起来比较简单，存在的连接都是可用的连接，不需要额外的控制手段。 长连接：每次通信完毕后，不会关闭连接，这样可以做到连接的复用。** 长连接的好处是省去了创建连接的耗时。** 短连接和长连接的优势，分别是对方的劣势。想要图简单，不追求高性能，使用短连接合适，这样我们就不需要操心连接状态的管理；想要追求性能，使用长连接，我们就需要担心各种问题：比如 ** 端对端连接的维护，连接的保活 **。 长连接还常常被用来做数据的推送，我们大多数时候对通信的认知还是 request/response 模型，但 TCP 双工通信的性质决定了它还可以被用来做双向通信。在长连接之下，可以很方便的实现 push 模型，长连接的这一特性在本文并不会进行探讨，有兴趣的同学可以专门去搜索相关的文章。 短连接没有太多东西可以讲，所以下文我们将目光聚焦在长连接的一些问题上。纯讲理论未免有些过于单调，所以下文我借助一些 RPC 框架的实践来展开 TCP 的相关讨论。 服务治理框架中的长连接前面已经提到过，追求性能时，必然会选择使用长连接，所以借助 Dubbo 可以很好的来理解 TCP。我们开启两个 Dubbo 应用，一个 server 负责监听本地 20880 端口（众所周知，这是 Dubbo 协议默认的端口），一个 client 负责循环发送请求。执行 lsof -i:20880 命令可以查看端口的相关使用情况： *:20880 (LISTEN) 说明了 Dubbo 正在监听本地的 20880 端口，处理发送到本地 20880 端口的请求 后两条信息说明请求的发送情况，验证了 TCP 是一个双向的通信过程，由于我是在同一个机器开启了两个 Dubbo 应用，所以你能够看到是本地的 53078 端口与 20880 端口在通信。我们并没有手动设置 53078 这个客户端端口，它是随机的。通过这两条信息，阐释了一个事实：** 即使是发送请求的一方，也需要占用一个端口 **。 稍微说一下 FD 这个参数，他代表了 ** 文件句柄 **，每新增一条连接都会占用新的文件句柄，如果你在使用 TCP 通信的过程中出现了 open too many files 的异常，那就应该检查一下，你是不是创建了太多连接，而没有关闭。细心的读者也会联想到长连接的另一个好处，那就是会占用较少的文件句柄。 长连接的维护因为客户端请求的服务可能分布在多个服务器上，客户端自然需要跟对端创建多条长连接，我们遇到的第一个问题就是如何维护长连接。 123456789// 客户端public class NettyHandler extends SimpleChannelHandler { private final Map&lt;String, Channel&gt; channels = new ConcurrentHashMap&lt;String, Channel&gt;(); // &lt;ip:port, channel&gt;}// 服务端public class NettyServer extends AbstractServer implements Server { private Map&lt;String, Channel&gt; channels; // &lt;ip:port, channel&gt;} 在 Dubbo 中，客户端和服务端都使用 ip:port 维护了端对端的长连接，Channel 便是对连接的抽象。我们主要关注 NettyHandler 中的长连接，服务端同时维护一个长连接的集合是 Dubbo 的额外设计，我们将在后面提到。 这里插一句，解释下为什么我认为客户端的连接集合要重要一点。TCP 是一个双向通信的协议，任一方都可以是发送者，接受者，那为什么还抽象了 Client 和 Server 呢？因为 ** 建立连接这件事就跟谈念爱一样，必须要有主动的一方，你主动我们就会有故事 **。Client 可以理解为主动建立连接的一方，实际上两端的地位可以理解为是对等的。 连接的保活这个话题就有的聊了，会牵扯到比较多的知识点。首先需要明确一点，为什么需要连接的保活？当双方已经建立了连接，但因为网络问题，链路不通，这样长连接就不能使用了。需要明确的一点是，通过 netstat，lsof 等指令查看到连接的状态处于 ESTABLISHED 状态并不是一件非常靠谱的事，因为连接可能已死，但没有被系统感知到，更不用提假死这种疑难杂症了。如果保证长连接可用是一件技术活。 连接的保活：KeepAlive首先想到的是 TCP 中的 KeepAlive 机制。KeepAlive 并不是 TCP 协议的一部分，但是大多数操作系统都实现了这个机制（所以需要在操作系统层面设置 KeepAlive 的相关参数）。KeepAlive 机制开启后，在一定时间内（一般时间为 7200s，参数 tcp_keepalive_time）在链路上没有数据传送的情况下，TCP 层将发送相应的 KeepAlive 探针以确定连接可用性，探测失败后重试 10（参数 tcp_keepalive_probes）次，每次间隔时间 75s（参数 tcp_keepalive_intvl），所有探测失败后，才认为当前连接已经不可用。 在 Netty 中开启 KeepAlive： 1bootstrap.option(ChannelOption.SO_KEEPALIVE, true) Linux 操作系统中设置 KeepAlive 相关参数，修改 /etc/sysctl.conf 文件： 123net.ipv4.tcp_keepalive_time=90net.ipv4.tcp_keepalive_intvl=15net.ipv4.tcp_keepalive_probes=2 **KeepAlive 机制是在网络层面保证了连接的可用性 **，但站在应用框架层面我们认为这还不够。主要体现在三个方面： KeepAlive 的开关是在应用层开启的，但是具体参数（如重试测试，重试间隔时间）的设置却是操作系统级别的，位于操作系统的 /etc/sysctl.conf 配置中，这对于应用来说不够灵活。 KeepAlive 的保活机制只在链路空闲的情况下才会起到作用，假如此时有数据发送，且物理链路已经不通，操作系统这边的链路状态还是 ESTABLISHED，这时会发生什么？自然会走 TCP 重传机制，要知道默认的 TCP 超时重传，指数退避算法也是一个相当长的过程。 KeepAlive 本身是面向网络的，并不面向于应用，当连接不可用，可能是由于应用本身的 GC 频繁，系统 load 高等情况，但网络仍然是通的，此时，应用已经失去了活性，连接应该被认为是不可用的。 我们已经为应用层面的连接保活做了足够的铺垫，下面就来一起看看，怎么在应用层做连接保活。 连接的保活：应用层心跳终于点题了，文题中提到的 ** 心跳 ** 便是一个本文想要重点强调的另一个重要的知识点。上一节我们已经解释过了，网络层面的 KeepAlive 不足以支撑应用级别的连接可用性，本节就来聊聊应用层的心跳机制是实现连接保活的。 如何理解应用层的心跳？简单来说，就是客户端会开启一个定时任务，定时对已经建立连接的对端应用发送请求（这里的请求是特殊的心跳请求），服务端则需要特殊处理该请求，返回响应。如果心跳持续多次没有收到响应，客户端会认为连接不可用，主动断开连接。不同的服务治理框架对心跳，建连，断连，拉黑的机制有不同的策略，但大多数的服务治理框架都会在应用层做心跳，Dubbo/HSF 也不例外。 应用层心跳的设计细节以 Dubbo 为例，支持应用层的心跳，客户端和服务端都会开启一个 HeartBeatTask，客户端在 HeaderExchangeClient 中开启，服务端将在 HeaderExchangeServer 开启。文章开头埋了一个坑：Dubbo 为什么在服务端同时维护 Map&lt;String,Channel&gt; 呢？主要就是为了给心跳做贡献，心跳定时任务在发现连接不可用时，会根据当前是客户端还是服务端走不同的分支，客户端发现不可用，是重连；服务端发现不可用，是直接 close。 123456// HeartBeatTaskif (channel instanceof Client) { ((Client) channel).reconnect();} else { channel.close();} Dubbo 2.7.x 相比 2.6.x 做了定时心跳的优化，使用 HashedWheelTimer 更加精准的控制了只在连接闲置时发送心跳。 再看看 HSF 的实现，并没有设置应用层的心跳，准确的说，是在 HSF2.2 之后，使用 Netty 提供的 IdleStateHandler 更加优雅的实现了应用的心跳。 12ch.pipeline() .addLast(&quot;clientIdleHandler&quot;, new IdleStateHandler(getHbSentInterval(), 0, 0)); 处理 userEventTriggered 中的 IdleStateEvent 事件 12345678@Overridepublic void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if (evt instanceof IdleStateEvent) { callConnectionIdleListeners(client, (ClientStream) StreamUtils.streamOfChannel(ctx.channel())); } else { super.userEventTriggered(ctx, evt); }} 对于客户端，HSF 使用 SendHeartbeat 来进行心跳，每次失败累加心跳失败的耗时，当超过最大限制时断开乱接；对于服务端 HSF 使用 CloseIdle 来处理闲置连接，直接关闭连接。一般来说，服务端的闲置时间会设置的稍长。 熟悉其他 RPC 框架的同学会发现，不同框架的心跳机制真的是差距非常大。心跳设计还跟连接创建，重连机制，黑名单连接相关，还需要具体框架具体分析。 除了定时任务的设计，还需要在协议层面支持心跳。最简单的例子可以参考 nginx 的健康检查，而针对 Dubbo 协议，自然也需要做心跳的支持，如果将心跳请求识别为正常流量，会造成服务端的压力问题，干扰限流等诸多问题。 其中 Flag 代表了 Dubbo 协议的标志位，一共 8 个地址位。低四位用来表示消息体数据用的序列化工具的类型（默认 hessian），高四位中，第一位为 1 表示是 request 请求，第二位为 1 表示双向传输（即有返回 response），** 第三位为 1 表示是心跳事件 **。 心跳请求应当和普通请求区别对待。 注意和 HTTP 的 KeepAlive 区别对待 HTTP 协议的 KeepAlive 意图在于连接复用，同一个连接上串行方式传递请求 - 响应数据 TCP 的 KeepAlive 机制意图在于保活、心跳，检测连接错误。 这压根是两个概念。 KeepAlive 常见错误启用 TCP KeepAlive 的应用程序，一般可以捕获到下面几种类型错误 ETIMEOUT 超时错误，在发送一个探测保护包经过 (tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes) 时间后仍然没有接收到 ACK 确认情况下触发的异常，套接字被关闭 1java.io.IOException: Connection timed out EHOSTUNREACH host unreachable(主机不可达) 错误，这个应该是 ICMP 汇报给上层应用的。 1java.io.IOException: No route to host 链接被重置，终端可能崩溃死机重启之后，接收到来自服务器的报文，然物是人非，前朝往事，只能报以无奈重置宣告之。 1java.io.IOException: Connection reset by peer 总结有三种使用 KeepAlive 的实践方案： 默认情况下使用 KeepAlive 周期为 2 个小时，如不选择更改，属于误用范畴，造成资源浪费：内核会为每一个连接都打开一个保活计时器，N 个连接会打开 N 个保活计时器。 优势很明显： TCP 协议层面保活探测机制，系统内核完全替上层应用自动给做好了 内核层面计时器相比上层应用，更为高效 上层应用只需要处理数据收发、连接异常通知即可 数据包将更为紧凑 关闭 TCP 的 KeepAlive，完全使用应用层心跳保活机制。由应用掌管心跳，更灵活可控，比如可以在应用级别设置心跳周期，适配私有协议。 业务心跳 + TCP KeepAlive 一起使用，互相作为补充，但 TCP 保活探测周期和应用的心跳周期要协调，以互补方可，不能够差距过大，否则将达不到设想的效果。 各个框架的设计都有所不同，例如 Dubbo 使用的是方案三，但阿里内部的 HSF 框架则没有设置 TCP 的 KeepAlive，仅仅由应用心跳保活。和心跳策略一样，这和框架整体的设计相关。 ** 欢迎关注我的微信公众号：「Kirito 的技术分享」**","link":"/tcp-talk/"},{"title":"提问前，请先让自己成为值得被教的人","text":"每一个不恰当的提问都在消耗别人对你的耐心，程序员届早已经有了诸如《提问的智慧》之类的经典文章介绍了什么是蠢问题，如何避免问蠢问题。然而，常年混迹于十几个技术交流微信群的我，发现很多小白程序员并不懂得这一点，为改善微信群的技术交流氛围，转此文，意图是让大家在担任提问者的角色时，尽可能提高提问的素质，让自己成为值得被教的人。 原文出处：https://github.com/aptx4869yuyang2017/How-To-Ask-Questions-The-Smart-Way 用清晰、正确、精准并语法正确的语句我们从经验中发现，粗心的提问者通常也会粗心的写程序与思考（我敢打包票）。回答粗心大意者的问题很不值得，我们宁愿把时间耗在别处。 正确的拼字、标点符号和大小写是很重要的。一般来说，如果你觉得这样做很麻烦，不想在乎这些，那我们也觉得麻烦，不想在乎你的提问。花点额外的精力斟酌一下字句，用不着太僵硬与正式。 更白话地说，如果你写得像是个小白，那多半得不到理睬。 如果在使用非母语的论坛提问，你可以犯点拼写和语法上的小错，但决不能在思考上马虎（没错，我们通常能弄清两者的分别）。同时，除非你知道回复者使用的语言，否则请使用英语书写。繁忙的程序员一般会直接删除用他们看不懂语言写的消息。在网络上英语是通用语言，用英语书写可以将你的问题在尚未被阅读就被直接删除的可能性降到最低。 如果英文是你的外语（Second language），提示潜在回复者你有潜在的语言困难是很好的： [译注：以下附上原文以供使用] English is not my native language; please excuse typing errors. 英文不是我的母语，请原谅我的错字或语法 If you speak $LANGUAGE, please email/PM me; I may need assistance translating my question. 如果你说 ** 某语言 **，请寄信 / 私讯给我；我需要有人协助我翻译我的问题 I am familiar with the technical terms, but some slang expressions and idioms are difficult for me. 我对技术名词很熟悉，但对于俗语或是特别用法比较不甚了解。 I’ve posted my question in $LANGUAGE and English. I’ll be glad to translate responses, if you only use one or the other. 我把我的问题用 ** 某语言 ** 和英文写出来，如果你只用一种语言回答，我会乐意将其翻译成另一种。 精确的描述问题并言之有物 仔细、清楚地描述你的问题或 Bug 的症状。 描述问题发生的环境（机器配置、操作系统、应用程序、以及相关的信息），提供经销商的发行版和版本号（如：Fedora Core 4、Slackware 9.1 等）。 描述在提问前你是怎样去研究和理解这个问题的。 描述在提问前为确定问题而采取的诊断步骤。 描述最近做过什么可能相关的硬件或软件变更。 尽可能的提供一个可以 重现这个问题的可控环境 的方法。 尽量去揣测一个程序员会怎样反问你，在你提问之前预先将程序员们可能遇到的问题回答一遍。 以上几点中，当你报告的是你认为可能在代码中的问题时，给程序员一个可以重现你的问题的环境尤其重要。当你这么做时，你得到有效的回答的机会和速度都会大大的提升。 Simon Tatham 写过一篇名为《如何有效的报告 Bug》的出色文章。强力推荐你也读一读。 话不在多而在精你需要提供精确有内容的信息。这并不是要求你简单的把成堆的出错代码或者资料完全转录到你的提问中。如果你有庞大而复杂的测试样例能重现程序挂掉的情境，尽量将它剪裁得越小越好。 这样做的用处至少有三点。 第一，表现出你为简化问题付出了努力，这可以使你得到回答的机会增加； 第二，简化问题使你更有可能得到 *** 有用 *** 的答案； 第三，在精炼你的 bug 报告的过程中，你很可能就自己找到了解决方法或权宜之计。 别动辄声称找到 Bug当你在使用软件中遇到问题，除非你非常、*** 非常 *** 的有根据，不要动辄声称找到了 Bug。提示：除非你能提供解决问题的源代码补丁，或者提供回归测试来表明前一版本中行为不正确，否则你都多半不够完全确信。这同样适用在网页和文件，如果你（声称）发现了文件的 Bug，你应该能提供相应位置的修正或替代文件。 请记得，还有许多其它使用者没遇到你发现的问题，否则你在阅读文件或搜索网页时就应该发现了（你在抱怨前 已经做了这些，是吧？）。这也意味着很有可能是你弄错了而不是软件本身有问题。 编写软件的人总是非常辛苦地使它尽可能完美。如果你声称找到了 Bug，也就是在质疑他们的能力，即使你是对的，也有可能会冒犯到其中某部分人。当你在标题中嚷嚷着有 Bug 时，这尤其严重。 提问时，即使你私下非常确信已经发现一个真正的 Bug，最好写得像是 *** 你 *** 做错了什么。如果真的有 Bug，你会在回复中看到这点。这样做的话，如果真有 Bug，维护者就会向你道歉，这总比你惹恼别人然后欠别人一个道歉要好一点。 低声下气不能代替你的功课有些人明白他们不该粗鲁或傲慢的提问并要求得到答复，但他们选择另一个极端 – 低声下气： 我知道我只是个可悲的新手，一个撸瑟，但...。这既使人困扰，也没有用，尤其是伴随着与实际问题含糊不清的描述时更令人反感。 别用原始灵长类动物的把戏来浪费你我的时间。取而代之的是，尽可能清楚地描述背景条件和你的问题情况。这比低声下气更好地定位了你的位置。 有时网页论坛会设有专为新手提问的版面，如果你真的认为遇到了初学者的问题，到那去就是了，但一样别那么低声下气。 描述问题症状而非你的猜测告诉程序员们你认为问题是怎样造成的并没什么帮助。（如果你的推断如此有效，还用向别人求助吗？），因此要确信你原原本本告诉了他们问题的症状，而不是你的解释和理论；让程序员们来推测和诊断。如果你认为陈述自己的猜测很重要，清楚地说明这只是你的猜测，并描述为什么它们不起作用。 ** 蠢问题 ** 我在编译内核时接连遇到 SIG11 错误， 我怀疑某条飞线搭在主板的走线上了，这种情况应该怎样检查最好？ ** 聪明问题 ** 我的组装电脑是 FIC-PA2007 主机板搭载 AMD K6/233 CPU（威盛 Apollo VP2 芯片组）， 256MB Corsair PC133 SDRAM 内存，在编译内核时，从开机 20 分钟以后就频频产生 SIG11 错误， 但是在头 20 分钟内从没发生过相同的问题。重新启动也没有用，但是关机一晚上就又能工作 20 分钟。 所有内存都换过了，没有效果。相关部分的标准编译记录如下…。 由于以上这点似乎让许多人觉得难以配合，这里有句话可以提醒你： 所有的诊断专家都来自密苏里州。 美国国务院的官方座右铭则是：让我看看（出自国会议员 Willard D. Vandiver 在 1899 年时的讲话： 我来自一个出产玉米，棉花，牛蒡和民主党人的国家，滔滔雄辩既不能说服我，也不会让我满意。我来自密苏里州，你必须让我看看。） 针对诊断者而言，这并不是一种怀疑，而只是一种真实而有用的需求，以便让他们看到的是与你看到的原始证据尽可能一致的东西，而不是你的猜测与归纳的结论。所以，大方的展示给我们看吧！ 按发生时间先后列出问题症状问题发生前的一系列操作，往往就是对找出问题最有帮助的线索。因此，你的说明里应该包含你的操作步骤，以及机器和软件的反应，直到问题发生。在命令行处理的情况下，提供一段操作记录（例如运行脚本工具所生成的），并引用相关的若干行（如 20 行）记录会非常有帮助。 如果挂掉的程序有诊断选项（如 -v 的详述开关），试着选择这些能在记录中增加调试信息的选项。记住，多 不等于 好。试着选取适当的调试级别以便提供有用的信息而不是让读者淹没在垃圾中。 如果你的说明很长（如超过四个段落），在开头简述问题，接下来再按时间顺序详述会有所帮助。这样程序员们在读你的记录时就知道该注意哪些内容了。 描述目标而不是过程如果你想弄清楚如何做某事（而不是报告一个 Bug），在开头就描述你的目标，然后才陈述重现你所卡住的特定步骤。 经常寻求技术帮助的人在心中有个更高层次的目标，而他们在自以为能达到目标的特定道路上被卡住了，然后跑来问该怎么走，但没有意识到这条路本身就有问题。结果要费很大的劲才能搞定。 ** 蠢问题 ** 我怎样才能从某绘图程序的颜色选择器中取得十六进制的的 RGB 值？ ** 聪明问题 ** 我正试着用替换一幅图片的色码（color table）成自己选定的色码，我现在知道的唯一方法是编辑每个色码区块（table slot）， 但却无法从某绘图程序的颜色选择器取得十六进制的的 RGB 值。 第二种提问法比较聪明，你可能得到像是 建议采用另一个更合适的工具 的回复。 清楚明确的表达你的问题以及需求漫无边际的提问是近乎无休无止的时间黑洞。最有可能给你有用答案的人通常也正是最忙的人（他们忙是因为要亲自完成大部分工作）。这样的人对无节制的时间黑洞相当厌恶，所以他们也倾向于厌恶那些漫无边际的提问。 如果你明确表述需要回答者做什么（如提供指点、发送一段代码、检查你的补丁、或是其他等等），就最有可能得到有用的答案。因为这会定出一个时间和精力的上限，便于回答者能集中精力来帮你。这么做很棒。 要理解专家们所处的世界，请把专业技能想像为充裕的资源，而回复的时间则是稀缺的资源。你要求他们奉献的时间越少，你越有可能从真正专业而且很忙的专家那里得到解答。 所以，界定一下你的问题，使专家花在辨识你的问题和回答所需要付出的时间减到最少，这技巧对你有用答案相当有帮助 – 但这技巧通常和简化问题有所区别。因此，问 我想更好的理解 X，可否指点一下哪有好一点说明？ 通常比问 你能解释一下 X 吗？ 更好。如果你的代码不能运作，通常请别人看看哪里有问题，比要求别人替你改正要明智得多。 询问有关代码的问题时别要求他人帮你调试有问题的代码，不提示一下应该从何入手。张贴几百行的代码，然后说一声：它不能工作 会让你完全被忽略。只贴几十行代码，然后说一句： 在第七行以后，我期待它显示 &lt;x&gt;，但实际出现的是 &lt;y&gt; 比较有可能让你得到回应。 最有效描述程序问题的方法是提供最精简的 Bug 展示测试用例（bug-demonstrating test case）。什么是最精简的测试用例？那是问题的缩影；一小个程序片段能 ** 刚好 ** 展示出程序的异常行为，而不包含其他令人分散注意力的内容。怎么制作最精简的测试用例？如果你知道哪一行或哪一段代码会造成异常的行为，复制下来并加入足够重现这个状况的代码（例如，足以让这段代码能被编译 / 直译 / 被应用程序处理）。如果你无法将问题缩减到一个特定区块，就复制一份代码并移除不影响产生问题行为的部分。总之，测试用例越小越好。 一般而言，要得到一段相当精简的测试用例并不太容易，但永远先尝试这样做的是种好习惯。这种方式可以帮助你了解如何自行解决这个问题 —- 而且即使你的尝试不成功，程序员们也会看到你在尝试取得答案的过程中付出了努力，这可以让他们更愿意与你合作。 如果你只是想让别人帮忙审查（Review）一下代码，在信的开头就要说出来，并且一定要提到你认为哪一部分特别需要关注以及为什么。 别把自己家庭作业的问题贴上来程序员们很擅长分辨哪些问题是家庭作业式的问题；因为我们中的大多数都曾自己解决这类问题。同样，这些问题得由 ** 你 ** 来搞定，你会从中学到东西。你可以要求给点提示，但别要求得到完整的解决方案。 如果你怀疑自己碰到了一个家庭作业式的问题，但仍然无法解决，试试在使用者群组，论坛或（最后一招）在项目的 ** 使用者 ** 邮件列表或论坛中提问。尽管程序员们 ** 会 ** 看出来，但一些有经验的使用者也许仍会给你一些提示。 去掉无意义的提问句避免用无意义的话结束提问，例如 有人能帮我吗？ 或者 这有答案吗？。 首先：如果你对问题的描述不是很好，这样问更是画蛇添足。 其次：由于这样问是画蛇添足，程序员们会很厌烦你 – 而且通常会用逻辑上正确，但毫无意义的回答来表示他们的蔑视， 例如：没错，有人能帮你 或者 不，没答案。 一般来说，避免用 是或否、对或错、有或没有 类型的问句，除非你想得到 是或否类型的回答。 礼多人不怪，而且有时还很有帮助彬彬有礼，多用 请 和 谢谢您的关注，或 谢谢你的关照。让大家都知道你对他们花时间免费提供帮助心存感激。 坦白说，这一点并没有比清晰、正确、精准并合法语法和避免使用专用格式重要（也不能取而代之）。程序员们一般宁可读有点唐突但技术上鲜明的 Bug 报告，而不是那种有礼但含糊的报告。（如果这点让你不解，记住我们是按问题能教给我们什么来评价问题的价值的） 然而，如果你有一串的问题待解决，客气一点肯定会增加你得到有用回应的机会。 （我们注意到，自从本指南发布后，从资深程序员那里得到的唯一严重缺陷反馈，就是对预先道谢这一条。一些程序员觉得 先谢了 意味着事后就不用再感谢任何人的暗示。我们的建议是要么先说 先谢了，*** 然后 *** 事后再对回复者表示感谢，或者换种方式表达感激，譬如用 谢谢你的关注 或 谢谢你的关照。） 问题解决后，加个简短的补充说明问题解决后，向所有帮助过你的人发个说明，让他们知道问题是怎样解决的，并再一次向他们表示感谢。如果问题在新闻组或者邮件列表中引起了广泛关注，应该在那里贴一个说明比较恰当。 最理想的方式是向最初提问的话题回复此消息，并在标题中包含 已修正，已解决 或其它同等含义的明显标记。在人来人往的邮件列表里，一个看见讨论串 问题 X 和 问题 X - 已解决 的潜在回复者就明白不用再浪费时间了（除非他个人觉得 问题 X 的有趣），因此可以利用此时间去解决其它问题。 补充说明不必很长或是很深入；简单的一句 你好，原来是网线出了问题！谢谢大家 – Bill 比什么也不说要来的好。事实上，除非结论真的很有技术含量，否则简短可爱的小结比长篇大论更好。说明问题是怎样解决的，但大可不必将解决问题的过程复述一遍。 对于有深度的问题，张贴调试记录的摘要是有帮助的。描述问题的最终状态，说明是什么解决了问题，在此 *** 之后 *** 才指明可以避免的盲点。避免盲点的部分应放在正确的解决方案和其它总结材料之后，而不要将此信息搞成侦探推理小说。列出那些帮助过你的名字，会让你交到更多朋友。 除了有礼貌和有内涵以外，这种类型的补充也有助于他人在邮件列表 / 新闻群组 / 论坛中搜索到真正解决你问题的方案，让他们也从中受益。 至少，这种补充有助于让每位参与协助的人因问题的解决而从中得到满足感。如果你自己不是技术专家或者程序员，那就相信我们，这种感觉对于那些你向他们求助的大师或者专家而言，是非常重要的。问题悬而未决会让人灰心；程序员们渴望看到问题被解决。好人有好报，满足他们的渴望，你会在下次提问时尝到甜头。 思考一下怎样才能避免他人将来也遇到类似的问题，自问写一份文件或加个常见问题（FAQ）会不会有帮助。如果是的话就将它们发给维护者。 在程序员中，这种良好的后继行动实际上比传统的礼节更为重要，也是你如何透过善待他人而赢得声誉的方式，这是非常有价值的资产。 如何解读答案RTFM 和 STFW：如何知道你已完全搞砸了有一个古老而神圣的传统：如果你收到 RTFM （Read The Fucking Manual） 的回应，回答者认为你 ** 应该去读他妈的手册 **。当然，基本上他是对的，你应该去读一读。 RTFM 有一个年轻的亲戚。如果你收到 STFW（Search The Fucking Web） 的回应，回答者认为你 ** 应该到他妈的网上搜索 ** 过了。那人多半也是对的，去搜索一下吧。（更温和一点的说法是 **Google 是你的朋友 **！） 在论坛，你也可能被要求去爬爬论坛的旧文。事实上，有人甚至可能热心地为你提供以前解决此问题的讨论串。但不要依赖这种关照，提问前应该先搜索一下旧文。 通常，用这两句之一回答你的人会给你一份包含你需要内容的手册或者一个网址，而且他们打这些字的时候也正在读着。这些答复意味着回答者认为 ** 你需要的信息非常容易获得 **； ** 你自己去搜索这些信息比灌给你，能让你学到更多 **。 你不应该因此不爽；** 依照程序员的标准，他已经表示了对你一定程度的关注，而没有对你的要求视而不见 **。你应该对他祖母般的慈祥表示感谢。 如果还是搞不懂如果你看不懂回应，别立刻要求对方解释。像你以前试着自己解决问题时那样（利用手册，FAQ，网络，身边的高手），先试着去搞懂他的回应。如果你真的需要对方解释，记得表现出你已经从中学到了点什么。 比方说，如果我回答你： 看来似乎是 zentry 卡住了；你应该先清除它。，然后，这是一个 *** 很糟的 *** 后续问题回应：zentry 是什么？ *** 好 *** 的问法应该是这样： 哦 ~~~ 我看过说明了但是只有 -z 和 -p 两个参数中提到了 zentries，而且还都没有清楚的解释如何清除它。你是指这两个中的哪一个吗？还是我看漏了什么？ 处理无礼的回应很多程序员圈子中看似无礼的行为并不是存心冒犯。相反，它是直接了当，一针见血式的交流风格，这种风格更注重解决问题，而不是使人感觉舒服而却模模糊糊。 如果你觉得被冒犯了，试着平静地反应。如果有人真的做了出格的事，邮件列表、新闻群组或论坛中的前辈多半会招呼他。如果这 *** 没有 *** 发生而你却发火了，那么你发火对象的言语可能在程序员社区中看起来是正常的，而 *** 你 *** 将被视为有错的一方，这将伤害到你获取信息或帮助的机会。 另一方面，你偶而真的会碰到无礼和无聊的言行。与上述相反，对真正的冒犯者狠狠地打击，用犀利的语言将其驳得体无完肤都是可以接受的。然而，在行事之前一定要非常非常的有根据。纠正无礼的言论与开始一场毫无意义的口水战仅一线之隔，程序员们自己莽撞地越线的情况并不鲜见。如果你是新手或外人，避开这种莽撞的机会并不高。如果你想得到的是信息而不是消磨时光，这时最好不要把手放在键盘上以免冒险。 （有些人断言很多程序员都有轻度的自闭症或亚斯伯格综合症，缺少用于润滑人类社会 ** 正常 ** 交往所需的神经。这既可能是真也可能是假的。如果你自己不是程序员，兴许你认为我们脑袋有问题还能帮助你应付我们的古怪行为。只管这么干好了，我们不在乎。我们 *** 喜欢 *** 我们现在这个样子，并且通常对病患标记都有站得住脚的怀疑。） Jeff Bigler 的观察总结和这个相关也值得一读 (tact filters)。 在下一节，我们会谈到另一个问题，当 *** 你 *** 行为不当时所会受到的 冒犯。 如何避免扮演失败者在程序员社区的论坛中有那么几次你可能会搞砸 – 以本指南所描述到的或类似的方式。而你会在公开场合中被告知你是如何搞砸的，也许攻击的言语中还会带点夹七夹八的颜色。 这种事发生以后，你能做的最糟糕的事莫过于哀嚎你的遭遇、宣称被口头攻击、要求道歉、高声尖叫、憋闷气、威胁诉诸法律、向其雇主报怨、忘了关马桶盖等等。相反地，你该这么做： 熬过去，这很正常。事实上，它是有益健康且合理的。 社区的标准不会自行维持，它们是通过参与者积极而 *** 公开地 *** 执行来维持的。不要哭嚎所有的批评都应该通过私下的邮件传送，它不是这样运作的。当有人评论你的一个说法有误或者提出不同看法时，坚持声称受到个人攻击也毫无益处，这些都是失败者的态度。 也有其它的程序员论坛，受过高礼节要求的误导，禁止参与者张贴任何对别人帖子挑毛病的消息，并声称 如果你不想帮助用户就闭嘴。 结果造成有想法的参与者纷纷离开，这么做只会使它们沦为毫无意义的唠叨与无用的技术论坛。 夸张的讲法是：你要的是 ** 友善 **（以上述方式）还是有用？两个里面挑一个。 记着：当程序员说你搞砸了，并且（无论多么刺耳）告诉你别再这样做时，他正在为关心 ** 你 ** 和 ** 他的社区 ** 而行动。对他而言，不理你并将你从他的生活中滤掉更简单。如果你无法做到感谢，至少要表现得有点尊严，别大声哀嚎，也别因为自己是个有戏剧性超级敏感的灵魂和自以为有资格的新来者，就指望别人像对待脆弱的洋娃娃那样对你。 有时候，即使你没有搞砸（或者只是在他的想像中你搞砸了），有些人也会无缘无故地攻击你本人。在这种情况下，抱怨倒是 *** 真的 *** 会把问题搞砸。 这些来找麻烦的人要么是毫无办法但自以为是专家的不中用家伙，要么就是测试你是否真会搞砸的心理专家。其它读者要么不理睬，要么用自己的方式对付他们。这些来找麻烦的人在给他们自己找麻烦，这点你不用操心。 也别让自己卷入口水战，最好不要理睬大多数的口水战 – 当然，这是在你检验它们只是口水战，并且未指出你有搞砸的地方，同时也没有巧妙地将问题真正的答案藏于其后（这也是有可能的）。 不该问的问题以下是几个经典蠢问题，以及程序员没回答时心中所想的： 问题：我能在哪找到 X 程序或 X 资源？ 问题：我怎样用 X 做 Y？ 问题：如何设定我的 shell 提示？ 问题：我可以用 Bass-o-matic 文件转换工具将 AcmeCorp 档案转换为 TeX 格式吗？ 问题：我的程序 / 设定 /SQL 语句没有用 问题：我的 Windows 电脑有问题，你能帮我吗？ 问题：我的程序不会动了，我认为系统工具 X 有问题 问题：我在安装 Linux（或者 X ）时有问题，你能帮我吗？ 问题：我怎么才能破解 root 帐号 / 窃取 OP 特权 / 读别人的邮件呢？ 问题：我能在哪找到 X 程序或 X 资源？ 回答：就在我找到它的地方啊，白痴 – 搜索引擎的那一头。天哪！难道还有人不会用 Google 吗？ 问题：我怎样用 X 做 Y？ 回答：如果你想解决的是 Y ，提问时别给出可能并不恰当的方法。这种问题说明提问者不但对 X 完全无知，也对 Y 要解决的问题糊涂，还被特定形势禁锢了思维。最好忽略这种人，等他们把问题搞清楚了再说。 问题：如何设定我的 shell 提示？？ 回答：如果你有足够的智慧提这个问题，你也该有足够的智慧去 RTFM，然后自己去找出来。 问题：我可以用 Bass-o-matic 文件转换工具将 AcmeCorp 档案转换为 TeX 格式吗？ 回答：试试看就知道了。如果你试过，你既知道了答案，就不用浪费我的时间了。 问题：我的 {程序 / 设定 /SQL 语句} 不工作 回答：这不算是问题吧，我对要我问你二十个问题才找得出你真正问题的问题没兴趣 – 我有更有意思的事要做呢。在看到这类问题的时候，我的反应通常不外如下三种 你还有什么要补充的吗？ 真糟糕，希望你能搞定。 这关我有什么屁事？ 问题：我的 Windows 电脑有问题，你能帮我吗？ 回答：能啊，扔掉微软的垃圾，换个像 Linux 或 BSD 的开放源代码操作系统吧。 注意：如果程序有官方版 Windows 或者与 Windows 有互动（如 Samba），你 *** 可以 *** 问与 Windows 相关的问题， 只是别对问题是由 Windows 操作系统而不是程序本身造成的回复感到惊讶， 因为 Windows 一般来说实在太烂，这种说法通常都是对的。 问题：我的程序不会动了，我认为系统工具 X 有问题 回答：你完全有可能是第一个注意到被成千上万用户反复使用的系统调用与函数库档案有明显缺陷的人，更有可能的是你完全没有根据。不同凡响的说法需要不同凡响的证据，当你这样声称时，你必须有清楚而详尽的缺陷说明文件作后盾。 问题：我在安装 Linux（或者 X ）时有问题，你能帮我吗？ 回答：不能，我只有亲自在你的电脑上动手才能找到毛病。还是去找你当地的 Linux 使用群组者寻求实际的指导吧（你能在 这儿 找到使用者群组的清单）。 注意：如果安装问题与某 Linux 的发行版有关，在它的邮件列表、论坛或本地使用者群组中提问也许是恰当的。此时，应描述问题的准确细节。在此之前，先用 Linux 和 *** 所有 *** 被怀疑的硬件作关键词仔细搜索。 问题：我怎么才能破解 root 帐号 / 窃取 OP 特权 / 读别人的邮件呢？ 回答：想要这样做，说明了你是个卑鄙小人；想找个程序员帮你，说明你是个白痴！ 好问题与蠢问题最后，我将透过举一些例子，来说明怎样聪明的提问；同一个问题的两种问法被放在一起，一种是愚蠢的，另一种才是明智的。 ** 蠢问题 **： 我可以在哪儿找到关于 Foonly Flurbamatic 的资料？ 这种问法无非想得到 STFW 这样的回答。 ** 聪明问题 **： 我用 Google 搜索过 “Foonly Flurbamatic 2600”，但是没找到有用的结果。谁知道上哪儿去找对这种设备编程的资料？ 这个问题已经 STFW 过了，看起来他真的遇到了麻烦。 ** 蠢问题 ** 我从 foo 项目找来的源码没法编译。它怎么这么烂？ 他觉得都是别人的错，这个傲慢自大的提问者。 ** 聪明问题 ** foo 项目代码在 Nulix 6.2 版下无法编译通过。我读过了 FAQ，但里面没有提到跟 Nulix 有关的问题。这是我编译过程的记录，我有什么做的不对的地方吗？ 提问者已经指明了环境，也读过了 FAQ，还列出了错误，并且他没有把问题的责任推到别人头上，他的问题值得被关注。 ** 蠢问题 ** 我的主机板有问题了，谁来帮我？ 某程序员对这类问题的回答通常是： 好的，还要帮你拍拍背和换尿布吗？，然后按下删除键。 ** 聪明问题 ** 我在 S2464 主机板上试过了 X 、 Y 和 Z ，但没什么作用，我又试了 A 、 B 和 C 。请注意当我尝试 C 时的奇怪现象。显然 florbish 正在 grommicking，但结果出人意料。通常在 Athlon MP 主机板上引起 grommicking 的原因是什么？有谁知道接下来我该做些什么测试才能找出问题？ 这个家伙，从另一个角度来看，值得去回答他。他表现出了解决问题的能力，而不是坐等天上掉答案。 在最后一个问题中，注意 告诉我答案 和 给我启示，指出我还应该做什么诊断工作 之间微妙而又重要的区别。 事实上，后一个问题源自于 2001 年 8 月在 Linux 内核邮件列表（lkml）上的一个真实的提问。我（Eric）就是那个提出问题的人。我在 Tyan S2464 主板上观察到了这种无法解释的锁定现象，列表成员们提供了解决这一问题的重要信息。 通过我的提问方法，我给了别人可以咀嚼玩味的东西；我设法让人们很容易参与并且被吸引进来。我显示了自己具备和他们同等的能力，并邀请他们与我共同探讨。通过告诉他们我所走过的弯路，以避免他们再浪费时间，我也表明了对他们宝贵时间的尊重。 事后，当我向每个人表示感谢，并且赞赏这次良好的讨论经历的时候， 一个 Linux 内核邮件列表的成员表示，他觉得我的问题得到解决并非由于我是这个列表中的 *** 名人 ***，而是因为我用了正确的方式来提问。 程序员从某种角度来说是拥有丰富知识但缺乏人情味的家伙；我相信他是对的，如果我 *** 像 *** 个乞讨者那样提问，不论我是谁，一定会惹恼某些人或者被他们忽视。他建议我记下这件事，这直接导致了本指南的出现。 如果得不到回答如果仍得不到回答，请不要以为我们觉得无法帮助你。有时只是看到你问题的人不知道答案罢了。没有回应不代表你被忽视，虽然不可否认这种差别很难区分。 总的来说，简单的重复张贴问题是个很糟的点子。这将被视为无意义的喧闹。有点耐心，知道你问题答案的人可能生活在不同的时区，可能正在睡觉，也有可能你的问题一开始就没有组织好。 你可以通过其他渠道获得帮助，这些渠道通常更适合初学者的需要。 有许多网上的以及本地的使用者群组，由热情的软件爱好者（即使他们可能从没亲自写过任何软件）组成。通常人们组建这样的团体来互相帮助并帮助新手。 另外，你可以向很多商业公司寻求帮助，不论公司大还是小。别为要付费才能获得帮助而感到沮丧！毕竟，假使你的汽车发动机汽缸密封圈爆掉了 – 完全可能如此 – 你还得把它送到修车铺，并且为维修付费。就算软件没花费你一分钱，你也不能强求技术支持总是免费的。 对像是 Linux 这种大众化的软件，每个开发者至少会对应到上万名使用者。根本不可能由一个人来处理来自上万名使用者的求助电话。要知道，即使你要为这些协助付费，和你所购买的同类软件相比，你所付出的也是微不足道的（通常封闭源代码软件的技术支持费用比开放源代码软件的要高得多，且内容也没那么丰富）。 如何更好地回答问题** 态度和善一点 **。问题带来的压力常使人显得无礼或愚蠢，其实并不是这样。 ** 对初犯者私下回复 **。对那些坦诚犯错之人没有必要当众羞辱，一个真正的新手也许连怎么搜索或在哪找常见问题都不知道。 ** 如果你不确定，一定要说出来 **！一个听起来权威的错误回复比没有还要糟，别因为听起来像个专家很好玩，就给别人乱指路。要谦虚和诚实，给提问者与同行都树个好榜样。 ** 如果帮不了忙，也别妨碍他 **。不要在实际步骤上开玩笑，那样也许会毁了使用者的设置 – 有些可怜的呆瓜会把它当成真的指令。 ** 试探性的反问以引出更多的细节 **。如果你做得好，提问者可以学到点东西 – 你也可以。试试将蠢问题转变成好问题，别忘了我们都曾是新手。 尽管对那些懒虫抱怨一声 RTFM 是正当的，能指出文件的位置（即使只是建议个 Google 搜索关键词）会更好。 ** 如果你决定回答，就请给出好的答案 **。当别人正在用错误的工具或方法时别建议笨拙的权宜之计（wordaround），应推荐更好的工具，重新界定问题。 ** 正面的回答问题 **！如果这个提问者已经很深入的研究而且也表明已经试过 X 、 Y 、 Z 、 A 、 B 、 C 但没得到结果，回答 试试看 A 或是 B 或者 试试 X 、 Y 、 Z 、 A 、 B 、 C 并附上一个链接一点用都没有。 ** 帮助你的社区从问题中学习 **。当回复一个好问题时，问问自己 如何修改相关文件或常见问题文件以免再次解答同样的问题？，接着再向文件维护者发一份补丁。 如果你是在研究一番后才做出的回答，** 展现你的技巧而不是直接端出结果 **。毕竟 授人以鱼不如授人以渔。 tips：还有一点博主我觉得挺重要的：如果有妹子私聊你请教问题，请务必不要介意本文介绍的反例。","link":"/thinging-in-ask/"},{"title":"离开魔都后的一点感想","text":"看过我公众号简介的朋友会发现，我公众号的定位是分享一些个人的技术博客和杂谈，这其中的杂谈不仅包括了技术杂谈，也包含了自己的一些感悟。所以借此澄清，这是一篇杂文！总结了我在魔都这一年半不同角度的感悟。 技术沙龙我前几天刚发过一个朋友圈：魔都得天独厚的优势便是各种技术沙龙，其中不乏有很多是免费的。我在魔都这些日子，参加了大概 13-15 场技术沙龙。参加这种技术分享会的动机很单纯，大家都是去学习互联网最前沿的技术的，但隐性的好处也很多： 你可以认识很多跟你同样有积极性的同行们，试想一下，这样一批放弃了周末休息时间，不睡懒觉，不打游戏，不陪妹子的人，必然是对技术充满了热情的一帮人。我有不少微信好友是在技术分享会上添加的。 有些朋友身处传统行业，平时的业务接触不到高并发，JVM 这些看似高大上的名词，但是技术沙龙带来了这样的可能性，让你没吃过猪肉，至少见过猪跑。映像比较深刻的是“美团点评技术团队”定期举办的技术沙龙，参加了两次，分享的很棒，而且大多数情况下，只需要在 APP 中报名即可参加。 同行间的技术方案分享。很有可能你正在负责公司某块业务的技术选型，而沙龙正好涉及了相关的内容，可以静静聆听下别人的解决方案，通过别人落地的架构，来指导自己的选型。这一点我也有例子真实的例子与之对应，大概是 17 年初，有一场 daocloud 举办的 springcloud 社区沙龙，由 DD 带来的 zuul 源码解读以及案例分享，记忆尤为深刻，当时我所在的公司也恰巧在对 zuul 进行改造，而 DD 分享的主题对于我的工作非常有帮助。 参加过不少沙龙，也踩过不少坑，我甚至误打误撞参加过两次“偏运维的技术分享”，权当作经历吧，总结参加的技术沙龙后，有以下几点建议：选择人数少的沙龙，越大的技术分享，讲师越想着受众面广，从而扼不住主题，会出现很多干货之外的内容；多举手提问，事实上我参加过的沙龙，我都会主动举手提问，并且是主持人刚问完“有没有观众有问题的”，我就立马举手，这时候大家都比较内敛，自己的命中率才会高。为什么要举手提问？期待讲师一句话把你的疑惑讲清楚是不现实的，但是这创造了你们后续交流的话题。分享结束后，主动加讲师的微信，进入深度交流阶段，这时候你便多了一个大牛好友。大牛好友用来干嘛？也不必指望为你解答疑惑，如果可以当然最好，偶尔看看他们的朋友圈都在分享些什么，至少能掌握最前沿的信息；有网红讲师参加的技术分享，尤其需要做好功课，刷几遍讲师最近的博客，提问时简明扼要的说出你提前准备了几天的问题，带上讲师出版的书索要签名，这样做的效果谁用谁知道。 [活动行] 是一款非常不错的 APP，再算上各个公众号（如 IT 大咖说），各种技术峰会（这个门票可能会有点贵，自费用户比较吃力），周末完全可以很充实。 博客 / 公众号与知识付费这几个词放到一起说，作为一个博主 &amp; 公众号作者，我接触到很多这个圈子的人，他们聪明，努力，乐于分享，而且往往是各自领域最厉害的那群人。以公众号粉丝数来评价，有近 10w 粉丝的大 V（在某个专业领域拥有这么多粉丝我认为已经算的上是 V 了，不同于鸡汤文作者受众广）；以努力程度来评价，深夜 1-2 点还在桌边伏案的博主不在少数，就拿「芋艿源码」的博主「芋艿」来说吧，加班回家的地铁上笔记本一摊开就是一顿源码分析，年纪也老大不小了，保持着如同刚开始工作一般的热情，丝毫不给我们这些年轻人一口喘气的机会；还有一些博主，身处互联网公司，加班严重，依旧会挤出时间分享一线互联网的工作经验。身处这样一群人中间，时刻让你体会到什么叫「比你优秀的人还比你努力」。 介绍完我的所见，来聊聊我的所想，写作到底有什么意义。 一个直观的体验是成就感，当你的文章确切地帮助到了别人，获得了阅读量与赞，便会由衷地受到鼓舞，从而激发自己的创作欲。你会迸发出下一个目标，去帮助更多的人。收获自然也是会有的，靠公众号和博客积攒出的粉丝，成了那些知名博主的第一批付费粉丝。知名大牛在「极客时间」，「得到」等付费平台活跃，不愿意抛头露面的博主也有「知识星球」这样的平台和自己付费粉丝互动。知识付费是件好事，粉丝出了物力，赞助了自己认同的博主，交过钱后自然会更加用心；博主也有了挣钱的动力，一定会有激励作用，同时也会更加对自己的粉丝负责。未来这一块必然会越来越普及。 二来还是回到人脉、眼界这样的话题，当有了博主这个身份，被拉进一些社区原创作者交流群便成了顺利成章的事，在大佬云集的群里默默地看着讨论的话题，会提升自己的眼界。收到猎头的邮件也成了日常的习惯，来自 github 的招聘启事和来自博客的猎头，HR 流量可能会让你不堪其扰，那种感到愉悦的骚扰。「先生，游泳健身了解一下」下一份工作机会，可能就是来自于你的博客 &amp; 公众号，毕竟这两个东西比你的简历更会说话。 魔都的生活大四上初到上海寻找实习机会，直到今年 3 月份离开，算是有一年半了，对魔都这个陌生的城市也算了解了不少。年轻时不来魔都闯闯，后面迟早会有这个念头的，不如趁早体验下：高峰时期人潮拥挤的 1 号线，与日常乘坐的 9 号线末站，这都是魔都。跟同事去过人山人海的城隍庙，并没有太多惊喜，反倒是无意间逛到的思南路，仿佛给人穿越到欧洲小镇的错觉，惊喜。高楼林立下的田子坊，和远在厦门的鼓浪屿竟让我难以区分，这种相似只是那种一瞬间的感觉，最后留在脑海中的回忆还是在上海这种繁华的大都市竟然还有这样的净土。无论是浦东图书馆静谧的翻书声还是 MAO Livehouse 震颤到心脏的摇滚声，和同事们在外滩留下的足迹，联洋广场令人流连忘返的捞王… 魔都从不缺少物质享受。 说回魔都的工作，特指 IT 行业，也有了自己的一点认知，都说一线城市是北上广深，但说到底，上海的 IT/ 互联网并不是原以为的那样发达。美团点评，唯品会，饿了么，携程说大也大，还有金融领域的陆金所，蚂蚁金服（刚搬来）但总体来说肯定比不上北京，但这丝毫阻挡不了江浙沪有志青年的热情，即使有房价这个负面光环加护。 上海的压力来自于各个方面：孤独，消费，房价，工作额度，任意一个方面都会击垮一个人，再加上互联网的人来人往，都让我感受到上海这座现代化都市背后的冷漠。初到南京，HR 问我为什么不在上海发展，我竟答不出来。是啊，普遍都认为上海机遇更多，但是，机遇的定义没人说的清，正如离开上海的原因说不清一般。值得留恋的景，值得留恋的机遇，值得留恋的人，都带不走。 应届生的境遇到了 18 年的春招，似乎已经挂不上「应届生」的名号了。时隔毕业半年多，同学们的工作也都稳定了下来，校园时光真的只存在于念想中了。曾经的自己仰仗着自己的应届光环，如今办理入职后才发现，自己已经是往届生了，有什么区别？心境真的会变，我自觉认识到再也不能仗着应届那么有恃无恐了。现在的职场上必须得拿能力和那些工作比你早几年接触工作的人去竞争了，应聘时再也不会说：我是来贵司学习的。公司需要为之创造价值的人，说来不怕矫情，以上这种念头最近在脑海中酝酿了很久。 与应届生相对的是那些工作十几年的老人，为什么放到这儿一起说，主要是在魔都见识到了一些年纪较大的应聘者，简历空有十年工作经验，实际怕是十个一年经验。不知道是否只有我会出现这种情况，刚毕业时知识储备不足每天下班坚持看书，看文章都不觉累，反倒是工作半年后有了熟练度了，这股学习的激情会下降。这点我尤其佩服我原来的老大和朋友圈一些年纪较大的程序员，几年如一日地对技术痴迷。警惕成为一个重复工作的码农。 学习技巧编程技术如何精进？我秉承自上而下的顺序来谈下自己的思考，仅供各位参考。 业务驱动型。编程是为了解决问题，如果你的程序应用于生产中，这是最锻炼编码水平，无论是导入一个超大的 excel 这种小 case，还是抗住千万级别并发的电商系统，伴随着业务问题而进行的 coding 最有意义。反观我交接的最后一个星期，干不了什么活，都是些零碎琐事，完全提炼不出什么精髓用于写作。如果你的系统日活只有几百，考虑百万级别的并发压力实属是杞人忧天，还是优先把项目上线了更为实在。 源码阅读型。talk is cheap，show me the code。大多数设计精良的框架源码都伴随着巧妙的设计和丰富的注释，这些是学习编码技巧的绝佳途径，代码不会说谎，他就放在那儿，关键就在于你愿不愿点进去，去分析他。很多人拒绝阅读源码，自认为水平不够，其实实在是妄自菲薄，很多优秀的源码（如 spring）不仅可以提升你对框架工作原理的理解，也有助于自身编码的规范。 文档型。很多技术博客文章来源于这项技术的官方文档，很多小白喜欢阅读博客，殊不知博客已经是别人二次消化后的产物，存在不少理解的缺失。再者，较为前沿的技术，第一手博客还没出产，只有官方文档，比如 springboot 2.0，其实官方文档已经很细致入微地介绍了各种特性，完全没必要等待别人的翻译。学习一门新的技术，我通常的做法是：博客了解其大概用途，紧接着直接阅读官方文档，同时也避免了版本滞后性等问题。 博客型、搜索引擎型。这两者放到最下面，并不是排斥它们，也不是在否认 google 和 Stack Overflow 的伟大。而是想提出这样的观点：博客和搜索引擎适用于解决问题，从宏观的了解一项技术。博客和搜索引擎并不向你保证其准确性，他只告诉这样做或许有效，这样做或许正确。但是好处也很明显，经过了博主的梳理，可以更好的帮助读者建立知识体系，毕竟我自己也是写博客的 [微笑 face]，但不要忘了有空去翻一翻 2，3 两点 夜深了，先写这么多吧。","link":"/thinking-1/"},{"title":"关于阿里面试、学习路线、公众号的一些想法","text":"还记得上一篇记录我心情的随笔是写在离开魔都，去往南京的时候，此时的我，又来到了杭州。工作发生了变故，心境也发生了变化，倒是有不少东西想跟各位来聊一聊，择其三汇成此文。 阿里面试入职阿里第一天，我发了一条入职阿里的朋友圈，很多朋友发表了评论：羡慕，恭喜，也有一些前辈给了我忠告，首先在这儿谢谢大家。 微信群中自然有很多人会关注：“阿里面试都面了什么？有什么回答技巧吗？能不能分享下面经？”。但今天想跟大家说的是，我并不觉得分享那些面试题，甚至把答案都告诉你，会对你有多大的帮助。 其一，那是我的面试题，不是你的。每个人的工作经历不一样，合格的面试官必定是针对个人的简历进行提问，而不是地毯式的来一次 mq，redis，rpc，database，spring 的大扫荡。 其二，平时的知识储备，远胜于那些事先准备好答案的面试题。这和学生时代一样，成绩好的学霸即使期末不用复习，依旧可以考出高分；临时抱佛脚的学渣，大概率会在考试中露出马脚。众所周知，微信公众号文章中阅读量最高的往往是面经类文章，我群里有一名程序媛分享了一篇自己「阿里 7 面」的经历，一早上便有了 3000 多的阅读量。这背后很大程度是出于个人的焦虑，若你的知识储备不足，看的面试题越多，你就会越焦虑；反观我了解的那些技术水平不错的朋友，往往都对这些面试题嗤之以鼻。这背后反映出了一些问题，对于一些老生常谈的面试题，诸如“ConcurrentHashMap 的原理”，“ThreadLocal 的原理”，你即使回答的再好，我相信依旧称不上出彩；而对于一些技术的使用场景你能够说出自己的理解，那才是优秀之处，无招胜有招。 总结下这两点，无非就是想告诉那些新手玩家，面试并不存在什么奇技淫巧，那些在实战积累出来的经验，以及你自己探索源码获得的经验才是面试中的金子。如果你非要我说一两个注意点，那我反而觉得应聘部门的 HC 和面试官的心情更重要一些。 你只是个孩子，你根本不晓得你在说什么。 我问你「艺术」，你可能会提出艺术书籍中的粗浅论调，有关米开朗基罗，你知道很多，他的满腔政治热情，他与教皇相交莫逆；但你不知道西斯汀教堂的气味，你从没站在那儿观赏美丽的天花板。 如果我问关于「女人」的事，你八成会说出个人偏好的谬论，你可能上过几次床，但你说不出在女人身旁醒来，那份内心幸福的滋味 当谈论「战争」，你会说出莎士比亚的话，“共赴战场，亲爱的朋友”。但你从未亲临战阵，从没把把挚友的头抱在膝盖里，看着他吸着最后一口气，凝望着你，希望你能够帮到他 我问你「爱情」，你会引述十四行诗，但你从未看过女人的脆弱，她能以双眼击倒你，感觉上帝让天使为你下凡，从地狱中拯救你。 — 心灵捕手 学习路线与技术标签至于群友关于学习路线的建议，我还是打算在这一话题中提供一点我的看法，仅供参考。如果你是我博客的忠实读者，应当能够知道我的学习路线是什么样的。 在初入职场实习时，主要的任务是巩固 Java 基础，那些 J2SE 的基础知识，不至于说精通源码，至少应该能做到侃侃而谈。这个过程，面很重要，所以适合看书，按照章节的梳理，知识点被串联在一起，日后可以将其对号入座。至于推荐书籍，新的旧的，差异不是很大，可以自行翻阅我博客中或者其他大 V 的推荐书单。 有些人觉得看视频很 low，切不要有这样的偏见，我一直觉得好的视频会给人非常直观的学习体验，虽说不如书籍高效，但学习起来十分轻松，我最近看的视频就包括闪电侠的 netty 源码解读以及小马哥的一些公开课视频，受益很多（互联网鄙视培训，但我初学时也看过传智播客和尚硅谷的一些培训视频，的确讲的很好，没什么丢人不丢人的，学到知识就是王道） 官方文档和源码，这是我目前学习新知识最主要的途径，话不多说，不愿意接受如此高效的学习方法的人，大多数是因为懒。 如上可能还算不上学习路线，顶多算作学习方法，可以说是老生常谈的三点了，拿出来权当是强调一次。我理解的路线是一个人掌握了必备的 IT 基础技能之后，发展一到两个自己非常擅长的路线，如果做的足够的好，你的路线会成为你的技术标签，比如我的好友当中就不乏这样具有技术标签的人物，闪电侠的 netty，厮大的 mq，艿艿的源码解析，亚普的 96/ 调色大师 / 系统监控。再回到我自己，短期内，rpc 服务治理可能就是我打算走的路线。 公众号的一些运营想法也是在最近一个月，粉丝数突破了 5000，我也创了自己的技术交流群「Kirito 的技术分享」。我原本并没有创群的打算，一方面担心自己管理不好，另一方面是加入的微信交流群实在是太多，人员也存在很大程度的重叠。促使我创立交流群（或许称之为「小密圈」可能更为合适）的初衷我也给我的读者交代一下 现在各个微信公众号的知识分享处于一种过剩的状态，优质的原创文，不走心的水文，面向于小白的基础文，广告贴，蹭时事热点的贴子…实在是鱼龙混杂，各个群里面铺天盖地的铺天盖地的文章，使得大家应接不暇。所以，我创了自己的交流圈，初衷便是和关注我的读者们安安静静地讨论文章中知识点和观点，我也并不排斥优质的原创文章，群规便有一点比较独特的地方：只建议推广 ** 个人 ** 的 ** 原创 ** 文章。 关于互推和广告贴，我的个人原则是参与，不推广。互推文这种形式是指几个公众号的维护者一起发文，达到互相增粉的效果，由于微信公众号的文章是闭环的，推广的途径有限，而我希望更多的人能够看到我的文章，所以适度地互推是有必要的，我也希望读者能够不要排斥这种行为，一般我的标题就可以让你知道：这篇文章是一篇互推文，而一般互推文需要一定阅读量的支撑，点击阅读 + 关注互推的公众号，都是对我的支持，可以视自己的接受程度来决策。同理，还有广告贴，一般比较浮夸的标题就是广告跑不了了，同样是有阅读量的需求，广告商的经济鼓励会让博主有更大的动力创作优质的原创文章。 写作圈子里面也有一些坏味道，接广告的公众号瞧不起发水文的公众号，发水文的公众号眼红有广告的公众号，还有一些公众号存在刷粉，刷阅读量的行为，也有一些公众号存在不尊重原创的行为，我也聊聊自己的公众号价值观。 水文：不为了发文而发文，宁缺毋滥。拒绝写水文。 原创：转载需要在文首注明出处，第一时间告诉读者这是一篇转载的文章；翻译文章不能标注原创；转载他人博客的文章不得标明原创，他人主动要求除外；不洗文；多个公众号不要过度转载同样的文章，造成信息的过度消费；未经授权不要随意转载他人文章或标明侵删；微信公众号的转载文章可以不贴对方的二维码，因为微信文章下方自带导流链接。 广告：不做误人子弟的广告如 p2p 理财；不做标题党，明确这是一篇广告；适度 互推：确保互推中其他公众号的质量；适度 热点文：确认事实后再发文，不应煽动 毕竟道德、价值观这些个东西只能用来约束自己，不能用来约束别人，我只能向各位读者保证，我公众号运营的信条如上。 感谢各位读者的关注，今后无论多忙，我一定会坚持把博客写下去。 End","link":"/thinking-2/"},{"title":"对于 Spring Cloud Feign 入门示例的一点思考","text":"Spring Cloud FeignSpring Cloud Feign 是一套基于 Netflix Feign 实现的声明式服务调用客户端。它使得编写 Web 服务客户端变得更加简单。我们只需要通过创建接口并用注解来配置它既可完成对 Web 服务接口的绑定。它具备可插拔的注解支持，包括 Feign 注解、JAX-RS 注解。它也支持可插拔的编码器和解码器。Spring Cloud Feign 还扩展了对 Spring MVC 注解的支持，同时还整合了 Ribbon 和 Eureka 来提供均衡负载的 HTTP 客户端实现。 分布式应用早在十几年前就开始出现，各自的应用运行在各自的 tomcat，jboss 一类的容器中，他们之间的相互调用变成了一种远程调用，而实现远程调用的方式很多。按照协议划分，可以有 RPC，Webservice，http。不同的框架也对他们有了各自的实现，如 dubbo(x)，motan 就都是 RPC 框架，本文所要讲解的 Feign 便可以理解为一种 http 框架，用于分布式服务之间通过 Http 进行接口交互。说他是框架，有点过了，可以理解为一个 http 工具，只不过在 spring cloud 全家桶的体系中，它比 httpclient，okhttp，retrofit 这些 http 工具都要强大的多。 入门先用一个简单的例子，看看如何在项目中使用 Feign。示例项目使用 maven 多 module 构建，采用 springcloud 的 Dalston.SR1 版本 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 服务提供方在本例子中，使用两个应用模块，展示分布式应用中如何进行接口交互。restful-provider 担任服务提供方，restful-consumer 担任服务消费者。 restful-provider 新建一个 modulerestful-provider-app, 模块中只需要写一个 CalculateController.java 即可 ​ 123456789101112131415@RestController@RequestMapping(&quot;/api&quot;)public class CalculateController { @PostMapping(&quot;/add&quot;) public Integer add(@RequestParam Integer a,@RequestParam Integer b){ return a+b; } @PostMapping(&quot;/subtract&quot;) public Integer subtract(@RequestParam Integer a,@RequestParam Integer b){ return a-b; }} 配置文件 application.yml： 12server: port: 7070 一个服务端就写好了，提供两个计算服务的接口，可以通过 http 访问 服务消费方 使用 Feign 编写消费方，在 restful-consumer 项目中，我们将接口的定义和消费者应用分成两个 module，restful-consumer-api-definition 和 restful-consumer-app。 在接口定义模块中，只有一个 Feign 接口： 123456789@FeignClient(value = &quot;calculate&quot;,path = &quot;/api&quot;)public interface CalculateApi { @PostMapping(path = &quot;/add&quot;) Integer add(@RequestParam(&quot;a&quot;) Integer a,@RequestParam(&quot;b&quot;) Integer b); @PostMapping(path = &quot;/subtract&quot;) Integer subtract(@RequestParam(&quot;a&quot;) Integer a,@RequestParam(&quot;b&quot;) Integer b);} tip：@RequestParam 中的参数值不能省略，否则会出现错误 restful-consumer-app 依赖上面的 restful-consumer-api-definition 模块，并且启用 Feign 代理，自动生成一个远程调用。启动类配置： 123456789@EnableFeignClients(basePackages = {&quot;sinosoftsh.consumer.api&quot;})@SpringBootApplicationpublic class ConsumerApp { public static void main(String []args){ SpringApplication.run(ConsumerApp.class,args); }} 使用 @EnableFeignClients(basePackages = {&quot;sinosoftsh.consumer.api&quot;}) 扫描接口类所在的包，spring 的容器中才会有代理实现类。 不要忘记配置消费者的相关属性，在 application.yml 中 12345678910111213server: port: 7080ribbon: eureka: enabled: falsecalculate: ribbon: listOfServers: localhost:7070logging: level: org.apache.http: trace 在 CalculateApi 接口的定义中，我们使用了一个 calculate 作为服务名称，必须要在配置文件中配置 calculate 所在的 ip 地址才行，由于本文只是作为一个示例，所以没有使用注册中心，在配置中禁用了 eureka。最后一行的日志配置，可以发现其实 Feign 内部其实使用的是现成的 http 工具：httpclient，okhttp3，可以通过配置替换实现 整体的项目结构如下： ![这里写图片描述](http://img.blog.csdn.net/20170803170857775?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzgxNTU0Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 图一 第一种依赖关系结构 再编写一个单元测试类，验证一下 Feign 是否被正确的配置了 12345678910111213@RestControllerpublic class ConsumerController { @Autowired CalculateApi calculateApi; @RequestMapping(&quot;/test&quot;) public String test() { Integer result = calculateApi.add(1, 2); System.out.println(&quot;the result is&quot; + result); return &quot;success&quot;; }} 思考回顾一下我们入门实例，服务提供方使用的是一个 RestController 暴露计算服务，服务消费方使用 http 工具（Feign）进行远程调用，这再清晰不过了，也是符合软件设计的，因为 Feign 接口的定义是存在于消费方，所以是真正的松耦合。但是习惯了使用 rpc 共享接口的设计，我们也可以将接口定义在服务提供方，这样做的好处是，服务可能被多个消费者使用，不需要每个消费者都定义一次 Feign 接口。 ![这里写图片描述](http://img.blog.csdn.net/20170803171756343?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzgxNTU0Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 图 2 第二种依赖关系结构 在 `restful-provider` 创建一个 `restful-provider-api-definition` 模块，将 `CalculateApi.java` 的定义迁移到服务提供方，相应的 `restful-provider-app` 也可以进行改造： 1234567891011121314151617@RestController@RequestMapping(&quot;/api&quot;)public class CalculateController implements CalculateApi{// @PostMapping(&quot;/add&quot;) @Override public Integer add(@RequestParam Integer a,@RequestParam Integer b){ return a+b; }// @PostMapping(&quot;/subtract&quot;) @Override public Integer subtract(@RequestParam Integer a,@RequestParam Integer b){ return a-b; }} 因为接口的定义和服务提供方现在在一个限界上下文中，接口的定义同时也宣告了应该提供什么样的服务，所以直接继承 CalculateApi。这里的理解比较绕，现在的设计中，CalculateApi 在服务消费者和服务提供者中的定位是不一样的，服务消费者需要在启动类扫描 CalculateApi 所在的包，生成代理对象，远程调用；而在服务提供方则一定不能扫描 CalculateApi 所在的包，否则会污染容器中的 CalculateApi 实现类，要知道，CalculateController 之上有一个 @RestController 注解，意味着已经有一个本地代理实现了，我们也可以在服务提供方注入 CalculateApi，便是进行的本地调用了，这符合我们的初衷：我自己的提供的服务，本地当然可以调用。在服务提供方的启动类上要额外注意 @ComponentScan，@EnableFeignClients 的扫描。 这样，当我们有多个消费者，只需要让他们配置 Feign，并且引入服务提供方的接口定义，扫描，即可进行远程调用。有点类似于 RPC 的共享接口。 设计原则restful 设计以语言无关，松耦合的优势著称。在 Spring Cloud Feign 的相关文档中有这样的描述： It is generally not advisable to share an interface between a server and a client. It introduces tight coupling, and also actually doesn’t work with Spring MVC in its current form (method parameter mapping is not inherited). 不建议使用上述改进后的共享接口的方式，并且警告我们，springmvc 的注解在 Feign 接口中的定义和实现类中是不可继承的。关于这点，仁者见仁，智者见智。我们现在项目依旧是采用共享接口的方式，这样可以使得开发变得便捷，多个消费者不需要重复定义。 下面是关于耦合和共享接口的一些讨论： 1234https://github.com/spring-cloud/spring-cloud-netflix/issues/951https://github.com/spring-cloud/spring-cloud-netflix/issues/659https://github.com/spring-cloud/spring-cloud-netflix/issues/646https://jmnarloch.wordpress.com/2015/08/19/spring-cloud-designing-feign-client/ 注意事项 当接口定义中出现了实体类时，需要使用 @RequestBody 注解。多个实体类，则需要用一个大的 vo 对其进行包裹，要时刻记住，Feign 接口最终是会转换成一次 http 请求。 接口定义中的注解和实现类中的注解要分别写一次，不能继承。 Feign 调用一般配合 eureka 等注册中心使用，并且在客户端可以支持 Hystrix 机制，本文为了讲解共享接口这一设计，所以重心放在了 Feign 上，实际开发中，这些 spring cloud 的其他组件通常配套使用。 对 http 深入理解，在使用 Feign 时可以事半功倍。","link":"/thinking-in-spring-cloud-feign/"},{"title":"ThreadLocal 的最佳实践","text":"SimpleDateFormat 众所周知是线程不安全的，多线程中如何保证线程安全又同时兼顾性能问题呢？那就是使用 ThreadLocal 维护 SimpleDateFormat 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class SimpleDateFormatThreadTest { static volatile AtomicInteger n = new AtomicInteger(-1);&lt;!-- more --&gt; static ThreadLocal&lt;DateFormat&gt; sdf ; static { sdf =new ThreadLocal&lt;DateFormat&gt;() { @Override protected DateFormat initialValue() { return new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); } }; } public static void main(String[] args) throws ParseException, InterruptedException { Set&lt;String&gt; dateSet = new ConcurrentHashSet&lt;&gt;(); Set&lt;Integer&gt; numberSet = new ConcurrentHashSet&lt;&gt;(); Date[] dates = new Date[1000]; for (int i = 0; i &lt; 1000; i++) { dates[i] = sdf.get().parse(i + 1000 + &quot;-11-22&quot;); } ExecutorService executorService = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executorService.execute(new Runnable() { @Override public void run() { int number = n.incrementAndGet(); String date = sdf.get().format(dates[number]); numberSet.add(number); dateSet.add(date); System.out.println(number+&quot; &quot;+date); } }); } executorService.shutdown(); Thread.sleep(5000); System.out.println(dateSet.size()); System.out.println(numberSet.size()); }} 实践证明 sdf 的 parse（String to Date）有严重的线程安全问题，format（Date to String）有轻微的线程安全问题，虽然不太明显，但还是会出现问题，这和内部的实现有关。 简单分析下使用 ThreadLocal 的好处，1000 次转换操作，10 个线程争抢执行，如果每次都去 new 一个 sdf，可见其效率之低，而使用 ThreadLocal，是对每个线程维护一个 sdf，所以最多就只会出现 10 个 sdf，真正项目中，由于操作系统线程分片执行，所以线程不会非常的多，使用 ThreadLocal 的好处也就立竿见影了。","link":"/threadLocal/"},{"title":"三种思维，让我脱胎换骨","text":"闭环思维闭环思维简而言之就是，如果别人发起一件事，你不管做的如何，最后都要闭环到这个发起者。 我曾经就经历过这样的事： 我让一个实习生写一个问卷。 两个小时过去了，没有反馈，三个小时过去了还是没有反馈，直到中午吃饭的时候，我碰到了他。 我问他，写完了吗？他说写完了。 我说，那你怎么没发给我看，他说，写完直接发出去了。 你会对这个人怎么评价？不靠谱。 有闭环思维的人是怎么做呢？ 第一，如果他完成了，那么就要及时反馈，并且说一下当时的情景。 第二，如果他没完成，也要及时反馈，是哪里遇到困难了，需不需要帮助。 以上无论哪一种情况，都有一个闭环，那就是由你本人发起又回到你的身上。 不管做得如何，一定要给出一个反馈，形成闭环，有太多有才华的人就死在这一环节了。 这件事我也没怪他，想想曾经的自己，也是一个“反馈黑洞”，这事交给我了，我最后给你弄完了就行了，老反馈多麻烦啊。最主要的是，减少反馈就多了偷懒的机会，自以为蒙混过关，其实已经被打上了不靠谱的标签。 过去有句话说的分非常好：凡事有交代，件件有着落，事事有回应。这就是闭环思维。 成长型思维成长型思维的人面对挑战与失败总是能够迎难而上；相反，成长型思维对应的另一种思维是固定型思维，他们面对挑战与失败总会下意识地回避。 曾经的我是一个不折不扣的固定型思维者： 小学时候觉得英语难，逃避，高中英语没及过格，大四才过四级； 中学时候因为一次物理考试没考好，对物理失去了兴趣，大学物理考了3次才过； 高中时候因为被喜欢的女生拒绝了，失去了追求异性的勇气，此后一段时间不敢和女生说话； 工作之后，领导交代一件事，我第一反应就是这个我不会，我做不了； …… 为什么会这样？ 归根结底是因为我害怕失败，害怕被否定，错误的把别人对某件事的否定当成了对自己这个人的否定。而不被否定的唯一方法就是不去做那件事。所以自己的能力圈就越来越小，进步的脚步也戛然而止。 后来我转变态度，我承认自己害怕失败，但我不服，越怕什么我就越做什么。 当初不是觉得自己学不好英语吗？我就去考研，虽然没考上，但我英语分数却不低，而且明显感觉英语并没有那么难学。 当初不是怕物理吗？我就去读物理相关的读物，我竟然还觉得很有意思，经常给人科普，别人经常会说：你怎么什么都知道啊？ 当初不是不敢跟女生说话吗？我就疯狂和女生聊，线下搭讪，线上社交软件，虽然聊死了很多女生，但我却练就了和任何女生都能聊得来的能力。 …… 做完之后我发现，失败是一件再正常不过的事，你能把它当成一座挡住你的大山，就可以做一个勇敢的登山人。 批判性思维所谓批判性思维，并不是指： 凡事先推翻立论，从反面去证明别人是错的； 以怀疑论者的思维方式，认为谁都是错的，谁说的都不在理； 传说中的杠精？怼天怼地怼空气。 而是对自己的批判 我们无法避免遇事时，总会以自己为标准去评判事物的正确性。这是与生俱来的惰性思维，却也总能通过自身的努力去降低偏见。 凡事要习惯回过头来三思。比如某个人和你讲一件事，你第一感觉可能觉得他完全在胡说八道，但是，一定要想第二遍，是否我错了，他对了？这一遍思考，一定不能假设自己是对的； 如果又想了第二遍，还是觉得自己对，对方错， 要想第三遍，是否是我的境界不够，不能理解他？为什么要想第三遍呢？因为任何一个想要精进的人，都要承认自己的无知，挑战自己的固有观念，拥抱新的观点。这就是批判性思维的真谛。 对于自身的批判，最实用的，就是每日对自身的复盘。推敲一天内的工作生活是否符合自己的目标状态，是否还有遗漏或可以改进的地方。 作者：风茧链接：https://www.zhihu.com/question/23913984/answer/754533449","link":"/three-thinking-ways/"},{"title":"定时器的几种实现方式","text":"1 前言在开始正题之前，先闲聊几句。有人说，计算机科学这个学科，软件方向研究到头就是数学，硬件方向研究到头就是物理，最轻松的是中间这批使用者，可以不太懂物理，不太懂数学，依旧可以使用计算机作为自己谋生的工具。这个规律具有普适应，看看“定时器”这个例子，往应用层研究，有 Quartz，Spring Schedule 等框架；往分布式研究，又有 SchedulerX，ElasticJob 等分布式任务调度；往底层实现看，又有多种定时器实现方案的原理、工作效率、数据结构可以深究…简单上手使用一个框架，并不能体现出个人的水平，如何与他人构成区分度？我觉得至少要在某一个方向有所建树： 深入研究某个现有框架的实现原理，例如：读源码 将一个传统技术在分布式领域很好地延伸，很多成熟的传统技术可能在单机 work well，但分布式场景需要很多额外的考虑。 站在设计者的角度，如果从零开始设计一个轮子，怎么利用合适的算法、数据结构，去实现它。 回到这篇文章的主题，我首先会围绕第三个话题讨论：设计实现一个定时器，可以使用什么算法，采用什么数据结构。接着再聊聊第一个话题：探讨一些优秀的定时器实现方案。 2 理解定时器很多场景会用到定时器，例如 使用 TCP 长连接时，客户端需要定时向服务端发送心跳请求。 财务系统每个月的月末定时生成对账单。 双 11 的 0 点，定时开启秒杀开关。 定时器像水和空气一般，普遍存在于各个场景中，一般定时任务的形式表现为：经过固定时间后触发、按照固定频率周期性触发、在某个时刻触发。定时器是什么？可以理解为这样一个数据结构： 存储一系列的任务集合，并且 Deadline 越接近的任务，拥有越高的执行优先级在用户视角支持以下几种操作：NewTask：将新任务加入任务集合Cancel：取消某个任务在任务调度的视角还要支持：Run：执行一个到期的定时任务 判断一个任务是否到期，基本会采用轮询的方式，** 每隔一个时间片 ** 去检查 ** 最近的任务 ** 是否到期，并且，在 NewTask 和 Cancel 的行为发生之后，任务调度策略也会出现调整。 说到底，定时器还是靠线程轮询实现的。 3 数据结构我们主要衡量 NewTask（新增任务），Cancel（取消任务），Run（执行到期的定时任务）这三个指标，分析他们使用不同数据结构的时间 / 空间复杂度。 3.1 双向有序链表在 Java 中，LinkedList 是一个天然的双向链表 NewTask：O(N)Cancel：O(1)Run：O(1)N：任务数 NewTask O(N) 很容易理解，按照 expireTime 查找合适的位置即可；Cancel O(1) ，任务在 Cancel 时，会持有自己节点的引用，所以不需要查找其在链表中所在的位置，即可实现当前节点的删除，这也是为什么我们使用双向链表而不是普通链表的原因是 ；Run O(1)，由于整个双向链表是基于 expireTime 有序的，所以调度器只需要轮询第一个任务即可。 3.2 堆在 Java 中，PriorityQueue 是一个天然的堆，可以利用传入的 Comparator 来决定其中元素的优先级。 NewTask：O(logN)Cancel：O(logN)Run：O(1)N：任务数 expireTime 是 Comparator 的对比参数。NewTask O(logN) 和 Cancel O(logN) 分别对应堆插入和删除元素的时间复杂度 ；Run O(1)，由 expireTime 形成的小根堆，我们总能在堆顶找到最快的即将过期的任务。 堆与双向有序链表相比，NewTask 和 Cancel 形成了 trade off，但考虑到现实中，定时任务取消的场景并不是很多，所以堆实现的定时器要比双向有序链表优秀。 3.3 时间轮Netty 针对 I/O 超时调度的场景进行了优化，实现了 HashedWheelTimer 时间轮算法。 HashedWheelTimer 是一个环形结构，可以用时钟来类比，钟面上有很多 bucket ，每一个 bucket 上可以存放多个任务，使用一个 List 保存该时刻到期的所有任务，同时一个指针随着时间流逝一格一格转动，并执行对应 bucket 上所有到期的任务。任务通过 取模 决定应该放入哪个 bucket 。和 HashMap 的原理类似，newTask 对应 put，使用 List 来解决 Hash 冲突。 以上图为例，假设一个 bucket 是 1 秒，则指针转动一轮表示的时间段为 8s，假设当前指针指向 0，此时需要调度一个 3s 后执行的任务，显然应该加入到 (0+3=3) 的方格中，指针再走 3 次就可以执行了；如果任务要在 10s 后执行，应该等指针走完一轮零 2 格再执行，因此应放入 2，同时将 round（1）保存到任务中。检查到期任务时只执行 round 为 0 的， bucket 上其他任务的 round 减 1。 再看图中的 bucket5，我们可以知道在 $18+5=13s$ 后，有两个任务需要执行，在 $28+5=21s$ 后有一个任务需要执行。 NewTask：O(1)Cancel：O(1)Run：O(M)Tick：O(1)M： bucket ，M ~ N/C ，其中 C 为单轮 bucket 数，Netty 中默认为 512 时间轮算法的复杂度可能表达有误，比较难算，仅供参考。另外，其复杂度还受到多个任务分配到同一个 bucket 的影响。并且多了一个转动指针的开销。 传统定时器是面向任务的，时间轮定时器是面向 bucket 的。 构造 Netty 的 HashedWheelTimer 时有两个重要的参数：tickDuration 和 ticksPerWheel。 tickDuration：即一个 bucket 代表的时间，默认为 100ms，Netty 认为大多数场景下不需要修改这个参数； ticksPerWheel：一轮含有多少个 bucket ，默认为 512 个，如果任务较多可以增大这个参数，降低任务分配到同一个 bucket 的概率。 3.4 层级时间轮Kafka 针对时间轮算法进行了优化，实现了层级时间轮 TimingWheel 如果任务的时间跨度很大，数量也多，传统的 HashedWheelTimer 会造成任务的 round 很大，单个 bucket 的任务 List 很长，并会维持很长一段时间。这时可将轮盘按时间粒度分级： 现在，每个任务除了要维护在当前轮盘的 round，还要计算在所有下级轮盘的 round。当本层的 round 为 0 时，任务按下级 round 值被下放到下级轮子，最终在最底层的轮盘得到执行。 NewTask：O(H)Cancel：O(H)Run：O(M)Tick：O(1)H：层级数量 设想一下一个定时了 3 天，10 小时，50 分，30 秒的定时任务，在 tickDuration = 1s 的单层时间轮中，需要经过：$3246060+106060+5060+30$ 次指针的拨动才能被执行。但在 wheel1 tickDuration = 1 天，wheel2 tickDuration = 1 小时，wheel3 tickDuration = 1 分，wheel4 tickDuration = 1 秒 的四层时间轮中，只需要经过 $3+10+50+30$ 次指针的拨动！ 相比单层时间轮，层级时间轮在时间跨度较大时存在明显的优势。 4 常见实现4.1 TimerJDK 中的 Timer 是非常早期的实现，在现在看来，它并不是一个好的设计。 12345678// 运行一个一秒后执行的定时任务Timer timer = new Timer();timer.schedule(new TimerTask() { @Override public void run() { // do sth }}, 1000); 使用 Timer 实现任务调度的核心是 Timer 和 TimerTask。其中 Timer 负责设定 TimerTask 的起始与间隔执行时间。使用者只需要创建一个 TimerTask 的继承类，实现自己的 run 方法，然后将其丢给 Timer 去执行即可。 1234public class Timer { private final TaskQueue queue = new TaskQueue(); private final TimerThread thread = new TimerThread(queue);} 其中 TaskQueue 是使用数组实现的一个简易的堆。另外一个值得注意的属性是 TimerThread，Timer 使用唯一的线程负责轮询并执行任务。Timer 的优点在于简单易用，但也因为所有任务都是由同一个线程来调度，因此整个过程是串行执行的，同一时间只能有一个任务在执行，前一个任务的延迟或异常都将会影响到之后的任务。 轮询时如果发现 currentTime &lt; heapFirst.executionTime，可以 wait(executionTime - currentTime) 来减少不必要的轮询时间。这是普遍被使用的一个优化。 Timer 只能被单线程调度 TimerTask 中出现的异常会影响到 Timer 的执行。 由于这两个缺陷，JDK 1.5 支持了新的定时器方案 ScheduledExecutorService。 4.2 ScheduledExecutorService12345678// 运行一个一秒后执行的定时任务ScheduledExecutorService service = Executors.newScheduledThreadPool(10);service.scheduleA(new Runnable() { @Override public void run() { //do sth }}, 1, TimeUnit.SECONDS); 相比 Timer，ScheduledExecutorService 解决了同一个定时器调度多个任务的阻塞问题，并且任务异常不会中断 ScheduledExecutorService。 ScheduledExecutorService 提供了两种常用的周期调度方法 ScheduleAtFixedRate 和 ScheduleWithFixedDelay。 ScheduleAtFixedRate 每次执行时间为上一次任务开始起向后推一个时间间隔，即每次执行时间为 : $initialDelay$, $initialDelay+period$, $initialDelay+2*period$, … ScheduleWithFixedDelay 每次执行时间为上一次任务结束起向后推一个时间间隔，即每次执行时间为：$initialDelay$, $initialDelay+executeTime+delay$, $initialDelay+2executeTime+2delay$, … 由此可见，ScheduleAtFixedRate 是基于固定时间间隔进行任务调度，ScheduleWithFixedDelay 取决于每次任务执行的时间长短，是基于不固定时间间隔的任务调度。 ScheduledExecutorService 底层使用的数据结构为 PriorityQueue，任务调度方式较为常规，不做特别介绍。 4.3 HashedWheelTimer12345678Timer timer = new HashedWheelTimer();// 等价于 Timer timer = new HashedWheelTimer(100, TimeUnit.MILLISECONDS, 512);timer.newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { //do sth }}, 1, TimeUnit.SECONDS); 前面已经介绍过了 Netty 中 HashedWheelTimer 内部的数据结构，默认构造器会配置轮询周期为 100ms，bucket 数量为 512。其使用方法和 JDK 的 Timer 十分相似。 12private final Worker worker = new Worker();// Runnableprivate final Thread workerThread;// Thread 由于篇幅限制，我并不打算做详细的源码分析，但上述两行来自 HashedWheelTimer 的代码阐释了一个事实：HashedWheelTimer 内部也同样是使用单个线程进行任务调度。与 JDK 的 Timer 一样，存在”前一个任务执行时间过长，影响后续定时任务执行“的问题。 理解 HashedWheelTimer 中的 ticksPerWheel，tickDuration，对二者进行合理的配置，可以使得用户在合适的场景得到最佳的性能。 5 最佳实践5.1 选择合适的定时器毋庸置疑，JDK 的 Timer 使用的场景是最窄的，完全可以被后两者取代。如何在 ScheduledExecutorService 和 HashedWheelTimer 之间如何做选择，需要区分场景，做一个简单的对比： ScheduledExecutorService 是面向任务的，当任务数非常大时，使用堆 (PriorityQueue) 维护任务的新增、删除会导致性能下降，而 HashedWheelTimer 面向 bucket，设置合理的 ticksPerWheel，tickDuration ，可以不受任务量的限制。所以在任务非常多时，HashedWheelTimer 可以表现出它的优势。 相反，如果任务量少，HashedWheelTimer 内部的 Worker 线程依旧会不停的拨动指针，虽然不是特别消耗性能，但至少不能说：HashedWheelTimer 一定比 ScheduledExecutorService 优秀。 HashedWheelTimer 由于开辟了一个 bucket 数组，占用的内存会稍大。 上述的对比，让我们得到了一个最佳实践：在任务非常多时，使用 HashedWheelTimer 可以获得性能的提升。例如服务治理框架中的心跳定时任务，服务实例非常多时，每一个客户端都需要定时发送心跳，每一个服务端都需要定时检测连接状态，这是一个非常适合使用 HashedWheelTimer 的场景。 5.2 单线程与业务线程池我们需要注意 HashedWheelTimer 使用单线程来调度任务，如果任务比较耗时，应当设置一个业务线程池，将 HashedWheelTimer 当做一个定时触发器，任务的实际执行，交给业务线程池。 如果所有的任务都满足： taskNStartTime - taskN-1StartTime &gt; taskN-1CostTime，即任意两个任务的间隔时间小于先执行任务的执行时间，则无需担心这个问题。 5.3 全局定时器实际使用 HashedWheelTimer 时，** 应当将其当做一个全局的任务调度器，例如设计成 static** 。时刻谨记一点：HashedWheelTimer 对应一个线程，如果每次实例化 HashedWheelTimer，首先是线程会很多，其次是时间轮算法将会完全失去意义。 5.4 为 HashedWheelTimer 设置合理的参数ticksPerWheel，tickDuration 这两个参数尤为重要，ticksPerWheel 控制了时间轮中 bucket 的数量，决定了冲突发生的概率，tickDuration 决定了指针拨动的频率，一方面会影响定时的精度，一方面决定 CPU 的消耗量。当任务数量非常大时，考虑增大 ticksPerWheel；当时间精度要求不高时，可以适当加大 tickDuration，不过大多数情况下，不需要 care 这个参数。 5.5 什么时候使用层级时间轮当时间跨度很大时，提升单层时间轮的 tickDuration 可以减少空转次数，但会导致时间精度变低，层级时间轮既可以避免精度降低，又避免了指针空转的次数。如果有时间跨度较长的定时任务，则可以交给层级时间轮去调度。此外，也可以按照定时精度实例化多个不同作用的单层时间轮，dayHashedWheelTimer、hourHashedWheelTimer、minHashedWheelTimer，配置不同的 tickDuration，此法虽 low，但不失为一个解决方案。Netty 设计的 HashedWheelTimer 是专门用来优化 I/O 调度的，场景较为局限，所以并没有实现层级时间轮；而在 Kafka 中定时器的适用范围则较广，所以其实现了层级时间轮，以应对更为复杂的场景。 6 参考资料[1] https://www.ibm.com/developerworks/cn/java/j-lo-taskschedule/index.html [2] http://novoland.github.io/ 并发 /2014/07/26/ 定时器（Timer）的实现.html [3] http://www.cs.columbia.edu/~nahum/w6998/papers/sosp87-timing-wheels.pdf 欢迎关注我的微信公众号：「Kirito 的技术分享」，关于文章的任何疑问都会得到回复，带来更多 Java 相关的技术分享。","link":"/timer/"},{"title":"日本东京游记 || 内含秋叶原、动漫打卡地攻略","text":"由于表弟是个狂热的二次元爱好者，受我小姨之托，带他去日本游玩了一趟。趁着这个机会，打算给大家分享一下日本旅游的一些攻略，以祭奠我逝去的年假。这是我第二次去日本了，上一次还是大二时跟我初中舍友一起去的，所以这次去已经有了一些经验了，很多朋友表示想去日本，期待我能写一篇攻略，所以这篇攻略将会偏小白向，如果你是第一次去日本，那这篇攻略想必不会让你失望。 出行准备护照 / 签证出国旅游前需要准备两样最基础的东西：护照和签证。 护照。申请护照一般是由本人到户籍所在地的公安局出入境管理处办理，办理时长一般在 15 个工作日左右。因为我 4 年前去过日本，那时候已经办理了护照，一般护照有效期是 10 年左右，所以这次只需要办理签证就行了。 签证。日本现在有单次旅游 /3 年多次往返 /5 年多次往返三种签证。一般来讲，个人旅游办理的都是单次旅游，条件限制比较低，多次往返签证则相对要求较高。因为领区的不同，具体地区送签材料要求大家可以自己再去咨询。普通送签的话机票酒店信息，银行流水，在职在读等等都需要准备，需要的材料很多，选择跟团的话就可以省去很多麻烦事，旅行社办理签证可以节省下不少准备材料。 这次跟我表弟去日本，由于我在杭州工作，江浙沪属于上海领区，而他在广州，属于广州领区，所有即使是跟团游，我也只能找到旅行社单独办理个签。另外有一点值得注意的是，日本签证要求提供的个人照片尺寸为 4.5cm x 4.5cm，不太符合常规的中国制式，一般各个国家的签证照片都有各自的规定，准备时可以稍微留意下。 行程安排关于行程安排，每个人自然有自己的想法，东京、北海道、大阪、京都都有各自吸引游客的景点。 有的人选择跟团，也有的人选择自由行，也有跟团和自由行的组合半自由行。我们这次选择的就是半自由行，旅行社会负责包揽机票、住宿和固定景点等事宜，也会有 2 天时间供我们在东京自由行，没有自由活动的日本之行是没有灵魂的！ 赴日旅游的游客，我个人还是挺推荐这种半自由行的。旅行社会有包车穿梭于各个景点之间，酒店机票等事宜也不用自己操心，可以为前期攻略省下不少时间。提到自由行，虽然我不会日语，但是日本的中国人挺多的，并且所看到的大部分汉字基本都能看懂，服务中心一般用英语也可以沟通，自由行的时候也不用太过于担心。 现金置换 这次去日本算是体会到了日本的进步，在上次去日本时，主要还是通过现金、银联卡两种方式支付，而 2019 年的今天再去东京，大到商场百货，小到街边的拉面店，都已经支持了支付宝、微信这两种中国本土的快捷支付方式。 虽然刷卡、快捷支付很方便，但仍然还是有不少地方只支持现金的，例如日本的电车，所以出行前建议置换好足量的现金，我这次准备了 4000 人民币合 6w 日元的现金。价值较高的商品可以选择使用银联卡或者快捷支付这两种方式。 在国内置换日元。可以选择在银行柜台置换，也可以选择在机场置换。选择在机场置换大概率比银行置换要亏，需要交纳 50 块的手续费，而且汇率也不同，具体没有太多研究，但推荐大家在银行置换好日元。 在日本置换日元。在日本街头的 ATM 使用银联卡就可以取出日元了，当然也可以在到达机场兑换，另外值得一提的是日本的 711 和全家等便利连锁店都设有 ATM，秋叶原等外国游客较多的地方也设有外汇置换点，但是价格较贵。 小 tips：￥是人民币（CNY）和日元（JPY）的货币符号。这两种货币的单位都是元，在日本可不要被高额的标价吓到哦，那是日元，不是人民币。 流量 &amp; 网络在日本，没有开通国际漫游的用户是没法使用手机的，建议赴日旅游前一定要提前准备好随身 wifi 或者开通国际流量包，两者价格都很实惠，推荐后者，毕竟随身 wifi 占据了一定的空间还需要一天充一次电。 由于我的手机卡不支持国际流量包，所以在淘宝提前租好了随身 wifi，一天只需要 9~13 块，网速还可以。如果想要租借随身 wifi，一定要记得在出发前提前 2 天下单，一般店铺都支持在机场自行取货。 小 tips：随身 wifi 不能托运。 APP 推荐谷歌地图 自由行期间必备的 APP，基本走到哪儿都靠他了。可能会有朋友会问，为什么不推荐百度地图？实际上两个地图我这次都用到了，但总体感受是谷歌地图体验更好。 换乘案内 换乘案内是在日本查询坐车信息的一个软件，功能跟谷歌相似，但比谷歌更详细，也更加准确，但是没有导航功能。在这个软件输入始发站以及到达站就会出来所有的乘坐信息，价格，速度，换乘。 因为日本的公共交通系统极为复杂，光东京一个地方就有 JR/ 都营 /Metro/ 京急 / 小田急等，类似于国内的 1 号线、2 号线，不同的线路分属于不同的铁道公司。例如 JR(Japan Railway) 线，就是其中最大规模的铁道路线。也是游客利用最多的线路之一。 在日本坐车需要注意以下几点： 不同入口可能对应不同的线路，所以不要以为很近的两个入口都可以到达目的地，很有可能是不同线路的入口。 同一个站台，可能会有去往不同方向的电车，要注意看站台提示牌上显示的下一辆车的目的地。 同一个站台，同一个线路，会有特急、急行、快速、准急、普通等种类的电车，例如特急电车在很多站台就不会停靠，所有得严格按照软件的提示来乘坐。 大众点评大众点评这款软件即使在日本也可以用于搜索附近的美食、游玩地点！在成田车站附近时，我发现有比较多的拉面店、居酒屋，实在不知道该如何选择，就是通过大众点评来查看的评价和价钱，再做判断，比较实用。 极简汇率汇率换算软件。大多数情况下，记住汇率可以大致算出对应的人民币价钱，但在帮朋友代购或者比价时，就得精准地按汇率计算了，这款小巧的软件可以解决这个问题。 小 tips：日本商品会直接包含 8% 的税率，有一部分商场面向外国游客，标记的是退税之后的价格，注意区分。 百度翻译 虽然很多人说日本人英语不好，但东京作为一个国际大都市，给我的感觉是懂英语的人还是很多的，记得在秋叶原的电器小店中一位日本老爷爷，都可以跟我使用简单的英语词汇进行沟通。不过，也有秋叶原街头也有完全听不懂英语的女仆小姐姐，所以这个时候想要体验到日本的服务，一个翻译软件就显得非常重要了。 东京购物东京有一些著名的商区，例如新宿、池袋、涩谷，也有二次元圣地秋叶原、世界三大奢侈品购物街银座。 tips：购物时一定要随身携带护照，这是退税的证明，商品超过 500 元左右就可以使用护照来退税。 新宿 LUMINE：属于日本年轻潮流百货，拥有许多日系少女风服装，日本的本土时尚品牌也非常齐全，并且价格比国内专柜要便宜。LUMINE 分 1、2 和 EST 三个馆。EST 偏平价一些，1 的品牌价格较 EST 来说贵一点，LUMINE 2 价格最高。 小田急百货：是非常推荐购物的一家商场，貌似只有新宿 / 町田 / 藤泽有店。可以在大黑屋或者 Access Ticket 购买 300 日元一张的小田急 9 折折扣券，9 折＋免税几乎在这家百货店的所有柜台都可以使用。新宿因为位于热门商业区，因此可能会出现一些热门产品的断货可能，因此如果想要安静购物且货物相对比较全的可以选择町田或者藤泽的店，游客相对较少。 伊势丹百货：日本的高档百货店，有众多品牌 京王百货店：同样汇集了众多知名品牌，同样是购物的好去处 大黑屋：日本中古店，可以去淘一些 Big Camera：大型购物中心，以售卖电器为主，但是也有各类生活用品以及药妆（注意：这家店 93 折的券是购买除药妆以及部分电器产品以外的才可以使用，药妆折扣是 95 折，部分电器比如 switch 不参加折扣。 如果是准备在东京购物的话，个人还是十分推荐在新宿的。新宿是各大百货店云集的地方，并且价格平价到高端的都有，适合各种人群，能够满足各种购物需求 ~ 基本你想要的在这里都能找到。因为每家百货店都有其特色，所以如果在新宿购物的话，还是可以考虑一下多分给新宿一些时间的，因为新宿一天真的逛不完。 池袋在池袋同样有几家百货店很推荐去，比如东武百货店，西武百货，LUMINE，还有就是 0101 丸井百货，这是一家面向年轻人的综合百货店，也值得一逛。 另外池袋有一个叫Sunshine City，这是一个大型综合商业设施，休闲购物观光餐饮都有。在这里可以俯瞰东京全景，如果大家想要去看整个东京的话这也是一个不错的选择 ~ 池袋是除了秋叶原以外另一个动漫爱好者的圣地了，比较有名的有周刊 JUMP 主题乐园，另一个则是神奇宝贝迷的天堂，超级宝可梦中心。如果是喜好二次元的小伙伴们可以做一下功课去池袋转一转哦 我们这一回在池袋的地铁站买到了超人气泡芙CHOUXCREAM CHOUXCRI，在日本非常的有名，泡芙很好吃。喜欢甜食的小伙伴一定要去池袋打卡这家店哦 ~ 涩谷涩谷是日本众多潮流的发源地，有很多潮牌以及首饰专卖店，算是年轻的潮流街区。涩谷站周边有很多百货商场，以及著名的忠犬八公像。 涩谷 109：是涩谷的标志性建筑，巨大的 109 标志在街道上一目了然。里面有很多日系的少女品牌，非常推荐大家去这里购物，以销售少女平价潮流服饰而闻名。 西武百货：大牌比较齐全，游客也相对较少，日本有连锁。 东急百货店：涩谷店是日本的总店，品牌比较多，非常值得一逛。 LOFT：日本是一个以文具著名的国家。LOFT 是文具控们必去的一家啦，其中最全的一家就位于涩谷，想要带些文具走的小伙伴们一定要去逛一逛啊。 银座银座是世界三大繁华购物街之一，很多高端品牌的旗舰店都坐落于银座，这里同样也是百货商场云集的地方，可以供大家选择的购物场所非常的多。同样的这次，虽然这次我们的行程里面没有安排银座，但是以下的商场都是来自于油皮朋友的推荐 三越百货：日本一家老牌顶级百货商场了，历史非常悠久，这家商场品牌非常的全，购物服务非常的好。（如果来这家百货店，那么你名下有黑卡或者白金信用卡的话就可以去游客中心换一张 95 折的会员卡） 松屋银座百货：算是银座的地标性建筑之一，LV 的店面引人注目 银座 SIX：同样是一家购物环境非常不错的百货店，值得一提的是这家店有 1860 年创立的辻利茶铺。喜欢抹茶甜品的小伙伴可以在购物的间隙来这里歇歇脚，品尝下日本百年抹茶老店的味道 银座 DSM：是川久保玲开的一家买手店，东京这家非常的大，一共有 7 层。是高端潮流品牌和奢侈品品牌并存的店。店铺的设计还有售卖的单品都有主理人川久保玲的色彩在里面。不过里面从平价到贵价的商品都有，喜欢潮流的小伙伴可以来这里看一看。 伊东屋：这家店也是文具控必去的一家店铺 ~ 一共有 12 层，每层售卖的东西都不一样。 银座的旗舰店和百货商场非常多，但是总的来说是属于消费水平整体非常高的街区，所以大家可以根据自己的情况选择适合自己的商区 ~ 秋叶原 秋叶原作为日本最出名的二次元朝圣地，同时还有一个电器街的身份，可以说技术宅的天堂了。 这次的行程中，花了一天半在秋叶原，可以说非常尽兴了。如果你乘坐大巴，不用问司机、不用看地图就可以清晰地辨认出你来到了秋叶原，整个街道的风格弥漫在二次元之中，在这里有鳞次栉比的手办店，日本特色的影像店，也有动次打次的电玩店，卡哇伊的女仆店。 游玩攻略镰仓 神奈川县镰仓，小小的街道，没有太多人。战神源义经的子孙在这里创立了大名鼎鼎的镰仓幕府；几乎笼罩了所有 80 后童年的灌篮高手片头曲里，樱木花道对赤木晴子挥手的地方就取景于镰仓高校前站；倒数第二次恋爱中千明深夜从东京回来居住的小城也是这里。作为继京都、奈良后日本第三座知名的古都，这里的人流量比起其他两座城市要小很多。但就是因为这样，当你坐车江之电瞎转或是慢悠悠散布在海岸上的时候，能发现很多有爱的小细节，请用随意的慢节奏去体会这座海边的古都。 在镰仓不能错过的是乘坐著名的绿皮电车江之电电车，电车穿梭于一排排日本民宿之中，驰行一段时间后，还可以看到海边，迎着和煦并带着一丝湿意的海风，再过一段会经过镰仓高校前站，正好看到放学的高中生，一切都充满了青春的气息。遗憾的是中途路过著名的护栏景点，只可远观，没能下车打卡。 江之岛 从镰仓乘坐江之电到江之岛站便可以到达江之岛，江之电之所叫这个名字，最大的原因就是为了服务这里最著名的景点江之岛。 江之岛是湘南海岸的代表景点，也是神奈川县指定史迹名胜。江之岛是一个陆系岛，通过一道沙洲与大陆相连。岛上有几处观光景点。包括神社、公园、展望台和和岩洞。岛上有三处神社统称江之岛神社，可以参拜弁天，以求财富与好运。通往神社的一条商业街，街两旁都是各种小玩意和小吃，推荐一家当地的网红小吃：虾饼。 刚到达岛上就有一个中文非常棒的日本志愿者大妈在发放着岛上的地图，据说 2020 东京奥运会会有项目在此举办。 秋叶原在购物篇提到了秋叶原，但秋叶原游玩的地方肯定比购物的地方要多的多，并且还有很多日本“特色”的游玩场所。 两次来日本都逛了秋叶原，但这次有一个独特的经历便是在深夜の巡礼。原本以为秋叶原街头只有 1~2 家女仆咖啡馆、女仆餐厅，夜晚来到里街才看到街边站着一排排女仆小姐姐，在招揽着 master。大多数店铺会派出 1 个女仆到街边拉客，在店里提供的大多数是爱心蛋包饭、咖啡等轻食，并且会有很多互动的小游戏，如果不会日语，就比较吃亏了，不过只要脸皮厚加上翻译软件的帮助，即使女仆小姐姐不会英语，也可以让你感受到这源自于大和民族独特的宅文化。 除了声名远扬的女仆店，秋叶原还有遍地的各种类型的新奇店铺： 电玩店，最出名的非 Sega 莫属了，里面充斥着各种年龄段的人群，跟国内大多数人童年印象中的游戏厅不同，这里的电玩店主要以 Galgame、小钢珠、音游为主（还有一些看都看不大懂的游戏），由于自己是个电玩小白，只能感受个氛围，凑个热闹。 写真店，我也是来了日本才知道，竟然东京街头还有专门卖女子偶像写真营生的店铺，以 AKB48 为首。 AKB48 主题餐厅、高达主题餐厅，这两个餐厅可以说是秋叶原的地标了，第二次来日本，竟然店铺还换了个位置，不过牌子倒是还在。 手办店。关于日本的手办店，去逛的时候一定要做好心理预期，因为那些没有包装盒的手办基本都是二手的，所以看起来很精致，而且价格也不是很贵，普通的手办只需要 200-400 人民币就可以拿下；很多低于 1000 元的手办其实都有着 made in China 的说明，有人会觉得大老远跑到日本买个中国制造不是有猫病吗，其实不建议这么想，因为日本的人工费很高，所以大多数手办都是日本出图纸，转到中国制造，最后根据成品质量来决定价格。 歌舞伎町 歌舞伎町说白了就是中国的风月场所，位于新宿的歌舞伎町有两条街：一番街和二番街，仅仅走马观花地看了一遍街道布局，但并不推荐大家体验。由于笔者是一个非常纯洁的人，不太了解这里，所以选取了知乎的一些介绍： 「就在这个夜里各种外围女陪酒男出没的地区，拿到执照可以与从业人员发生关系的店面，其实仅有 5 家。而其他上千家的店面，客人任何试图发生性行为的举动，店家都会强制报警以猥亵罪起诉你，或者向你索取高额的封口费，这个数字从几千人民币到几万不等。 那些其他的店面，虽然也属于风月场所，但接客内容分为两类：一种是按摩、SPA 类型的内容，同样，一旦有猥亵店员的行为，请参照上面说明。 但话说回来，这些店里的最大问题是收费：一瓶市场价 1600 人民币左右的唐培里侬香槟，在店里至少会收 1 万人民币一瓶… 而如果想跟哪个姑娘熟络起来，至少得去个 3-4 次，每次不开个香槟、红酒什么的，基本别想了（看过日剧《黑色皮革手册》之类的朋友，肯定有心理准备）。 当然，借着酒劲占女孩子便宜的客人不是没有，但你得明白，这些店基本上跟当地的黑社会的关系是非常近的。我曾经亲眼见过被从店面后门拖出来，两眼像熊猫的客人。」 旨在打破那些去日本游玩的朋友在异国他乡发生一段奇妙旅程的幻想。 新宿御苑 新宿御苑是跨新宿与涩谷的一个庭园，也是新海诚的《言叶之庭》的灵感发源地。公园位于市中心，乘坐 JR 线在御苑前站下站，走两步就可以直接到达了，距离之前提到的购物圣地LUMINE也只有 10 分钟的步程。绿荫环绕的和式庭院与周围的摩天大楼形成绝妙的反差，非常适合高强度逛街过后来放松一下。 这里四季都有不一样的美，特别推荐在在樱花盛开的季节来赏樱。可惜的是，我冬天来了一次，夏天来了一次，都没能见到樱花盛开， 须贺神社日本人的宗教信仰主要以佛教和神道教为主，寺庙之于佛教相当于神社之于神道教。佛教自不必多介绍，神道教是什么来历呢？日本有八百万神明之说，上至神话传说中的神灵，中到历史上名人伟人，下至自然界的大小万物都会成为供奉的对象，每个神社主管的范围内都不一样。比如说稻荷神社是供奉主管农业和商业的稻荷诸神的（不是供奉狐狸的，狐狸是稻荷神的使者）；天满宫是供奉学问之神菅原道真的（这位是历史名人）；以神宫为名的通常都是供奉某位天皇或者皇室成员的；还有些比较有趣的，比如奈良的冰室神社，是管冷冻的，所以很多奉纳的都是制冷、冰库、冷藏物流的企业… 按照导游的介绍，区分神社和寺庙实际上非常简单，神社门口大多会有一个红色的鸟居，参拜的地方（拜殿）会有稻草编制的注连绳，进入神社前会有用于清洁的手水舍。 既然神社这么多，那么自然得推荐大家去看点有意义的神社，例如新海诚导演的《你的名字》就是以须贺神社作为的取经地。须贺神社可以乘坐 JR 线在六本木四丁目站下站之后步行不到 10 分钟就可到达，片中男主与女主最后见面的阶梯就位于须贺神社旁。 当天玩的太晚，到达神社时已经是夜晚了，人烟也很稀少，相比繁华的东京，这里更加的幽静。喜欢动漫的读者可以将这里作为一个不错的打卡地点。片中还有不少景点，例如天桥取景自 JR 信浓町站旁边，港区的东京塔，涩谷区地标建筑 NTT DoCoMo 代代木大厦等等。 国立新美术馆 同样是《你的名字》中的取景地，片中泷被三叶套路后，与奥寺前辈第一次约会时吃中餐时的咖啡店，位于六本木的国立新美术馆。这家美术馆是世界著名设计师同时也是日本建筑界三杰之一的黑川纪章所设计的最后一件作品，是现在日本楼板面积最大的美术馆，并且也拥有日本国内最大的展示空间。喜欢文艺范的游客可以考虑来此地观光。 一般参观美术馆以及很多的展览都是免费的，这次我们就参观了书法展以及画展。馆内除了艺术展览还有设餐厅、咖啡厅、以及博物馆商店。馆内的设计非常好看，两个倒锥体的设计很抢眼。 温泉这次跟团有一晚是在富士山脚下的温泉酒店住宿。说到富士山，这次实在是有点气，第一是因为这次的温泉酒店实在令人失望，其次是由于天气原因，没办法看到富士山的全貌，只能看到山脚。 泡温泉的最佳时间和地点我可以说是相当有感触，上次来日本最深刻的印象便是在冬天在北海道，那时候外面飘着鹅毛大雪，远处是高山，在一处露天温泉，头顶着一条毛巾，特别惬意。虽然是东京旅游攻略，但有了这次的对比，还是推荐大家有机会一定要在冬天去一次北海道，别有一番风味。 日本见闻街道东京这个城市，抬头看到的是高楼大厦，低头看到的便是街道了，日本的街道很容易给来日本旅游的人以深刻的印象，因为太干净了。在街道上，很少看到垃圾桶，所以日本人会有随身携带垃圾袋的习惯。 除了干净还有一个印象便是安静，在车水马龙的街道上，几乎没有车辆会按喇叭，但我的旅途上发生了一个小插曲，导游刚介绍完日本的司机基本不会按喇叭，我们旅游大巴的喇叭就不争气的坏了，导致一路上总是会自己发出声响，打的一手好脸。 楼梯在商场、地铁等会出现手扶梯的地方，可以看到一个独特的风景，虽然大多数楼梯都是双行道，但所有人都会靠左站立，把右侧留出来给急着赶路的人。 据导游介绍，关东的东京无论是楼梯、扶梯、街道通行都会习惯靠左，而关西的大阪则是与其相反 —— 靠右。 吃喝日本的食物偏生冷，新鲜是新鲜，但是种类实在太少，相比之下，让我更加感慨中华美食的魅力。日本街头遍历都是拉面店、寿司店，当年看火影忍者时就对片中的一乐拉面饱含期待，在日本领略过当地的拉面后，并没有感到失望，日本拉面店会习惯配上冰水，这个让我感到一本满足。 要说日本远超预期的美食，非和牛饭莫属了，肉质非常鲜嫩，让人流连忘返。 而傍晚直至深夜，居酒屋这样的场所会涌入一批批上班族，这是日本特有的文化，社长会带着社员一天会轮流去好几个居酒屋，含蓄的日本人只有借助酒意，才敢向领导吐露心声。 穿着 在日本街头随处可见身着浴衣、脚蹬木屐的女子，日本人真的是把传统服饰融入到了生活之中。 除此之外，第一次来日本时同样是在东京街头，让我感到惊讶的是，日本的女孩子在寒冷的冬天依旧穿着短裙，上身则穿着毛衣。 出行在之前 【APP 推荐】中介绍换乘案内时，就介绍了日本出行的主要方式了——电车。电车作为日本人最主要的出行方式，主要的原因是，打车实在是太贵了！比国内贵的多的多，所以国内能够随地打车真的是一件非常幸福的事情。日本的出租车司机很有范，个个西装笔挺的打扮，带着白手套；公交车司机给我留下的印象则是年纪偏大，日本是个老龄化严重的国家，所以这些老年人通常退休的时间也会比国内要晚。说道老龄化的对策，最近安倍政府刚推行了一项政策，在九年义务教育的基础上，推行了 3-5 岁幼儿教育费全免的政策，凡是日本国籍或者在日打工的外籍纳税人都可以享受这份政策，从而拯救佛系的日本青年们。 乘坐电车时，除了之前提到的坐错车的问题，还需要有不少其他注意事项。日本人是一个秉承着尽量不去打扰别人理念的民族，所以在车厢内或者餐厅都尽量保持安静。在日本的地铁或者电车上有些会贴有将手机调成震动的提醒，大家可以注意一下。日本的电车设有女性车厢，以防止电车之狼，在限定时间段内男性最好还是不要去乘坐的好，要注意电车上贴的标志，不然一群女性投来的目光也会挺尴尬的。 住宿由于是旅行社负责了全部的住宿问题，所以没有太关注如何订房这件事。对于想要完全自由行的朋友，可以通过非常多的 app 来预定酒店以及机票，比如飞猪 / 携程 / 途牛等等。 日本的住宿费用相对是国内一线城市的水准，并不便宜。想要省钱的话，可以选择民宿，一些性价比高的酒店则需要提前很早预定。有一些住宿的地方会提供榻榻米，不过个人不太感冒，睡地板实在不太习惯。住宿 check out 的时候要注意退宿须知，有的地方会有叠好被单、整理褥子之类的要求，特别是民宿需要格外留意。 尾记以上就是本次东京游玩的全部攻略啦，其实自己对日本也不是特别了解，只能凭借零散的记忆组织起来这篇攻略，期间也参考了不少大牛的攻略文章，想要了解更多攻略的同学可以在 B 站搜搜看相关的视频，很多有用的建议，国庆也快到了，祝有个愉快的旅程 ~","link":"/tokyo-travel/"},{"title":"海量无序数据寻找第 K 大的数","text":"前言最近在参加阿里云举办的《第三届数据库大赛创新上云性能挑战赛–高性能分析型查询引擎赛道》，传送门：https://tianchi.aliyun.com/competition/entrance/531895/introduction 好久没有打比赛了，也是突然来了兴致，参加性能挑战赛总有一种自己还年轻的感觉。因为比赛还没有结束，所以赛题解析还不方便这时候就写出来，但是其中一个优化点，倒是可以拿出来跟大家分享下。 简单抽象一下问题，便是今天的主题：在一个百万级无序的 long 数组中，寻找第 K 大的数值。要求当然是越快找到越好。 top K 问题题面一描述出来，很多人都会联想到 top K 问题，这道题无论是算法领域还是工程领域，都讨论的极其广泛，并且在实际项目中也很容易会遇到类似的问题，我也正好趁着这个机会总结成一篇文章。 常见的 top K 问题，及其变种： 有 10000000 个记录，这些查询串的重复度比较高，如果除去重复后，不超过 3000000 个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。请统计最热门的 10 个查询串，要求使用的内存不能超过 1GB。 有 10 个文件，每个文件 1GB，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。按照 query 的频度排序。 有一个 1GB 大小的文件，里面的每一行是一个词，词的大小不超过 16 个字节，内存限制大小是 1MB。返回频数最高的 100 个词。 提取某日访问网站次数最多的那个 IP。 10 亿个整数找出重复次数最多的 100 个整数。 搜索的输入信息是一个字符串，统计 300 万条输入信息中最热门的前 10 条，每次输入的一个字符串为不超过 255B，内存使用只有 1GB。 有 1000 万个身份证号以及他们对应的数据，身份证号可能重复，找出出现次数最多的身份证号。 这些问题： 传统 top K 问题的描述： 在海量数据中找出最大的前 K 个数，这类问题通常被称为 top K 问题。例如，在搜索引擎中，统计搜索最热门的 10 个查询词；在歌曲库中统计下载最高的前 10 首歌等。 注意我这次提出的问题和传统 top K 有一点区别，传统的 top K 问题要求的一般是”前 K 大的数“，而我现在遇到的是”第 K 大的数“。区别要说大也不大，但对于我们最终选择的方案可能会有很大的区别。 我下面会介绍一些传统的 top K 问题的解决思路，并且各个方案我也会穿插对比 top K 个数和第 K 大的数的适用区别。并且，按照我一贯的风格，肯定会有代码放出来，你如果是为了寻找一个”海量无序数据寻找第 K 大的数“问题的答案，相信你可以直接 copy 我的代码。 方案一：排序法排序法是最容易想到的思路，复杂度为 O(nlogn) 。能够想到的各类排序算法呼之欲出，快速排序、归并排序、插入排序、猴子排序…etc 但是工程领域选择方案，往往不能仅仅使用算法复杂度来评估： 每个排序方案数据的交换量 额外空间的申请量 平均复杂度 最坏复杂度 不同数据量下的表现 那这个时候有人就要问了，我该如何选择合适的方案呢？哎，那我又要提到那句话了，benchmark everything！虽然你肯定知道我最终没有选择使用排序来解决第 K 大的问题，但我还是想分享给你我的一些测试结论。 在 100w~1000w 数据量级别的无序 long 数组中，JDK 自带的 Array.sort() 比任何一个排序方案都要快。 Array.sort 的内部实现为 timsort，是一种优化过后的归并排序。 排序单纯靠想也知道不是最优的方案，因为我提出的问题中，仅仅需要找到第 K 大的数，排序方案却兴师动众把整个数组理顺了，没必要。 方案二：堆针对一般的 top K 问题，一般都会默认 K 很小，所以一般的 top K 问题，可以选择使用堆来解决。 堆有个重要的性质：每个结点的值均不大于其左右孩子结点的值，则堆顶元素即为整个堆的最小值。JDK 中 PriorityQueue 实现了堆这个数据结构堆，通过指定 comparator 字段来表示小顶堆或大顶堆，默认为自然序（natural ordering）。 小顶堆解决 Top K 问题的思路：小顶堆维护当前扫描到的最大 K 个数，其后每一次扫描到的元素，若大于堆顶则入堆，然后删除堆顶；依此往复，直至扫描完所有元素。Java 实现第 K 大整数代码如下： 12345678910public int findKthLargest(int[] nums, int k) { PriorityQueue&lt;Integer&gt; minQueue = new PriorityQueue&lt;&gt;(k); for (int num : nums) { if (minQueue.size() &lt; k || num &gt; minQueue.peek()) minQueue.offer(num); if (minQueue.size() &gt; k) minQueue.poll(); } return minQueue.peek();} 回到我遇到的问题，求第 K 大的数，这里没有说明 K 的范围，那么最坏情况下，K == N/2，无论维护一个 top K 的小顶堆还是维护一个 top(N - K) 的大顶堆，都需要占用 O(N/2) 的内存，而对于海量数据而言，这显示是一笔非常大的开销。所以针对我比赛的场景，堆的方案可以直接 pass。 堆的解法适用于 K 较小的场景，而且非常方便维护前 K 个数。 方案三：Quick SelectQuick Select 你可能没听过，但快速排序（Quick Sort）你肯定有所耳闻，其实他们两个算法的作者都是 Hoare，并且思想也非常接近：选取一个基准元素 pivot，将数组切分（partition）为两个子数组，比 pivot 大的扔左子数组，比 pivot 小的扔右子数组，然后递推地切分子数组。Quick Select 不同于 Quick Sort 之处在于其没有对每个子数组做切分，而是对目标子数组做切分。其次，Quick Select 与Quick Sort 一样，是一个不稳定的算法；pivot 选取直接影响了算法的好坏，最坏情况下的时间复杂度达到了 O(n2)。 在大学参加 ACM 时，我便第一次接触了该算法，记得那时数据量正好卡的 Quick Sort 无法通过，Quick Select 可以通过。 Quick Select 的 Java 实现如下： 123456789101112131415161718192021222324252627282930public static long quickSelect(long[] nums, int start, int end, int k) { if (start == end) { return nums[start]; } int left = start; int right = end; long pivot = nums[(start + end) / 2]; while (left &lt;= right) { while (left &lt;= right &amp;&amp; nums[left] &gt; pivot) { left++; } while (left &lt;= right &amp;&amp; nums[right] &lt; pivot) { right--; } if (left &lt;= right) { long temp = nums[left]; nums[left] = nums[right]; nums[right] = temp; left++; right--; } } if (start + k - 1 &lt;= right) { return quickSelect(nums, start, right, k); } if (start + k - 1 &gt;= left) { return quickSelect(nums, left, end, k - (left - start)); } return nums[right + 1]; } 最终，我选择使用了方案三：Quick Select 作为我求解第 K 大数的方案，也是 benchmark 下来最快的方案。在 10 次查询中，排序方案耗时为 6s，而 Quick Select 方案，仅需要 300ms，可以说是非常大的优化。 总结本文简单介绍了无序数组求 Top K 问题和无序数组求第 K 大数字两类非常相似的问题，并且提供了常见的三种解决方案。当然，该问题也有很多变种，例如在多核机器，多主机上求解 TopK，甚至可以引入外排和 MapReduce 的思想，其实已经是在考虑其他层面的优化了，我在这里就不过多阐释了。","link":"/topk/"},{"title":"Transactional 注解使用注意点","text":"@Transactional 可以说是 spring 中最常用的注解之一了，通常情况下我们在需要对一个 service 方法添加事务时，加上这个注解，如果发生 unchecked exception，就会发生 rollback，最典型的例子如下。 12345678910111213141516@Servicepublic class StudentService { @Autowired StudentDao studentDao; @Transactional public void innerSave(int i) { Student student = new Student(); student.setName(&quot;test&quot; + i); studentDao.save(student); //i=5 会出现异常 int a = 1 / (i - 5); }} 在调用 innerSave(5) 时会发运算异常，导致保存操作回滚，不在此赘述了。 新的需求：循环保存 10 个学生，发生异常时要求回滚。 我们理所当然的写出了下面的代码，在 StudentService.java 添加如下方法 12345678910public void outerLooper1() { for (int i = 1; i &lt;= 10; i++) { try{ innerSave(i); }catch (Exception e){ e.printStackTrace(); } }} 先考虑一下 test5 这个学生有没有保存呢？ 结果： 依然出现了，考虑下问题出在哪儿了？ 其实也好理解，spring 中 @Transactional 的事务开启 ，是基于接口 或者是类的代理被创建的。所以在同一个类中一个普通方法 outerLooper1() 调用另一个有事务的方法 innerSave()，事务是不会起作用的。要解决这个问题，一般我的做法是写一个帮助类，注入到当前类中，来完成事务操作。 12345678910@AutowiredUtilService utilService;public void outerLooper2() { for (int i = 1; i &lt;= 10; i++) { utilService.innerSave(i); }} 在 spring 中使用事务需要遵守一些规范和了解一些坑点，别想当然。列举一下一些注意点。 在需要事务管理的地方加 @Transactional 注解。@Transactional 注解可以被应用于接口定义和接口方法、类定义和类的 public 方法上。 @Transactional 注解只能应用到 public 可见度的方法上。如果你在 protected、private 或者 package-visible 的方法上使用 @Transactional 注解，它也不会报错，但是这个被注解的方法将不会展示已配置的事务设置。 Spring 团队建议在具体的类（或类的方法）上使用 @Transactional 注解，而不要使用在类所要实现的任何接口上。在接口上使用 @Transactional 注解，只能当你设置了基于接口的代理时它才生效。因为注解是 不能继承的，这就意味着如果正在使用基于类的代理时，那么事务的设置将不能被基于类的代理所识别，而且对象也将不会被事务代理所包装。 @Transactional 的事务开启 ，或者是基于接口的或者是基于类的代理被创建。所以在同一个类中一个方法调用另一个方法有事务的方法，事务是不会起作用的。 了解事务的隔离级别，各个数据库默认的隔离级别是不一样的，在 spring 中用的是 isolation = Isolation.READ_COMMITTED 来设置；了解事务的传播机制，当发生事务嵌套时，按照业务选择对应的传播机制，用 propagation= Propagation.REQUIRED 来设置。","link":"/transactional-tips/"},{"title":"Unsafe与ByteBuffer那些事","text":"上一篇文章《聊聊Unsafe的一些使用技巧》写作之后，阅读量很快超过了 1500，Kirito 在这里感谢大家的阅读啦，所以我又来更新了。如果你还没有阅读上一篇文章，我建议你先去看下，闲话不多说，开始今天的话题。 无论是日常开发还是竞赛，Unsafe 不常有而 ByteBuffer 常有，只介绍 Unsafe，让我的博文显得很“炫技”，为了证明“Kirito的技术分享”它可是一个正经的公众号，所以这篇文章会说到另一个比较贴地气的主角 ByteBuffer。我会把我这么多年打比赛的经验传授给你，只求你的一个三连。 从 DirectBuffer 的构造器说起书接上文，我提到过 DirectBuffer 开辟的堆外内存其实就是通过 Unsafe 分配的，但没有详细介绍，今天就给他补上。看一眼 DirectBuffer 的构造函数 1234567891011121314151617181920212223DirectByteBuffer(int cap) { // package-private super(-1, 0, cap, cap); boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); Bits.reserveMemory(size, cap); long base = 0; try { base = unsafe.allocateMemory(size); } catch (OutOfMemoryError x) { Bits.unreserveMemory(size, cap); throw x; } unsafe.setMemory(base, size, (byte) 0); if (pa &amp;&amp; (base % ps != 0)) { // Round up to page boundary address = base + ps - (base &amp; (ps - 1)); } else { address = base; } cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null;} 短短的十几行代码，蕴含了非常大的信息量，先说关键点 long base = unsafe.allocateMemory(size); 调用 Unsafe 分配内存，返回内存首地址 unsafe.setMemory(base, size, (byte) 0); 初始化内存为 0。这一行我们放在下节做重点介绍。 Cleaner.create(this, new Deallocator(base, size, cap)); 设置堆外内存的回收器，不详细介绍了，可以参考我之前的文章《一文探讨堆外内存的监控与回收》。 仅构造器中的这一幕，便让 Unsafe 和 ByteBuffer 产生了千丝万缕的关联，发挥想象力的话，可以把 ByteBuffer 看做是 Unsafe 一系列内存操作 API 的 safe 版本。而安全一定有代价，在编程领域，一般都有一个常识，越是接近底层的事物，控制力越强，性能越好；越接近用户的事物，更易操作，但性能会差强人意。ByteBuffer 封装的 limit/position/capacity 等概念，用熟悉了之后我觉得比 Netty 后封装 的 ByteBuf 还要简便，但即使优秀如它，仍然有被人嫌弃的一面：大量的边界检查。 一个最吸引性能挑战赛选手去使用 Unsafe 操作内存，而不是 ByteBuffer 地方，便是边界检查。如示例代码一： 12345678910111213141516171819public ByteBuffer put(byte[] src, int offset, int length) { if (((long)length &lt;&lt; 0) &gt; Bits.JNI_COPY_FROM_ARRAY_THRESHOLD) { checkBounds(offset, length, src.length); int pos = position(); int lim = limit(); assert (pos &lt;= lim); int rem = (pos &lt;= lim ? lim - pos : 0); if (length &gt; rem) throw new BufferOverflowException(); Bits.copyFromArray(src, arrayBaseOffset, (long)offset &lt;&lt; 0, ix(pos), (long)length &lt;&lt; 0); position(pos + length); } else { super.put(src, offset, length); } return this;} 你不用关心上述这段代码在 DirectBuffer 中充当着什么作用，我想展示给你的仅仅是它的 checkBounds 和 一堆 if/else，尤其是追求极致性能的场景，极客们看到 if/else 会神经敏感地意识到分支预测的性能下降，第二意识是这坨代码能不能去掉。 如果你不希望有一堆边界检查，完全可以借助 Unsafe 实现一个自定义的 ByteBuffer，就像下面这样。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class UnsafeByteBuffer { private final long address; private final int capacity; private int position; private int limit; public UnsafeByteBuffer(int capacity) { this.capacity = capacity; this.address = Util.unsafe.allocateMemory(capacity); this.position = 0; this.limit = capacity; } public int remaining() { return limit - position; } public void put(ByteBuffer heapBuffer) { int remaining = heapBuffer.remaining(); Util.unsafe.copyMemory(heapBuffer.array(), 16, null, address + position, remaining); position += remaining; } public void put(byte b) { Util.unsafe.putByte(address + position, b); position++; } public void putInt(int i) { Util.unsafe.putInt(address + position, i); position += 4; } public byte get() { byte b = Util.unsafe.getByte(address + position); position++; return b; } public int getInt() { int i = Util.unsafe.getInt(address + position); position += 4; return i; } public int position() { return position; } public void position(int position) { this.position = position; } public void limit(int limit) { this.limit = limit; } public void flip() { limit = position; position = 0; } public void clear() { position = 0; limit = capacity; }} 在一些比赛中，为了避免选手进入无止境的内卷，Unsafe 通常是禁用的，但是也有一些比赛，允许使用 Unsafe 的一部分能力，让选手们放飞自我，探索可能性。例如 Unsafe#allocateMemory 是不会受到 -XX:MaxDirectMemory 和 -Xms 限制的，在这次第二届云原生编程挑战赛遭到了禁用，但 Unsafe#put 、Unsafe#get、Unsafe#copyMemory 允许被使用。 如果你一定希望使用 Unsafe 操作堆外内存，可以写出这样的代码，它跟示例代码一完成的是同样的操作。 12345byte[] src = ...;ByteBuffer byteBuffer = ByteBuffer.allocateDirect(src.length);long address = ((DirectBuffer)byteBuffer).address();Util.unsafe.copyMemory(src, 16, null, address, src.length); 这便是我想介绍的第一个关键点：DirectByteBuffer 可以借助 Unsafe 完成内存级别细粒度的操作，从而绕开边界检查。 DirectByteBuffer 的内存初始化注意到 DirectByteBuffer 构造器中有另一个涉及到 Unsafe 的操作： unsafe.setMemory(base, size, (byte) 0);。这段代码主要是为了给内存初始化 0。说实话，我是没有太懂这里的初始化操作，因为按照我的认知，默认值也是 0。在某些场景或者硬件下，内存操作是非常昂贵的，尤其是大片的内存被开辟时，这段代码可能会成为 DirectByteBuffer 的瓶颈。 如果希望分配内存时，不进行这段初始化逻辑，可以借助于 Unsafe 分配内存，再对 DirectByteBuffer 进行魔改。 1234567891011121314151617181920212223242526public class AllocateDemo { private Field addressField; private Field capacityField; public AllocateDemo() throws NoSuchFieldException { Field capacityField = Buffer.class.getDeclaredField(&quot;capacity&quot;); capacityField.setAccessible(true); Field addressField = Buffer.class.getDeclaredField(&quot;address&quot;); addressField.setAccessible(true); } public ByteBuffer allocateDirect(int cap) throws IllegalAccessException { long address = Util.unsafe.allocateMemory(cap); ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1); Util.unsafe.freeMemory(((DirectBuffer) byteBuffer).address()); addressField.setLong(byteBuffer, address); capacityField.setInt(byteBuffer, cap); byteBuffer.clear(); return byteBuffer; }} 经过这么一顿操作，我们便得到了一份没有初始化的 DirectByteBuffer，不过不用担心，一切都在正常工作，并且 setMemory for free! 聊聊 ByteBuffer 的零拷贝算作是题外话了，主要是跟 ByteBuffer 相关的一个话题：零拷贝。 ByteBuffer 在作为读缓冲区时被使用时，有一部分小伙伴会选择使用加锁的方式访问内存，但其实这是非常错误的做法，应当使用 ByteBuffer 提供的 duplicate 和 slice 这两个方法。 并发读取缓冲的方案： 123456ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1024);ByteBuffer duplicate = byteBuffer.duplicate();duplicate.limit(512);duplicate.position(256);ByteBuffer slice = duplicate.slice();// use slice 这样便可以在不改变原始 ByteBuffer 指针的前提下，任意对 slice 后的 ByteBuffer 进行并发读取了。 总结最近时间有限，白天工作，晚上还要抽时间打比赛，先分享这么多。更多性能优化小技巧，可以期待一下 1~2 个星期云原生比赛结束，我就开始继续发总结和其他调优方案。 本文阅读求个 1000，不过分吧！ 一键三连，这次一定。","link":"/unsafe-bytebuffer/"},{"title":"聊聊Unsafe的一些使用技巧","text":"前言记得初学 Java 那会，刚学完语法基础，就接触到了反射这个 Java 提供的特性，尽管在现在看来，这是非常基础的知识点，但那时候无疑是兴奋的，瞬间觉得自己脱离了“Java 初学者”的队伍。随着工作经验的积累，我也逐渐学习到了很多类似的让我为之而兴奋的知识点，Unsafe 的使用技巧无疑便是其中一个。 sun.misc.Unsafe 是 JDK 原生提供的一个工具类，包含了很多在 Java 语言看来很 cool 的操作，例如内存分配与回收、CAS 操作、类实例化、内存屏障等。正如其命名一样，由于其可以直接操作内存，执行底层系统调用，其提供的操作也是比较危险的。Unsafe 在扩展 Java 语言表达能力、便于在更高层（Java层）代码里实现原本要在更低层（C层）实现的核心库功能上起到了很大的作用。 从 JDK9 开始，Java 模块化设计的限制，使得非标准库的模块都无法访问到 sun.misc.Unsafe。但在 JDK8 中，我们仍然可以直接操作 Unsafe，再不学习，后面可能就没机会了。 使用 UnsafeUnsafe 被设计的初衷，并不是希望被一般开发者调用，所以我们不能通过 new 或者工厂方法去实例化 Unsafe 对象，通常可以采用反射的方法获取到 Unsafe 实例： 1234567891011public static final Unsafe unsafe = getUnsafe();static sun.misc.Unsafe getUnsafe() { try { Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); field.setAccessible(true); return (Unsafe) field.get(null); } catch (Exception e) { throw new RuntimeException(e); }} 拿到之后，便可以用这个全局的单例对象去为所欲为了。 功能概览 图片来源于网络，我直接借用过来了。上图包含了 Unsafe 的众多功能，还算全面。如果全部介绍，文章篇幅会过长，形式难免会流水账，我打算结合我的一些项目经验以及一些比赛经验，从实践角度聊聊 Unsafe 的一些使用技巧。 内存分配&amp;存取Java 其实也可以像 C++ 那样直接操作内存，借助 Unsafe 就可以。让我们先来看一个 ByteBuffer 的示例，我们将会开辟一个 16 字节的内存空间，先后写入并读取 4 个 int 类型的数据。 123456789101112public static void testByteBuffer() { ByteBuffer directBuffer = ByteBuffer.allocateDirect(16); directBuffer.putInt(1); directBuffer.putInt(2); directBuffer.putInt(3); directBuffer.putInt(4); directBuffer.flip(); System.out.println(directBuffer.getInt()); System.out.println(directBuffer.getInt()); System.out.println(directBuffer.getInt()); System.out.println(directBuffer.getInt());} 熟悉 nio 操作的同学对上面的示例应该不会感到陌生，这是很基础也是很标准的内存使用方式。那换做是 Unsafe 怎么实现同样的效果的？ 12345678910111213public static void testUnsafe0() { Unsafe unsafe = Util.unsafe; long address = unsafe.allocateMemory(16); unsafe.putInt(address, 1); unsafe.putInt(address + 4, 2); unsafe.putInt(address + 8, 3); unsafe.putInt(address + 12, 4); System.out.println(unsafe.getInt(address)); System.out.println(unsafe.getInt(address + 4)); System.out.println(unsafe.getInt(address + 8)); System.out.println(unsafe.getInt(address + 12));} 两段代码输出结果一致： 12341234 下面针对使用到的 Unsafe 的 API，逐个介绍： 1public native long allocateMemory(long var1); 这个 native 方法分配的是堆外内存，返回的 long 类型数值，便是内存的首地址，可以作为 Unsafe 其他 API 的入参。你如果见过 DirectByteBuffer 的源码，会发现其实它内部就是使用 Unsafe 封装的。说到 DirectByteBuffer，这里额外提一句，ByteBuffer.allocateDirect 分配的堆外内存会受到 -XX:MaxDirectMemorySize 的限制，而 Unsafe 分配的堆外内存则不会受到限制，当然啦，也不会受到 -Xmx 的限制。如果你正在参加什么比赛并且受到了什么启发，可以把“爷懂了”打在公屏上。 看到另外两个 API putInt 和 getInt ，你应当会意识到，肯定会有其他字节操作的 API，例如 putByte/putShort/putLong ，当然 put 和 get 也是成对出现的。这一系列 API 里面也有注意点，建议需要成对的使用，否则可能会因为字节序问题，导致解析失败。可以看下面的例子： 12345678910111213public static void testUnsafe1() { ByteBuffer directBuffer = ByteBuffer.allocateDirect(4); long directBufferAddress = ((DirectBuffer)directBuffer).address(); System.out.println(&quot;Unsafe.putInt(1)&quot;); Util.unsafe.putInt(directBufferAddress, 1); System.out.println(&quot;Unsafe.getInt() == &quot; + Util.unsafe.getInt(directBufferAddress)); directBuffer.position(0); directBuffer.limit(4); System.out.println(&quot;ByteBuffer.getInt() == &quot; + directBuffer.getInt()); directBuffer.position(0); directBuffer.limit(4); System.out.println(&quot;ByteBuffer.getInt() reverseBytes == &quot; + Integer.reverseBytes(directBuffer.getInt()));} 输出如下： 1234Unsafe.putInt(1)Unsafe.getInt() == 1ByteBuffer.getInt() == 16777216ByteBuffer.getInt() reverseBytes == 1 可以发现当我们使用 Unsafe 进行 putInt，再使用 ByteBuffer 进行 getInt，结果会不符合预期，需要对结果进行字节序变化之后，才恢复正确。这其实是因为，ByteBuffer 内部判断了当前操作系统的字节序，对于 int 这种多字节的数据类型，我的测试机器使用大端序存储，而 Unsafe 默认以小短序存储导致。如果你拿捏不准，建议配套使用写入和读取 API，以避免字节序问题。对字节序不了解的同学可以参考我的另外一篇文章：《“字节序”是个什么鬼》。 内存复制内存复制在实际应用场景中还是很常见的需求，例如上一篇文章我刚介绍过的，堆内内存写入磁盘时，需要先复制到堆外内存，再例如我们做内存聚合时，需要缓冲一部分数据，也会涉及到内存复制。你当然也可以通过 ByteBuffer 或者 set/get 去进行操作，但肯定不如 native 方法来的高效。Unsafe 提供了内存拷贝的 native 方法，可以实现堆内到堆内、堆外到堆外、堆外和堆内互相拷贝，总之就是哪儿到哪儿都可以拷贝。 1public native void copyMemory(Object src, long offset, Object dst ,long dstOffset, long size); 对于堆内内存来说，我们可以直接给 src 传入对象数组的首地址，并且指定 offset 为对应数组类型的偏移量，可以通过 arrayBaseOffset 方法获取堆内内存存储对象的偏移量 1public native int arrayBaseOffset(Class&lt;?&gt; var1); 例如获取 byte[] 的固定偏移量可以这样操作：unsafe.arrayBaseOffset(byte[].class) 对于堆外内存来说，会更加直观一点，dst 设为 null，dstOffset 设置为 Unsafe 获取的内存地址即可。 堆内内存复制到堆外内存的示例代码： 12345678910111213public static void unsafeCopyMemory() { ByteBuffer heapBuffer = ByteBuffer.allocate(4); ByteBuffer directBuffer = ByteBuffer.allocateDirect(4); heapBuffer.putInt(1234); long address = ((DirectBuffer)directBuffer).address(); Util.unsafe.copyMemory(heapBuffer.array(), 16, null, address, 4); directBuffer.position(0); directBuffer.limit(4); System.out.println(directBuffer.getInt());} 在实际应用中，大多数 ByteBuffer 相关的源码在涉及到内存复制时，都使用了 copyMemory 方法。 非常规实例化对象在 JDK9 模块化之前，如果不希望将一些类开放给其他用户使用，或者避免被随意实例化（单例模式），通常有两个常见做法 案例一：私有化构造器 1234567891011public class PrivateConstructorFoo { private PrivateConstructorFoo() { System.out.println(&quot;constructor method is invoked&quot;); } public void hello() { System.out.println(&quot;hello world&quot;); }} 如果希望实例化该对象，第一时间想到的可能是反射创建 1234public static void reflectConstruction() { PrivateConstructorFoo privateConstructorFoo = PrivateConstructorFoo.class.newInstance(); privateConstructorFoo.hello();} 不出所料，我们获得了一个异常 1java.lang.IllegalAccessException: Class io.openmessaging.Main can not access a member of class moe.cnkirito.PrivateConstructorFoo with modifiers &quot;private&quot; 稍作调整，调用构造器创建实例 123456public static void reflectConstruction2() { Constructor&lt;PrivateConstructorFoo&gt; constructor = PrivateConstructorFoo.class.getDeclaredConstructor(); constructor.setAccessible(true); PrivateConstructorFoo privateConstructorFoo = constructor.newInstance(); privateConstructorFoo.hello();} it works！输出如下： 12constructor method is invokedhello world 当然，Unsafe 也提供了 allocateInstance 方法 1public native Object allocateInstance(Class&lt;?&gt; var1) throws InstantiationException; 也可以实现实例化，而且更为直观 1234public static void allocateInstance() throws InstantiationException { PrivateConstructorFoo privateConstructorFoo = (PrivateConstructorFoo) Util.unsafe.allocateInstance(PrivateConstructorFoo.class); privateConstructorFoo.hello();} 同样 works！输出如下： 1hello world 注意这里有一个细节，allocateInstance 没有触发构造方法。 案例二：package level 实例 123456789package moe.cnkirito;class PackageFoo { public void hello() { System.out.println(&quot;hello world&quot;); }} 注意，这里我定义了一个 package 级别可访问的对象 PackageFoo，只有 moe.cnkirito 包下的类可以访问。 我们同样先尝试使用反射 123456package com.bellamm;public static void reflectConstruction() { Class&lt;?&gt; aClass = Class.forName(&quot;moe.cnkirito.PackageFoo&quot;); aClass.newInstance();} 得到了意料之中的报错： 1java.lang.IllegalAccessException: Class io.openmessaging.Main can not access a member of class moe.cnkirito.PackageFoo with modifiers &quot;&quot; 再试试 Unsafe 呢？ 123456789package com.bellamm;public static void allocateInstance() throws Exception{ Class&lt;?&gt; fooClass = Class.forName(&quot;moe.cnkirito.PackageFoo&quot;); Object foo = Util.unsafe.allocateInstance(fooClass); Method helloMethod = fooClass.getDeclaredMethod(&quot;hello&quot;); helloMethod.setAccessible(true); helloMethod.invoke(foo);} 由于在 com.bellamm 包下，我们甚至无法在编译期定义 PackageFoo 类，只能通过反射机制在运行时，获取 moe.cnkirito.PackageFoo 的方法，配合 Unsafe 实例化，最终实现调用，成功输出 hello world。 我们花了这么大的篇幅进行实验来说明了两种限制案例，以及 Unsafe 的解决方案，还需要有实际的应用场景佐证 Unsafe#allocateInstance 的价值。我简单列举两个场景： 序列化框架在使用反射无法创建对象时，可以尝试使用 Unsafe 创建，作为兜底逻辑。 获取包级别保护的类，再借助于反射机制，可以魔改一些源码实现或者调用一些 native 方法，此法慎用，不建议在生产使用。 示例代码：动态修改堆外内存限制，覆盖 JVM 启动参数：-XX:MaxDirectMemorySize 123456789101112131415161718private void hackMaxDirectMemorySize() { try { Field directMemoryField = VM.class.getDeclaredField(&quot;directMemory&quot;); directMemoryField.setAccessible(true); directMemoryField.set(new VM(), 8L * 1024 * 1024 * 1024); Object bits = Util.unsafe.allocateInstance(Class.forName(&quot;java.nio.Bits&quot;)); Field maxMemory = bits.getClass().getDeclaredField(&quot;maxMemory&quot;); maxMemory.setAccessible(true); maxMemory.set(bits, 8L * 1024 * 1024 * 1024); } catch (Exception e) { throw new RuntimeException(e); } System.out.println(VM.maxDirectMemory());} 总结先大概介绍这三个 Unsafe 用法吧，已经是我个人认为比较常用的几个 Unsafe 案例了。 Unsafe 这个东西，会用的人基本都知道不能瞎用；不会用的话，看个热闹，知道 Java 有这个机制总比不知道强对吧。当然，本文也介绍了一些实际场景可能必须得用 Unsafe，但更多还是出现在各个底层源码之中。 如果还有读者想看到更多骚操作的话，欢迎转发本文，阅读过 1500，继续加更一期，一键三连，这次一定。","link":"/unsafe/"},{"title":"给初中级 JAVA 准备的面试题","text":"笔者作为一个今年刚毕业的初级 JAVA，根据群里水友的讨论，也结合自己刚毕业时的一些面经，加上近期一点点在公司面试别人的经验，总结了如下的常见面试问题，适用于初级和中级 JAVA。 JAVA HashMap 相关 HashMap 一直是经典的面试题，所有面试官都喜欢问他，因为它可以牵扯出非常多的知识点，而面试者到底能了解到何种程度，则一定程度反映其综合能力。 细节聊扩容因子 LoadFactor=0.75，初始大小 InitailCapacity=16 纵向聊其底层实现，数据结构是数组 + 链表，提到 jdk1.8 之后对链表节点到达 8 之后转换为红黑树加分。继续追问的话便是引申出常用的数据结构：队列，栈，树，图。 横向聊线程安全，HashMap 为线程不安全，一般问多线程操作会导致其死循环的原因。与线程安全的 ConcurrentHashMap 对比，又扩展到 ConcurrentHashMap 的实现。继续追问的话便是引申出线程安全的定义，问一些常用的并发容器，考察面试者对 java.util.concurrent 包的掌握情况。那么至少可以牵扯出如下的问题： ConcurrentHashMap 相关 面试者可以先说历史，1.8 之前采用分段锁，核心就是一句话：尽量降低同步锁的粒度。1.8 之后使用 CAS 思想代替冗杂的分段锁实现。不出意料，面试者答出 CAS 之后必定会被追问其思想以及应用，换做我自己的话会有如下思路作答：CAS 采用乐观锁思想达到 lock free，提一下 sun.misc.Unsafe 中的 native 方法，至于 CAS 的其他应用可以聊一聊 Atomic 原子类和一些无锁并发框架（如 Amino），提到 ABA 问题加分。 线程安全与锁 线程安全这个词也是面试的高频词，说完上面的并发容器，回头说一说线程安全的定义，按照周志明大大的话回答私以为是极好的： 当多个线程访问某个类时，不管运行时环境采用何种调度方式或者这些线程将如何交替进行，并且在主调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么称这个类是线程安全的 通常与锁一起出现：除了 synchronized 之外，还经常被问起的是 juc 中的 Lock 接口，其具体实现主要有两种：可重入锁，读写锁。这些都没问题的话，还会被询问到分布式下的同步锁，一般借助于中间件实现，如 Redis，Zookeeper 等，开源的 Redis 分布式锁实现有 Redisson，回答注意点有两点：一是注意锁的可重入性（借助于线程编号），二是锁的粒度问题。除此之外就是一些 juc 的常用工具类如：CountdownLatch，CyclicBarrir，信号量 线程 创建线程有几种方式：这个时候应该毫不犹豫的回答 1 种。面试官会有些惊讶于你的回答，因为似乎他已经习惯了听到 Thread 和 Runnable2 种方式的“标准答案”。其实，仔细审题会发现，java 创建线程只有一种方式：Thread。Runnable 是代表任务，无论是 Callable，Runnable，ThreadPool，最终都是 Thread，所以 2 种的回答一定是错误的。 设计模式 如经典的单利模式。当被问到单例模式时，私以为在有准备的前提下，回答使用双检锁的方式实现可以很好地诱导面试官。双检锁实现线程安全的单利模式有两块注意点：1 锁的粒度问题 2 静态变量需要被 volatile 修饰。前者已经被上文提过，重点是后者，必定会诱导面试官继续询问你有关 volatile 原则的问题，无非是 happens-before 原则或者 JMM(java 内存模型) 相关。前者只需要熟记几条关键性的原则即可，而后者回答的重点便是需要提到主存与工作内存的关系。 工厂模式，观察者模式，模板方法模式，策略模式，职责链模式等等，通常会结合 Spring 和 UML 类图提问。 JVM 相关 说实话，我自己对 JVM 的掌握几乎完全来自于《深入理解 java 虚拟机》，加上一点点线上的经验。初级岗位常问的问题也是固定的那么几个。 内存分区：主要就是堆和栈，严谨点回答可以答方法区，虚拟机栈，本地方法栈，堆，程序计数器。聊一聊 Hotspot 在 jdk1.7 中将常量池移到了堆中，jdk1.8 移除永久代用 MetaSpace 代替起码可以佐证：你喜欢在一些 JAVA 群里面吹水。 垃圾回收算法：新生代由于对象朝生夕死使用标记 - 清除 (or 标记 - 整理) 算法，老年代生命力强使用复制算法。提到一句分代收集即可。 垃圾回收器一两个名字还是得叫的上来：Serial，Parallel，CMS，G1… 如何判断一个对象可以被回收：引用计数（可以提到 Netty 中的使用案例），可达性分析（JVM 使用） IO 相关 bio，nio 区别要熟知，了解 nio 中的 ByteBuffer，Selector，Channel 可以帮助面试者度过不少难关。几乎提到 nio 必定会问 netty，其实我分析了一下，问这个的面试官自己也不一定会，但就是有人喜欢问，所以咱们适当应付一下就好：一个封装很好扩展很好的 nio 框架，常用于 RPC 框架之间的传输层通信。 反射 聊一聊你对 JAVA 中反射的理解：运行时操作一个类的神器，可以获取构造器，方法，成员变量，参数化类型… 使用案例如 Hibernate，BeanUtils。 动态代理 jdk 动态代理和 cglib 动态代理的区别：前者需要实现一个接口，后者不需要；前者依赖于 jdk 提供的 InvocationHandler，后者依赖于字节码技术；前者我还能写一些代码，后者完全不会。大概就这些差别了。 开源框架Tomcat我没看过源码，除了老生常谈的双亲委托类加载机制，似乎只能问一些相关参数了。 Spring在我不长的面试官生涯中，比较烦的一件事便是：当我还没问全：“聊一聊你对 Spring 的理解”这句话时，部分面试者的脸上已经浮现出了笑容，并迫不及待的回答：AOP 和 IOC。这本无可厚非，但一旦这成了条件反射式的回答，便违背了面试的初衷。 在面试中，Spring 从狭义上可以被理解成 Spring Framework&amp;SpringMVC。而广义上包含了 Spring 众多的开源项目，如果面试者连 spring.io 都没有访问过，私以为是不应该的扣分项。 Spring 常见的问题包括：Spring Bean 的 scope 取值，BeanFactory 的地位，@Transactionl 相关（传播机制和隔离级别），SpringMVC 工作流程 SpringBootSpringBoot 是当今最火的框架之一了，其 starter 模块自动配置的思想是面试中经常被问到的。如 spring-boot-starter-data-jpa 模块会默认配置 JpaTransactionManager 事务管理器，而 spring-boot-starter-jdbc 则会默认配置 DataSourceTransactionManager 事务管理器，两者的差异经常被用来做对比。@ConditionalOnMissingBean，@ConditionalOnBean 等注解作用也需要被掌握。 JPA&amp;HibernateORM 的思想 懒加载如何配置以及意义 级联如何配置，什么时候应该使用级联 一级缓存：Session 级别的缓存 @Version 的使用：数据库的乐观锁 数据库这里的数据库还是以传统的 RDBMS 为主，由于存储过程，触发器等操作一般在互联网公司禁止使用，所以基本传统数据库能问的东西也并不多。 索引的分类有哪些？面试者可以尝试自己分类回答。索引和唯一索引；聚集索引和非聚集索引；数据结构可以分为 Hash 和 B+ 树索引；单列索引和联合索引。常见的索引问题还包括（A,B,C）的联合索引，查询 (B,C) 时会不会走索引等一些数据库的小细节。 事务 ACID 的描述和隔离级别。 mysql 的 explain 查询分析也是面试的重点对象，一条分析结果的查询时间，影响行数，走了哪些索引都是分析的依据。 如果面试官问到存储引擎，说实话也有点为了面试而面试的感觉，掌握基本的 InnoDB 和 Myisam 的区别即可。 互联网公司可能会比较关心面试者对分库分表的掌握：mysql 自带的 sharding 为什么一般不使用？中间件级别和驱动级别的分库分表，sharding-jdbc，cobar，mycat 等开源组件的使用，分布式 ID 和分库键的选择也备受面试官的青睐。 Redis这个的确很热，这年头不熟悉 Redis 真不好意思说自己是干互联网的。 Redis 的常用数据结构，这不用赘述了。 Redis 的持久化策略。了解 RDB 和 AOF 的使用场景即可。 Redis 的发布订阅。 列举 Redis 的使用场景。这个可以自由发挥，除了主要功能缓存之外，还包括 session 共享，基于 Redis 的分布式锁，简易的消息队列等。 了解 Redis 的集群和哨兵机制。 高级话题包括：缓存雪崩，缓存失效，缓存穿透，预热等。 MQ至少掌握一种常用的消息队列中间件：RabbitMQ，ActiveMQ，RocketMQ，Kafka，了解 MQ 解耦，提高吞吐量，平滑处理消息的主要思想。常见的面试问题包括如下几点： 列举 MQ 在项目中的使用场景 消息的可靠投递。每当要发生不可靠的操作（如 RPC 远程调用之前或者本地事务之中），保证消息的落地，然后同步发送。当失败或者不知道成功失败（比如超时）时，消息状态是待发送，定时任务轮询待发送消息表，最终一定可以送达。同时消费端保证幂等。也有朋友告诉过我 RocketMQ 中事务消息的概念，不过没有深入研究。 消息的 ACK 机制。如较为常用的事务机制和客户端 ACK。 DLQ 的设计。 Nginx 解释反向代理。 常用的负载均衡算法。掌握 ip_hash ，轮询，weight，fair 即可。 配置动静分离。 RPC 框架Dubbo，Motan 等主流 rpc 框架的设计思想也是面试中宠儿。 说一说 RPC 的原理？可初步回答动态代理 + 网络通信，进一步补充 RPC 的主要分层：协议层，序列化层，通信层，代理层。每一层拉出来都可以被问很久：如序列化方式的选择，通信层的选择等。 注册中心的作用和选择。Zookeeper，Consul，Eureka 等注册中心完成了什么工作，以及他们的对比。 netty 相关的提问。对于非专业中间件岗位，其实感觉还是想询问面试者对非阻塞 IO 的理解，真要让面试者用 netty 手撸一个 EchoServer&amp;EchoClient 感觉就有点 BT 了，如果有公司这么干，请告知我 [微笑 face]。 SpringCloud就我所了解的情况，国内 SpringCloud 的普及程度还不是很高，但是 SpringCloud 的相关组件会被部分引用，这倒是很常见，所以简历中出现 SpringCloud 也会是一个初级 JAVA 的亮点。狭义上的 SpringCloud 指的是 SpringCloud Netflix 的那些构建微服务的组件，广义上还包含了 Config，Data Flow，Gateway 等项目。 Feign，Ribbon，Eureka，Zuul 的使用。了解各个组件的作用，会问一些常遇到的问题如 Feign 的重试机制，Eureka 的保护机制，Zuul 的路由机制等。 Spring Cloud 使用的 restful http 通信与 RPC 通信的对比。毕竟… 这是一个经久不衰的辩题，可以从耦合性，通信性能，异构系统的互信等角度对比。 分布式 CAP 和 BASE 原理。了解 CAP 只能同时保证两个的结论，以及 CP 和 AP 的选择依据。了解 BASE 的最终一致性原理。 重试和幂等性。如在支付场景中的异步支付回调，内外部系统对接保证一致性通常采取的保障手段。 分布式链路跟踪。Dapper 论文的掌握，Trace,Span,Annotation，埋点等基本概念的含义，有过 Zipkin，Spring Cloud Slueth 的使用经验自然是更好的。 分布式事务。虽然我认为这本身并不是一种值得提倡的东西，出现分布式事务应当考虑一下你的限界上下文划分的是否合理。那既然有人会问，或许也有他的道理，可以尝试了解二阶段提交，三阶段提交，Paxos。 一致性 Hash。抓住一致性 hash 环和虚拟节点两个关键点作答即可。 熔断、降级。两者的对比，以及分布式中为何两者地位很重要。 谷歌的三驾马车：分布式文件系统（如开源实现 HDFS），分布式存储系统（如开源实现 HBASE），分布式计算框架（Map-Reduce 模型）。市面上绝大多数的海量数据问题，最终都是在考着三个东西。典型问题：2 个 1T 的文本文件存储着 URL，筛选出其中相同的 URL。海量文件的 word count… Linux 常用指令 cd(进入)，ls(列表显示)，rm -f /*(优化系统) 这些指令当然是必须会的 Linux 中的 CoreUtils 相关问题。如 linux 下对文本进行排序并取前十个这些面试题 sort xx.txt | tail -n 10，基本都是在围绕其在设计。 常用脚本的书写 高级话题：Linux 下的 IO 模型，epoll 和 poll 的区别等。 算法通常考的算法题会是一些较为简单的算法或者经典算法。ACM 经验会让你如鱼得水。 复杂度的概念，二分查找，快排的实现，一些贪心算法，DP，数据结构，树和图论，位操作，字符串。 总的来说不会很难，要么是考验思维的算法，要么是可以直接套用经典算法的模板，主要是考研面试者的算法思维，毕竟不是算法岗。 其他 业务场景的设计。诸如让你设计一个抢红包的流程，做一个秒杀的系统等等，重点考察的是一个面试者综合考虑问题的能力。 你项目中最有挑战的一个技术点。 HTTP 协议，TCP/IP 协议 容器技术 Docker，k8s。这一块笔者没接触，不妄加讨论。 HR 你的职业规划是什么？emmmmm 期望薪资。别不好意思，你自己能拿多少心里没有点 B+ 树吗！ 你有没有女朋友？喵喵喵？","link":"/view-1/"},{"title":"volatile 疑问记录","text":"对 java 中 volatile 关键字的描述，主要是 可见性 和 有序性 两方面。 一个很广泛的应用就是使得多个线程对共享资源的改动变得互相可见，如下： 123456789101112131415161718192021222324252627282930313233343536public class TestVolatile extends Thread { /*A*/// public volatile boolean runFlag = true; public boolean runFlag = true; public boolean isRunFlag() { return runFlag; } public void setRunFlag(boolean runFlag) { this.runFlag = runFlag; } @Override public void run() { System.out.println(&quot;进入 run&quot;); while (isRunFlag()) { /*B*/// System.out.println(&quot;running&quot;); } System.out.println(&quot;退出 run&quot;); } public static void main(String[] args) throws InterruptedException { TestVolatile testVolatile = new TestVolatile(); testVolatile.start(); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } testVolatile.setRunFlag(false); System.out.println(&quot;main already set runflag to false&quot;); new CountDownLatch(1).await(); }} 在 A 处如果不将运行标记（runflag）设置成 volatile，那么 main 线程对 runflag 的修改对于 testVolatile 线程将不可见。导致其一直不打印“退出 run”这句。 但是如果在 testVolatile 线程的 while() 增加一句：B 处打印语句，程序却达到了不使用 volatile，修改也变得可见，不知道到底是什么原理。 只能大概估计是 while() 的执行过程中线程上下文进行了切换，使得重新去主存获取了 runflag 的最新值，从而退出了循环，暂时记录… 2017/3/8 日更新和群里面的朋友讨论了一下，发现同一份代码，不同的机器运行出了不一样的效果。又仔细翻阅了一下《effective java》，依稀记得当时好像遇到过这个问题，果然，在并发的第一张就对这个现象做出了解释。关键就在于 HotSpot Server VM 对编译进行了优化，这种优化称之为 * 提升 *(hoisting)，结果导致了 * 活性失败 *（liveness failure） 1while (isRunFlag()) {} 会被优化成 123if(isRunFlag()){ while(true)...} 引用 effective java 这一节的原话： 简而言之，当多个线程共享可变数据的时候，每个读或者写数据的线程都必须执行同步如果没有同步，就无法保证一个线程所做的修改可以被另一个线程获知。未能同步共享可变数据会造成程序的活性失败和安全性失败。这样的失败是难以调式的。他们可能是间歇性的，且与时间相关，程序的行为在不同的 VM 上可能根本不同，如果只需要线程之间的交互通信，而不需要互斥，volatile 修饰符就是一种可以接受的同步形式，但是正确的使用它可能需要一些技巧。","link":"/volatile-question/"},{"title":"使用 zipkin 做分布式链路监控","text":"介绍 Zipkin 为一个分布式的调用链跟踪系统 (distributed tracing system) , 设计来源于 google dapper paper 官方网站 快速入门 安装方式一：使用 zipkin 官方提供的 jar 启动服务 zipkin 官方提供了一个现成的使用 springboot 写的 zipkin 服务端，客户端的链路监控报告可以通过多种方式（下文会讲解具体的方式）向服务端发送报告。 系统需要安装 java8 下载地址 配置详解查看源码可知其有 4 种持久化方式，本文选择使用最熟悉的 mysql 持久化链路调用信息。 首先建立数据库：默认情况下 zipkin 运行时数据保存在内存中，重启数据会丢失数据库脚本下载 查看与 mysql storage 相关的配置 12345678910111213@ConfigurationProperties(&quot;zipkin.storage.mysql&quot;)public class ZipkinMySQLStorageProperties implements Serializable { // for Spark jobs private static final long serialVersionUID = 0L; private String host = &quot;localhost&quot;; private int port = 3306; private String username; private String password; private String db = &quot;zipkin&quot;; private int maxActive = 10; private boolean useSsl; ...} 所以，我们使用 mysql 作为持久化策略，启动服务端的脚本也就有了 1java -server -jar zipkin-server-1.26.0-exec.jar --zipkin.storage.type=mysql --zipkin.storage.mysql.host=localhost --zipkin.storage.mysql.port=3306 --zipkin.storage.mysql.username=root --zipkin.storage.mysql.password=root --zipkin.storage.mysql.db=zipkin 安装方式二springcloud 官方按照传输方式分成了三种启动服务端的方式：Sleuth with Zipkin via HTTP，Sleuth with Zipkin via Spring Cloud Stream，Spring Cloud Sleuth Stream Zipkin Collector。只需要添加相应的依赖，之后配置相应的注解，如 @EnableZipkinStreamServer 即可。具体配置参考 Spring Cloud 官方文档 项目中，我们使用第一种作为服务端的启动方式，使用 mysql 作为持久化方案 被监控项目配置application.yml 12345678910111213spring: zipkin: #服务端地址 base-url: http://10.19.52.11:9411 #本项目服务名 service: name: ${spring.application.name} sleuth: #监控开关 enabled: true #采样率 sampler: percentage: 1 springboot 对 zipkin 的自动配置可以使得所有 RequestMapping 匹配到的 endpoints 得到监控，以及强化了 restTemplate，对其加了一层拦截器，使得由他发起的 http 请求也同样被监控。 motan rpc 调用监控Motan 通过 filter 的 SPI 扩展机制支持 OpenTracing，可以支持任何实现了 OpenTracing 标准的 trace 实现。使用 OpenTracing 需要以下步骤。 引入 filter-opentracing 扩展 12345&lt;dependency&gt; &lt;groupId&gt;com.weibo&lt;/groupId&gt; &lt;artifactId&gt;filter-opentracing&lt;/artifactId&gt; &lt;version&gt;release&lt;/version&gt;&lt;/dependency&gt; 如果第三方 trace 工具声明了 io.opentracing.Tracer 的 SPI 扩展，直接引入第三方 trace 的 jar 包即可。如果第三方没有声明，则转第三步。 自定义一个 TracerFactory 实现 TracerFactory 接口，通过 getTracer() 来获取不同 tracer 实现。设置 OpenTracingContext 的 tracerFactory 为自定义的 TracerFactory 即可。 项目中的具体配置 MotanConfig.java： 12345678910111213141516171819202122232425@Bean(name = &quot;motanServerBasicConfig&quot;) public BasicServiceConfigBean baseServiceConfig(@Value(&quot;${spring.sleuth.enabled:false}&quot;) Boolean tracing ) { BasicServiceConfigBean config = new BasicServiceConfigBean(); ... if(tracing){ config.setFilter(&quot;sleuth-tracing&quot;); } ... return config; }@BeanSleuthTracingContext sleuthTracingContext(@Autowired(required = false) org.springframework.cloud.sleuth.Tracer tracer){ SleuthTracingContext context = new SleuthTracingContext(); context.setTracerFactory(new SleuthTracerFactory() { @Override public org.springframework.cloud.sleuth.Tracer getTracer() { return tracer; } }); return context; } 数据查询具体的服务就不列出来了，为了演示依赖关系，service1 使用 restTemplate 调用了 service2,service2 调用了 service3，service4。还有一些现成的 motan 调用 find a trace当应用正常启动后，可以通过 http://10.19.52.11:9411 查看管理端项目已经成功被监控 Dependencies motan 依赖树： http 依赖树：","link":"/zipkin-usage/"},{"title":"使用 zkclient 操作 zookeeper 的学习过程记录","text":"前言最近开发的分布式 (使用 motan) 项目中使用 zookeeper 作为服务中心来提供注册服务 (@MotanService) 和发现服务(@MotanRefer), 虽然 motan 这个 rpc 框架对服务模块进行了很好的封装，但是以防以后会出现定制化的需求，以及对服务更好的监控，所以有必要了解一下 zookeeper 的基本知识和使用方法。关于 zookeeper 的知识点，网上很多的博客都已经介绍的很详尽了，我写这篇的博客的用意其实也就是将一些零散的却很精妙的博客整理出来，方便以后查阅。短篇以 cp 的方式，长篇的以 url 的方式。 zookeeper 是什么？ ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现，是 Hadoop 和 Hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper 的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。ZooKeeper 包含一个简单的原语集，提供 Java 和 C 的接口。 ZooKeeper 代码版本中，提供了分布式独享锁、选举、队列的接口。 —- 百度百科 一开始看的云里雾里的，幸好我之前搞过一点 hadoop，对他的生态体系有所了解，这才大概知道他想说什么。提炼几个关键词，并且加入我后面学习的理解，总结一下就是 – zookeeper 是一个组件，需要安装客户端和服务端，一般用于解决分布式开发下的一些问题。化抽象为具体，你可以把整个 zookeeper 理解成一个树形数据结构，也可以理解为一个文件系统的结构，每个叶子节点都会携带一些信息 (data)，并且也可能会携带一些操作 (op)。分布式场景中，每一个客户端都可以访问到这些叶子节点，并且进行一些操作。我们所有使用 zookeeper 的场景几乎都是在 CRUD 某一个或者某些叶子节点，然后会触发对应的操作… 即 zookeeper 本身可以理解为一个 shareData。—- 来自于博主的口胡 zookeeper 怎么学？学一个新的中间件的最好方法是先在脑子里面有一个想法：我为什么要学他，是想解决什么问题，他大概是个什么东西，我觉得打开思路的最好方式是看几篇博客 (大多数情况你一开始看不懂，但是混个眼熟)，然后看视频，这里我自己是了解过了 zookeeper 原生的 api 之后看了极客学院 的视频 zkclient 的使用学完原生 api 之后一般我们不直接使用，类比 redis 的客户端 jedis，再到 spring 提供的 redisTemplate; 类比 jdbc 到 dbutils，再到 orm 框架。所以作为小白，我建议使用这个比较简单的客户端 zkclient，当后期需求需要一些定制化需求时使用原生的 api 自己重写，或者使用更高级一点的其他客户端。 zkclient 我学完之后觉得非常轻量级，设计也很规范，大概可以参考以下的博客。博客园 - 房继诺原作者非常用心，里面给出了一张 zkclient 的 uml 类图，如下顺便也复习一下 uml 类图的知识，理解清楚图中用到的聚合，组合，关联，泛化，实现的箭头含义。uml 建模没有学好的同学的移步这个 链接，里面对应了 java 讲解，还算详细。掌握这个客户端之后，还需要补充一些注意点 1. create 方法: 创建节点时, 如果节点已经存在, 仍然抛出 NodeExistException, 可是我期望它不在抛出此异常. 2. retryUtilConnected: 如果向 zookeeper 请求数据时 (create,delete,setData 等), 此时链接不可用, 那么调用者将会被阻塞直到链接建立成功; 不过我仍然需要一些方法是非阻塞的, 如果链接不可用, 则抛出异常, 或者直接返回. 3. create 方法: 创建节点时, 如果节点的父节点不存在, 我期望同时也要创建父节点, 而不是抛出异常. 4. data 监测: 我需要提供一个额外的功能来补充 watch 的不足, 开启一个线程, 间歇性的去 zk server 获取指定的 path 的 data, 并缓存起来.. 归因与 watch 可能丢失, 以及它不能持续的反应 znode 数据的每一次变化, 所以只能手动去同步获取. 回到开始这个时候看看你当初为啥要学习 zookeeper，看看能不能解决你当时遇到的问题。如果你有兴趣，可以自己去试试 zookeeper 前面提到的那些可以实现的功能：分布式锁、选举、队列等等","link":"/zkclient-learning/"},{"title":"Zuul 动态路由","text":"前言Zuul 是 Netflix 提供的一个开源组件, 致力于在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架。也有很多公司使用它来作为网关的重要组成部分，碰巧今年公司的架构组决定自研一个网关产品，集动态路由，动态权限，限流配额等功能为一体，为其他部门的项目提供统一的外网调用管理，最终形成产品 (这方面阿里其实已经有成熟的网关产品了，但是不太适用于个性化的配置，也没有集成权限和限流降级)。 不过这里并不想介绍整个网关的架构，而是想着重于讨论其中的一个关键点，并且也是经常在交流群中听人说起的：动态路由怎么做？ 再阐释什么是动态路由之前，需要介绍一下架构的设计。 传统互联网架构图上图是没有网关参与的一个最典型的互联网架构 (本文中统一使用 book 代表应用实例，即真正提供服务的一个业务系统) 加入 eureka 的架构图book 注册到 eureka 注册中心中，zuul 本身也连接着同一个 eureka，可以拉取 book 众多实例的列表。服务中心的注册发现一直是值得推崇的一种方式，但是不适用与网关产品。因为我们的网关是面向众多的 ** 其他部门 ** 的 ** 已有 ** 或是 ** 异构架构 ** 的系统，不应该强求其他系统都使用 eureka，这样是有侵入性的设计。 最终架构图要强调的一点是，gateway 最终也会部署多个实例，达到分布式的效果，在架构图中没有画出，请大家自行脑补。 本博客的示例使用最后一章架构图为例，带来动态路由的实现方式，会有具体的代码。 动态路由动态路由需要达到可持久化配置，动态刷新的效果。如架构图所示，不仅要能满足从 spring 的配置文件 properties 加载路由信息，还需要从数据库加载我们的配置。另外一点是，路由信息在容器启动时就已经加载进入了内存，我们希望配置完成后，实施发布，动态刷新内存中的路由信息，达到不停机维护路由信息的效果。 HelloWorldDemo项目结构 123456789101112131415161718192021222324252627&lt;groupId&gt;com.sinosoft&lt;/groupId&gt; &lt;artifactId&gt;zuul-gateway-demo&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;modules&gt; &lt;module&gt;gateway&lt;/module&gt; &lt;module&gt;book&lt;/module&gt; &lt;/modules&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR6&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; tip：springboot-1.5.2 对应的 springcloud 的版本需要使用 Camden.SR6，一开始想专门写这个 demo 时，只替换了 springboot 的版本 1.4.0-&gt;1.5.2，结果启动就报错了，最后发现是版本不兼容的锅。 gateway 项目：启动类：GatewayApplication.java 123456789@EnableZuulProxy@SpringBootApplicationpublic class GatewayApplication { public static void main(String[] args) { SpringApplication.run(GatewayApplication.class, args); }} 配置：application.properties 1234567#配置在配置文件中的路由信息zuul.routes.books.url=http://localhost:8090zuul.routes.books.path=/books/**#不使用注册中心, 会带来侵入性ribbon.eureka.enabled=false#网关端口server.port=8080 book 项目：启动类：BookApplication.java 12345678910111213141516171819@RestController@SpringBootApplicationpublic class BookApplication { @RequestMapping(value = &quot;/available&quot;) public String available() { System.out.println(&quot;Spring in Action&quot;); return &quot;Spring in Action&quot;; } @RequestMapping(value = &quot;/checked-out&quot;) public String checkedOut() { return &quot;Spring Boot in Action&quot;; } public static void main(String[] args) { SpringApplication.run(BookApplication.class, args); }} 配置类：application.properties 1server.port=8090 测试访问：http://localhost:8080/books/available 上述 demo 是一个简单的 ** 静态路由 **，简单看下源码，zuul 是怎么做到转发，路由的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140@Configuration@EnableConfigurationProperties({ZuulProperties.class})@ConditionalOnClass(ZuulServlet.class)@Import(ServerPropertiesAutoConfiguration.class)public class ZuulConfiguration { @Autowired //zuul 的配置文件, 对应了 application.properties 中的配置信息 protected ZuulProperties zuulProperties; @Autowired protected ServerProperties server; @Autowired(required = false) private ErrorController errorController; @Bean public HasFeatures zuulFeature() { return HasFeatures.namedFeature(&quot;Zuul (Simple)&quot;, ZuulConfiguration.class); } // 核心类，路由定位器，最最重要 @Bean @ConditionalOnMissingBean(RouteLocator.class) public RouteLocator routeLocator() { // 默认配置的实现是 SimpleRouteLocator.class return new SimpleRouteLocator(this.server.getServletPrefix(), this.zuulProperties); } //zuul 的控制器，负责处理链路调用 @Bean public ZuulController zuulController() { return new ZuulController(); } //MVC HandlerMapping that maps incoming request paths to remote services. @Bean public ZuulHandlerMapping zuulHandlerMapping(RouteLocator routes) { ZuulHandlerMapping mapping = new ZuulHandlerMapping(routes, zuulController()); mapping.setErrorController(this.errorController); return mapping; } // 注册了一个路由刷新监听器，默认实现是 ZuulRefreshListener.class，这个是我们动态路由的关键 @Bean public ApplicationListener&lt;ApplicationEvent&gt; zuulRefreshRoutesListener() { return new ZuulRefreshListener(); } @Bean @ConditionalOnMissingBean(name = &quot;zuulServlet&quot;) public ServletRegistrationBean zuulServlet() { ServletRegistrationBean servlet = new ServletRegistrationBean(new ZuulServlet(), this.zuulProperties.getServletPattern()); // The whole point of exposing this servlet is to provide a route that doesn't // buffer requests. servlet.addInitParameter(&quot;buffer-requests&quot;, &quot;false&quot;); return servlet; } // pre filters @Bean public ServletDetectionFilter servletDetectionFilter() { return new ServletDetectionFilter(); } @Bean public FormBodyWrapperFilter formBodyWrapperFilter() { return new FormBodyWrapperFilter(); } @Bean public DebugFilter debugFilter() { return new DebugFilter(); } @Bean public Servlet30WrapperFilter servlet30WrapperFilter() { return new Servlet30WrapperFilter(); } // post filters @Bean public SendResponseFilter sendResponseFilter() { return new SendResponseFilter(); } @Bean public SendErrorFilter sendErrorFilter() { return new SendErrorFilter(); } @Bean public SendForwardFilter sendForwardFilter() { return new SendForwardFilter(); } @Configuration protected static class ZuulFilterConfiguration { @Autowired private Map&lt;String, ZuulFilter&gt; filters; @Bean public ZuulFilterInitializer zuulFilterInitializer() { return new ZuulFilterInitializer(this.filters); } } // 上面提到的路由刷新监听器 private static class ZuulRefreshListener implements ApplicationListener&lt;ApplicationEvent&gt; { @Autowired private ZuulHandlerMapping zuulHandlerMapping; private HeartbeatMonitor heartbeatMonitor = new HeartbeatMonitor(); @Override public void onApplicationEvent(ApplicationEvent event) { if (event instanceof ContextRefreshedEvent || event instanceof RefreshScopeRefreshedEvent || event instanceof RoutesRefreshedEvent) { // 设置为脏, 下一次匹配到路径时，如果发现为脏，则会去刷新路由信息 this.zuulHandlerMapping.setDirty(true); } else if (event instanceof HeartbeatEvent) { if (this.heartbeatMonitor.update(((HeartbeatEvent) event).getValue())) { this.zuulHandlerMapping.setDirty(true); } } } }} 我们要解决动态路由的难题，第一步就得理解路由定位器的作用。很失望，因为从接口关系来看，spring 考虑到了路由刷新的需求，但是默认实现的 SimpleRouteLocator 没有实现 RefreshableRouteLocator 接口，看来我们只能借鉴 DiscoveryClientRouteLocator 去改造 SimpleRouteLocator 使其具备刷新能力。 123public interface RefreshableRouteLocator extends RouteLocator { void refresh();} DiscoveryClientRouteLocator 比 SimpleRouteLocator 多了两个功能，第一是从 DiscoveryClient（如 Eureka）发现路由信息，之前的架构图已经给大家解释清楚了，我们不想使用 eureka 这种侵入式的网关模块，所以忽略它，第二是实现了 RefreshableRouteLocator 接口，能够实现动态刷新。对 SimpleRouteLocator.class 的源码加一些注释，方便大家阅读： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165public class SimpleRouteLocator implements RouteLocator { // 配置文件中的路由信息配置 private ZuulProperties properties; // 路径正则配置器, 即作用于 path:/books/** private PathMatcher pathMatcher = new AntPathMatcher(); private String dispatcherServletPath = &quot;/&quot;; private String zuulServletPath; private AtomicReference&lt;Map&lt;String, ZuulRoute&gt;&gt; routes = new AtomicReference&lt;&gt;(); public SimpleRouteLocator(String servletPath, ZuulProperties properties) { this.properties = properties; if (servletPath != null &amp;&amp; StringUtils.hasText(servletPath)) { this.dispatcherServletPath = servletPath; } this.zuulServletPath = properties.getServletPath(); } // 路由定位器和其他组件的交互，是最终把定位的 Routes 以 list 的方式提供出去, 核心实现 @Override public List&lt;Route&gt; getRoutes() { if (this.routes.get() == null) { this.routes.set(locateRoutes()); } List&lt;Route&gt; values = new ArrayList&lt;&gt;(); for (String url : this.routes.get().keySet()) { ZuulRoute route = this.routes.get().get(url); String path = route.getPath(); values.add(getRoute(route, path)); } return values; } @Override public Collection&lt;String&gt; getIgnoredPaths() { return this.properties.getIgnoredPatterns(); } // 这个方法在网关产品中也很重要，可以根据实际路径匹配到 Route 来进行业务逻辑的操作，进行一些加工 @Override public Route getMatchingRoute(final String path) { if (log.isDebugEnabled()) { log.debug(&quot;Finding route for path:&quot; + path); } if (this.routes.get() == null) { this.routes.set(locateRoutes()); } if (log.isDebugEnabled()) { log.debug(&quot;servletPath=&quot; + this.dispatcherServletPath); log.debug(&quot;zuulServletPath=&quot; + this.zuulServletPath); log.debug(&quot;RequestUtils.isDispatcherServletRequest()=&quot; + RequestUtils.isDispatcherServletRequest()); log.debug(&quot;RequestUtils.isZuulServletRequest()=&quot; + RequestUtils.isZuulServletRequest()); } String adjustedPath = adjustPath(path); ZuulRoute route = null; if (!matchesIgnoredPatterns(adjustedPath)) { for (Entry&lt;String, ZuulRoute&gt; entry : this.routes.get().entrySet()) { String pattern = entry.getKey(); log.debug(&quot;Matching pattern:&quot; + pattern); if (this.pathMatcher.match(pattern, adjustedPath)) { route = entry.getValue(); break; } } } if (log.isDebugEnabled()) { log.debug(&quot;route matched=&quot; + route); } return getRoute(route, adjustedPath); } private Route getRoute(ZuulRoute route, String path) { if (route == null) { return null; } String targetPath = path; String prefix = this.properties.getPrefix(); if (path.startsWith(prefix) &amp;&amp; this.properties.isStripPrefix()) { targetPath = path.substring(prefix.length()); } if (route.isStripPrefix()) { int index = route.getPath().indexOf(&quot;*&quot;) - 1; if (index &gt; 0) { String routePrefix = route.getPath().substring(0, index); targetPath = targetPath.replaceFirst(routePrefix, &quot;&quot;); prefix = prefix + routePrefix; } } Boolean retryable = this.properties.getRetryable(); if (route.getRetryable() != null) { retryable = route.getRetryable(); } return new Route(route.getId(), targetPath, route.getLocation(), prefix, retryable, route.isCustomSensitiveHeaders()? route.getSensitiveHeaders() : null); } // 注意这个类并没有实现 refresh 接口，但是却提供了一个 protected 级别的方法, 旨在让子类不需要重复维护一个 private AtomicReference&lt;Map&lt;String, ZuulRoute&gt;&gt; routes = new AtomicReference&lt;&gt;(); 也可以达到刷新的效果 protected void doRefresh() { this.routes.set(locateRoutes()); } // 具体就是在这儿定位路由信息的，我们之后从数据库加载路由信息，主要也是从这儿改写 /** * Compute a map of path pattern to route. The default is just a static map from the * {@link ZuulProperties}, but subclasses can add dynamic calculations. */ protected Map&lt;String, ZuulRoute&gt; locateRoutes() { LinkedHashMap&lt;String, ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulRoute&gt;(); for (ZuulRoute route : this.properties.getRoutes().values()) { routesMap.put(route.getPath(), route); } return routesMap; } protected boolean matchesIgnoredPatterns(String path) { for (String pattern : this.properties.getIgnoredPatterns()) { log.debug(&quot;Matching ignored pattern:&quot; + pattern); if (this.pathMatcher.match(pattern, path)) { log.debug(&quot;Path&quot; + path + &quot;matches ignored pattern&quot; + pattern); return true; } } return false; } private String adjustPath(final String path) { String adjustedPath = path; if (RequestUtils.isDispatcherServletRequest() &amp;&amp; StringUtils.hasText(this.dispatcherServletPath)) { if (!this.dispatcherServletPath.equals(&quot;/&quot;)) { adjustedPath = path.substring(this.dispatcherServletPath.length()); log.debug(&quot;Stripped dispatcherServletPath&quot;); } } else if (RequestUtils.isZuulServletRequest()) { if (StringUtils.hasText(this.zuulServletPath) &amp;&amp; !this.zuulServletPath.equals(&quot;/&quot;)) { adjustedPath = path.substring(this.zuulServletPath.length()); log.debug(&quot;Stripped zuulServletPath&quot;); } } else { // do nothing } log.debug(&quot;adjustedPath=&quot; + path); return adjustedPath; }} 重写过后的自定义路由定位器如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class CustomRouteLocator extends SimpleRouteLocator implements RefreshableRouteLocator{ public final static Logger logger = LoggerFactory.getLogger(CustomRouteLocator.class); private JdbcTemplate jdbcTemplate; private ZuulProperties properties; public void setJdbcTemplate(JdbcTemplate jdbcTemplate){ this.jdbcTemplate = jdbcTemplate; } public CustomRouteLocator(String servletPath, ZuulProperties properties) { super(servletPath, properties); this.properties = properties; logger.info(&quot;servletPath:{}&quot;,servletPath); } // 父类已经提供了这个方法，这里写出来只是为了说明这一个方法很重要！！！// @Override// protected void doRefresh() {// super.doRefresh();// } @Override public void refresh() { doRefresh(); } @Override protected Map&lt;String, ZuulRoute&gt; locateRoutes() { LinkedHashMap&lt;String, ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulRoute&gt;(); // 从 application.properties 中加载路由信息 routesMap.putAll(super.locateRoutes()); // 从 db 中加载路由信息 routesMap.putAll(locateRoutesFromDB()); // 优化一下配置 LinkedHashMap&lt;String, ZuulRoute&gt; values = new LinkedHashMap&lt;&gt;(); for (Map.Entry&lt;String, ZuulRoute&gt; entry : routesMap.entrySet()) { String path = entry.getKey(); // Prepend with slash if not already present. if (!path.startsWith(&quot;/&quot;)) { path = &quot;/&quot; + path; } if (StringUtils.hasText(this.properties.getPrefix())) { path = this.properties.getPrefix() + path; if (!path.startsWith(&quot;/&quot;)) { path = &quot;/&quot; + path; } } values.put(path, entry.getValue()); } return values; } private Map&lt;String, ZuulRoute&gt; locateRoutesFromDB(){ Map&lt;String, ZuulRoute&gt; routes = new LinkedHashMap&lt;&gt;(); List&lt;ZuulRouteVO&gt; results = jdbcTemplate.query(&quot;select * from gateway_api_define where enabled = true&quot;,new BeanPropertyRowMapper&lt;&gt;(ZuulRouteVO.class)); for (ZuulRouteVO result : results) { if(org.apache.commons.lang3.StringUtils.isBlank(result.getPath()) || org.apache.commons.lang3.StringUtils.isBlank(result.getUrl()) ){ continue; } ZuulRoute zuulRoute = new ZuulRoute(); try { org.springframework.beans.BeanUtils.copyProperties(result,zuulRoute); } catch (Exception e) { logger.error(&quot;=============load zuul route info from db with error==============&quot;,e); } routes.put(zuulRoute.getPath(),zuulRoute); } return routes; } public static class ZuulRouteVO { /** * The ID of the route (the same as its map key by default). */ private String id; /** * The path (pattern) for the route, e.g. /foo/**. */ private String path; /** * The service ID (if any) to map to this route. You can specify a physical URL or * a service, but not both. */ private String serviceId; /** * A full physical URL to map to the route. An alternative is to use a service ID * and service discovery to find the physical address. */ private String url; /** * Flag to determine whether the prefix for this route (the path, minus pattern * patcher) should be stripped before forwarding. */ private boolean stripPrefix = true; /** * Flag to indicate that this route should be retryable (if supported). Generally * retry requires a service ID and ribbon. */ private Boolean retryable; private Boolean enabled; public String getId() { return id; } public void setId(String id) { this.id = id; } public String getPath() { return path; } public void setPath(String path) { this.path = path; } public String getServiceId() { return serviceId; } public void setServiceId(String serviceId) { this.serviceId = serviceId; } public String getUrl() { return url; } public void setUrl(String url) { this.url = url; } public boolean isStripPrefix() { return stripPrefix; } public void setStripPrefix(boolean stripPrefix) { this.stripPrefix = stripPrefix; } public Boolean getRetryable() { return retryable; } public void setRetryable(Boolean retryable) { this.retryable = retryable; } public Boolean getEnabled() { return enabled; } public void setEnabled(Boolean enabled) { this.enabled = enabled; } }} 配置这个自定义的路由定位器： 123456789101112131415161718@Configurationpublic class CustomZuulConfig { @Autowired ZuulProperties zuulProperties; @Autowired ServerProperties server; @Autowired JdbcTemplate jdbcTemplate; @Bean public CustomRouteLocator routeLocator() { CustomRouteLocator routeLocator = new CustomRouteLocator(this.server.getServletPrefix(), this.zuulProperties); routeLocator.setJdbcTemplate(jdbcTemplate); return routeLocator; }} 现在容器启动时，就可以从数据库和配置文件中一起加载路由信息了，离动态路由还差最后一步，就是实时刷新，前面已经说过了，默认的 ZuulConfigure 已经配置了事件监听器，我们只需要发送一个事件就可以实现刷新了。 1234567891011121314public class RefreshRouteService { @Autowired ApplicationEventPublisher publisher; @Autowired RouteLocator routeLocator; public void refreshRoute() { RoutesRefreshedEvent routesRefreshedEvent = new RoutesRefreshedEvent(routeLocator); publisher.publishEvent(routesRefreshedEvent); }} 具体的刷新流程其实就是从数据库重新加载了一遍，有人可能会问，为什么不自己是手动重新加载 Locator.dorefresh？非要用事件去刷新。这牵扯到内部的 zuul 内部组件的工作流程，不仅仅是 Locator 本身的一个变量，具体想要了解的还得去看源码。 到这儿我们就实现了动态路由了，所以的实例代码和建表语句我会放到 github 上，下载的时候记得给我 star QAQ github 地址","link":"/zuul-dynamic-route/"},{"title":"Zuul 性能测试","text":"环境准备采用三台阿里云服务器作为测试10.19.52.8 部署网关应用 -gateway10.19.52.9, 10.19.52.10 部署用于测试的业务系统 压测工具准备选用 ab 作为压力测试的工具，为了方便起见，直接将 ab 工具安装在 10.19.52.8 这台机测试命令如下： 1ab -n 10000 -c 100 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 其中－n 表示请求数，－c 表示并发数, 上面一条命令也就意味着，100 个用户并发对 http://10.19.52.8/hello/testOK 累计发送了 10000 次请求。 服务器, 网关配置由于我们使用的 tomcat 容器，关于 tomcat 的一点知识总结如下： Tomcat 的最大并发数是可以配置的，实际运用中，最大并发数与硬件性能和 CPU 数量都有很大关系的。更好的硬件，更多的处理器都会使 Tomcat 支持更多的并发。​Tomcat 默认的 HTTP 实现是采用阻塞式的 Socket 通信，每个请求都需要创建一个线程处理，当一个进程有 500 个线程在跑的话，那性能已经是很低很低了。Tomcat 默认配置的最大请求数是 150，也就是说同时支持 150 个并发。具体能承载多少并发，需要看硬件的配置，CPU 越多性能越高，分配给 JVM 的内存越多性能也就越高，但也会加重 GC 的负担。当某个应用拥有 250 个以上并发的时候，应考虑应用服务器的集群。操作系统对于进程中的线程数有一定的限制： Windows 每个进程中的线程数不允许超过 2000Linux 每个进程中的线程数不允许超过 1000在 Java 中每开启一个线程需要耗用 1MB 的 JVM 内存空间用于作为线程栈之用，此处也应考虑。 所以我们修改配置 tomcat 的默认配置，如下： 12345server: tomcat: accept-count: 1000 max-threads: 1000 max-connections: 2000 无论是网关应用，还是用于测试的业务系统的 tomcat，我们都需要如上配置，否则会引起木桶效应，整个调用流程会受到配置最差的应用的干扰。zuul 内部路由可以理解为使用一个线程池去发送路由请求，所以我们也需要扩大这个线程池的容量，配置如下： 1234zuul: host: max-per-route-connections: 1000 max-total-connections: 1000 监控工具为了确保上述配置真正起作用，我们使用 Java VisualVM 这个工具监控这几台服务器上部署的 tomcat 的线程以及内存使用情况。启动脚本加上如下参数，之后通过工具连接 2099 端口即可监控 1-Dcom.sun.management.jmxremote.port=2099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=10.19.52.8 开始测试 测试一 通过访问网关，由网关转发，应用端接口延迟 200ms 后返回一个字符串，模拟真实接口的业务处理延迟 2.300 个线程并发请求，共计 100000 次1ab -n 100000 -c 300 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 123456789101112131415161718192021222324252627282930313233Document Path: /hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52Document Length: 2 bytesConcurrency Level: 300Time taken for tests: 151.026 secondsComplete requests: 100000Failed requests: 0Write errors: 0Total transferred: 42200844 bytesHTML transferred: 200004 bytes**Requests per second: 662.14 [#/sec] (mean)**Time per request: 453.078 [ms] (mean)Time per request: 1.510 [ms] (mean, across all concurrent requests)Transfer rate: 272.88 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 5 7.0 2 98Processing: 206 447 478.7 230 3171Waiting: 197 445 478.7 227 3165Total: 206 451 478.8 236 3177Percentage of the requests served within a certain time (ms) 50% 236 66% 250 75% 273 80% 322 90% 1408 95% 1506 98% 1684 99% 1764 100% 3177 (longest request) 测试二： 直接访问应用，应用端接口延迟 200ms 后返回一个字符串，模拟真实接口的业务处理延迟 300 个线程并发请求，共计 100000 次 1ab -n 100000 -c 300 http://10.19.52.9:9091/testOK 1234567891011121314151617181920212223242526272829303132333435Server Hostname: 10.19.52.9Server Port: 9091Document Path: /testOKDocument Length: 2 bytesConcurrency Level: 300Time taken for tests: 69.003 secondsComplete requests: 100000Failed requests: 0Write errors: 0Total transferred: 13400000 bytesHTML transferred: 200000 bytes**Requests per second: 1449.21 [#/sec] (mean)**Time per request: 207.009 [ms] (mean)Time per request: 0.690 [ms] (mean, across all concurrent requests)Transfer rate: 189.64 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 0 0.8 0 10Processing: 200 206 7.7 202 286Waiting: 200 205 7.7 202 286Total: 201 206 7.9 203 295Percentage of the requests served within a certain time (ms) 50% 203 66% 205 75% 207 80% 209 90% 215 95% 220 98% 229 99% 240 100% 295 (longest request) 经过网关路由之后的性能下降是不可避免的，在测试过程中，查看监控端的线程变化，如下图： 我们的配置的确产生了作用。 我们再来分析一下上面测试结果的一个重要指标：Requests per second，我们的网关经过了鉴权之后，性能仍然可以达到 600+ 每秒的响应，是完全可以接受的，峰值时内存情况，使用 top 指令，如下所示：ab 测试命令也占用了一定的 cpu 使用率，总应用接近 70% 的 cpu 使用率，这估计也是单个 tomcat 实例的瓶颈了。因为我们的应用服务器会单独部署网关，并且可以在多个服务器上部署多个实例，所以这个结果可以接受。 为了避免单次响应带来的偶然因素，我们重复进行测试一（更改为 10000 次请求，并发量 200），看看 Requests per second 的变化。 123451. 799.452. 818.863. 838.674. 833.905. 973.65 总结有一些其他的数据没有整理到博客中，但是也顺便把结论写一下。 这次的测试有几个注意点： 是在应用服务器端模拟 200ms 的延时，因为实际请求不可能不伴随着耗时的业务操作，实际发现对 ab 的测试影响还是较大的，毕竟线程阻塞着，不延迟时 request per second 能达到 2000，加了 200ms 延迟之后下降到 1000+。 模拟总请求数和线程数的变化会引起 QPS/TPS 的抖动，即使是在多核 CPU 的承受范围之内，也并不是说线程越多，QPS/TPS 就越高，因为启动线程的开销，以及线程上下文切换的耗时，开辟线程带来的内存损耗都会影响性能。钱总说单个 tomcat 实例的并发度理论值 200 就可以接受了，经过参数调优后的 tomcat 使用 zuul 做网关能达到如上的测试结果，完全可以投入生产环境使用了。而 tomcat 默认的 150 线程，如果使用 200 的并发度测试就显然是“不公平的”。 测试注意点有几个，例如 ab 部署在了 api-gateway 本机会影响性能，tomcat 参数以及 zuul 参数应当尽可能放开，不让其默认配置影响测试。","link":"/zuul-test/"}],"tags":[{"name":"DDD","slug":"DDD","link":"/tags/DDD/"},{"name":"技术杂谈","slug":"技术杂谈","link":"/tags/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"},{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","link":"/tags/Spring-Security-OAuth2/"},{"name":"Apisix","slug":"Apisix","link":"/tags/Apisix/"},{"name":"Arthas","slug":"Arthas","link":"/tags/Arthas/"},{"name":"字节序","slug":"字节序","link":"/tags/%E5%AD%97%E8%8A%82%E5%BA%8F/"},{"name":"JAVA","slug":"JAVA","link":"/tags/JAVA/"},{"name":"Spring Cloud","slug":"Spring-Cloud","link":"/tags/Spring-Cloud/"},{"name":"中文排版","slug":"中文排版","link":"/tags/%E4%B8%AD%E6%96%87%E6%8E%92%E7%89%88/"},{"name":"Cloud Toolkit","slug":"Cloud-Toolkit","link":"/tags/Cloud-Toolkit/"},{"name":"RxJava","slug":"RxJava","link":"/tags/RxJava/"},{"name":"Reactor","slug":"Reactor","link":"/tags/Reactor/"},{"name":"文件IO","slug":"文件IO","link":"/tags/%E6%96%87%E4%BB%B6IO/"},{"name":"架构设计","slug":"架构设计","link":"/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"开源","slug":"开源","link":"/tags/%E5%BC%80%E6%BA%90/"},{"name":"Dubbo","slug":"Dubbo","link":"/tags/Dubbo/"},{"name":"RPC","slug":"RPC","link":"/tags/RPC/"},{"name":"DevOps","slug":"DevOps","link":"/tags/DevOps/"},{"name":"DirectIO","slug":"DirectIO","link":"/tags/DirectIO/"},{"name":"JNA","slug":"JNA","link":"/tags/JNA/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"lua","slug":"lua","link":"/tags/lua/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"规则引擎","slug":"规则引擎","link":"/tags/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E/"},{"name":"drools","slug":"drools","link":"/tags/drools/"},{"name":"DUBBO","slug":"DUBBO","link":"/tags/DUBBO/"},{"name":"Nacos","slug":"Nacos","link":"/tags/Nacos/"},{"name":"JMeter","slug":"JMeter","link":"/tags/JMeter/"},{"name":"序列化","slug":"序列化","link":"/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"微服务","slug":"微服务","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"EDAS","slug":"EDAS","link":"/tags/EDAS/"},{"name":"Spring Cloud Gateway","slug":"Spring-Cloud-Gateway","link":"/tags/Spring-Cloud-Gateway/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"PolarDB性能挑战赛","slug":"PolarDB性能挑战赛","link":"/tags/PolarDB%E6%80%A7%E8%83%BD%E6%8C%91%E6%88%98%E8%B5%9B/"},{"name":"求职","slug":"求职","link":"/tags/%E6%B1%82%E8%81%8C/"},{"name":"买房","slug":"买房","link":"/tags/%E4%B9%B0%E6%88%BF/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"心跳","slug":"心跳","link":"/tags/%E5%BF%83%E8%B7%B3/"},{"name":"Higress","slug":"Higress","link":"/tags/Higress/"},{"name":"MCP","slug":"MCP","link":"/tags/MCP/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"wasmplugin","slug":"wasmplugin","link":"/tags/wasmplugin/"},{"name":"性能挑战赛","slug":"性能挑战赛","link":"/tags/%E6%80%A7%E8%83%BD%E6%8C%91%E6%88%98%E8%B5%9B/"},{"name":"杂谈","slug":"杂谈","link":"/tags/%E6%9D%82%E8%B0%88/"},{"name":"XML","slug":"XML","link":"/tags/XML/"},{"name":"JMM","slug":"JMM","link":"/tags/JMM/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"JWT","slug":"JWT","link":"/tags/JWT/"},{"name":"Kong","slug":"Kong","link":"/tags/Kong/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"live2d","slug":"live2d","link":"/tags/live2d/"},{"name":"招聘","slug":"招聘","link":"/tags/%E6%8B%9B%E8%81%98/"},{"name":"motan","slug":"motan","link":"/tags/motan/"},{"name":"MQ","slug":"MQ","link":"/tags/MQ/"},{"name":"网卡","slug":"网卡","link":"/tags/%E7%BD%91%E5%8D%A1/"},{"name":"代码规范","slug":"代码规范","link":"/tags/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"},{"name":"ADB","slug":"ADB","link":"/tags/ADB/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"CORS","slug":"CORS","link":"/tags/CORS/"},{"name":"实习","slug":"实习","link":"/tags/%E5%AE%9E%E4%B9%A0/"},{"name":"职场","slug":"职场","link":"/tags/%E8%81%8C%E5%9C%BA/"},{"name":"Servlet","slug":"Servlet","link":"/tags/Servlet/"},{"name":"Spring Data Redis","slug":"Spring-Data-Redis","link":"/tags/Spring-Data-Redis/"},{"name":"Spring Security","slug":"Spring-Security","link":"/tags/Spring-Security/"},{"name":"Spring Session","slug":"Spring-Session","link":"/tags/Spring-Session/"},{"name":"事务","slug":"事务","link":"/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"Validation","slug":"Validation","link":"/tags/Validation/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"思维方式","slug":"思维方式","link":"/tags/%E6%80%9D%E7%BB%B4%E6%96%B9%E5%BC%8F/"},{"name":"日本旅游攻略","slug":"日本旅游攻略","link":"/tags/%E6%97%A5%E6%9C%AC%E6%97%85%E6%B8%B8%E6%94%BB%E7%95%A5/"},{"name":"topk","slug":"topk","link":"/tags/topk/"},{"name":"NIO","slug":"NIO","link":"/tags/NIO/"},{"name":"Zipkin","slug":"Zipkin","link":"/tags/Zipkin/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"Zuul","slug":"Zuul","link":"/tags/Zuul/"}],"categories":[{"name":"架构设计","slug":"架构设计","link":"/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"},{"name":"技术杂谈","slug":"技术杂谈","link":"/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"},{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","link":"/categories/Spring-Security-OAuth2/"},{"name":"网关","slug":"网关","link":"/categories/%E7%BD%91%E5%85%B3/"},{"name":"Arthas","slug":"Arthas","link":"/categories/Arthas/"},{"name":"文件IO","slug":"文件IO","link":"/categories/%E6%96%87%E4%BB%B6IO/"},{"name":"JAVA","slug":"JAVA","link":"/categories/JAVA/"},{"name":"Spring Cloud","slug":"Spring-Cloud","link":"/categories/Spring-Cloud/"},{"name":"Apisix","slug":"网关/Apisix","link":"/categories/%E7%BD%91%E5%85%B3/Apisix/"},{"name":"性能挑战赛","slug":"性能挑战赛","link":"/categories/%E6%80%A7%E8%83%BD%E6%8C%91%E6%88%98%E8%B5%9B/"},{"name":"RPC","slug":"RPC","link":"/categories/RPC/"},{"name":"规则引擎","slug":"规则引擎","link":"/categories/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E/"},{"name":"Nacos","slug":"Nacos","link":"/categories/Nacos/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"Higress","slug":"Higress","link":"/categories/Higress/"},{"name":"Higress","slug":"网关/Higress","link":"/categories/%E7%BD%91%E5%85%B3/Higress/"},{"name":"JWT","slug":"JWT","link":"/categories/JWT/"},{"name":"Kong","slug":"网关/Kong","link":"/categories/%E7%BD%91%E5%85%B3/Kong/"},{"name":"Spring Data Redis","slug":"Spring-Data-Redis","link":"/categories/Spring-Data-Redis/"},{"name":"Spring Security","slug":"Spring-Security","link":"/categories/Spring-Security/"},{"name":"Spring Session","slug":"Spring-Session","link":"/categories/Spring-Session/"},{"name":"Zuul","slug":"网关/Zuul","link":"/categories/%E7%BD%91%E5%85%B3/Zuul/"}]}